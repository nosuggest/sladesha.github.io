<!DOCTYPE html>



  

<html class="theme-next muse use-motion" lang="zh-Hans">


<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="自然语言," />










<meta name="description" content="在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券安慰”，”特权享受”等行为，有效的降低了用户的流失。根据实际的业务营销效果，在模型上线后，abtest">
<meta name="keywords" content="自然语言">
<meta property="og:type" content="article">
<meta property="og:title" content="基于自然语言识别下的流失用户预警">
<meta property="og:url" content="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/index.html">
<meta property="og:site_name" content="Sladesal&#39;s Algorithm World">
<meta property="og:description" content="在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券安慰”，”特权享受”等行为，有效的降低了用户的流失。根据实际的业务营销效果，在模型上线后，abtest检验下模型识别用户人群进行营销后的流失率比随意营销下降9.2%，效果显著。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1129359-44f62ac8f31504a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1129359-685ed65594d6c27b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1129359-c2eca9e5da73375b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1129359-e20c4b10905b9090.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2017-12-01T15:08:35.315Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="基于自然语言识别下的流失用户预警">
<meta name="twitter:description" content="在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券安慰”，”特权享受”等行为，有效的降低了用户的流失。根据实际的业务营销效果，在模型上线后，abtest检验下模型识别用户人群进行营销后的流失率比随意营销下降9.2%，效果显著。">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/1129359-44f62ac8f31504a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/"/>





  <title>基于自然语言识别下的流失用户预警 | Sladesal's Algorithm World</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sladesal's Algorithm World</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">复盘是最锋利的剑</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="沙韬伟 sladesal">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/1.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sladesal's Algorithm World">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">基于自然语言识别下的流失用户预警</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-15T23:06:35+08:00">
                2017-08-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-comment-o"></i>
              </span>
              
                <a href="/2017/08/15/基于自然语言识别下的流失用户预警/#SOHUCS" itemprop="discussionUrl">
                  <span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2017/08/15/基于自然语言识别下的流失用户预警/" itemprop="commentsCount"></span>
                </a>
              
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3,565 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  15 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，<strong>通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券安慰”，”特权享受”等行为，有效的降低了用户的流失。</strong>根据实际的业务营销效果，在模型上线后，<strong>abtest检验下模型识别用户人群进行营销后的流失率比随意营销下降9.2%，效果显著。</strong></p>
</blockquote>
<p>当前文本文义识别存在一些问题：<br>（1）准确率而言，很多线上数据对特征分解的过程比较粗糙，<strong>很多直接基于df或者idf结果进行排序</strong>，在算法设计过程中，<strong>也是直接套用模型</strong>，只是工程上的实现，缺乏统计意义上的分析；</p>
<p>（2）<strong>文本越多，特征矩阵越稀疏，计算过程越复杂</strong>。常规的文本处理过程中只会对文本对应的特征值进行排序，其实在文本选择中，可以先剔除相似度较高的文本，这个课题比较大，后续会单独开一章进行研究；</p>
<p>（3）<strong>扩展性较差</strong>。比如我们这次做的流失用户预警是基于电商数据，你拿去做通信商的用户流失衡量的话，其质量会大大下降，所以重复开发的成本较高，这个属于非增强学习的硬伤，目前也在攻克这方面的问题。</p>
<p>首先，我们来看下，整个算法设计的思路：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.通过hive将近期的用户评价hadoop文件下载为若干个text文件</span><br><span class="line">2.通过R语言将若干个text整合读取为一个R内的dataframe</span><br><span class="line">3.利用R里面的正则函数将文本中的异常符号‘#！@￥%%’，英文，标点等去除</span><br><span class="line">（这边可以在hive里面提前处理好，也可以在后续的分词过程中利用停顿词去除）</span><br><span class="line">4.文本分词，这边可以利用R中的Rwordseg，jiebaR等，我写这篇文章之前看到很多现有的语义分析的文章中，Rwordseg用的挺多，所以这边我采用了jiebaR</span><br><span class="line">5.文本分词特征值提取,常见的包括互信息熵，信息增益，tf-idf，本文采取了tf-idf，剩余方法会在后续文章中更新</span><br><span class="line">6.模型训练</span><br><span class="line">      这边我采取的方式是利用概率模型naive bayes+非线性模型random forest先做标签训练，最后用nerual network对结果进行重估</span><br><span class="line">（原本我以为这样去做会导致很严重的过拟合，但是在实际操作之后发现，过拟合并不是很严重，至于原因我也不算很清楚，后续抽空可以研究一下）</span><br></pre></td></tr></table></figure></p>
<p>下面，我们来剖析文本分类识别的每一步</p>
<h1 id="定义用户属性"><a href="#定义用户属性" class="headerlink" title="定义用户属性"></a>定义用户属性</h1><p>首先，我们定义了已经存在的流失用户及非流失用户，易购的用户某品类下的购买周期为27天，针对前60天-前30天下单购物的用户，观察近30天是否有下单行为，如果有则为非流失用户，如果没有则为流失用户。提取每一个用户最近一次商品评价作为msg。</p>
<h1 id="文本合成"><a href="#文本合成" class="headerlink" title="文本合成"></a>文本合成</h1><p>通过hive -e的方式下载到本地，会形成text01，text02…等若干个文本，通过R进行<strong>文本整合</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#先设置文本路径</span><br><span class="line">path &lt;- &quot;C:/Users/17031877/Desktop/Nlp/answer/Cmsg&quot;</span><br><span class="line">completepath &lt;- list.files(path, pattern = &quot;*.txt$&quot;, full.names = TRUE)</span><br><span class="line"></span><br><span class="line">#批量读入文本</span><br><span class="line">readtxt &lt;- function(x) &#123;</span><br><span class="line">  ret &lt;- readLines(x)                   #每行读取</span><br><span class="line">  return(paste(ret, collapse = &quot;&quot;))     #通过paste将每一行连接起来</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#lappy批量操作，形成list，个人感觉对非关系数据，list处理更加便捷</span><br><span class="line">msg &lt;- lapply(completepath, readtxt)</span><br><span class="line"></span><br><span class="line">#用户属性</span><br><span class="line">user_status &lt;- list.files(path, pattern = &quot;*.txt$&quot;)</span><br><span class="line"></span><br><span class="line">#stringsAsFactors=F，避免很多文本被读成因子类型</span><br><span class="line">comment &lt;- as.data.frame(cbind(user_status, unlist(msg)),stringsAsFactors = F)</span><br><span class="line">colnames(comment) &lt;- c(&quot;user_status&quot;, &quot;msg&quot;)</span><br></pre></td></tr></table></figure></p>
<p>基础的数据整合就完成了。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-44f62ac8f31504a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="数据整理"><a href="#数据整理" class="headerlink" title="数据整理"></a>数据整理</h1><p>也可以看到，基础数据读取完成后，还是很多评论会有一些<strong>不规则的数据</strong>，包括‘#￥%……&amp;’，英文，数字，下面通过正则、停顿词的方式进行处理：</p>
<h2 id="正则化处理"><a href="#正则化处理" class="headerlink" title="正则化处理"></a>正则化处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#直接处理</span><br><span class="line">comment$msg &lt;- gsub(pattern = &quot; &quot;, replacement =&quot;&quot;, comment$msg)  #gsub是字符替换函数，去空格</span><br><span class="line">comment$msg&lt;- gsub(&quot;[[:digit:]]*&quot;, &quot;&quot;, comment$msg) #清除数字[a-zA-Z]</span><br><span class="line">comment$msg&lt;- gsub(&quot;[a-zA-Z]&quot;, &quot;&quot;, comment$msg)   #清除英文字符</span><br><span class="line">comment$msg&lt;- gsub(&quot;\\.&quot;, &quot;&quot;, comment$msg)      #清除全英文的dot符号</span><br><span class="line">--------------------------------------------------------------------------------------------------</span><br><span class="line">#如果是常做nlp处理，可以写成函数打包，后期直接library就可以了</span><br><span class="line">#数值删除</span><br><span class="line">removeNumbers =</span><br><span class="line">    function(x)&#123;</span><br><span class="line">    ret = gsub(&quot;[0-9]&quot;,&quot;&quot;,x)</span><br><span class="line">    return(ret)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">#字符删除</span><br><span class="line">removeLiters =</span><br><span class="line">    function(x)&#123;</span><br><span class="line">    ret = gsub(&quot;[a-z|A-Z]&quot;,&quot;&quot;,x)</span><br><span class="line">    return(ret)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">#各种操作符处理,\s表示空格,\r表示回车,\n表示换行</span><br><span class="line">removeActions =</span><br><span class="line">    function(x)&#123;</span><br><span class="line">    ret = gsub(&quot;\\s|\\r|\\n&quot;, &quot;&quot;, x)</span><br><span class="line">    return(ret)</span><br><span class="line">    &#125;</span><br><span class="line">comment$msg=removeNumbers(comment$msg)</span><br><span class="line">comment$msg=removeLiters(comment$msg)</span><br><span class="line">comment$msg=removeActions (comment$msg)</span><br></pre></td></tr></table></figure>
<p>这边需要对正则化里面的一些表示有所了解，详细可以百度，一般我都是具体需求具体去看，因为太多，自己又懒，所以没记</p>
<h2 id="停顿词"><a href="#停顿词" class="headerlink" title="停顿词"></a>停顿词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#加载jiebaR包</span><br><span class="line">library(jiebaR)</span><br><span class="line"></span><br><span class="line">#找jiebaR存停顿词的地方，自行将需要处理掉的符号存进去，我这边是C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8</span><br><span class="line">tagger&lt;-worker(stop_word=&quot;C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8&quot;)</span><br></pre></td></tr></table></figure>
<p>至于位置可以通过直接输入<code>worker()</code>查看，</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-685ed65594d6c27b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>当前的是没有stop_word的，所有词存储的位置在：C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/下</p>
<h1 id="文本分词"><a href="#文本分词" class="headerlink" title="文本分词"></a>文本分词</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#jieba 分词,去除停顿词</span><br><span class="line">library(jiebaR)</span><br><span class="line">tagger&lt;-worker(stop_word=&quot;C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8&quot;)</span><br><span class="line">words=list()</span><br><span class="line">for (i in 1:nrow(comment))&#123;</span><br><span class="line">    tmp=tagger[comment[i,2]]</span><br><span class="line">    words=c(words,list(tmp))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>直接先分词，但是分词结果会存在很多只有一个字比如‘的’、‘你’、‘我’等或者很多无意义的长句‘中华人民共和国’、‘长使英雄泪满襟’等，需要把这些<strong>词长异常</strong>明显无意义的词句去掉。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#词长统计</span><br><span class="line">whole_words_set=unlist(words)</span><br><span class="line">whole_words_set_rank=data.frame(table(whole_words_set))</span><br><span class="line"></span><br><span class="line">whole_words_set_dealed=c()</span><br><span class="line">for (i in 1:nrow(whole_words_set_rank))&#123;</span><br><span class="line">    tmp=nchar(as.character(whole_words_set_rank[i,1]))</span><br><span class="line">    whole_words_set_dealed=c(whole_words_set_dealed,tmp)</span><br><span class="line">&#125;</span><br><span class="line">whole_words_set_dealed=cbind(whole_words_set_rank,whole_words_set_dealed)</span><br><span class="line">whole_words_set_dealed=whole_words_set_dealed[whole_words_set_dealed$whole_words_set_dealed&gt;1&amp;whole_words_set_dealed$whole_words_set_dealed&lt;5,]</span><br><span class="line">whole_words_set_dealed=whole_words_set_dealed[order(whole_words_set_dealed$Freq,decreasing=T),]</span><br><span class="line"></span><br><span class="line">#words的删除异常值,排序</span><br><span class="line">whole_words_set_sequence=words</span><br><span class="line">key_word=nrow(words)</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">    for (j in 1:length(words[[i]]))&#123;</span><br><span class="line">    tmp=ifelse(nchar(words[[i]][j])&gt;1 &amp; nchar(words[[i]][j])&lt;5,words[[i]][j],&apos;&apos;)</span><br><span class="line">    whole_words_set_sequence[[i]][j]=tmp</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">    whole_words_set_sequence[[i]]=whole_words_set_sequence[[i]][whole_words_set_sequence[[i]]!=&apos;&apos;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="tf-idf词特征值重要性排序"><a href="#tf-idf词特征值重要性排序" class="headerlink" title="tf-idf词特征值重要性排序"></a>tf-idf词特征值重要性排序</h1><p>首先，我们大致看一下排序的数据依旧：</p>
<p>TF = 某词在文章中出现的次数/文章包含的总词数（或者文章有价值词次数）<br>DF = （包含某词的文档数）/（语料库的文档总数）<br>IDF = log（（语料库的文档总数）/（包含某词的文档数+1））<br>这边的+1是为了避免（语料库的文档总数）/（包含某词的文档数）=1，log(1)=0，使得最后的重要性中出现0的情况，与有意义的前提相互驳斥。<br>TF-IDF = TF*IDF</p>
<p>分别看下，里面的每一项的意义：<br>TF，我们可以看出，<strong>在同一个评论中，词数出现的越多，代表这个词越能成为这篇文章的代表</strong>，当然前提是非无意义的助词等。</p>
<p>IDF，我们可以看出，<strong>所以评论中，包含目标词的评论的占比，占比数越高，目标词的意义越大</strong>，假设1000条评论中，“丧心病狂”在一条评论里面重复了10次，但是其他999条里面一次也没有出现，那就算“丧心病狂”非常能代表这条评论，但是在做文本集特征考虑的情况下，它的价值也是不大的。</p>
<p>下面，我们来看代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#tfidf_partone 为对应的tf</span><br><span class="line">tdidf_partone=whole_words_set_sequence</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp1=as.data.frame(prop.table(table(whole_words_set_sequence[[i]])))</span><br><span class="line">tdidf_partone[[i]]=tmp1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#tdidf_partfour 为对应的idf</span><br><span class="line">tdidf_parttwo=unique(unlist(whole_words_set_sequence))</span><br><span class="line">tdidf_max=length(tdidf_parttwo)</span><br><span class="line">tdidf_partthree=tdidf_parttwo</span><br><span class="line">for (i in 1:tdidf_max)&#123;</span><br><span class="line">tmp=0</span><br><span class="line">aimed_word=tdidf_parttwo[i]</span><br><span class="line">    for (j in 1:key_word)&#123;</span><br><span class="line">    tmp=tmp+sum(tdidf_parttwo[i] %in% whole_words_set_sequence[[j]])</span><br><span class="line">    &#125;</span><br><span class="line">tdidf_partthree[i]=log(as.numeric(key_word)/(tmp+1))</span><br><span class="line">&#125;</span><br><span class="line">tdidf_partfour=cbind(tdidf_parttwo,tdidf_partthree)</span><br><span class="line">tdidf_partfive=tdidf_partone</span><br><span class="line">colnames(tdidf_partfour)&lt;-c(&apos;Var1&apos;,&apos;Freq1&apos;)</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tdidf_partfive[[i]]=merge(tdidf_partone[[i]],tdidf_partfour,by=c(&quot;Var1&quot;))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#计算tf-idf结果，并排序key_word</span><br><span class="line">tdidf_partsix=tdidf_partfive</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp=tdidf_partfive[[i]][,2:3]</span><br><span class="line">tdidf_partsix[[i]][,2]=as.numeric(tmp[,1])*as.numeric(tmp[,2])</span><br><span class="line">tdidf_partsix[[i]]=tdidf_partsix[[i]][order(tdidf_partsix[[i]][,2],decreasing=T),][]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">key_word=c()</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp=tdidf_partsix[[i]][1:5,1]</span><br><span class="line">key_word=rbind(key_word,as.character(tmp))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>理论上讲，如果这边数据存储方式用的是data.frame的话，可以利用spply、apply等批量处理函数，这边用得是list的方式，对lpply不是很熟悉的我，选择了for的循环，后续这边会优化一下，这样太消耗资源了。</p>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><p>这边，我最后采取的是概率模型naive bayes+非线性模型random forest先做标签训练，最后用nerual network对结果进行重估方式，但是在训练过程中，我还有几种模型的尝试，这边也一并贴出来给大家做参考。</p>
<h2 id="数据因子化的预处理"><a href="#数据因子化的预处理" class="headerlink" title="数据因子化的预处理"></a>数据因子化的预处理</h2><p>这边得到了近400维度的有效词，现在将每一维度的词遍做一维的feature，同时，此处的feature的意义为要么评论存在该词，要么评论中不存在该词的0-1问题，<strong>需要因子化一下</strong>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#整合数据</span><br><span class="line">well_dealed_data=cbind(as.character(comment[,1]),key_word)</span><br><span class="line">names=as.data.frame(table(key_word))[,1]</span><br><span class="line">names_count=length(names)</span><br><span class="line">names=as.matrix(names,names_count,1)</span><br><span class="line">feature_matrix=matrix(rep(0,names_count*key_word),key_word,names_count)</span><br><span class="line">for (i in 1:names_count)&#123;</span><br><span class="line">    for(j in 1:key_word)&#123;</span><br><span class="line">    feature_matrix[j,i]=ifelse(names[i] %in% key_word[j,],1,0)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#art=1,literature=-1,标签0-1化</span><br><span class="line">feature_matrix=cbind(well_dealed_data[,1],feature_matrix)</span><br><span class="line">feature_matrix[feature_matrix[,1]==&apos;aimed&apos;,1]=&apos;1&apos;</span><br><span class="line">feature_matrix[feature_matrix[,1]==&apos;unaimed&apos;,1]=&apos;-1&apos;</span><br><span class="line"></span><br><span class="line">feature_matrix=as.data.frame(feature_matrix)</span><br><span class="line"></span><br><span class="line">num=1:(ncol(feature_matrix)-1)</span><br><span class="line">value_name=paste(&quot;feature&quot;,num)</span><br><span class="line">value_name=c(&apos;label&apos;,value_name)</span><br><span class="line">colnames(feature_matrix)=value_name</span><br><span class="line"></span><br><span class="line">#feature0-1化</span><br><span class="line">for (i in 1:ncol(feature_matrix))&#123;</span><br><span class="line">feature_matrix[,i]=as.factor(as.numeric(as.character(feature_matrix[,i])))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="数据切分训练测试"><a href="#数据切分训练测试" class="headerlink" title="数据切分训练测试"></a>数据切分训练测试</h2><p>这边就不适用切分函数了，自己写了一个更加快速。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n_index=sample(1:nrow(feature_matrix),round(0.7*nrow(feature_matrix)))</span><br><span class="line">train_feature_matrix=feature_matrix[n_index,]</span><br><span class="line">test_feature_matrix=feature_matrix[-n_index,]</span><br></pre></td></tr></table></figure></p>
<h2 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="backpropagation-neural-network"><a href="#backpropagation-neural-network" class="headerlink" title="backpropagation neural network"></a>backpropagation neural network</h3><p><strong>这边需要用网格算法对size和decay进行交叉检验，这边不贴细节，可以百度搜索详细过程。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(nnet)</span><br><span class="line">nn &lt;- nnet(label~., data=train_feature_matrix, size=2, decay=0.01, maxit=1000, linout=F, trace=F)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">nn.predict_train = predict(nn,train_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),nn.predict_train)</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">nn.predict_test = predict(nn,test_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),nn.predict_test)</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="Linear-Support-Vector-Machine"><a href="#Linear-Support-Vector-Machine" class="headerlink" title="Linear Support Vector Machine"></a>Linear Support Vector Machine</h3><p><strong>这边需要用网格算法对cost进行交叉检验，这边不贴细节，可以百度搜索详细过程。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">svmfit &lt;- svm(label~., data=train_feature_matrix, kernel = &quot;linear&quot;, cost = 10, scale = FALSE) # linear svm, scaling turned OFF</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">svmfit.predict_train=predict(svmfit, train_feature_matrix, type = &quot;probabilities&quot;)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(svmfit.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">svmfit.predict_test = predict(svmfit,test_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(svmfit.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h3><p><strong>这边我没调参，我觉得这边做的好坏在于数据预处理中剩下来的特征词</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">sms_classifier &lt;- naiveBayes(train_feature_matrix[,-1], train_feature_matrix$label)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">sms.predict_train=predict(sms_classifier, train_feature_matrix)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(sms.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">sms.predict_test = predict(sms_classifier,test_feature_matrix)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(sms.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p><strong>这边因为是最后的整合模型，需要调参的地方比较多，首先根据oob确定在mtry=log（feature）下的最优trees数量，在根据确定的trees的数量，反过来去确定mtry的确定值。除此之外，还需要对树的最大深度，子节点的停止条件做交叉模拟，是整体模型训练过程中最耗时的地方</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(randomForest)</span><br><span class="line">randomForest=randomForest(train_feature_matrix[,-1], train_feature_matrix$label)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">rf.predict_train=predict(randomForest, train_feature_matrix)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(rf.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">rf.predict_test = predict(randomForest,test_feature_matrix)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(rf.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<p>就单模型下的test集合的准确率如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-c2eca9e5da73375b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>整体上看，nnet是过拟合的，所以在测试集上的效果折扣程度最大；naive bayes模型的拟合效果应该是最弱的，但是好在它的开发成本低，逻辑简单，有统计意义；svm和randomforest这边的效果不相上下。本次训练的数据量在20w条左右，理论上讲再扩大数据集的话，randomforest的效果应该会稳定，svm会下降，nnet会上升。</p>
<h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p><img src="http://upload-images.jianshu.io/upload_images/1129359-e20c4b10905b9090.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这边的train_data的准确率在92.1%，test_data的准确率在84.3%，与理想的test_data90%以上的准确率还是有差距，所以后续准备：<br>1.细化流失用户的定义方式，当前定义过于笼统粗糙<br>2.以RNN的模型去替代BpNN去做整合训练，探索特征到特征本身的激活会对结果的影响<br>3.重新定义词重要性，考虑互信息熵及isolation forest的判别方式</p>
<p>最后谢谢大家的阅读。</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>有你们的支持，我们一定会不会停息</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/image/wechatpay.jpg" alt="沙韬伟 sladesal 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/image/alipay.jpg" alt="沙韬伟 sladesal 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/自然语言/" rel="tag"># 自然语言</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/09/数据预处理-异常值识别/" rel="next" title="数据预处理-异常值识别">
                <i class="fa fa-chevron-left"></i> 数据预处理-异常值识别
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/19/深度学习下的电商商品推荐/" rel="prev" title="深度学习下的电商商品推荐">
                深度学习下的电商商品推荐 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="SOHUCS"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/image/1.png"
                alt="沙韬伟 sladesal" />
            
              <p class="site-author-name" itemprop="name">沙韬伟 sladesal</p>
              <p class="site-description motion-element" itemprop="description">前滴滴出行风控算法负责人，hp实验室研究员</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="mailto:stw386@sina.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/seven_xi/activities" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-globe"></i>知乎</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://www.jianshu.com" target="_blank" title="简书">
                    
                      <i class="fa fa-fw fa-globe"></i>简书</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#定义用户属性"><span class="nav-number">1.</span> <span class="nav-text">定义用户属性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#文本合成"><span class="nav-number">2.</span> <span class="nav-text">文本合成</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据整理"><span class="nav-number">3.</span> <span class="nav-text">数据整理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化处理"><span class="nav-number">3.1.</span> <span class="nav-text">正则化处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#停顿词"><span class="nav-number">3.2.</span> <span class="nav-text">停顿词</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#文本分词"><span class="nav-number">4.</span> <span class="nav-text">文本分词</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tf-idf词特征值重要性排序"><span class="nav-number">5.</span> <span class="nav-text">tf-idf词特征值重要性排序</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#模型训练"><span class="nav-number">6.</span> <span class="nav-text">模型训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据因子化的预处理"><span class="nav-number">6.1.</span> <span class="nav-text">数据因子化的预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据切分训练测试"><span class="nav-number">6.2.</span> <span class="nav-text">数据切分训练测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型训练-1"><span class="nav-number">6.3.</span> <span class="nav-text">模型训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#backpropagation-neural-network"><span class="nav-number">6.3.1.</span> <span class="nav-text">backpropagation neural network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Support-Vector-Machine"><span class="nav-number">6.3.2.</span> <span class="nav-text">Linear Support Vector Machine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#贝叶斯分类器"><span class="nav-number">6.3.3.</span> <span class="nav-text">贝叶斯分类器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林"><span class="nav-number">6.3.4.</span> <span class="nav-text">随机森林</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型集成"><span class="nav-number">6.3.5.</span> <span class="nav-text">模型集成</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">沙韬伟 sladesal</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">51.2k</span>
  
</div>


<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  




  
    <script type="text/javascript">
    (function(){
      var appid = 'cytllX3jJ';
      var conf = 'df80cce337485e83adcdb34af93e3dbb';
      var width = window.innerWidth || document.documentElement.clientWidth;
      if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){
        window.changyan.api.config({appid:appid,conf:conf})});
      }
    })();
    </script>
    <script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script>
  









  





  

  

  

  
  

  

  

  

</body>
</html>
