<!DOCTYPE html>



  

<html class="theme-next muse use-motion" lang="zh-Hans">


<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="NLP," />










<meta name="description" content="update:17.12.20 : 关于IDF处描述，经@余海跃同学提醒，细化了解释内容，感谢！   在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券">
<meta name="keywords" content="NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="基于自然语言识别下的流失用户预警">
<meta property="og:url" content="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/index.html">
<meta property="og:site_name" content="SladeSha&#39;s Algorithm World">
<meta property="og:description" content="update:17.12.20 : 关于IDF处描述，经@余海跃同学提醒，细化了解释内容，感谢！   在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券安慰”，”特权享受”等行为，有效的降低了用户的流失。根据实际的业务营销效果，在模">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/1.png">
<meta property="og:image" content="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/2.png">
<meta property="og:image" content="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/3.png">
<meta property="og:image" content="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/4.png">
<meta property="og:image" content="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/5.png">
<meta property="og:updated_time" content="2019-05-10T10:15:45.263Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="基于自然语言识别下的流失用户预警">
<meta name="twitter:description" content="update:17.12.20 : 关于IDF处描述，经@余海跃同学提醒，细化了解释内容，感谢！   在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券安慰”，”特权享受”等行为，有效的降低了用户的流失。根据实际的业务营销效果，在模">
<meta name="twitter:image" content="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/"/>





  <title>基于自然语言识别下的流失用户预警 | SladeSha's Algorithm World</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SladeSha's Algorithm World</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">微信公众号：ml_trip</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-me">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            ME
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sladesha.github.io/2017/08/15/基于自然语言识别下的流失用户预警/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="沙韬伟 sladesha">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/1.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SladeSha's Algorithm World">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">基于自然语言识别下的流失用户预警</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-15T23:06:35+08:00">
                2017-08-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4,055 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  17 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>update:</strong><br><strong>17.12.20 : 关于IDF处描述，经@余海跃同学提醒，细化了解释内容，感谢！</strong></p>
<hr>
<blockquote>
<p>在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，<strong>通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券安慰”，”特权享受”等行为，有效的降低了用户的流失。</strong>根据实际的业务营销效果，在模型上线后，<strong>abtest检验下模型识别用户人群进行营销后的流失率比随意营销下降9.2%，效果显著。</strong></p>
</blockquote>
<p>当前文本文义识别存在一些问题：<br>（1）准确率而言，很多线上数据对特征分解的过程比较粗糙，<strong>很多直接基于df或者idf结果进行排序</strong>，在算法设计过程中，<strong>也是直接套用模型</strong>，只是工程上的实现，缺乏统计意义上的分析；</p>
<p>（2）<strong>文本越多，特征矩阵越稀疏，计算过程越复杂</strong>。常规的文本处理过程中只会对文本对应的特征值进行排序，其实在文本选择中，可以先剔除相似度较高的文本，这个课题比较大，后续会单独开一章进行研究；</p>
<p>（3）<strong>扩展性较差</strong>。比如我们这次做的流失用户预警是基于电商数据，你拿去做通信商的用户流失衡量的话，其质量会大大下降，所以重复开发的成本较高，这个属于非增强学习的硬伤，目前也在攻克这方面的问题。</p>
<p>首先，我们来看下，整个算法设计的思路：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.通过hive将近期的用户评价hadoop文件下载为若干个text文件</span><br><span class="line">2.通过R语言将若干个text整合读取为一个R内的dataframe</span><br><span class="line">3.利用R里面的正则函数将文本中的异常符号‘#！@￥%%’，英文，标点等去除</span><br><span class="line">（这边可以在hive里面提前处理好，也可以在后续的分词过程中利用停顿词去除）</span><br><span class="line">4.文本分词，这边可以利用R中的Rwordseg，jiebaR等，我写这篇文章之前看到很多现有的语义分析的文章中，Rwordseg用的挺多，所以这边我采用了jiebaR</span><br><span class="line">5.文本分词特征值提取,常见的包括互信息熵，信息增益，tf-idf，本文采取了tf-idf，剩余方法会在后续文章中更新</span><br><span class="line">6.模型训练</span><br><span class="line">这边我采取的方式是利用概率模型naive bayes+非线性模型random forest先做标签训练，最后用nerual network对结果进行重估</span><br><span class="line">（原本我以为这样去做会导致很严重的过拟合，但是在实际操作之后发现，过拟合并不是很严重，至于原因我也不算很清楚，后续抽空可以研究一下）</span><br></pre></td></tr></table></figure></p>
<p>下面，我们来剖析文本分类识别的每一步</p>
<h1 id="定义用户属性"><a href="#定义用户属性" class="headerlink" title="定义用户属性"></a>定义用户属性</h1><p>首先，我们定义了已经存在的流失用户及非流失用户，易购的用户某品类下的购买周期为27天，针对前60天-前30天下单购物的用户，观察近30天是否有下单行为，如果有则为非流失用户，如果没有则为流失用户。提取每一个用户最近一次商品评价作为msg。</p>
<h1 id="文本合成"><a href="#文本合成" class="headerlink" title="文本合成"></a>文本合成</h1><p>通过hive -e的方式下载到本地，会形成text01，text02…等若干个文本，通过R进行<strong>文本整合</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#先设置文本路径</span><br><span class="line">path &lt;- &quot;C:/Users/17031877/Desktop/Nlp/answer/Cmsg&quot;</span><br><span class="line">completepath &lt;- list.files(path, pattern = &quot;*.txt$&quot;, full.names = TRUE)</span><br><span class="line"></span><br><span class="line">#批量读入文本</span><br><span class="line">readtxt &lt;- function(x) &#123;</span><br><span class="line">ret &lt;- readLines(x)                   #每行读取</span><br><span class="line">return(paste(ret, collapse = &quot;&quot;))     #通过paste将每一行连接起来</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#lappy批量操作，形成list，个人感觉对非关系数据，list处理更加便捷</span><br><span class="line">msg &lt;- lapply(completepath, readtxt)</span><br><span class="line"></span><br><span class="line">#用户属性</span><br><span class="line">user_status &lt;- list.files(path, pattern = &quot;*.txt$&quot;)</span><br><span class="line"></span><br><span class="line">#stringsAsFactors=F，避免很多文本被读成因子类型</span><br><span class="line">comment &lt;- as.data.frame(cbind(user_status, unlist(msg)),stringsAsFactors = F)</span><br><span class="line">colnames(comment) &lt;- c(&quot;user_status&quot;, &quot;msg&quot;)</span><br></pre></td></tr></table></figure></p>
<p>基础的数据整合就完成了。</p>
<p><img src="/2017/08/15/基于自然语言识别下的流失用户预警/1.png" alt=""></p>
<h1 id="数据整理"><a href="#数据整理" class="headerlink" title="数据整理"></a>数据整理</h1><p>也可以看到，基础数据读取完成后，还是很多评论会有一些<strong>不规则的数据</strong>，包括‘#￥%……&amp;’，英文，数字，下面通过正则、停顿词的方式进行处理：</p>
<h2 id="正则化处理"><a href="#正则化处理" class="headerlink" title="正则化处理"></a>正则化处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#直接处理</span><br><span class="line">comment$msg &lt;- gsub(pattern = &quot; &quot;, replacement =&quot;&quot;, comment$msg)  #gsub是字符替换函数，去空格</span><br><span class="line">comment$msg&lt;- gsub(&quot;[[:digit:]]*&quot;, &quot;&quot;, comment$msg) #清除数字[a-zA-Z]</span><br><span class="line">comment$msg&lt;- gsub(&quot;[a-zA-Z]&quot;, &quot;&quot;, comment$msg)   #清除英文字符</span><br><span class="line">comment$msg&lt;- gsub(&quot;\\.&quot;, &quot;&quot;, comment$msg)      #清除全英文的dot符号</span><br><span class="line">--------------------------------------------------------------------------------------------------</span><br><span class="line">#如果是常做nlp处理，可以写成函数打包，后期直接library就可以了</span><br><span class="line">#数值删除</span><br><span class="line">removeNumbers =</span><br><span class="line">function(x)&#123;</span><br><span class="line">ret = gsub(&quot;[0-9]&quot;,&quot;&quot;,x)</span><br><span class="line">return(ret)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#字符删除</span><br><span class="line">removeLiters =</span><br><span class="line">function(x)&#123;</span><br><span class="line">ret = gsub(&quot;[a-z|A-Z]&quot;,&quot;&quot;,x)</span><br><span class="line">return(ret)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#各种操作符处理,\s表示空格,\r表示回车,\n表示换行</span><br><span class="line">removeActions =</span><br><span class="line">function(x)&#123;</span><br><span class="line">ret = gsub(&quot;\\s|\\r|\\n&quot;, &quot;&quot;, x)</span><br><span class="line">return(ret)</span><br><span class="line">&#125;</span><br><span class="line">comment$msg=removeNumbers(comment$msg)</span><br><span class="line">comment$msg=removeLiters(comment$msg)</span><br><span class="line">comment$msg=removeActions (comment$msg)</span><br></pre></td></tr></table></figure>
<p>这边需要对正则化里面的一些表示有所了解，详细可以百度，一般我都是具体需求具体去看，因为太多，自己又懒，所以没记</p>
<h2 id="停顿词"><a href="#停顿词" class="headerlink" title="停顿词"></a>停顿词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#加载jiebaR包</span><br><span class="line">library(jiebaR)</span><br><span class="line"></span><br><span class="line">#找jiebaR存停顿词的地方，自行将需要处理掉的符号存进去，我这边是C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8</span><br><span class="line">tagger&lt;-worker(stop_word=&quot;C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8&quot;)</span><br></pre></td></tr></table></figure>
<p>至于位置可以通过直接输入<code>worker()</code>查看，</p>
<p><img src="/2017/08/15/基于自然语言识别下的流失用户预警/2.png" alt=""><br>当前的是没有stop_word的，所有词存储的位置在：C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/下</p>
<h1 id="文本分词"><a href="#文本分词" class="headerlink" title="文本分词"></a>文本分词</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#jieba 分词,去除停顿词</span><br><span class="line">library(jiebaR)</span><br><span class="line">tagger&lt;-worker(stop_word=&quot;C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8&quot;)</span><br><span class="line">words=list()</span><br><span class="line">for (i in 1:nrow(comment))&#123;</span><br><span class="line">tmp=tagger[comment[i,2]]</span><br><span class="line">words=c(words,list(tmp))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>直接先分词，但是分词结果会存在很多只有一个字比如‘的’、‘你’、‘我’等或者很多无意义的长句‘中华人民共和国’、‘长使英雄泪满襟’等，需要把这些<strong>词长异常</strong>明显无意义的词句去掉。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#词长统计</span><br><span class="line">whole_words_set=unlist(words)</span><br><span class="line">whole_words_set_rank=data.frame(table(whole_words_set))</span><br><span class="line"></span><br><span class="line">whole_words_set_dealed=c()</span><br><span class="line">for (i in 1:nrow(whole_words_set_rank))&#123;</span><br><span class="line">tmp=nchar(as.character(whole_words_set_rank[i,1]))</span><br><span class="line">whole_words_set_dealed=c(whole_words_set_dealed,tmp)</span><br><span class="line">&#125;</span><br><span class="line">whole_words_set_dealed=cbind(whole_words_set_rank,whole_words_set_dealed)</span><br><span class="line">whole_words_set_dealed=whole_words_set_dealed[whole_words_set_dealed$whole_words_set_dealed&gt;1&amp;whole_words_set_dealed$whole_words_set_dealed&lt;5,]</span><br><span class="line">whole_words_set_dealed=whole_words_set_dealed[order(whole_words_set_dealed$Freq,decreasing=T),]</span><br><span class="line"></span><br><span class="line">#words的删除异常值,排序</span><br><span class="line">whole_words_set_sequence=words</span><br><span class="line">key_word=nrow(words)</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">for (j in 1:length(words[[i]]))&#123;</span><br><span class="line">tmp=ifelse(nchar(words[[i]][j])&gt;1 &amp; nchar(words[[i]][j])&lt;5,words[[i]][j],&apos;&apos;)</span><br><span class="line">whole_words_set_sequence[[i]][j]=tmp</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">whole_words_set_sequence[[i]]=whole_words_set_sequence[[i]][whole_words_set_sequence[[i]]!=&apos;&apos;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="tf-idf词特征值重要性排序"><a href="#tf-idf词特征值重要性排序" class="headerlink" title="tf-idf词特征值重要性排序"></a>tf-idf词特征值重要性排序</h1><p>首先，我们大致看一下排序的数据依旧：</p>
<p>TF = 某词在文章中出现的次数/文章包含的总词数（或者文章有价值词次数）<br>DF = （包含某词的文档数）/（语料库的文档总数）<br>IDF = log（（语料库的文档总数）/（包含某词的文档数+1））<br>这边的+1是为了避免（语料库的文档总数）/（包含某词的文档数）=1，log(1)=0，使得最后的重要性中出现0的情况，与有意义的前提相互驳斥。<br>TF-IDF = TF*IDF</p>
<p>分别看下，里面的每一项的意义：<br>TF，我们可以看出，<strong>在同一个评论中，词数出现的越多，代表这个词越能成为这篇文章的代表</strong>，当然前提是非无意义的助词等。</p>
<p>IDF，我们可以看出，<strong>所以评论中，包含目标词的评论的占比，占比数越高，目标词的意义越大</strong>，假设1000条评论中，“丧心病狂”在一条评论里面重复了10次，但是其他999条里面一次也没有出现，那就算“丧心病狂”非常能代表这条评论，但是在做文本集特征考虑的情况下，它的价值也是不大的。</p>
<p><strong>注意，经@余海跃同学提醒，这边的IDF解释不清晰，详细剖析如下：</strong><br>首先idf的定义是如下这样的：<br><img src="/2017/08/15/基于自然语言识别下的流失用户预警/3.png" alt=""><br>（D为所有文章，d为单篇文章）<br>通常，会考虑类似近拉普拉斯平滑（+1）这样的方法修正idf值，在NLP领域，真正的意思如你所理解的：随着single word出现在的doc数量的增加，idf值应该是下降的，我们认为，一个词在越多文档中出现，该词代表文章的概述的能力越弱。其实，我在《应用：基于自然语言识别下的流失用户预警》实际R代码编写过程中也是这么去做的，但是当时我考虑了另一个方面：电商的评论与传统的文学文本差异还是很大的，单条评论中独特的词（只出现过一次的词或者短句）非常之多。这意味着：如果原封不动的按照idf去计算的话，最后识别出来的判别标签，也就是’文本分词特征值’会变得非常多，而且对泛化情况的识别能力非常的差。体现在对做后续的有监督分类的时候，如果不做处理会造成异常过拟合的问题。所以，我这边表述的想法是将idf值过大的一些词，也就是single word出现的doc过少的一些词剔除，再根据剩余的其他特征词计算idf提取关键特征词，我这边设定的阈值范围是：特征词至少在3.5%以上的评论中出现过。当然，你完全可以选择另外一种方法，完全按照idf计算，在最后做特分类之前，做特征筛选，去除掉一些冗余特征词变量。</p>
<p>下面，我们来看代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#tfidf_partone 为对应的tf</span><br><span class="line">tdidf_partone=whole_words_set_sequence</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp1=as.data.frame(prop.table(table(whole_words_set_sequence[[i]])))</span><br><span class="line">tdidf_partone[[i]]=tmp1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#tdidf_partfour 为对应的idf</span><br><span class="line">tdidf_parttwo=unique(unlist(whole_words_set_sequence))</span><br><span class="line">tdidf_max=length(tdidf_parttwo)</span><br><span class="line">tdidf_partthree=tdidf_parttwo</span><br><span class="line">for (i in 1:tdidf_max)&#123;</span><br><span class="line">tmp=0</span><br><span class="line">aimed_word=tdidf_parttwo[i]</span><br><span class="line">for (j in 1:key_word)&#123;</span><br><span class="line">tmp=tmp+sum(tdidf_parttwo[i] %in% whole_words_set_sequence[[j]])</span><br><span class="line">&#125;</span><br><span class="line">tdidf_partthree[i]=log(as.numeric(key_word)/(tmp+1))</span><br><span class="line">&#125;</span><br><span class="line">tdidf_partfour=cbind(tdidf_parttwo,tdidf_partthree)</span><br><span class="line">tdidf_partfive=tdidf_partone</span><br><span class="line">colnames(tdidf_partfour)&lt;-c(&apos;Var1&apos;,&apos;Freq1&apos;)</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tdidf_partfive[[i]]=merge(tdidf_partone[[i]],tdidf_partfour,by=c(&quot;Var1&quot;))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#计算tf-idf结果，并排序key_word</span><br><span class="line">tdidf_partsix=tdidf_partfive</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp=tdidf_partfive[[i]][,2:3]</span><br><span class="line">tdidf_partsix[[i]][,2]=as.numeric(tmp[,1])*as.numeric(tmp[,2])</span><br><span class="line">tdidf_partsix[[i]]=tdidf_partsix[[i]][order(tdidf_partsix[[i]][,2],decreasing=T),][]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">key_word=c()</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp=tdidf_partsix[[i]][1:5,1]</span><br><span class="line">key_word=rbind(key_word,as.character(tmp))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>理论上讲，如果这边数据存储方式用的是data.frame的话，可以利用spply、apply等批量处理函数，这边用得是list的方式，对lpply不是很熟悉的我，选择了for的循环，后续这边会优化一下，这样太消耗资源了。</p>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><p>这边，我最后采取的是概率模型naive bayes+非线性模型random forest先做标签训练，最后用nerual network对结果进行重估方式，但是在训练过程中，我还有几种模型的尝试，这边也一并贴出来给大家做参考。</p>
<h2 id="数据因子化的预处理"><a href="#数据因子化的预处理" class="headerlink" title="数据因子化的预处理"></a>数据因子化的预处理</h2><p>这边得到了近400维度的有效词，现在将每一维度的词遍做一维的feature，同时，此处的feature的意义为要么评论存在该词，要么评论中不存在该词的0-1问题，<strong>需要因子化一下</strong>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#整合数据</span><br><span class="line">well_dealed_data=cbind(as.character(comment[,1]),key_word)</span><br><span class="line">names=as.data.frame(table(key_word))[,1]</span><br><span class="line">names_count=length(names)</span><br><span class="line">names=as.matrix(names,names_count,1)</span><br><span class="line">feature_matrix=matrix(rep(0,names_count*key_word),key_word,names_count)</span><br><span class="line">for (i in 1:names_count)&#123;</span><br><span class="line">for(j in 1:key_word)&#123;</span><br><span class="line">feature_matrix[j,i]=ifelse(names[i] %in% key_word[j,],1,0)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#art=1,literature=-1,标签0-1化</span><br><span class="line">feature_matrix=cbind(well_dealed_data[,1],feature_matrix)</span><br><span class="line">feature_matrix[feature_matrix[,1]==&apos;aimed&apos;,1]=&apos;1&apos;</span><br><span class="line">feature_matrix[feature_matrix[,1]==&apos;unaimed&apos;,1]=&apos;-1&apos;</span><br><span class="line"></span><br><span class="line">feature_matrix=as.data.frame(feature_matrix)</span><br><span class="line"></span><br><span class="line">num=1:(ncol(feature_matrix)-1)</span><br><span class="line">value_name=paste(&quot;feature&quot;,num)</span><br><span class="line">value_name=c(&apos;label&apos;,value_name)</span><br><span class="line">colnames(feature_matrix)=value_name</span><br><span class="line"></span><br><span class="line">#feature0-1化</span><br><span class="line">for (i in 1:ncol(feature_matrix))&#123;</span><br><span class="line">feature_matrix[,i]=as.factor(as.numeric(as.character(feature_matrix[,i])))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="数据切分训练测试"><a href="#数据切分训练测试" class="headerlink" title="数据切分训练测试"></a>数据切分训练测试</h2><p>这边就不适用切分函数了，自己写了一个更加快速。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n_index=sample(1:nrow(feature_matrix),round(0.7*nrow(feature_matrix)))</span><br><span class="line">train_feature_matrix=feature_matrix[n_index,]</span><br><span class="line">test_feature_matrix=feature_matrix[-n_index,]</span><br></pre></td></tr></table></figure></p>
<h2 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="backpropagation-neural-network"><a href="#backpropagation-neural-network" class="headerlink" title="backpropagation neural network"></a>backpropagation neural network</h3><p><strong>这边需要用网格算法对size和decay进行交叉检验，这边不贴细节，可以百度搜索详细过程。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(nnet)</span><br><span class="line">nn &lt;- nnet(label~., data=train_feature_matrix, size=2, decay=0.01, maxit=1000, linout=F, trace=F)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">nn.predict_train = predict(nn,train_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),nn.predict_train)</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">nn.predict_test = predict(nn,test_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),nn.predict_test)</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="Linear-Support-Vector-Machine"><a href="#Linear-Support-Vector-Machine" class="headerlink" title="Linear Support Vector Machine"></a>Linear Support Vector Machine</h3><p><strong>这边需要用网格算法对cost进行交叉检验，这边不贴细节，可以百度搜索详细过程。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">svmfit &lt;- svm(label~., data=train_feature_matrix, kernel = &quot;linear&quot;, cost = 10, scale = FALSE) # linear svm, scaling turned OFF</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">svmfit.predict_train=predict(svmfit, train_feature_matrix, type = &quot;probabilities&quot;)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(svmfit.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">svmfit.predict_test = predict(svmfit,test_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(svmfit.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h3><p><strong>这边我没调参，我觉得这边做的好坏在于数据预处理中剩下来的特征词</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">sms_classifier &lt;- naiveBayes(train_feature_matrix[,-1], train_feature_matrix$label)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">sms.predict_train=predict(sms_classifier, train_feature_matrix)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(sms.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">sms.predict_test = predict(sms_classifier,test_feature_matrix)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(sms.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p><strong>这边因为是最后的整合模型，需要调参的地方比较多，首先根据oob确定在mtry=log（feature）下的最优trees数量，在根据确定的trees的数量，反过来去确定mtry的确定值。除此之外，还需要对树的最大深度，子节点的停止条件做交叉模拟，是整体模型训练过程中最耗时的地方</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(randomForest)</span><br><span class="line">randomForest=randomForest(train_feature_matrix[,-1], train_feature_matrix$label)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">rf.predict_train=predict(randomForest, train_feature_matrix)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(rf.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">rf.predict_test = predict(randomForest,test_feature_matrix)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(rf.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<p>就单模型下的test集合的准确率如下：</p>
<p><img src="/2017/08/15/基于自然语言识别下的流失用户预警/4.png" alt=""><br>整体上看，nnet是过拟合的，所以在测试集上的效果折扣程度最大；naive bayes模型的拟合效果应该是最弱的，但是好在它的开发成本低，逻辑简单，有统计意义；svm和randomforest这边的效果不相上下。本次训练的数据量在20w条左右，理论上讲再扩大数据集的话，randomforest的效果应该会稳定，svm会下降，nnet会上升。</p>
<h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p><img src="/2017/08/15/基于自然语言识别下的流失用户预警/5.png" alt=""></p>
<p>这边的train_data的准确率在92.1%，test_data的准确率在84.3%，与理想的test_data90%以上的准确率还是有差距，所以后续准备：<br>1.细化流失用户的定义方式，当前定义过于笼统粗糙<br>2.以RNN的模型去替代BpNN去做整合训练，探索特征到特征本身的激活会对结果的影响<br>3.重新定义词重要性，考虑互信息熵及isolation forest的判别方式</p>
<p>最后谢谢大家的阅读。</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>打赏的大佬可以联系我，赠送超赞的算法资料</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/image/wechatpay.jpg" alt="沙韬伟 sladesha 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/image/alipay.jpg" alt="沙韬伟 sladesha 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/09/数据预处理-异常值识别/" rel="next" title="数据预处理-异常值识别">
                <i class="fa fa-chevron-left"></i> 数据预处理-异常值识别
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/19/深度学习下的电商商品推荐/" rel="prev" title="深度学习下的电商商品推荐">
                深度学习下的电商商品推荐 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="SOHUCS"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/image/1.png"
                alt="沙韬伟 sladesha" />
            
              <p class="site-author-name" itemprop="name">沙韬伟 sladesha</p>
              <p class="site-description motion-element" itemprop="description">前滴滴出行风控算法负责人，前HP-AILab算法研究员</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">71</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-globe"></i>知乎</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.jianshu.com/u/79b57248a6c3" target="_blank" title="简书">
                    
                      <i class="fa fa-fw fa-globe"></i>简书</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://github.com/sladesha" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:stw386@sina.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/slade_sha" target="_blank" title="Twitter">
                    
                      <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#定义用户属性"><span class="nav-number">1.</span> <span class="nav-text">定义用户属性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#文本合成"><span class="nav-number">2.</span> <span class="nav-text">文本合成</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据整理"><span class="nav-number">3.</span> <span class="nav-text">数据整理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化处理"><span class="nav-number">3.1.</span> <span class="nav-text">正则化处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#停顿词"><span class="nav-number">3.2.</span> <span class="nav-text">停顿词</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#文本分词"><span class="nav-number">4.</span> <span class="nav-text">文本分词</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tf-idf词特征值重要性排序"><span class="nav-number">5.</span> <span class="nav-text">tf-idf词特征值重要性排序</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#模型训练"><span class="nav-number">6.</span> <span class="nav-text">模型训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据因子化的预处理"><span class="nav-number">6.1.</span> <span class="nav-text">数据因子化的预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据切分训练测试"><span class="nav-number">6.2.</span> <span class="nav-text">数据切分训练测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型训练-1"><span class="nav-number">6.3.</span> <span class="nav-text">模型训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#backpropagation-neural-network"><span class="nav-number">6.3.1.</span> <span class="nav-text">backpropagation neural network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Support-Vector-Machine"><span class="nav-number">6.3.2.</span> <span class="nav-text">Linear Support Vector Machine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#贝叶斯分类器"><span class="nav-number">6.3.3.</span> <span class="nav-text">贝叶斯分类器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林"><span class="nav-number">6.3.4.</span> <span class="nav-text">随机森林</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型集成"><span class="nav-number">6.3.5.</span> <span class="nav-text">模型集成</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">沙韬伟 sladesha</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">136.2k</span>
  
</div>


<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  


  

  

</body>
</html>
