<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[博客导读公告(置顶)]]></title>
      <url>/2029/01/20/%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%AF%BB%E5%85%AC%E5%91%8A/</url>
      <content type="html"><![CDATA[<p><img src="/2029/01/20/博客导读公告/wechat.png" alt=""><br><a id="more"></a></p>
<p><strong>首先，我很荣幸您出现在我的个人博客，下面请允许我花费您3分钟左右的时间，简单的为您介绍一下博客中的相关功能，这将极大的提高您在后续阅读中的体验：</strong></p>
<p>打个广告，欢迎各位老爷关注我的微信公众号：ml_trip，期待与大家交流！</p>
<h2 id="我的个人介绍"><a href="#我的个人介绍" class="headerlink" title="我的个人介绍"></a>我的个人介绍</h2><p>大家可以在首页的标题下面找到me这个图标，点击即可，里面有我的个人介绍：<br><img src="/2029/01/20/博客导读公告/个人介绍.png" alt=""></p>
<p>我的个人简历下载地址在me跳转页面中的位置如下：<br><img src="/2029/01/20/博客导读公告/cv.png" alt=""></p>
<hr>
<h2 id="快速阅读"><a href="#快速阅读" class="headerlink" title="快速阅读"></a>快速阅读</h2><p>您可在任何一篇文章的右侧看到红色方框：</p>
<ul>
<li>如果您对其中部分内容感兴趣，可直接点击绿色方框内的文章，会自动跳转到您关心的模块；</li>
<li>如果您想要看到我的<strong>更多联系方式</strong>，可以点击蓝色模块中的站点概览；</li>
<li>如果您不想红色方框影响您的阅读，在黑色条块的最下方的小叉点击即可。<br><img src="/2029/01/20/博客导读公告/read.png" alt=""></li>
</ul>
<hr>
<h2 id="赞助激励"><a href="#赞助激励" class="headerlink" title="赞助激励"></a>赞助激励</h2><p>如果您觉得我写的东西对您有一些帮助，在您宽裕的情况下可以在文章下面的打赏中给我发一个小红包，或者直接扫描下面的二维码，感谢您对我的认可：<br><img src="/2029/01/20/博客导读公告/pay.png" alt=""></p>
<p><strong>如果您还是一个学生或者您正处于人生的低谷，感谢您对我的认可，打赏就不需要了，我会一如既往的给大家整理工作中的一些想法和心得</strong></p>
<hr>
<h2 id="自定义搜索"><a href="#自定义搜索" class="headerlink" title="自定义搜索"></a>自定义搜索</h2><p>为了方便大家找到自己关心的内容，建议直接点击搜索图标：<br><img src="/2029/01/20/博客导读公告/search.png" alt=""></p>
<p>比如搜索svm，有模糊匹配结果如下：<br><img src="/2029/01/20/博客导读公告/content.png" alt=""></p>
<hr>
<h2 id="标签检索"><a href="#标签检索" class="headerlink" title="标签检索"></a>标签检索</h2><p>大家可以在首页的标题下面找到标签这个图标，点击即可：<br><img src="/2029/01/20/博客导读公告/tag.png" alt=""></p>
<p>该类别下生成了标签云，为较为仔细的文章内容概括：<br><img src="/2029/01/20/博客导读公告/cloud.png" alt=""></p>
<hr>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>因为本博客部署在GitHub，如果您有时候遇到打不开网页的问题，建议您重复刷新或者收藏<a href="https://www.jianshu.com/u/79b57248a6c3" target="_blank" rel="noopener">slade_sal简书地址</a>，两者内容是一致的</li>
<li>如果您有任何疑惑或者疑问都可以通过站点概览的邮箱联系我，非常愿意解答您的问题</li>
<li>如果您遇到算法学习过程的困难，需要内推或者就业方向建议，也可以通过站点概览的邮箱联系我，非常愿意和您进行交流</li>
<li>绝大多数代码都可以在我的Github上找到，所有的数据都经过脱敏处理，您可以放心使用，希望对您有所帮助</li>
</ul>
<p>最后，感谢大家一路以来对我的认可。</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 公告 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[PKUseg在货运领域的评测]]></title>
      <url>/2019/01/14/PKUseg%E5%9C%A8%E8%B4%A7%E8%BF%90%E9%A2%86%E5%9F%9F%E7%9A%84%E8%AF%84%E6%B5%8B/</url>
      <content type="html"><![CDATA[<p>先说结论，再和大家闲聊，对比jieba与PKUseg在公路货运切词能力上：</p>
<ul>
<li>默认模型下，jieba效果优于PKUseg</li>
<li>PKUseg提供场景精细化的预训练（还没有提供入口），长远来讲适合专业领域使用</li>
<li>PKUseg在特定的场景下有令人惊喜的效果（地址切分）</li>
</ul>
<p>给大家的建议就是，如果大家赶时间求稳定适应范围需要非常广的时候，目前来说jieba是非常好的选择，如果说在面临一些精细化领域的特殊需求的时候，可以用PKUseg进行一波尝试，有意外惊喜。</p>
<hr>
<p>那是一个风和日丽的早上，突然群里老大发出一条消息：<br><img src="/2019/01/14/PKUseg在货运领域的评测/boss.png" alt=""><br>我感觉我的心脏有一丝隐隐作痛的感觉，人在办公室坐，活从天上来，虽然身后站着一堆催上线的产品，我还是屈服于老大的正义（淫威），简单测评了新出来的PKUseg与Jieba在公路货运/运输行业上的效果对比。</p>
<p>在我们的热词数据库中已经有人工切词完成的2万多条货运的词条：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">description	standard</span><br><span class="line">高博集团装货卸宝华	高博 集团 装货 卸 宝华</span><br><span class="line">北安到吉林农安饲料90吨每吨105	北安 到 吉林 农安 饲料 90吨 每吨 105</span><br><span class="line">需要4个车	需要 4个 车</span><br><span class="line">叶张公路装香闵路曲吴路两卸	叶张公路 装 香闵路 曲吴路 两卸</span><br><span class="line">从福通物流到吴滩镇	从 福通 物流 到 吴滩镇</span><br><span class="line">霞浦宏霞路到中通物流	霞浦宏霞路 到 中通物流</span><br><span class="line">石大路3场到德兴西门山	石大路 3场 到 德兴 西门山</span><br><span class="line">公园西路装	公园 西路 装</span><br><span class="line">不押车每吨150	不 押车 每吨 150</span><br><span class="line">速订价钱好商量	速订 价钱 好商量</span><br><span class="line">慈溪胜山装	慈溪 胜山装</span><br><span class="line">好装好卸高价急走	好装好卸 高价急走</span><br><span class="line">九顶山路与东方大道位置装货可以配货	九顶 山路 与 东方 大道 位置 装货 可以 配货</span><br><span class="line">要二部	要 二部</span><br><span class="line">青浦工业园区久远路提货到奉贤新杨公路进仓	青浦 工业园区 久远路 提货 到 奉贤 新杨公路 进仓</span><br><span class="line">园光路装博学南路卸	园光路 装 博学南路 卸</span><br><span class="line">公兴装卸荣昌广顺	公兴 装卸 荣昌 广顺</span><br><span class="line">打备注电话18458331112	打 备注 电话 18458331112</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>首先看，不加任何词库，预训练下的，最后的效果对比：<br><img src="/2019/01/14/PKUseg在货运领域的评测/1.png" alt=""><br>可以看到，<strong>在默认的分词模型下，jieBa分词还是拥有绝对优势的</strong>，但是在pkuSeg的git里面<img src="/2019/01/14/PKUseg在货运领域的评测/2.png" alt=""></p>
<p>所以我想看看能不能进行一下预训练下后再对比一下，可惜的是我在git（<a href="https://github.com/sladesha/pkuseg-python#相关论文" target="_blank" rel="noopener">git地址传送门</a>）上找了半天也没有找到预训练的入口，只有已经被官方预训练好的词库<br><img src="/2019/01/14/PKUseg在货运领域的评测/3.png" alt=""><br>等有时间了，可以邮件沟通一下再补充这个部分的效果对比，我觉得，应该还是有提升的。</p>
<p>但是，在我们实际去测的过程中，我们发现了一些差异话的东西比较有意思。我们其实现在在做一个语音发货的产品，涉及到把一串地址切分开的需求：<br><img src="/2019/01/14/PKUseg在货运领域的评测/4.png" alt=""></p>
<p>其中涉及到<strong>地址切分</strong>的时候，jieba的能力会比如PKUseg要弱不少，比如“山西大同”，“上海浦东”，我们需要把一级二级地址切开的时候，PKUseg可以做到，而jieba并不能按照需求切块。所以，我们已经打算在地址模块切换PKUseg的模型来适应了。<br><img src="/2019/01/14/PKUseg在货运领域的评测/5.png" alt=""></p>
<p>最后吐槽一下，虽然我知道PKUseg需要加载模型，但是一加载就是一二十秒也是有点夸张了。酒浆，各位下回见。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 开源项目 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[pandas中的问题记录]]></title>
      <url>/2018/10/23/pandas%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
      <content type="html"><![CDATA[<p>最近发现pandas的一个问题，记录一下：<br>有一组数据（test.txt）如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">20181016</span>    <span class="number">14830680298903273</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953069</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953079</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953089</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953099</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953019</span></span><br></pre></td></tr></table></figure></p>
<p>剖析出来看，数据是按照<code>\t</code>进行分隔的：<code>&#39;20181016\t14830680298903273\n&#39;</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'test.txt'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">line = f.readline()</span><br><span class="line">print(line)</span><br></pre></td></tr></table></figure></p>
<p>我平时一直在用pandas去读数据，所以我很熟练的写下来如下的代码：<br><code>pd.read_table(&#39;test.txt&#39;,header=None)</code><br>然后发现，第一列变成了科学记数法的方式进行存储了：</p>
<p><img src="/2018/10/23/pandas中的问题记录/1.png" alt=""></p>
<p>很明显，科学记数法是可以转换的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def as_number(value):</span><br><span class="line">try:</span><br><span class="line">return &apos;&#123;:.0f&#125;&apos;.format(value)</span><br><span class="line">except:</span><br><span class="line">return value</span><br><span class="line"></span><br><span class="line"># 应用到目标列去即可</span><br><span class="line">data.uid.apply(as_number)</span><br></pre></td></tr></table></figure></p>
<p>诡异的事情发生了，对于14830680298903273在<code>as_number</code>函数转换下变成了14830680298903272，理论上讲14830680298903273没有小数部分不存在四舍五入的原因，网上搜了也没有很明确的解释，初步讨论后猜测应该是pandas在用float64去存这种长度过长的数字的时候有精度丢失的问题。</p>
<p>要解决也是很简单的：</p>
<ul>
<li>用open的形式打开，在切割逐步去用list进行append，在合并</li>
<li>用read_table的函数的时候，默认是用float64去存在的，<a href="http://pandas.pydata.org/pandas-docs/stable/basics.html?highlight=astype#selecting-columns-based-on-dtype" target="_blank" rel="noopener">改成object去存(dtype=object)</a></li>
<li>在生产数据的时候，对于这种过长的数据采取str的形式去存</li>
</ul>
<p>也是给自己提个醒，要规范一下自己的数据存储操作，并养成数据核对的习惯。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。<br><img src="https://upload-images.jianshu.io/upload_images/1129359-654dc61c581d94e1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[YoutubeNet的数据答疑]]></title>
      <url>/2018/10/16/YoutubeNet%E7%9A%84%E6%95%B0%E6%8D%AE%E7%AD%94%E7%96%91/</url>
      <content type="html"><![CDATA[<p>实在是太忙了，抽空给大家解析一下之前写的<a href="https://github.com/sladesha/deep_learning/tree/master/YoutubeNetwork" target="_blank" rel="noopener">YoutubeNet</a>的数据是怎么构造的，协助大家可以自行构造一下。</p>
<p>这边和大家说一下，我没有上传数据的原因有两个：</p>
<ul>
<li>涉及公司的数据财产，不方便上传</li>
<li>懒得做脱敏处理</li>
<li>数据一共有1300多万条，传输实在不方便</li>
</ul>
<p>主要数据处理的部分在map_id_idx.py脚本下，其中包含all_item_20180624.txt和click_thirty_day_data_20180609.txt两个数据集合。</p>
<p>其中，all_item_20180624.txt是当日所有的商品集合：包含’Prd_Id’, ‘ItemId’, ‘BrandId’, ‘MsortId’和‘GenderId’五列，分别代表着商品id，skuid，低级品牌id，中级品牌id，产品性别，最后形如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5675</span>    <span class="number">50000055</span>    <span class="number">175</span>    <span class="number">1500</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2577</span>    <span class="number">50000056</span>    <span class="number">187</span>    <span class="number">66</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2002</span>    <span class="number">50000057</span>    <span class="number">63</span>    <span class="number">11</span>    <span class="number">2</span></span><br><span class="line"><span class="number">2007</span>    <span class="number">50000058</span>    <span class="number">137</span>    <span class="number">58</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2075</span>    <span class="number">50000060</span>    <span class="number">80</span>    <span class="number">50</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2348</span>    <span class="number">50000061</span>    <span class="number">138</span>    <span class="number">16</span>    <span class="number">2</span></span><br><span class="line"><span class="number">423</span>    <span class="number">50000062</span>    <span class="number">162</span>    <span class="number">237</span>    <span class="number">3</span></span><br><span class="line"><span class="number">469</span>    <span class="number">50000063</span>    <span class="number">10</span>    <span class="number">1500</span>    <span class="number">3</span></span><br><span class="line"><span class="number">1102</span>    <span class="number">50000064</span>    <span class="number">176</span>    <span class="number">11</span>    <span class="number">1</span></span><br><span class="line"><span class="number">1896</span>    <span class="number">50000066</span>    <span class="number">37</span>    <span class="number">27</span>    <span class="number">1</span></span><br><span class="line"><span class="number">2489</span>    <span class="number">50000067</span>    <span class="number">27</span>    <span class="number">44</span>    <span class="number">1</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>click_thirty_day_data_20180609.txt为近三十天的用户点击流，包含’UId’, ‘ItemId’, ‘clickTime’三列，分别代表着uid、点击的skuid，点击时间，最后形如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">34</span>    <span class="number">51668064</span>    <span class="number">1528602406</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51890512</span>    <span class="number">1528788389</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51884724</span>    <span class="number">1528788393</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51884720</span>    <span class="number">1528788399</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51884718</span>    <span class="number">1528788414</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51580974</span>    <span class="number">1528788442</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51854970</span>    <span class="number">1528788487</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51514910</span>    <span class="number">1528788499</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51855000</span>    <span class="number">1528788535</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51854990</span>    <span class="number">1528788569</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51854998</span>    <span class="number">1528788572</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>通过map_id_idx.py对所有的商品进行标序号，然后带入用户的点击流中，方便后期做embedding操作，就酱。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[GloVe向量化做文本分类]]></title>
      <url>/2018/09/25/GloVe%E5%90%91%E9%87%8F%E5%8C%96%E5%81%9A%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
      <content type="html"><![CDATA[<h1 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h1><p>在之前，我对向量化的方法一直局限在两个点，</p>
<p>第一种是常规方法的one-hot-encoding的方法，常见的比如tf-idf生成的0-1的稀疏矩阵来代表原文本：<br><img src="/2018/09/25/GloVe向量化做文本分类/1.png" alt=""></p>
<p>这种方法简单暴力，直接根据文本中的单词进行one-hot-encoding，但是数据量一但大了，这个单句话的one-hot-encoding结果会异常的长，而且没办法得到词与词之间的关系。</p>
<p>第二种是基于神经网络的方法，常见的比如word2vec，YouTubeNet：<br><img src="/2018/09/25/GloVe向量化做文本分类/2.png" alt=""></p>
<p>这种方法（这边以CBOW为例子）都是初始一个固定长度的随机向量作为每个单词的向量，制定一个目标词的向量，以上下文词向量的sum结果作为input进行前向传递，使得传递的结果和目标词向量尽可能一致，以修正初始的随机向量。<br>换句话说，就是刚开始，我随意定义生成一个vector代表一个词，然后通过上下文的联系去修正这个随机的vector。好处就是我们可以得到词与词之间的联系，而且单个词的表示不复杂，坏处就是需要大量的训练样本，毕竟涉及到了神经网络。</p>
<p>最近，我们突然发现了第三种方法，GloVe向量化。它也是开始的时候随机一个vector作为单词的表示，但是它不利用神经网络去修正，而是利用了一个自己构造的损失函数：<br><img src="/2018/09/25/GloVe向量化做文本分类/3.png" alt=""></p>
<p>通过我们已有的文章内容，去是的这个损失函数最小，这就变成了一个机器学习的方法了，相比较暴力的前馈传递，这也高快速和高效的多。同时，它还兼具了word2vec最后结果里面vector方法的优点，得到词与词之间的联系，而且单个词的表示不复杂。</p>
<p>这边就不展开GloVe算法的细节了，后面有空和大家补充，这个算法的构造非常巧妙，值得大家借鉴一下。</p>
<h1 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h1><p>刚才开门见山的聊了蛮久向量化，看起来和文本分类没什么关系，确实在通常意义上来讲，我们的最简单最常用的方法并不是向量化的方法，比如通过朴素贝叶斯，N-Grams这些方法来做分类识别。</p>
<h2 id="tfidf-N-grams"><a href="#tfidf-N-grams" class="headerlink" title="tfidf+N-grams"></a>tfidf+N-grams</h2><p>1.其实很简单，首先对语料库进行切词，维护自己的词典，做高频词的人工复审，将无意词进行stop_words归总<br><img src="/2018/09/25/GloVe向量化做文本分类/4.png" alt="对公司内部信息进行了一下处理，主要看分布趋势"></p>
<p>可以看到，高频词其实是非常非常少的，而且如果你真的去做了，你就会发现，”了”、“的”、“啊”这种语气词，和一些你公司相关的领域词汇会非常靠前，这些词作为stop_words会有效的降低训练成本、提高模型效果。</p>
<p>2.进行tf-idf，将词进行重赋权，字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降，有效的将向量化中的one hot encoding结果进行了修正。但是依然存在问题：在TFIDF算法中并没有体现出单词的位置信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sublinear_tf：replace tf with 1 + log(tf)</span></span><br><span class="line"><span class="comment"># max_df：用来剔出高于词频0.5的词</span></span><br><span class="line"><span class="comment"># token_pattern：(?u)\b\w+\b是为了匹配出长度为1及以上的词，默认的至少需要词长度为2</span></span><br><span class="line"><span class="comment"># ngram_range：这边我做了3-grams处理，如果只想朴素计算的话(1,1)即可</span></span><br><span class="line"><span class="comment"># max_features：随着我做了各种宽松的条件，最后生成的词维度会异常大，这边限制了前3万</span></span><br><span class="line">vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=<span class="keyword">True</span>, max_df=<span class="number">0.5</span>, token_pattern=<span class="string">r"(?u)\b\w+\b"</span>,ngram_range=(<span class="number">1</span>, <span class="number">3</span>), max_features=<span class="number">30000</span>)</span><br></pre></td></tr></table></figure>
<p>不得不说，python处理机器学习，深度学习的便捷程度是异常的高。</p>
<p>3.在经过TfidfVectorizer处理之后的结果是以稀疏矩阵的形式来存的，如果想看内容的话，可以用todense()转化为matrix来看。接下来，用贝叶斯来训练刚才得到的矩阵结果就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mnb_tri = MultinomialNB(alpha=<span class="number">0.001</span>)</span><br><span class="line">mnb_tri.fit(tri_train_set.tdm, tri_train_set.label)</span><br></pre></td></tr></table></figure>
<h2 id="tf-idf-n-grams-naive-bayes-lr"><a href="#tf-idf-n-grams-naive-bayes-lr" class="headerlink" title="tf-idf + n-grams + naive-bayes + lr"></a>tf-idf + n-grams + naive-bayes + lr</h2><p>这种方法是上面方法的升级版本，我们先看下架构：<br><img src="/2018/09/25/GloVe向量化做文本分类/5.png" alt="对公司内部信息进行了一下处理，主要看算法架构"></p>
<p>其实主要差异在于右侧的算法模型详细部分，我们做了一个由3-grams到3-grams+naive-bayes+lr的扩充，提升精度。</p>
<p>在模型的过程中，上面的第一步，都是一样的，在第二、三步有所差异：<br>2.在第二步中，我们除了要构造出一个3-grams的sparse matrix也需要构造出一个朴素的sparse matrix</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 朴素结果</span></span><br><span class="line">vectorizerby = TfidfVectorizer(stop_words=stpwrdlst, token_pattern=<span class="string">r"(?u)\b\w+\b"</span>, max_df=<span class="number">0.5</span>, sublinear_tf=<span class="keyword">True</span>,ngram_range=(<span class="number">1</span>, <span class="number">1</span>), max_features=<span class="number">100000</span>)</span><br></pre></td></tr></table></figure>
<p>3.不仅仅用bayes进行一次分类，而是根据3-grams和朴素情况下的sparse matrix进行预测，再用logistics regression来合并两个的结果做个stack进行0-1压缩。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造出一个3-grams的sparse matrix也需要构造出一个朴素的sparse matrix</span></span><br><span class="line">mnb_tri = MultinomialNB(alpha=<span class="number">0.001</span>)</span><br><span class="line">mnb_tri.fit(tri_train_set.tdm, tri_train_set.label)</span><br><span class="line">mnb_by = MultinomialNB(alpha=<span class="number">0.001</span>)</span><br><span class="line">mnb_by.fit(by_train_set.tdm, by_train_set.label)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加bias，cv选择最优正则结果，lbfgs配合l2正则</span></span><br><span class="line">lr = LogisticRegressionCV(multi_class=<span class="string">"ovr"</span>, fit_intercept=<span class="keyword">True</span>, Cs=np.logspace(<span class="number">-2</span>, <span class="number">2</span>, <span class="number">20</span>), cv=<span class="number">2</span>, penalty=<span class="string">"l2"</span>,solver=<span class="string">"lbfgs"</span>, tol=<span class="number">0.01</span>)</span><br><span class="line">re = lr.fit(adv_data[[<span class="string">'f1'</span>, <span class="string">'f2'</span>]], adv_data[<span class="string">'rep_label'</span>])</span><br></pre></td></tr></table></figure>
<p>总结一下上面两种方法，我觉得是入门快，效果也不错的小练手，也是完全可以作为我们开始一个项目的时候，用来做baseline的方法，主要是快啊～/斜眼笑</p>
<h2 id="GloVe-lr"><a href="#GloVe-lr" class="headerlink" title="GloVe+lr"></a>GloVe+lr</h2><p>因为我目前的带标签数据比较少，所以之前一直没有敢用word2vec去向量化作死，但是GloVe不存在这个问题啊，我就美滋滋的进行了一波。<br>首先，先讲下GloVe的使用：</p>
<ul>
<li><a href="https://github.com/stanfordnlp/GloVe" target="_blank" rel="noopener">https://github.com/stanfordnlp/GloVe</a> 在最大的代码抄袭网站下载(git clone)坦福大佬的代码，友情提醒，不要作死自己看了理论就觉得自己会写，自己搞个GloVe。(别问我是怎么知道的)</li>
<li>cd到对应目录下，vim demo.sh这个文件<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"></span><br><span class="line"><span class="comment"># Makes programs, downloads sample data, trains a GloVe model, and then evaluates it.</span></span><br><span class="line"><span class="comment"># One optional argument can specify the language used for eval script: matlab, octave or [default] python</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 请把make这边注释掉，这个是让你去下个demo，我们直接改成自己的数据</span></span><br><span class="line"><span class="comment"># make</span></span><br><span class="line"><span class="comment"># if [ ! -e text8 ]; then</span></span><br><span class="line"><span class="comment">#   if hash wget 2&gt;/dev/null; then</span></span><br><span class="line"><span class="comment">#     wget http://mattmahoney.net/dc/text8.zip</span></span><br><span class="line"><span class="comment">#   else</span></span><br><span class="line"><span class="comment">#     curl -O http://mattmahoney.net/dc/text8.zip</span></span><br><span class="line"><span class="comment">#   fi</span></span><br><span class="line"><span class="comment">#   unzip text8.zip</span></span><br><span class="line"><span class="comment">#   rm text8.zip</span></span><br><span class="line"><span class="comment"># fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CORPUS需要对应自己的欲训练的文档</span></span><br><span class="line">CORPUS=content.txt</span><br><span class="line">VOCAB_FILE=vocab.txt</span><br><span class="line">COOCCURRENCE_FILE=cooccurrence.bin</span><br><span class="line">COOCCURRENCE_SHUF_FILE=cooccurrence.shuf.bin</span><br><span class="line">BUILDDIR=build</span><br><span class="line">SAVE_FILE=vectors</span><br><span class="line">VERBOSE=2</span><br><span class="line">MEMORY=4.0</span><br><span class="line"><span class="comment"># 单词至少出现几次</span></span><br><span class="line">VOCAB_MIN_COUNT=3</span><br><span class="line"><span class="comment"># 向量长度</span></span><br><span class="line">VECTOR_SIZE=128</span><br><span class="line"><span class="comment"># 迭代次数</span></span><br><span class="line">MAX_ITER=30</span><br><span class="line"><span class="comment"># 窗口长度</span></span><br><span class="line">WINDOW_SIZE=15</span><br><span class="line">BINARY=2</span><br><span class="line">NUM_THREADS=8</span><br><span class="line">X_MAX=10</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ <span class="variable">$BUILDDIR</span>/vocab_count -min-count <span class="variable">$VOCAB_MIN_COUNT</span> -verbose <span class="variable">$VERBOSE</span> &lt; <span class="variable">$CORPUS</span> &gt; <span class="variable">$VOCAB_FILE</span>"</span></span><br><span class="line"><span class="variable">$BUILDDIR</span>/vocab_count -min-count <span class="variable">$VOCAB_MIN_COUNT</span> -verbose <span class="variable">$VERBOSE</span> &lt; <span class="variable">$CORPUS</span> &gt; <span class="variable">$VOCAB_FILE</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ <span class="variable">$BUILDDIR</span>/cooccur -memory <span class="variable">$MEMORY</span> -vocab-file <span class="variable">$VOCAB_FILE</span> -verbose <span class="variable">$VERBOSE</span> -window-size <span class="variable">$WINDOW_SIZE</span> &lt; <span class="variable">$CORPUS</span> &gt; <span class="variable">$COOCCURRENCE_FILE</span>"</span></span><br><span class="line"><span class="variable">$BUILDDIR</span>/cooccur -memory <span class="variable">$MEMORY</span> -vocab-file <span class="variable">$VOCAB_FILE</span> -verbose <span class="variable">$VERBOSE</span> -window-size <span class="variable">$WINDOW_SIZE</span> &lt; <span class="variable">$CORPUS</span> &gt; <span class="variable">$COOCCURRENCE_FILE</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ <span class="variable">$BUILDDIR</span>/shuffle -memory <span class="variable">$MEMORY</span> -verbose <span class="variable">$VERBOSE</span> &lt; <span class="variable">$COOCCURRENCE_FILE</span> &gt; <span class="variable">$COOCCURRENCE_SHUF_FILE</span>"</span></span><br><span class="line"><span class="variable">$BUILDDIR</span>/shuffle -memory <span class="variable">$MEMORY</span> -verbose <span class="variable">$VERBOSE</span> &lt; <span class="variable">$COOCCURRENCE_FILE</span> &gt; <span class="variable">$COOCCURRENCE_SHUF_FILE</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ <span class="variable">$BUILDDIR</span>/glove -save-file <span class="variable">$SAVE_FILE</span> -threads <span class="variable">$NUM_THREADS</span> -input-file <span class="variable">$COOCCURRENCE_SHUF_FILE</span> -x-max <span class="variable">$X_MAX</span> -iter <span class="variable">$MAX_ITER</span> -vector-size <span class="variable">$VECTOR_SIZE</span> -binary <span class="variable">$BINARY</span> -vocab-file <span class="variable">$VOCAB_FILE</span> -verbose <span class="variable">$VERBOSE</span>"</span></span><br><span class="line"><span class="variable">$BUILDDIR</span>/glove -save-file <span class="variable">$SAVE_FILE</span> -threads <span class="variable">$NUM_THREADS</span> -input-file <span class="variable">$COOCCURRENCE_SHUF_FILE</span> -x-max <span class="variable">$X_MAX</span> -iter <span class="variable">$MAX_ITER</span> -vector-size <span class="variable">$VECTOR_SIZE</span> -binary <span class="variable">$BINARY</span> -vocab-file <span class="variable">$VOCAB_FILE</span> -verbose <span class="variable">$VERBOSE</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$CORPUS</span>"</span> = <span class="string">'text8'</span> ]; <span class="keyword">then</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$1</span>"</span> = <span class="string">'matlab'</span> ]; <span class="keyword">then</span></span><br><span class="line">matlab -nodisplay -nodesktop -nojvm -nosplash &lt; ./<span class="built_in">eval</span>/matlab/read_and_evaluate.m 1&gt;&amp;2 </span><br><span class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$1</span>"</span> = <span class="string">'octave'</span> ]; <span class="keyword">then</span></span><br><span class="line">octave &lt; ./<span class="built_in">eval</span>/octave/read_and_evaluate_octave.m 1&gt;&amp;2</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ python eval/python/evaluate.py"</span></span><br><span class="line">python <span class="built_in">eval</span>/python/evaluate.py</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这边多说一下，<code>CORPUS=content.txt</code>这边content.txt里面的格式需要按照空格为分隔符进行存储，我之前一直以为是<code>\t</code>。</p>
<ul>
<li><p>直接sh demo.sh，你会得到vectors.txt，这个里面就对应每个词的向量表示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">天气 -0.754142 0.386905 -1.200074 -0.587121 0.758316 0.373824 0.342211 -1.275982 -0.300846 0.374902 -0.548544 0.595310 0.906426 0.029255 0.549932 -0.650563 -0.425185 1.689703 -1.063556 -0.790254 -1.191287 0.841529 1.080641 -0.082830 1.062107 -0.667727 0.573955 -0.604460 -0.601102 0.615299 -0.470923 0.039398 1.110345 1.071094 0.195431 -0.155259 -0.781432 0.457884 1.093532 -0.188207 -0.161646 0.246220 -0.346529 0.525458 0.617904 -0.328059 1.374414 1.020984 -0.959817 0.670894 1.091743 0.941185 0.902730 0.609815 0.752452 1.037880 -1.522382 0.085098 0.152759 -0.562690 -0.405502 0.299390 -1.143145 -0.183861 0.383053 -0.013507 0.421024 0.025664 -0.290757 -1.258696 0.913482 -0.967165 -0.131502 -0.324543 -0.385994 0.711393 1.870067 1.349140 -0.541325 -1.060084 0.078870 0.773146 0.358453 0.610744 0.407547 -0.552853 1.663435 0.120006 0.534927 0.219279 0.682160 -0.631311 1.071941 -0.340337 -0.503272 0.150010 1.347857 -1.024009 -0.181186 0.610240 -0.218312 -1.120266 -0.486539 0.264507 0.266192 0.347005 0.172728 0.613503 -0.131925 -0.727304 -0.504488 1.773406 -0.700505 -0.159963 -0.888025 -1.358476 0.540589 -0.243272 -0.236959 0.391855 -0.133703 -0.071120 1.050547 -1.087613 -0.467604 1.779341 -0.449409 0.949411</span><br><span class="line">好了 1.413075 -0.226177 -2.024229 -0.192003 0.628270 -1.227394 -1.054946 -0.900683 -1.958882 -0.133343 -1.014088 -0.434961 0.026207 -0.066139 0.608682 -0.362021 0.314323 0.261955 -0.571414 1.738899 -1.013223 0.503853 -0.536511 -0.212048 0.611990 -0.627851 0.297657 -0.187690 -0.565871 -0.234922 -0.845875 -0.767733 0.032470 1.508012 -0.204894 -0.495031 -0.159262 0.181380 0.050582 -0.333469 0.454832 -2.091174 0.448453 0.940212 0.882077 -0.617093 0.616782 -0.993445 -0.385087 0.251711 0.259918 -0.222614 -0.595131 0.661472 0.194740 0.619222 -1.253610 -0.838179 0.781428 -0.396697 -0.530109 0.022801 -0.558296 -0.656034 0.842634 -0.105293 0.586823 -0.603681 -0.605727 -0.556468 0.924275 -0.299228 -1.121538 0.237787 0.498935 -0.045423 0.171536 -1.026385 -0.262225 0.390662 1.263240 0.352172 0.261121 0.915840 1.522183 -0.498536 2.046169 0.012683 -0.073264 -0.361662 0.759529 -0.713268 0.281747 -0.811104 -0.002061 -0.802508 0.520559 0.092275 -0.623098 0.199694 -0.134896 -1.390617 0.911266 -0.114067 1.274048 1.108440 -0.266002 1.066987 0.514556 0.144796 -0.606461 0.197114 0.340205 -0.400785 -0.957690 -0.327456 1.529557 -1.182615 0.431229 -0.084865 0.513266 -0.022768 -0.092925 -0.553804 -2.269741 -0.078390 1.376199 -1.163337</span><br><span class="line">随意 0.410436 0.776917 -0.381131 0.969900 -0.804778 -0.785379 -0.887346 -1.463543 -1.574851 0.313285 0.685253 -0.918359 0.199073 -0.305374 -0.642721 0.098114 -0.723331 0.353159 0.042807 0.369208 -1.534930 -0.084871 0.020417 -0.384782 0.276833 -0.160028 1.107051 0.884343 -0.204381 -0.459738 -0.387128 0.125867 0.093569 1.192471 -0.473752 -0.314541 -1.029249 0.481447 1.358753 -1.688778 -0.113080 -0.401443 -0.958206 0.605638 1.083126 0.131617 0.092507 0.476506 0.801755 1.096883 -0.102036 0.461804 0.820297 -0.104053 -0.126638 0.957708 -0.722038 0.223686 0.583582 0.201246 -1.254708 0.770717 -1.271523 -0.584094 -1.142426 1.066567 0.071951 -0.182649 0.014365 -0.577141 0.037340 -0.166832 -0.247827 0.165994 1.143665 -0.258421 -0.335195 0.170218 -0.212838 0.013709 0.088847 0.663238 -0.597439 0.632847 0.370871 0.652707 0.306935 0.195127 -0.252443 0.588479 0.191633 -1.587564 0.564600 -0.306158 -0.648177 -0.488595 1.532795 -0.462473 -0.643878 1.292369 -0.051494 -1.032738 0.453587 0.411327 -0.469373 0.428398 -0.020839 0.307422 0.518331 -0.860913 -2.170098 -0.277532 -0.966210 0.615336 -0.924783 0.042679 1.289640 1.272992 1.367773 0.426600 -0.187254 -0.781009 1.331301 -0.088357 -1.113550 -0.262879 0.300137 0.437905</span><br><span class="line">..</span><br></pre></td></tr></table></figure>
</li>
<li><p>有了每个词的向量，我们这边采取了借鉴YoutubeNet网络的想法：<br><img src="/2018/09/25/GloVe向量化做文本分类/6.png" alt=""></p>
</li>
</ul>
<p>举个例子：存在一句话”我爱中国”，“我”的向量是[0.3,0.2,0.3]，”爱”的向量是[0.1,0.2,0.3]，“中国”的向量是[0.6,0.6,0.4]，那么average后就是[0.33,0.33,0.33]，然后这就类似一个特征为三的input。</p>
<p>这种方法的好处就是快捷，预处理的工作代价要小，随着数据量的增多，模型的效果要更加的好。</p>
<h1 id="效果对比"><a href="#效果对比" class="headerlink" title="效果对比"></a>效果对比</h1><p>最后这边粗略的给出一下业务数据对比：</p>
<table>
<thead>
<tr>
<th>experiment</th>
<th>date</th>
<th>intercepted_recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>3-grams</td>
<td>20180915</td>
<td>79.3%</td>
</tr>
<tr>
<td>3-grams</td>
<td>20180917</td>
<td>78.7%</td>
</tr>
<tr>
<td>3-grams+bayes+lr</td>
<td>20180915</td>
<td>83.4%</td>
</tr>
<tr>
<td>3-grams+bayes+lr</td>
<td>20180917</td>
<td>88.6%</td>
</tr>
<tr>
<td>gloVe+lr</td>
<td>20180915</td>
<td>93.1%</td>
</tr>
<tr>
<td>gloVe+lr</td>
<td>20180917</td>
<td>93.9%</td>
</tr>
</tbody>
</table>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，相关代码已经上传到我的<a href="https://github.com/sladesha/machine_learning" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Google团队在DNN的实际应用方式的整理]]></title>
      <url>/2018/08/29/Google%E5%9B%A2%E9%98%9F%E5%9C%A8DNN%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E6%96%B9%E5%BC%8F%E7%9A%84%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>很荣幸有机会和论文作者<a href="https://www.linkedin.com/in/mehmet-emre-sargin-0665047/" target="_blank" rel="noopener">Emre Sargin</a>关于之前发的<a href="https://zhuanlan.zhihu.com/p/38638747" target="_blank" rel="noopener">Deep Neural Networks for YouTube Recommendations</a>进行交流，梳理如下：</p>
<p><strong>提问对话汇总：</strong></p>
<ul>
<li>如何进行负采样的？</li>
</ul>
<p>构造了千万量级热门视频集合，每个用户的负采样结果来源于这个集合，会有一些筛选的tricks，比如剔除浏览过的商品，负采样的数量Google在200万条。（也就是说，在计算loss的时候，google的label是一个200万长度的向量，瑟瑟发抖.jpg。）</p>
<ul>
<li>推荐算法应用上，有什么评估方式和评估指标？</li>
</ul>
<p>主要基于线上进行小批量的abtest进行对比，在考虑ctr指标的同时也会综合全站的信息加以分析，同时对新颖程度和用户兴趣变换也是我们考察的对象。</p>
<ul>
<li>冷启动的解决方式？从来没有被点击过的video如何处理？新上的video如何处理？</li>
</ul>
<p>google的推荐基于多种推荐算法的组合，YouTubeNet主要解决的是热门商品的一个推荐问题，冷启动或者没有被点击的video会有其他算法进行计算。换句话说，解决不了。</p>
<ul>
<li>example age如何定义？</li>
</ul>
<p>user+vedio的组合形式，train过程中，是用户点击该vedio的时间距离当前时间的间隔；predict过程中，为0。该部分对模型的鲁棒性非常重要。</p>
<ul>
<li>是否遇到神经元死亡的问题？</li>
</ul>
<p>有，解决方案很常规，都是大家了解的，降低learning_rate，使用batchnormalization。</p>
<ul>
<li>是否预到过拟合？</li>
</ul>
<p>没，youtube的用户上亿，可以构造出上千亿的数据，过拟合的情况不明显。但是会存在未登录用户，我们会通过一些其他CRM类的算法补充构造出他们的基本信息，比如gender、age…</p>
<ul>
<li>vedio vector在哪边进行构造和修正？</li>
</ul>
<p>history click部分进行vedio embedding，并进行修正。另外，50是我们尝试的历史点击长度，20-30也有不错的效果。</p>
<ul>
<li>会有工程计算压力么？</li>
</ul>
<p>不存在，建议在GPU上计算，后面由于VPN网络信号抖动没听清，大概就说Google在训练模型的时候会有大量GPU支持，每天大概更新2-3模型，没有遇到什么计算瓶颈。</p>
<p><em>(以上为我个人针对提问结果的理解及总结)</em></p>
<p><strong>个人感想如下：有钱任性</strong></p>
<p><strong>最后，我觉得算法还是要适应实际情况，大公司的方法可以借鉴但是可能很多时候抄不来，也没条件抄。</strong></p>
<p><em>原问题如下（实际有删改）：<br>How to do video embedding?<br>Is there any pre-training?<br>How to use example age in the model?<br>How to deal dead ReLU neurons？<br>How to sample negative classes？<br>How does the video embedding generated？<br>How to recommend the video never been clicked and new uploaded videos？<br>How to do ab testing? What’s the metrics?<br>Have you facing overfitting？How to solve it?<br>There is any difficulty in calculating the embedding for millions of videos and users.<br>During input embedding generation, are they simply averaged?</em></p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。<br><img src="https://upload-images.jianshu.io/upload_images/1129359-654dc61c581d94e1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 论文解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Stanford Word Segmenter问题整理]]></title>
      <url>/2018/08/27/Segmenter%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>最近在做一些nlp相关的项目，在涉及到Stanford CoreNLP工具包处理中文分词的时候，发现耗时问题很严重：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Item</th>
<th style="text-align:center">time(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">jieba</td>
<td style="text-align:center">0.4</td>
</tr>
<tr>
<td style="text-align:center">snownlp</td>
<td style="text-align:center">7.4</td>
</tr>
<tr>
<td style="text-align:center">pynlpir</td>
<td style="text-align:center">0.8</td>
</tr>
<tr>
<td style="text-align:center"><strong>StanfordCoreNLP</strong></td>
<td style="text-align:center"><strong>21.5</strong></td>
</tr>
<tr>
<td style="text-align:center">pyltp</td>
<td style="text-align:center">5.3</td>
</tr>
</tbody>
</table>
<p>因为Stanford CoreNLP调用的是这个pipeline，而我们实际用的是切词功能，所以尝试只用它的切词部分功能，但是在做的过程中发现一些问题，整理如下：</p>
<p>官网给出的方法<a href="http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.stanford_segmenter" target="_blank" rel="noopener">nltk.tokenize.stanford_segmenter module</a>是这么写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize.stanford_segmenter <span class="keyword">import</span> StanfordSegmenter</span><br><span class="line">seg = StanfordSegmenter()</span><br><span class="line">seg.default_config(<span class="string">'zh'</span>)</span><br></pre></td></tr></table></figure>
<p>但是这个缺少各种数据路径的，是完全不通的。</p>
<p>然后度娘的top1的答案给出的解决方案是：<a href="http://www.52nlp.cn/python自然语言处理实践-在nltk中使用斯坦福中文分词器" target="_blank" rel="noopener"></a>`<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">segmenter = StanfordSegmenter(path_to_jar=<span class="string">"stanford-segmenter-3.4.1.jar"</span>, path_to_sihan_corpora_dict=<span class="string">"./data"</span>, path_to_model=<span class="string">"./data/pku.gz"</span>, path_to_dict=<span class="string">"./data/dict-chris6.ser.gz"</span>)</span><br></pre></td></tr></table></figure></p>
<p>如果你的nltk的版本比较新，恭喜你，你会遇到下面这个问题：<br><code>TypeError: expected str, bytes or os.PathLike object, not NoneType</code></p>
<p>我在<a href="https://stackoverflow.com/questions/45663121/about-stanford-word-segmenter/45668849" target="_blank" rel="noopener">stackoverflow</a>上找了半天，发现有如下的解决方案：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.parse.corenlp <span class="keyword">import</span> CoreNLPParser </span><br><span class="line">corenlp_parser = CoreNLPParser(<span class="string">'http://localhost:9001'</span>, encoding=<span class="string">'utf8'</span>)</span><br><span class="line">result = corenlp_parser.api_call(text, &#123;<span class="string">'annotators'</span>: <span class="string">'tokenize,ssplit'</span>&#125;)</span><br><span class="line">tokens = [token[<span class="string">'originalText'</span>] <span class="keyword">or</span> token[<span class="string">'word'</span>] <span class="keyword">for</span> sentence <span class="keyword">in</span> result[<span class="string">'sentences'</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以完美解决，原因之前作者也说了，据称升级版本后不兼容，各位看看就好<a href="https://github.com/nltk/nltk/issues/1808" target="_blank" rel="noopener">“TypeError: expected str, bytes or os.PathLike object, not NoneType” about Stanford NLP </a>。</p>
<p>这个坑花了我两个多小时（主要在下载各种gz包），希望大家能够避免。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[logistic regression一点理解]]></title>
      <url>/2018/08/14/regression%E4%B8%80%E7%82%B9%E7%90%86%E8%A7%A3/</url>
      <content type="html"><![CDATA[<p>Hexo没有办法用Latex，所以采取了截图的方式，更舒适的阅读体验可以参见<a href="https://www.jianshu.com/p/61ac39a57f9d" target="_blank" rel="noopener">logistic regression一点理解</a>。</p>
<p><img src="/2018/08/14/regression一点理解/1.png" alt=""></p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Tensorflow实现FFM]]></title>
      <url>/2018/08/06/%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0FFM/</url>
      <content type="html"><![CDATA[<p>没错，这次登场的是FFM。各大比赛中的“种子”算法，中国台湾大学Yu-Chin Juan荣誉出品，美团技术团队背书，Michael Jahrer的论文的field概念灵魂升华，土豪公司鉴别神器。通过引入field的概念，FFM把相同性质的特征归于同一个field，相当于把FM中已经细分的feature再次拆分，可不可怕，厉不厉害？好，让我们来看看怎么一个厉害法。</p>
<h1 id="FFM理论"><a href="#FFM理论" class="headerlink" title="FFM理论"></a>FFM理论</h1><h2 id="特征交互"><a href="#特征交互" class="headerlink" title="特征交互"></a>特征交互</h2><p>网上已经说烂了的美团技术团队给出的那张图：<br><img src="/2018/08/06/基于Tensorflow实现FFM/1.png" alt=""><br>针对Country这个变量，<br>FM的做法是one-hot-encoding，生成country_USA，country_China两个稀疏的变量，再进行embedding向量化。<br>FFM的做法是cross-one-hot-encoding，生成country_USA_Day_26/11/15，country_USA_Ad_type_Movie…M个变量，再进行embedding向量化。<br><img src="/2018/08/06/基于Tensorflow实现FFM/2.png" alt=""><br>就和上图一样，fm做出来的latent factor是二维的，就是给每个特征一个embedding结果；而暴力的ffm做出的latent factor是三维的，出来给特征embedding还考虑不同维度特征给不同的embedding结果，也是FFM中“field-aware”的由来。<br><img src="/2018/08/06/基于Tensorflow实现FFM/3.png" alt=""><br>同时从公式中看，对于xi这个特征为什么embedding的latent factor向量的V是Vifj，其实就是因为xi乘以的是xj，所以latent factor向量的信息提取才是field j，也就是fj。多说一句，网上很多给出的现成的代码，这边都是写错了的，都写着写着变成了vifi，可能是写的顺手。</p>
<p>都说到这里了，我再多说一句，为什么说ffm是土豪公司鉴别神器呢？我们看下仅仅是二次项，ffm需要计算的参数有 nfk 个，远多于FM模型的 nk个，而且由于每次计算都依赖于乘以的xj的field，所以，无法用fm的那个计算技巧(ab = 1/2(a+b)^2-a^2-b^2)，所以计算复杂度是 O(kn^2)。这种情况下，没有GPU就不要想了，有GPU的特征多于50个，而且又很离散的，没有个三位数的GPU也算了。之前我看美团说他们在用，我想再去看看他们的实际用的过程的时候，发现文章被删了，真的是可惜，我其实一直也想知道如何减轻这个变态的计算量的方法。</p>
<p>给个实例给大家看下以上的这些的应用：<br>依旧来自美团技术研发团队中给出的案例，有用户数据如下：<br><img src="/2018/08/06/基于Tensorflow实现FFM/4.png" alt=""><br>这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。<br><img src="/2018/08/06/基于Tensorflow实现FFM/5.png" alt=""><br>红色部分对应的是field，来自于原始特征的个数；蓝色部分对应的是feature，来自于原始特征onehot之后的个数。<br>对于特征Feature:<code>User=YuChin</code>而言，有<code>Movie=3Idiots</code>、<code>Genre=Comedy</code>、<code>Genre=Drama</code>、<code>Price</code>四项要交互：<br><img src="/2018/08/06/基于Tensorflow实现FFM/6.png" alt=""><br><code>User=YuChin</code>与<code>Movie=3Idiots</code>交互是<v1,2,v2,1>·1·1，也就是第一项，为什么是V1,2呢？因为<code>User=YuChin</code>是Featureindex=1，而交互的<code>Movie=3Idiots</code>是Fieldindex=2，同理V2,1也是这样的，以此类推，那么，FFM的组合特征有10项，如下图所示：<br><img src="/2018/08/06/基于Tensorflow实现FFM/7.png" alt=""><br>这就是一个案例的实际操作过程。</v1,2,v2,1></p>
<h2 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h2><p>为什么要把这个单拎出来说呢？我看了网上不少的对于特征的处理过程，版本实在是太多了，而且差异化也蛮大，这边就和大家一起梳理一下：<br>1.feature index * feature value<br>这个就是上面我这个实际案例的方式，对于分类变量采取onehot，对于连续变量之间进行值的点积，不做处理。优点是快速简单，不需要预处理，但是缺点也很明显，离群点影响，值的波动大等。</p>
<p>2.连续值离散化<br>这个方法借鉴了Cart里面对连续值的处理方式，就是把所有的连续值都当成一个分类变量处理。举例，现在有一个年龄age的连续变量[10,19,20,22,22,34]，这种方法就生成了age_10,age_19,age_20,age_22,age_34这些变量，如果连续值一多，这个方法带来的计算量就直线上升。</p>
<p>3.分箱下的连续值离散化<br>这种方法优化了第二种方法，举例解释，现在有一个年龄age的连续变量[10,19,20,22,22,34]，我们先建立一个map，[0,10):0,[10,20):1,[20,30):2,[30,100):3。原始的数据就变成了[1,1,2,2,2,3]，再进行2的连续值离散化方法，生成了age_1,age_2,age_3这几个变量，优化了计算量，而且使得结果更具有解释性。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="logisitc-loss"><a href="#logisitc-loss" class="headerlink" title="logisitc loss"></a>logisitc loss</h3><p>这个是官方指定的方法，是-1/1做二分类的时候常用的loss计算方法：<br><img src="/2018/08/06/基于Tensorflow实现FFM/8.png" alt=""><br>这边需要注意的是，在做的时候，需要把label拆分成-1/1而不是0/1，当我们预测正确的时候，pred<em>label&gt;0且越大正确的程度越高，相应的log项是越小的，整体loss越小；相反，如果我们预测的越离谱，pred</em>label&lt;0且越小离谱的程度越高，相应的log项是越大的，整体loss越大。</p>
<h3 id="交互熵"><a href="#交互熵" class="headerlink" title="交互熵"></a>交互熵</h3><p>我看到很多人的实现依旧用了<code>tf.nn.softmax_cross_entropy_with_logits</code>，其实就是多分类中的损失函数，和大家平时的图像分类、商品推荐召回一模一样：<br><img src="/2018/08/06/基于Tensorflow实现FFM/9.png" alt=""><br>这边需要注意的是，在做的时候，需要把label拆分成[1,0]和[0,1]进行计算。不得不说，大家真的是为了省事很机智(丧心病狂)啊！</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>我这边只给一些关键地方的代码，更多的去GitHub里面看吧。</p>
<h2 id="embedding-part"><a href="#embedding-part" class="headerlink" title="embedding part"></a>embedding part</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.v = tf.get_variable(<span class="string">'v'</span>, shape=[self.p, self.f, self.k], dtype=<span class="string">'float32'</span>,initializer=tf.truncated_normal_initializer(mean=<span class="number">0</span>, stddev=<span class="number">0.01</span>))</span><br></pre></td></tr></table></figure>
<p>看到了，这边生成的v就是上面Vffm的形式。</p>
<h2 id="inference-part"><a href="#inference-part" class="headerlink" title="inference part"></a>inference part</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(self.p):</span><br><span class="line"><span class="comment"># 寻找没有match过的特征，也就是论文中的j = i+1开始</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, self.p):</span><br><span class="line">print(<span class="string">'i:%s,j:%s'</span> % (i, j))</span><br><span class="line"><span class="comment"># vifj</span></span><br><span class="line">vifj = self.v[i, self.feature2field[j]]</span><br><span class="line"><span class="comment"># vjfi</span></span><br><span class="line">vjfi = self.v[j, self.feature2field[I]]</span><br><span class="line"><span class="comment"># vi · vj</span></span><br><span class="line">vivj = tf.reduce_sum(tf.multiply(vifj, vjfi))</span><br><span class="line"><span class="comment"># xi · xj</span></span><br><span class="line">xixj = tf.multiply(self.X[:, i], self.X[:, j])</span><br><span class="line">self.field_cross_interaction += tf.multiply(vivj, xixj)</span><br></pre></td></tr></table></figure>
<p>我这边强行拆开了写，这样看起来更清晰一点，注意这边的vifj和vjfi的生成，这边也可以看到找对于的field的方法是用了filed这个字典，这就是为什么不能用fm的点击技巧。</p>
<h2 id="loss-part"><a href="#loss-part" class="headerlink" title="loss part"></a>loss part</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -1/1情况下的logistic loss</span></span><br><span class="line">self.loss = tf.reduce_mean(tf.log(<span class="number">1</span> + tf.exp(-self.y * self.y_out)))</span><br></pre></td></tr></table></figure>
<p>这边记得论文中的负号，如果有batch的情况下记得求个平均再进行bp过程。</p>
<h1 id="论文结论"><a href="#论文结论" class="headerlink" title="论文结论"></a>论文结论</h1><p>原始的ffm论文中给出了一些结论，我们在实际使用中值得参考：</p>
<ul>
<li>k值不用太大，没啥提升<br><img src="/2018/08/06/基于Tensorflow实现FFM/10.png" alt=""></li>
<li>正则项lambda和学习率alpha需要着重调参<br><img src="/2018/08/06/基于Tensorflow实现FFM/11.png" alt=""></li>
<li>epoch别太大，既会拖慢速度，而且造成过拟合；在原论文中甚至要考虑用early-stopping避免过拟合，所以epoch=1，常规的来讲就可以了，论文中提到的early-stopping操作：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> Split the data set into a training set <span class="keyword">and</span> a validation set.</span><br><span class="line"><span class="number">2.</span> At the end of each epoch, use the validation set to calcu-</span><br><span class="line">late the loss.</span><br><span class="line"><span class="number">3.</span> If the loss goes up, record the number of epochs. Stop <span class="keyword">or</span></span><br><span class="line">go to step <span class="number">4.</span></span><br><span class="line"><span class="number">4.</span> If needed, use the full data set to re-train a model <span class="keyword">with</span></span><br><span class="line">the number of epochs obtained <span class="keyword">in</span> step <span class="number">3.</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>FFM是一个细化隐向量非常好的方法，虽然很简单，但还是有很多细节之处值得考虑，比如如何线上应用，如何可解释，如何求稀疏解等等。在部署实现FFM之前，我还是建议大家先上线FM，当效果真的走投无路的时候再考虑FFM，FFM在工业界的影响着实不如学术界那么强大，偷偷说一句，<strong>太慢了，真的是太慢了，慢死了</strong>，我宁可去用deepfm。</p>
<p>最后，给出代码实现的Github地址<a href="https://github.com/sladesha/machine_learning/tree/master/FFM" target="_blank" rel="noopener">FFM</a>，这边是我自己写的，理解理解算法可以，但是实际用的时候建议参考FFM的实现比较好的项目比如libffm，最近比较火的xlearn。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 特征交叉 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Tensorflow实现DeepFM]]></title>
      <url>/2018/07/30/%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0DeepFM/</url>
      <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>DeepFM，Ctr预估中的大杀器，哈工大与华为诺亚方舟实验室荣耀出品，算法工程师面试高频考题，有效的结合了神经网络与因子分解机在特征学习中的优点：同时提取到低阶组合特征与高阶组合特征，这样的称号我可以写几十条出来，这也说明了DeepFM确实是一个非常值得手动撸一边的算法。</p>
<p>当然，早就有一票人写了一车封装好的deepFM的模型，大家随便搜搜肯定也能搜到，既然这样，我就不再搞这些东西了，今天主要和大家过一遍，deepFM的代码是咋写的，手把手入门一下，说一些我觉得比较重要的地方，方便大家按需修改。（只列举了一部分，更多的解释参见GitHub代码中的注释）</p>
<p>本文的数据和部分代码构造参考了nzc大神的<a href="https://github.com/sladesha/dnn_ctr/blob/master/model/DeepFM.py" target="_blank" rel="noopener">deepfm</a>的Pytorch版本的写法，改成tensorflow的形式，需要看原版的自取。</p>
<h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="/2018/07/30/基于Tensorflow实现DeepFM/1.png" alt=""></p>
<p>DeepFM包含两部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取，两部分权重共享。<br>DeepFM的预测结果可以写为：<code>y = sigmoid(y(fm)+y(DNN))</code></p>
<h2 id="FM部分"><a href="#FM部分" class="headerlink" title="FM部分"></a>FM部分</h2><p><img src="/2018/07/30/基于Tensorflow实现DeepFM/2.png" alt=""></p>
<p>FM公式为：<br><img src="/2018/07/30/基于Tensorflow实现DeepFM/3.png" alt=""></p>
<p>我很久之前一篇文章细节讲过，这边就不多扯了，更多详见<a href="http://shataowei.com/2017/12/04/FM理论解析及应用/" target="_blank" rel="noopener">FM理论解析及应用</a>。</p>
<h2 id="DNN部分"><a href="#DNN部分" class="headerlink" title="DNN部分"></a>DNN部分</h2><p><img src="/2018/07/30/基于Tensorflow实现DeepFM/4.png" alt=""></p>
<p>这边其实和我上篇文章说的MLPS差距不大，也就是简单的全链接，差就差在input的构造，这边采取了embedding的思想，将每个feature转化成了embedded vector作为input，同时此处的input也是上面计算FM中的V，更多的大家看代码就完全了解了。</p>
<h1 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h1><p>我一共写了两个script，build_data.py和deepfm.py，也很好理解。build_data.py用来预处理数据，deepfm.py用来跑模型。</p>
<h2 id="build-data-py"><a href="#build-data-py" class="headerlink" title="build_data.py"></a>build_data.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, data.shape[<span class="number">1</span>]):</span><br><span class="line">target = data.iloc[:, I]</span><br><span class="line">col = target.name</span><br><span class="line">l = len(set(target))</span><br><span class="line"><span class="keyword">if</span> l &gt; <span class="number">10</span>:</span><br><span class="line">target = (target - target.mean()) / target.std()</span><br><span class="line">co_feature = pd.concat([co_feature, target], axis=<span class="number">1</span>)</span><br><span class="line">feat_dict[col] = cnt</span><br><span class="line">cnt += <span class="number">1</span></span><br><span class="line">co_col.append(col)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">us = target.unique()</span><br><span class="line">print(us)</span><br><span class="line">feat_dict[col] = dict(zip(us, range(cnt, len(us) + cnt)))</span><br><span class="line">ca_feature = pd.concat([ca_feature, target], axis=<span class="number">1</span>)</span><br><span class="line">cnt += len(us)</span><br><span class="line">ca_col.append(col)</span><br><span class="line">feat_dim = cnt</span><br><span class="line">feature_value = pd.concat([co_feature, ca_feature], axis=<span class="number">1</span>)</span><br><span class="line">feature_index = feature_value.copy()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> feature_index.columns:</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">in</span> co_col:</span><br><span class="line">feature_index[i] = feat_dict[I]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">feature_index[i] = feature_index[i].map(feat_dict[I])</span><br><span class="line">feature_value[i] = <span class="number">1.</span></span><br></pre></td></tr></table></figure>
<p>核心部分如上，重要的是做了两件事情，生成了feature_index和feature_value。</p>
<p>feature_index是把所有特征进行了标序，feature1，feature2……featurem，分别对应0，1，2，3，…m，但是，请注意<strong>分类变量需要拆分</strong>！就是说如果有性别：男|女|未知，三个选项。需要构造feature男，feature女，feature未知三个变量，而连续变量就不需要这样。</p>
<p>feature_value就是特征的值，连续变量按真实值填写，<strong>分类变量</strong>全部填写1。</p>
<p>更加形象的如下：<br><img src="/2018/07/30/基于Tensorflow实现DeepFM/5.png" alt=""></p>
<h2 id="deepfm-py"><a href="#deepfm-py" class="headerlink" title="deepfm.py"></a>deepfm.py</h2><h3 id="特征向量化"><a href="#特征向量化" class="headerlink" title="特征向量化"></a>特征向量化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征向量化，类似原论文中的v</span></span><br><span class="line">self.weight[<span class="string">'feature_weight'</span>] = tf.Variable(</span><br><span class="line">tf.random_normal([self.feature_sizes, self.embedding_size], <span class="number">0.0</span>, <span class="number">0.01</span>),</span><br><span class="line">name=<span class="string">'feature_weight'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次项中的w系数，类似原论文中的w</span></span><br><span class="line">self.weight[<span class="string">'feature_first'</span>] = tf.Variable(</span><br><span class="line">tf.random_normal([self.feature_sizes, <span class="number">1</span>], <span class="number">0.0</span>, <span class="number">1.0</span>),</span><br><span class="line">name=<span class="string">'feature_first'</span>)</span><br></pre></td></tr></table></figure>
<p>可以对照下面的公式看，更有感觉。<br><img src="/2018/07/30/基于Tensorflow实现DeepFM/6.png" alt=""></p>
<h3 id="deep网络部分的weight"><a href="#deep网络部分的weight" class="headerlink" title="deep网络部分的weight"></a>deep网络部分的weight</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># deep网络初始input：把向量化后的特征进行拼接后带入模型，n个特征*embedding的长度</span></span><br><span class="line">input_size = self.field_size * self.embedding_size</span><br><span class="line">init_method = np.sqrt(<span class="number">2.0</span> / (input_size + self.deep_layers[<span class="number">0</span>]))</span><br><span class="line">self.weight[<span class="string">'layer_0'</span>] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(input_size, self.deep_layers[<span class="number">0</span>])), dtype=np.float32</span><br><span class="line">)</span><br><span class="line">self.weight[<span class="string">'bias_0'</span>] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(<span class="number">1</span>, self.deep_layers[<span class="number">0</span>])), dtype=np.float32</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 生成deep network里面每层的weight 和 bias</span></span><br><span class="line"><span class="keyword">if</span> num_layer != <span class="number">1</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_layer):</span><br><span class="line">init_method = np.sqrt(<span class="number">2.0</span> / (self.deep_layers[i - <span class="number">1</span>] + self.deep_layers[I]))</span><br><span class="line">self.weight[<span class="string">'layer_'</span> + str(i)] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(self.deep_layers[i - <span class="number">1</span>], self.deep_layers[i])),</span><br><span class="line">dtype=np.float32)</span><br><span class="line">self.weight[<span class="string">'bias_'</span> + str(i)] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(<span class="number">1</span>, self.deep_layers[i])),</span><br><span class="line">dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># deep部分output_size + 一次项output_size + 二次项output_size</span></span><br><span class="line">last_layer_size = self.deep_layers[<span class="number">-1</span>] + self.field_size + self.embedding_size</span><br><span class="line">init_method = np.sqrt(np.sqrt(<span class="number">2.0</span> / (last_layer_size + <span class="number">1</span>)))</span><br><span class="line"><span class="comment"># 生成最后一层的结果</span></span><br><span class="line">self.weight[<span class="string">'last_layer'</span>] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(last_layer_size, <span class="number">1</span>)), dtype=np.float32)</span><br><span class="line">self.weight[<span class="string">'last_bias'</span>] = tf.Variable(tf.constant(<span class="number">0.01</span>), dtype=np.float32)</span><br></pre></td></tr></table></figure>
<p>input的地方需要注意一下，这边用了个技巧，直接把把向量化后的特征进行拉伸拼接后带入模型，原来的v是<code>batch*n个特征*embedding的长度</code>，直接改成了<code>batch*（n个特征*embedding的长度）</code>，这样的好处就是全值共享，又快又有效。</p>
<h3 id="网络传递部分"><a href="#网络传递部分" class="headerlink" title="网络传递部分"></a>网络传递部分</h3><p>都是一些正常的操作，稍微要注意一下的是交互项的计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># second_order</span></span><br><span class="line">self.sum_second_order = tf.reduce_sum(self.embedding_part, <span class="number">1</span>)</span><br><span class="line">self.sum_second_order_square = tf.square(self.sum_second_order)</span><br><span class="line">print(<span class="string">'sum_square_second_order:'</span>, self.sum_second_order_square)</span><br><span class="line"><span class="comment"># sum_square_second_order: Tensor("Square:0", shape=(?, 256), dtype=float32)</span></span><br><span class="line"></span><br><span class="line">self.square_second_order = tf.square(self.embedding_part)</span><br><span class="line">self.square_second_order_sum = tf.reduce_sum(self.square_second_order, <span class="number">1</span>)</span><br><span class="line">print(<span class="string">'square_sum_second_order:'</span>, self.square_second_order_sum)</span><br><span class="line"><span class="comment"># square_sum_second_order: Tensor("Sum_2:0", shape=(?, 256), dtype=float32)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1/2*((a+b)^2 - a^2 - b^2)=ab</span></span><br><span class="line">self.second_order = <span class="number">0.5</span> * tf.subtract(self.sum_second_order_square, self.square_second_order_sum)</span><br><span class="line"></span><br><span class="line">self.fm_part = tf.concat([self.first_order, self.second_order], axis=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'fm_part:'</span>, self.fm_part)</span><br></pre></td></tr></table></figure></p>
<p>直接实现了下面的计算逻辑：<br><img src="/2018/07/30/基于Tensorflow实现DeepFM/7.png" alt=""></p>
<h3 id="loss部分"><a href="#loss部分" class="headerlink" title="loss部分"></a>loss部分</h3><p>我个人重写了一下我认为需要正则的地方，和一些loss的计算方式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># loss</span></span><br><span class="line">self.out = tf.nn.sigmoid(self.out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss = tf.losses.log_loss(label,out) 也行，看大家想不想自己了解一下loss的计算过程</span></span><br><span class="line">self.loss = -tf.reduce_mean(</span><br><span class="line">self.label * tf.log(self.out + <span class="number">1e-24</span>) + (<span class="number">1</span> - self.label) * tf.log(<span class="number">1</span> - self.out + <span class="number">1e-24</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则：sum(w^2)/2*l2_reg_rate</span></span><br><span class="line"><span class="comment"># 这边只加了weight，有需要的可以加上bias部分</span></span><br><span class="line">self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg_rate)(self.weight[<span class="string">"last_layer"</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.deep_layers)):</span><br><span class="line">self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg_rate)(self.weight[<span class="string">"layer_%d"</span> % I])</span><br></pre></td></tr></table></figure></p>
<p>大家也可以直接按照我注释掉的部分简单操作，看个人的理解了。</p>
<h3 id="梯度正则"><a href="#梯度正则" class="headerlink" title="梯度正则"></a>梯度正则</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line">opt = tf.train.GradientDescentOptimizer(self.learning_rate)</span><br><span class="line">trainable_params = tf.trainable_variables()</span><br><span class="line">print(trainable_params)</span><br><span class="line">gradients = tf.gradients(self.loss, trainable_params)</span><br><span class="line">clip_gradients, _ = tf.clip_by_global_norm(gradients, <span class="number">5</span>)</span><br><span class="line">self.train_op = opt.apply_gradients(</span><br><span class="line">zip(clip_gradients, trainable_params), global_step=self.global_step)</span><br></pre></td></tr></table></figure>
<p>很多网上的代码跑着跑着就NAN了，建议加一下梯度的正则，反正也没多复杂。</p>
<h3 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a>执行结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">/Users/slade/anaconda3/bin/python /Users/slade/Documents/Personalcode/machine-learning/Python/deepfm/deepfm.py</span><br><span class="line">[<span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">3</span> <span class="number">4</span> <span class="number">6</span> <span class="number">5</span> <span class="number">7</span>]</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line">[<span class="number">6</span> <span class="number">0</span> <span class="number">8</span> <span class="number">2</span> <span class="number">4</span> <span class="number">1</span> <span class="number">7</span> <span class="number">3</span> <span class="number">5</span> <span class="number">9</span>]</span><br><span class="line">[<span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use SSE4<span class="number">.2</span> instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use AVX2 instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">embedding_part: Tensor("Mul:0", shape=(?, 39, 256), dtype=float32)</span></span><br><span class="line"><span class="string">first_order: Tensor("Sum:0", shape=(?, 39), dtype=float32)</span></span><br><span class="line"><span class="string">sum_square_second_order: Tensor("Square:0", shape=(?, 256), dtype=float32)</span></span><br><span class="line"><span class="string">square_sum_second_order: Tensor("Sum_2:0", shape=(?, 256), dtype=float32)</span></span><br><span class="line"><span class="string">fm_part: Tensor("concat:0", shape=(?, 295), dtype=float32)</span></span><br><span class="line"><span class="string">deep_embedding: Tensor("Reshape_2:0", shape=(?, 9984), dtype=float32)</span></span><br><span class="line"><span class="string">output: Tensor("Add_3:0", shape=(?, 1), dtype=float32)</span></span><br><span class="line"><span class="string">[&lt;tensorflow.python.ops.variables.Variable object at 0x10e2a9ba8&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112885ef0&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3c18&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3da0&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3f28&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3c50&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112a03dd8&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112a03b38&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x16eae5c88&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112b937b8&gt;]</span></span><br><span class="line"><span class="string">time all:7156</span></span><br><span class="line"><span class="string">epoch 0:</span></span><br><span class="line"><span class="string">the times of training is 0, and the loss is 8.54514</span></span><br><span class="line"><span class="string">the times of training is 100, and the loss is 1.60875</span></span><br><span class="line"><span class="string">the times of training is 200, and the loss is 0.681524</span></span><br><span class="line"><span class="string">the times of training is 300, and the loss is 0.617403</span></span><br><span class="line"><span class="string">the times of training is 400, and the loss is 0.431383</span></span><br><span class="line"><span class="string">the times of training is 500, and the loss is 0.531491</span></span><br><span class="line"><span class="string">the times of training is 600, and the loss is 0.558392</span></span><br><span class="line"><span class="string">the times of training is 800, and the loss is 0.51909</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>
<p>看了下没啥大问题。</p>
<h1 id="还有一些要说的"><a href="#还有一些要说的" class="headerlink" title="还有一些要说的"></a>还有一些要说的</h1><ul>
<li>build_data.py中我为了省事，只做了标准化，没有进行其他数据预处理的步骤，这个是错误的，大家在实际使用中请按照我在公众号里面给大家进行的数据预处理步骤进行，<strong>这个非常重要</strong>！</li>
<li>learing_rate是我随便设置的，在实际大家跑模型的时候，请务必按照1.0，1e-3，1e-6，三个节点进行二分调优。</li>
<li>如果你直接搬上面代码，妥妥过拟合，请在真实使用过程中，务必根据数据量调整batch的大小，epoch的大小，建议在每次传递完成后加上<code>tf.nn.dropout</code>进行dropout。</li>
<li>如果数据量连10万量级都不到，我还是建议用机器学习的方法，xgboost+lr，mixed logistics regression等等都是不错的方法。</li>
</ul>
<p>好了，最后附上全量代码的地址<a href="https://github.com/sladesha/deep_learning" target="_blank" rel="noopener">Github</a>，希望对大家有所帮助。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 特征交叉 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Tensorflow实现多层感知机网络MLPs]]></title>
      <url>/2018/07/25/%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%BD%91%E7%BB%9CMLPs/</url>
      <content type="html"><![CDATA[<p>之前在<a href="https://www.jianshu.com/p/f26e232bff57" target="_blank" rel="noopener">基于Tensorflow的神经网络解决用户流失概率问题</a>写了一个MLPs的网络，很多人在问，其实这个网络看起来很清晰，但是却写的比较冗长，这边优化了一个版本更方便大家修改后直接使用。</p>
<p><img src="/2018/07/25/基于Tensorflow实现多层感知机网络MLPs/1.png" alt="多层感知机网络"></p>
<p>直接和大家过一遍核心部分：</p>
<p>上次我们计算过程中，通过的是先定义好多层网络中每层的weight，在通过<code>tf.matual</code>进行层与层之间的计算，最后再通过tf.contrib.layers.l2_regularizer进行正则；而这次我们直接通过图像识别中经常使用的全连接（FC）的接口，只需要确定每层的节点数，通过<code>layers_nodes</code>进行声明，自动可以计算出不同层下的weight，更加清晰明了。另外，还增加了dropout的部分，降低过拟合的问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">din_all = tf.layers.batch_normalization(inputs=din_all, name=<span class="string">'b1'</span>)</span><br><span class="line">layer_1 = tf.layers.dense(din_all, self.layers_nodes[<span class="number">0</span>], activation=tf.nn.sigmoid,use_bias=<span class="keyword">True</span>,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name=<span class="string">'f1'</span>)</span><br><span class="line">layer_1 = tf.nn.dropout(layer_1, keep_prob=self.drop_rate[<span class="number">0</span>])</span><br><span class="line">layer_2 = tf.layers.dense(layer_1, self.layers_nodes[<span class="number">1</span>], activation=tf.nn.sigmoid,use_bias=<span class="keyword">True</span>,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name=<span class="string">'f2'</span>)</span><br><span class="line">layer_2 = tf.nn.dropout(layer_2, keep_prob=self.drop_rate[<span class="number">1</span>])</span><br><span class="line">layer_3 = tf.layers.dense(layer_2, self.layers_nodes[<span class="number">2</span>], activation=tf.nn.sigmoid,use_bias=<span class="keyword">True</span>,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name=<span class="string">'f3'</span>)</span><br></pre></td></tr></table></figure>
<p><code>tf.layers.dense</code>接口信息如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.dense(</span><br><span class="line">inputs,</span><br><span class="line">units,</span><br><span class="line">activation=<span class="keyword">None</span>,</span><br><span class="line">use_bias=<span class="keyword">True</span>,</span><br><span class="line">kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">bias_initializer=tf.zeros_initializer(),</span><br><span class="line">kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">trainable=<span class="keyword">True</span>,</span><br><span class="line">name=<span class="keyword">None</span>,</span><br><span class="line">reuse=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<ul>
<li>inputs：必需，即需要进行操作的输入数据。</li>
<li>units：必须，即神经元的数量。</li>
<li>activation：可选，默认为 None，如果为 None 则是线性激活。</li>
<li>use_bias：可选，默认为 True，是否使用偏置。</li>
<li>kernel_initializer：可选，默认为 None，即权重的初始化方法，如果为 None，则使用默认的 Xavier 初始化方法。</li>
<li>bias_initializer：可选，默认为零值初始化，即偏置的初始化方法。</li>
<li>kernel_regularizer：可选，默认为 None，施加在权重上的正则项。</li>
<li>bias_regularizer：可选，默认为 None，施加在偏置上的正则项。</li>
<li>activity_regularizer：可选，默认为 None，施加在输出上的正则项。</li>
<li>kernel_constraint，可选，默认为 None，施加在权重上的约束项。</li>
<li>bias_constraint，可选，默认为 None，施加在偏置上的约束项。</li>
<li>trainable：可选，默认为 True，布尔类型，如果为 True，则将变量添加到 GraphKeys.TRAINABLE_VARIABLES 中。</li>
<li>name：可选，默认为 None，卷积层的名称。</li>
<li>reuse：可选，默认为 None，布尔类型，如果为 True，那么如果 name 相同时，会重复利用。</li>
</ul>
<p>除此之外，之前我们定义y和y_的时候把1转化为[1,0]，转化为了[0,1]，增加了工程量，这次我们通过：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy_mean = -tf.reduce_mean(self.y_ * tf.log(self.output + <span class="number">1e-24</span>))</span><br><span class="line">self.loss = cross_entropy_mean</span><br></pre></td></tr></table></figure>
<p>直接进行计算，避免了一些无用功。</p>
<p>最后，之前对于梯度的值没有进行限制，会导致整体模型的波动过大，这次优化中也做了修改，如果大家需要也可以参考一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们用learning_rate_base作为速率η，来训练梯度下降的loss函数解，对梯度进行限制后计算loss</span></span><br><span class="line">opt = tf.train.GradientDescentOptimizer(self.learning_rate_base)</span><br><span class="line">trainable_params = tf.trainable_variables()</span><br><span class="line">gradients = tf.gradients(self.loss, trainable_params)</span><br><span class="line">clip_gradients, _ = tf.clip_by_global_norm(gradients, <span class="number">5</span>)</span><br><span class="line">self.train_op = opt.apply_gradients(zip(clip_gradients, trainable_params), global_step=self.global_step)</span><br></pre></td></tr></table></figure></p>
<p>MLPs是入门级别的神经网络算法，实际的工业开发中使用的频率也不高，<strong>后面我准备和大家过一下常见的FM、FFM、DeepFM、NFM、DIN、MLR等在工业开发中更为常见的网络，欢迎大家持续关注。</strong></p>
<p>完整代码已经上传到Github中。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。<br><img src="https://upload-images.jianshu.io/upload_images/1129359-654dc61c581d94e1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[伪标签半监督学习]]></title>
      <url>/2018/07/24/%E4%BC%AA%E6%A0%87%E7%AD%BE%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
      <content type="html"><![CDATA[<p>之前在训练YoutubeNet和DCN的时候，我都发现平台用户中基础用户的信息数据缺失率特别高，比如性别一栏准确填写的不足60%，所以我一直想调研一下有没有什么更好的填充方法，要保证既不能太复杂太耗时，也要有足够好的效果。</p>
<p>其实这个问题就是一个缺失值填充，之前的文章中也写过很多办法，常规的也总结过：</p>
<ul>
<li>均值、众数填充<br>最简单的填充，效果也惨不忍睹</li>
<li>根据没有缺失的数据线性回归填充<br>填充的好会造成共线性错误，填充的不好就没价值，很矛盾</li>
<li>剔除<br>丢失信息量</li>
<li>设置哑变量<br>会造成数据分布有偏</li>
<li>smote<br>连续值有效，离散值就无法实施了</li>
</ul>
<p>我在Google上看imbalance问题的时候，偶然看到了这个<a href="http://course.fast.ai/?spm=a2c4e.11153940.blogcont215222.9.378a3aadIvrayn" target="_blank" rel="noopener">视频教程</a>，上面讲了图像的缺失处理，提到了伪标签处理的半监督学习方式。我就在国内的论坛上找了下，阿里云技术论坛也同样注意到了这个问题，但是只给出了如下的粗糙的构思图：<br><img src="/2018/07/24/伪标签半监督学习/1.png" alt=""></p>
<p>有一份整理了的流程图，具体执行步骤总结，和大家一起看一下：<br><img src="/2018/07/24/伪标签半监督学习/2.png" alt=""></p>
<ul>
<li>将有标签部分数据分为两份：train_set&amp;validation_set，并训练出最优的model1</li>
<li>用model1对未知标签数据(test_set)进行预测，给出伪标签结果pseudo-labeled</li>
<li>将train_set中抽取一部分做新的validation_set，把剩余部分与pseudo-labeled部分融合作为新的train_set，训练出最优的model2</li>
<li>再用model2对未知标签数据(test_set)进行预测，得到最终的final result label</li>
</ul>
<p>我利用了已知标签的数据对这个方法进行测试，用了最简单的mixed logistic regression模型作为Basic Model，得到结果如下：<br><img src="/2018/07/24/伪标签半监督学习/3.png" alt=""><br>利用伪标签半监督的方式，同样的mixed logistic regression模型AUC值会提高0.1pp左右，效果还不错，而且实施并不复杂，大家可以在缺失值处理或者分类问题中应用尝试一下。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 特征刻画 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据处理 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[热传导-物质扩散算法应用于推荐]]></title>
      <url>/2018/07/19/%E7%83%AD%E4%BC%A0%E5%AF%BC-%E7%89%A9%E8%B4%A8%E6%89%A9%E6%95%A3%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E4%BA%8E%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<p>没有大量的数据，没有大量的人力就不能做好推荐么？当然不是，热传导/物质扩散推荐算法就是作为冷启动及小规模团队非常实用的推荐召回部分的算法。</p>
<p>目标是为a图中标有星号（不妨记为用户1）的用户推荐商品，该用户已经购买过的两件商品是我们可以利用的信息，用来给目标用户进行推荐。</p>
<p>物质扩散算法：<br><img src="/2018/07/19/热传导-物质扩散算法应用于推荐/1.png" alt=""><br>初始，我们认为每件被目标用户购买过的商品的信息量为1。<br>商品把自己的信息平均分给所有购买过它的用户，用户的信息值则是从所有商品所得到的信息值得总和，比如上图(b)中的第一个节点的信息就等于第一个商品平均分给三个用户的的平均信息1/3，再加上第四个商品平均分给两个用户的平均信息1/2，即为1/3+1/2=5/6；接下来，每一个用户再把自己的信息平均分给所有购买过的商品，商品的信息则是从所有用户收到的信息值得总和，如对于图(c)中的第一个商品，它的信息值就等于第一个用户信息值的一半，为5/12，加上第二个用户信息值的1/4，为5/24，再加上第三个用户信息值得一半，为1/6，总的能量值即为:5/12+5/24+1/6=19/24。</p>
<p>以上两个步骤加起来为从商品到商品信息扩散一步。针对大规模系统的推荐，为了保持实时性和效率，往往只需扩散三步以内。如果以一步为界，基于图(c)中的结果，则在目标用户没有购买过的所有商品中，第三个商品的信息值最大，因此基于物质扩散算法的推荐系统则会将此商品推荐给目标用户，同时可以得到对于用户1的商品得分排序，自然可以得到用户召回集。值得注意的是物质扩散这种算法得到的所有商品最后的信息值之和就等于初始时所有商品的信息值，即能量是守恒的，图(c)中所有商品的信息之和仍为2。</p>
<p>热传导算法：<br><img src="/2018/07/19/热传导-物质扩散算法应用于推荐/2.png" alt=""></p>
<p>初始，我们认为目标用户购买过的每件商品的信息量为1。<br>目标用户的信息等于所有他购买过的商品信息的平均值，如图(d)所示，目标用户购买了商品1和商品4，则该用户的信息值即为(1 + 1) / 2 = 1。再根据目标用户浏览过的商品给所以商品计算信息，第一个商品、第四个商品信息量为1/2，其他商品的信息量为0（因为目标用户没有买过），接下来根据每一个商品的信息计算其他的用户的信息，如图(d)中的第二个用户的信息就为商品1,2,3,4的信息的平均值（1/2 + 1/2）/4 = 1/2；再根据每个用户的信息量平均分配信息到每个商品，如图(e)中的第一个商品来自第一个、第二个、第三个用户的信息的和，即为1/2<em>1/2+1/2</em>1/3+1/2*/12=2/3。</p>
<p>以上两个步骤加起来为从商品到商品热传导一步。因此基于热传导算法的推荐系统则会将此信息量大的商品推荐给目标用户，同时可以得到对于用户1的商品得分排序，自然可以得到用户召回集。与物质扩散不同的是这种算法得到的所有商品最后的信息值之和就不一定等于初始时所有商品的信息值，即不满足守恒定律，这是因为在信息传到的第二步过程中，有的用户的信息可能会被多次计算，从而导致不守恒。</p>
<p>基于物质扩散和基于热传导的推荐算法的区别在于： 基于物质扩散的方法在进行个性化推荐时，系统的总信息是守恒的；而热传导在推荐过程中，目标用户（即被推荐用户）的收藏品将被视作信息初始点，负责提供能量，所以系统的总信息量随着传递步骤的增加是在不断增加的。</p>
<p>如果对物理比较熟悉的朋友很容易联想到凸透镜和凹透镜，是的，我个人在理解的时候也是这样迁移理解，原理上确实一致。 基于物质扩散的方法相当于凸透镜一样把用户历史点击的信息聚焦到了少量优势的skn上了； 基于热传导的方法相当于是凹透镜一样把用户的历史点击信息发散到了那些较不流行的物品上，从而提高了推荐的新颖多样性。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[量化评估推荐系统效果]]></title>
      <url>/2018/07/19/%E9%87%8F%E5%8C%96%E8%AF%84%E4%BC%B0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%95%88%E6%9E%9C/</url>
      <content type="html"><![CDATA[<p>推荐系统最有效的方法就是A/B test进行模型之间的对比，但是由于现实原因的局限，存在现实实时的困难性，所以，梳理了一些可以补充替代的指标如下，但是离线评估也存在相应的问题：<br>1) 数据集的稀疏性限制了适用范围，用户之间的交集稀疏。<br>2) 评价结果的客观性，由于用户的主观性，不管离线评测的结果如何，都不能得出用户是否喜欢某推荐系统的结论，只是一个近似的评估。<br>3) 深度评估指标的缺失。(如点击深度、购买客单价、购买商品类别、购买偏好)之间的关联关系。<br>4）冷启动<br>5）Exploration 和 Exploitation问题</p>
<h1 id="离线模型之间的评估"><a href="#离线模型之间的评估" class="headerlink" title="离线模型之间的评估"></a>离线模型之间的评估</h1><h2 id="召回集测试"><a href="#召回集测试" class="headerlink" title="召回集测试"></a>召回集测试</h2><ul>
<li><p>recall<br>命中skn个数/用户真实点击skn个数</p>
</li>
<li><p>precision<br>命中skn个数/所有预测出来的skn总数</p>
</li>
<li><p>F1-Measure<br>2/(1/recall+1/precison)</p>
</li>
<li><p>交互熵 </p>
</li>
<li>MAE</li>
<li>RMSE</li>
<li>相关性<br>常见的比如：Pearson、Spearman和Kendall’s Tau相关，其中Pearson是更具数值之间的相似度，Spearman是根据数值排序之间的相似度，Kendall’s Tau是加权下的数值排序之间的相似度。</li>
<li>基尼系数</li>
<li>信息熵</li>
</ul>
<h2 id="排序部分测试"><a href="#排序部分测试" class="headerlink" title="排序部分测试"></a>排序部分测试</h2><ul>
<li>NDCG（Normalize DCG）</li>
<li>RBP（rank-biased precision）</li>
</ul>
<p>RBP和NDCG指标的唯一不同点在于RBP把推荐列表中商品的浏览概率p按等比数列递减，而ND CG则是按照log调和级数形式。</p>
<h1 id="离线模型与在线模型之间的评估"><a href="#离线模型与在线模型之间的评估" class="headerlink" title="离线模型与在线模型之间的评估"></a>离线模型与在线模型之间的评估</h1><p>很多时候，我们需要确定离线模型的效果足够的健壮才能允许上线进行线上测试，那如何进行离线模型与线上模型的评估对比就是一个比较复杂的问题。</p>
<h2 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h2><ul>
<li>缺乏公平的测试数据<br>实际处理过程中，我们发现，所有的已知点击都是来自线上模型推荐的结果，所以极端情况下，线上的recall是100%</li>
<li>缺乏公认的衡量指标<br>在线下对比中，我们发现比如recall、precision、F1-Measure等指标都是大家约定俗成的，不存在很大的争议，而离线在线模型对比却没有一个准确公认的衡量指标</li>
</ul>
<h2 id="指标设计"><a href="#指标设计" class="headerlink" title="指标设计"></a>指标设计</h2><ul>
<li>online_offline_cover_rate&amp;first_click_hit_rate</li>
</ul>
<p>这一组指标是结合在一起看的，其中online_offline_cover_rate是指针对每一个用户计算理线模型推荐的商品与在线模型推荐的商品的重合个数/在线模型的推荐商品个数，online_offline_cover_rate越低代表离线模型相对在线模型越独立；first_click_hit_rate是指offline模型对用户每天第一次点击的命中率，也就是命中次数/总统计用户数。<br>结合这两个指标，我们可以得到在online_offline_cover_rate越低的情况下，却能覆盖线上用户真实点击的次数越多，代表offline模型的效果优于线上模型。</p>
<ul>
<li>online_precision_rate/offline_precision_rate</li>
</ul>
<p>离线模型的准确率和在线模型的准确率。<br>这边在实际计算的时候采取了一个技巧，针对某个推荐位计算在线模型准确率的时候，用的是从来没有浏览过这个推荐位的用户的浏览历史匹配这个用户这个推荐位的推荐结果。这样可以避免用户的点击结果受到推荐位推荐结果影响的问题。</p>
<p>举个例子：用户在推荐位A上没有浏览过，他的点击是不受推荐位A推荐的商品影响的，拿这个用户推荐位A我们给他线上推荐的结果作为线上模型的推荐结果去计算，这样才更加合理。</p>
<ul>
<li>online_recall_rate/offline_recall_rate</li>
</ul>
<p>离线模型的召回率和在线模型的召回率。<br>同上解释。</p>
<ul>
<li>roi_reall/roi_precision</li>
</ul>
<p>同上解释，只是把未来的点击作为match源更换成了加购物车、购买、收藏这些数据。</p>
<h1 id="其他评估方向"><a href="#其他评估方向" class="headerlink" title="其他评估方向"></a>其他评估方向</h1><h2 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h2><p>推荐覆盖率越高， 系统给用户推荐的商品种类就越多 ，推荐多样新颖的可能性就越大。如果一个推荐算法总是推荐给用户流行的商品，那么它的覆盖率往往很低，通常也是多样性和新颖性都很低的推荐。</p>
<h2 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h2><p>采用推荐列表间的相似度（hamming distance、Cosine Method），也就是用户的推荐列表间的重叠度来定义整体多样性。</p>
<h2 id="新颖性"><a href="#新颖性" class="headerlink" title="新颖性"></a>新颖性</h2><p>计算推荐列表中物品的平均流行度。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>用户满意度、用户问卷、信任度、鲁棒性、实时性、</p>
<h1 id="评测维度"><a href="#评测维度" class="headerlink" title="评测维度"></a>评测维度</h1><p>最后说一下评测维度分为如下3种，多角度评测：</p>
<ul>
<li>用户维度<br>主要包括用户的人口统计学信息、活跃度以及是不是新用户等。</li>
<li>物品维度<br>包括物品的属性信息、流行度、平均分以及是不是新加入的物品等。</li>
<li>时间维度<br>包括季节，是工作日还是周末，是白天还是晚上等。</li>
</ul>
<p>附常规评价指标的整理结果(来自论文Evaluation Metrics for Recommender Systems)：<br><img src="/2018/07/19/量化评估推荐系统效果/1.png" alt=""></p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐评估方法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow在GPU下的Poolallocator Message]]></title>
      <url>/2018/06/28/Tensorflow%E5%9C%A8GPU%E4%B8%8B%E7%9A%84Poolallocator%20Message/</url>
      <content type="html"><![CDATA[<p><img src="/2018/06/28/Tensorflow在GPU下的Poolallocator Message/1.jpg" alt=""><br><a id="more"></a></p>
<p>我在在用GPU跑我一个深度模型的时候，发生了以下的问题：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">2018-06-27 18:09:11.701458: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 63521 get requests, put_count=63521 evicted_count=1000 eviction_rate=0.0157428 and unsatisfied allocation rate=0.0173171</span><br><span class="line">2018-06-27 18:09:11.701503: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110</span><br><span class="line">Global_step 2000        Train_loss: 0.0758</span><br><span class="line">Global_step 3000        Train_loss: 0.0618</span><br><span class="line">Global_step 4000        Train_loss: 0.0564</span><br><span class="line">Global_step 5000        Train_loss: 0.0521</span><br><span class="line">Global_step 6000        Train_loss: 0.0492</span><br><span class="line">Global_step 7000        Train_loss: 0.0468</span><br><span class="line">Global_step 8000        Train_loss: 0.0443</span><br><span class="line">Global_step 9000        Train_loss: 0.0422</span><br><span class="line">Global_step 10000       Train_loss: 0.0410</span><br><span class="line">Global_step 11000       Train_loss: 0.0397</span><br><span class="line">Global_step 12000       Train_loss: 0.0383</span><br><span class="line">2018-06-27 18:13:59.743133: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 71532 get requests, put_count=71532 evicted_count=1000 eviction_rate=0.0139798 and unsatisfied allocation rate=0.0143013</span><br><span class="line">2018-06-27 18:13:59.743167: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>除了常规的loss数据之外，我看到穿插在之间的warming informations ，虽然最后的结果没有任何问题，但是我抱着好奇的心态在stackoverflow找到了原因：</p>
<blockquote>
<p>TensorFlow has multiple memory allocators, for memory that will be used in different ways. Their behavior has some adaptive aspects.<br><strong>In your particular case, since you’re using a GPU, there is a PoolAllocator for CPU memory that is pre-registered with the GPU for fast DMA. A tensor that is expected to be transferred from CPU to GPU, e.g., will be allocated from this pool.</strong><br>The PoolAllocators attempt to amortize the cost of calling a more expensive underlying allocator by keeping around a pool of allocated then freed chunks that are eligible for immediate reuse. Their default behavior is to grow slowly until the eviction rate drops below some constant. (The eviction rate is the proportion of free calls where we return an unused chunk from the pool to the underlying pool in order not to exceed the size limit.) In the log messages above, you see <strong>“Raising pool_size<em>limit</em>“ lines that show the pool size growing. Assuming that your program actually has a steady state behavior with a maximum size collection of chunks it needs, the pool will grow to accommodate it, and then grow no more.</strong> It behaves this way rather than simply retaining all chunks ever allocated so that sizes needed only rarely, or only during program startup, are less likely to be retained in the pool.<br>These messages should only be a cause for concern <strong>if you run out of memory</strong>. In such a case the log messages may help diagnose the problem. Note also that peak execution speed may only be attained after the memory pools have grown to the proper size.</p>
</blockquote>
<p>加粗部分解释机制、处理方式和原因。总结起来就是，PoolAllocator会有一个内存分配机制，GPU和CPU之间不是独立的可以相互传输，如果你使用的空间太多，他就会提高原有的预设的空间大小，如果够用了，就没有什么影响了，但是，需要注意的是，兄弟你的数据加载量太大了，看看是不是改改batch size，一次性少加载点数据，或者干掉隔壁同事的任务。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow代码解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[关于'Deep Neural Networks for YouTube Recommendations'的一些思考和实现]]></title>
      <url>/2018/06/26/%E5%85%B3%E4%BA%8EDeep-Neural-Networks-for-YouTube-Recommendations%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83%E5%92%8C%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/1.png" alt=""></p>
<a id="more"></a>
<p>论文 Deep Neural Networks for YouTube Recommendations 来自google的YouTube团队，发表在16年9月的RecSys会议。我想应该很多人都读过，之前参与了公司的推荐系统优化的项目，本来想从各大搜索引擎中寻找到现成的分析，但是出人意料的一无所获。Github上的代码实现也出奇的少以及不清晰，所以就借着这个机会和大家分享一下自己做的过程中的一些理论心得、工程坑、代码实现等等。</p>
<p><em>本文基于大家对Deep Neural Networks for YouTube Recommendations已经完成通读的基础上，不会做细致的论文解析，只会涉及到自己实现过程中的一些总结，如果没有论文了解，会非常不易理解。</em></p>
<h2 id="系统概览"><a href="#系统概览" class="headerlink" title="系统概览"></a>系统概览</h2><p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/2.png" alt=""><br>上面这张图可以说是比较详细的涵盖了基础框架部分，整体的模型的优点我就不详述了，包括规模容纳的程度大啊、鲁棒性好啊、实时性优秀啊、延展性好啊等等，网上很多水字数的文章很多，我们主要总结几个愿论文上的亮点和实际去做的时候需要注意的地方：</p>
<ul>
<li>DNN网络可以怎么改</li>
<li>负采样的“避坑”</li>
<li>example age有没有必要构造</li>
<li>user feature的选择方向</li>
<li>attention 机制的引入</li>
<li>video vectors的深坑</li>
<li>实时化的选择</li>
</ul>
<p>整体上来说，G厂的这套算法基于的是两个部分：matching+ranking，这个的也给我们带来了更大的工作量，在做的时候，分成两个部分，我们在实际处理的时候，通过recall rate来判断matching部分的好坏，通过<a href="https://www.cnblogs.com/eyeszjwang/articles/2368087.html" target="_blank" rel="noopener">NDCG</a>来判断排序部分的好坏。总体如下：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/3.png" alt=""></p>
<p>candidate generation就是我们matching的模块，目的是把百万级的商品、视频筛选出百级、千级的可排序的量级；再通过ranking模块，选出十位数的展示商品、视频作为最后的推送内容。之所以把推荐系统划分成Matching和Ranking两个阶段，主要是从性能方面考虑的。Matching阶段面临的是百万级，而Ranking阶段的算法则非常消耗资源，不可能对所有目标都算一遍，而且就算算了，其中大部分在Ranking阶段排名也很低，也是浪费计算资源。</p>
<h2 id="Matching-amp-Ranking-Problems"><a href="#Matching-amp-Ranking-Problems" class="headerlink" title="Matching &amp; Ranking Problems"></a>Matching &amp; Ranking Problems</h2><p>首先，我们都知道，G厂给出的这个解决方案用的就是基于DNN的超大规模多分类思想，即在时刻t，为用户U（上下文信息C）在视频库V中精准的预测出视频i的类别（每个具体的视频视为一个类别，i即为一个类别），用数学公式表达如下：</p>
<p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/4.png" alt=""></p>
<p>很显然上式为一个softmax多分类器的形式。向量u是<user, context="">信息的高纬“embedding”，而向量v则是视频 j 的embedding向量，通过u与v的点积大小来判断用户与对应视频的匹配程度。所以DNN的目标就是在用户信息和上下文信息为输入条件下学习视频的embedding向量v，从而得到用户的向量u。</user,></p>
<p>说完基本思想，让我们看看实际的效果对比：</p>
<h3 id="DNN网络可以怎么改：softmax及revise的考虑"><a href="#DNN网络可以怎么改：softmax及revise的考虑" class="headerlink" title="DNN网络可以怎么改：softmax及revise的考虑"></a>DNN网络可以怎么改：<strong>softmax及revise</strong>的考虑</h3><p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/5.png" alt=""></p>
<p>如我图中两处红色标记，论文中虽然给出了模型的整体流程，但是却没有指明，1处的video vectors需要单独的embedding还是沿用最下方的embedded video watches里面的已经embedding好的结果，我们称之为softmax问题；2处论文没有提及一个问题，就是在固定好历史watch的长度，比如过去20次浏览的video，可能存在部分用户所有的历史浏览video数量都不足20次，在average的时候，是应该除以固定长度（比如上述例子中的20）还是选择除以用户真实的浏览video数量，我们称之为revise问题。</p>
<p>根据我们的数据实测，效果对比如下：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/6.png" alt=""></p>
<p><code>nosoft:沿用最下方的embedded video watches里面的已经embedding好的结果</code><br><code>revise:除以用户真实的浏览video数量</code></p>
<p>我们尝试的去探求原因发现，nosoft比softmax好的原因在于user vector是由最下方的embedded video watches里面的已经embedding好的结果进行多次FC传递得来的，如果新增一个video embedded vector 的话，和FC传递得到的u vector的点积的意义就难以解释；revise比norevise好的原因是，实际在yoho!buy的购物场景下，用户的点击历史比较我们实际选取的window size要短不少，如果所有的用户都除以固定长度的话，大量的用户history click average的vector大小接近于0。</p>
<h3 id="DNN网络可以怎么改：神经元死亡及网络的内部构造"><a href="#DNN网络可以怎么改：神经元死亡及网络的内部构造" class="headerlink" title="DNN网络可以怎么改：神经元死亡及网络的内部构造"></a>DNN网络可以怎么改：<strong>神经元死亡及网络的内部构造</strong></h3><p>这是一个异常恶心还有没什么好方法的问题，在刚开始做的时候，我们遇到了一个常见的问题，神经元批量死亡的问题。在增加了batch normalization、clip_by_global_norm和exponential_decay learning rate 后有所缓解。</p>
<p>网络结构的变化比较常规，对比场景的激活函数，参考了论文中推荐的深度、节点数，效果对比如下：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/7.png" alt=""></p>
<p>虽然我们看到增加网络的深度（3–&gt;4）一定程度上会提高模型的命中率，增加leakyrelu的一层网络也可以有些许的提升，但是总的来说，对模型没有啥大的影响，所以在之后的实际模型中，我们选择了原论文中relu+relu+relu，1024+512+256的框架。</p>
<h3 id="负采样的“避坑”"><a href="#负采样的“避坑”" class="headerlink" title="负采样的“避坑”"></a><strong>负采样的“避坑”</strong></h3><p>我们都知道，算法写起来小半天就可以搞定，但是前期的数据处理要搞个小半个月都不一定能出来。作为爱省事的我，为了快速实现算法，没有重视负采样的部分，采取了列表页点击为label=1，未点击为label=0的方式，详情如下：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/8.png" alt=""></p>
<p>看上去没什么问题，省略了从全量样本中抽样作为负样本的复杂过程，实际上，我把代码狂改了n边效果也一直维持在1.57%，可以说是没有任何提升，在此过程期间，我还是了拿用户的尾次点击（last_record）进行训练，拿了有较多行为的用户的尾次点击（change_last_record）进行训练，效果很感人：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/9.png" alt=""></p>
<p>在我孤注一掷一致，选择按照原论文中说的，每次label=0的我不拿展现给用户但是用户没有点击的商品，而是随机从全量商品中抽取用户没有点击过的商品作为label=0的商品后，奇迹发生了：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/10.png" alt=""></p>
<p>事后我仔细分析了原因：<br>a.在当次展现的情况下，虽然用户只点击了click商品，其他商品没有点击，但是很多用户在后续浏览的时候未click的商品也在其他非列表页的地方进行click，我实际上将用户感兴趣的商品误标记为了负样本<br>b.后来我咨询看了论文，也发现了原论文中也提及到，展现商品极有可能为热门商品，虽然该商品该用户未点击，但是我们不能降低热门商品的权重（通过label=0的方式），实际上最后的数据也证明了这一点<br>c.“偷窥未来的行为”，如下图，原论文中指出input构造时候不能拿还未发生的点击，只能拿label=1产生时之前的所有历史点击作为input；同理，在构造label=0的时候，只能拿在label=0的时候已经上架的商品，由于训练时间的拉长，不能偷窥label=1发生时还未上架的商品作为label=0的负样本</p>
<p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/11.png" alt=""></p>
<h3 id="example-age有没有必要构造"><a href="#example-age有没有必要构造" class="headerlink" title="example age有没有必要构造"></a><strong>example age有没有必要构造</strong></h3><p>首先，先稍微解释一下我对example age的概念的理解。所有的训练数据其实都是历史数据，距离当前的时刻都过去了一段时间，站在当前来看，距离当前原因的数据，对当前的影响应该是越小的。就比如1年前我买了白色的铅笔，对我现在需要不需要再买一支黑色的钢笔的影响是微乎其微的。而example age其实就是给了每一条数据一个权重，引用一下原论文的描述<code>In (5b), the example age is expressed as tmax − tN where tmax is the maximum observed time in the training data</code>，我这边采取了(tmax − tN)/tmax的赋权方式：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/12.png" alt=""></p>
<p>很悲催的是，直观的离线训练数据并没有给出很直观的效果提升，但是由于评估机制的问题（我们后面会说到），我会在实际上线 做abtest的时候重新验证我的这个example age的点，但是可以肯定的是，理论和逻辑上，给样本数据进行权重的更改，是一个可以深挖的点，对线上的鲁棒性的增强是正向的。</p>
<h3 id="user-feature的选择方向"><a href="#user-feature的选择方向" class="headerlink" title="user feature的选择方向"></a><strong>user feature的选择方向</strong></h3><p>很不幸的是，在这一块的提升，确实没有论文中说的那么好，对于整个网络的贡献，以我做的实际项目的结果来说，history click embedded item &gt; history click embedded brand &gt; history click embedded sort &gt; user info &gt; example age &gt; others。不过，因为时间、数据质量、数据的真实性的原因，可能作为原始input的数据构造的就没有那么好。这边主要和大家说两个点：</p>
<p>1.topic数据<br>原论文中在第四节的RANKING中指出:<br><code>We observe that the most important signals are those that describe a user’s previous interaction with the item itself and other similar items, matching others’ experience in ranking ads</code><br>论文中还举出了比如用户前一天的每个频道（topic）的浏览视频个数，最后一次浏览距今时间，其实说白了就是强调了过去的行为汇总对未来的预测的作用，认为过去的行为贯穿了整体的用户点击轨迹。<br>除此之外，G厂大佬还认为一些用户排序性质的描述特征对后面的ranking部分的提高也是蛮重要的，这边还举出了用户视频评分的例子，更多的内容大家可以自己去看一下原论文的部分，应该都会有自己的体会。</p>
<p>回到我们的项目，因为yoho!buy是电商，我类比着做了用户每个类目（裤子、衣服、鞋子…）的历史浏览点击购买次数、最后一次点击距今时长等等的topic信息，提升不是很明显。但是在大家做G厂这边论文，准确率陷入困境的时候，可以尝试一下这边的思路。<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/13.png" alt=""></p>
<p>2.query infomation<br>相比于论文中的user information的添加，在实际模型测试中，我们发现，query的information的部分有更多的”遐想”。</p>
<p>原论文中点名指出user language and video language 做为basic info的重要性，这边给出的提升也是相对于user info有明显的增长的：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/14.png" alt=""></p>
<p>有提升也自然有该部分的缺点：<br>1.语言模型的处理复杂，耗时久<br>在该部分的处理中，我强行拖着隔壁组的nlp博士和我一起搞了一周，每天都加班的搞去做数据清理，句法分析，语句树解析。如果需要让一个常规做推荐的人去弄，会有各种各样的坑，而且耗时还久<br>2.语言新增问题<br>商品的标题这类的文本处理还好，毕竟每日更新的数据存在一个可控的范围，但是用户搜索内容的变化是巨大的，粗略估测一下，一周时间间隔后，原提纯文本数据和新提纯文本数据的交集覆盖率不到78%，这意味着要重复的做nlp工作</p>
<h3 id="attention-机制的引入"><a href="#attention-机制的引入" class="headerlink" title="attention 机制的引入"></a><strong>attention 机制的引入</strong></h3><p>attention 机制的引入是我老大的硬性需求，我这边也就做了下，如果不了解attention 机制的朋友，可以阅读以下这边文章：<a href="https://blog.csdn.net/qq_21190081/article/details/53083516" target="_blank" rel="noopener">Attention model</a>。</p>
<p>我通俗的解释一下，不准确但是方便理解，Attention model就是让你每一个input与你的output计算一个similarity，再通过这些similarities给出每个input的权重。但是，很明显，我们离线训练还好，既有input也有output，但是线上预测的时候，就没有output了，所以，我们采取了lastclick替代的方式：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/15.png" alt=""></p>
<p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/16.png" alt=""></p>
<p>不得不说，老祖宗传下来的东西确实有独到之处，但是在提升了近1pp的rate代价之下，会有一个让人头疼的问题耗时。因为每一个input的weight需要和output进行一次相似度计算，而且后续还要对计算出的相似度进行处理，原本只需要6-7小时训练完的模型，在我加了3层Multihead Attention后被拖到了一天。数据量还只采样了一半，确实需要斟酌带来的提升与投入的成本之间的平衡问题。</p>
<h3 id="video-vectors的深坑"><a href="#video-vectors的深坑" class="headerlink" title="video vectors的深坑"></a><strong>video vectors的深坑</strong></h3><p>G厂一句话，我们测断腿。这句话不是瞎说的，大家应该还记得一开始我给出的那张图，在最上面有一行不是很明显的小字：video vectors。G厂的大佬们既没有说这些video vectors该怎么构造，也没有说video vectors需不需要变动，留下了一个乐趣点让大家体验。</p>
<p>刚开始我很傻的用了我们最开始的embedded item作为video vectors，与模型FC出来的user vectors进行点击，计算top items。我来来回回测了一个月，老命都快改没了，最后提升rate到4pp。然而RNN随便跑跑就能到达3pp，我说很不服气的，所以拉着同事一起脑洞了一下，我们之前做图片相似度匹配的时候，喜欢把图片的向量拆成颜色+款式+性别，所以我们就借用了一下，改成了embedded item + embedded brand + embedded sort作为video vectors，历史总是给我们惊喜，效果上一下子就能大到5.2pp左右，这个点的提升应该是得来的最意外的，建议大家在用的时候考虑一下。<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/17.png" alt=""></p>
<h3 id="实时化的选择"><a href="#实时化的选择" class="headerlink" title="实时化的选择"></a><strong>实时化的选择</strong></h3><p>实时部署上，我们用了tensor flow serving，没什么好说的，给一下关键代码，大家看下自己仿一下就行，一般自己做做demo不需要，企业级上线才需要，企业级上线的那些大佬可能也比我有更多想法，所以就不展开了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">部署及用python作为Client进行调用的测试：</span><br><span class="line">#1.编译服务</span><br><span class="line">bazel build //tensorflow_serving/model_servers:tensorflow_model_server</span><br><span class="line">#2.启动服务</span><br><span class="line">bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9005 --model_name=test --model_base_path=/Data/sladesha/tmp/test/ </span><br><span class="line">#3.编译文件 </span><br><span class="line">bazel build //tensorflow_serving/test:test_client</span><br><span class="line">#4.注销报错的包</span><br><span class="line">注销：/Data/muc/serving/bazel-bin/tensorflow_serving/test/test_client.runfiles/org_tensorflow/tensorflow/contrib/image/__init__.pyc中的from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms</span><br><span class="line">参考：https://github.com/tensorflow/serving/issues/421</span><br><span class="line">#5.运行</span><br><span class="line">bazel-bin/tensorflow_serving/test/test_client --server=localhost:9005</span><br></pre></td></tr></table></figure>
<p>相关的问题，有大佬已经梳理好了，自取其他可选的一些参数设置：<a href="https://blog.csdn.net/langb2014/article/details/54317490" target="_blank" rel="noopener">tensorflow serving 参数设置</a>。</p>
<p>还有一些评估技巧，模型之间的对比技巧，这边就不细讲了，可借鉴的意义也不大。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>虽然早就读过这篇文章，但是实现之后，发现新收获仍然不少。我特别赞成清凇的一句话:’对于普通的学术论文，重要的是提供一些新的点子，而对于类似google这种工业界发布的paper，特别是带有practical lessons的paper，很值得精读。’<br>G厂的这个推荐代码和attention model的代码之前是准备放GitHub的，想想还是算了。一是之前也放过很多此代码，也没什么反馈，二是这两个代码自己写也不是很难，可以作为练手项目。</p>
<h2 id="鸣谢"><a href="#鸣谢" class="headerlink" title="鸣谢"></a>鸣谢</h2><p>以上我个人在Yoho!Buy团队在实践中的一点总结，不代表公司的任何言论，仅仅是我个人的观点。最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！溜了溜了。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 论文解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tf.nn.embedding_lookup]]></title>
      <url>/2018/06/11/Tensorflow%E4%B8%93%E9%A2%98tf-nn-embedding-lookup/</url>
      <content type="html"><![CDATA[<p><img src="/2018/06/11/Tensorflow专题tf-nn-embedding-lookup/1.jpg" alt=""><br><a id="more"></a></p>
<p><img src="/2018/06/11/Tensorflow专题tf-nn-embedding-lookup/2.jpg" alt=""><br>我觉得这张图就够了，实际上tf.nn.embedding_lookup的作用就是找到要寻找的embedding data中的对应的行下的vector。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup(params, ids, partition_strategy=<span class="string">'mod'</span>, name=<span class="keyword">None</span>, validate_indices=<span class="keyword">True</span>, max_norm=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p><strong><a href="https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup" target="_blank" rel="noopener">官方文档位置</a></strong>，其中，params是我们给出的，可以通过：<br>1.<code>tf.get_variable(&quot;item_emb_w&quot;, [self.item_count, self.embedding_size])</code>等方式生产服从[0,1]的均匀分布或者标准分布<br>2.<code>tf.convert_to_tensor</code>转化我们现有的array<br>然后，ids是我们要找的params中对应位置。</p>
<p>举个例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">data = np.array([[[<span class="number">2</span>],[<span class="number">1</span>]],[[<span class="number">3</span>],[<span class="number">4</span>]],[[<span class="number">6</span>],[<span class="number">7</span>]]])</span><br><span class="line">data = tf.convert_to_tensor(data)</span><br><span class="line">lk = [[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]]</span><br><span class="line">lookup_data = tf.nn.embedding_lookup(data,lk)</span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure></p>
<p>先让我们看下不同数据对应的维度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">76</span>]: data.shape</span><br><span class="line">Out[<span class="number">76</span>]: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">In [<span class="number">77</span>]: np.array(lk).shape</span><br><span class="line">Out[<span class="number">77</span>]: (<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">In [<span class="number">78</span>]: lookup_data</span><br><span class="line">Out[<span class="number">78</span>]: &lt;tf.Tensor <span class="string">'embedding_lookup_8:0'</span> shape=(<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>) dtype=int64&gt;</span><br></pre></td></tr></table></figure></p>
<p>这个是怎么做到的呢？关键的部分来了，看下图：<br><img src="/2018/06/11/Tensorflow专题tf-nn-embedding-lookup/3.png" alt=""><br>lk中的值，在要寻找的embedding数据中下找对应的index下的vector进行拼接。永远是look(lk)部分的维度+embedding(data)部分的除了第一维后的维度拼接。很明显，我们也可以得到，<strong>lk里面值是必须要小于等于embedding(data)的最大维度减一的</strong>。</p>
<p>以上的结果就是：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">79</span>]: data</span><br><span class="line">Out[<span class="number">79</span>]:</span><br><span class="line">array([[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>],</span><br><span class="line">[<span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">6</span>],</span><br><span class="line">[<span class="number">7</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">80</span>]: lk</span><br><span class="line">Out[<span class="number">80</span>]: [[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># lk[0]也就是[0,1]对应着下面sess.run(lookup_data)的结果恰好是把data中的[[2],[1]],[[3],[4]]</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">81</span>]: sess.run(lookup_data)</span><br><span class="line">Out[<span class="number">81</span>]:</span><br><span class="line">array([[[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>],</span><br><span class="line">[<span class="number">4</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[[[<span class="number">3</span>],</span><br><span class="line">[<span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]]]])</span><br></pre></td></tr></table></figure></p>
<p>最后，partition_strategy是用于当len(params) &gt; 1，params的元素分割不能整分的话，则前(max_id + 1) % len(params)多分一个id.<br>当partition_strategy = ‘mod’的时候，13个ids划分为5个分区：[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]，也就是是按照数据列进行映射，然后再进行look_up操作。<br>当partition_strategy = ‘div’的时候，13个ids划分为5个分区：[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]，也就是是按照数据先后进行排序标序，然后再进行look_up操作。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow代码解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tf.scan]]></title>
      <url>/2018/06/04/Tensorflow%E4%B8%93%E9%A2%98tf-scan/</url>
      <content type="html"><![CDATA[<p><img src="/2018/06/04/Tensorflow专题tf-scan/1.jpg" alt=""><br><a id="more"></a></p>
<p>tf.scan(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=None)</p>
<p>fn：计算函数<br>elems：以elems的第一维度的变量list作函数计算直到遍历完整个elems<br>initializer：fn计算的初始值，替代elems做第一次计算</p>
<p>举个好理解的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">z = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">x = tf.convert_to_tensor(x)</span><br><span class="line">z = tf.convert_to_tensor(z)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x,y)</span>:</span></span><br><span class="line"><span class="keyword">return</span> x+y</span><br><span class="line"></span><br><span class="line">g = tf.scan(fn=f,elems = x,initializer=z)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer)</span><br><span class="line"></span><br><span class="line">sess.run(g)</span><br></pre></td></tr></table></figure></p>
<p>会得到：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">97</span>]: sess.run(g)</span><br><span class="line">Out[<span class="number">97</span>]: array([<span class="number">11</span>, <span class="number">13</span>, <span class="number">16</span>], dtype=int32)</span><br></pre></td></tr></table></figure></p>
<p>详细的计算逻辑如下：<br>11 = 10(初始值initializer)+ 1(x[0])<br>13 = 11(上次的计算结果)+2(x[1])<br>16 = 13(上次的计算结果)+3(x[2])</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow代码解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[写给想转行机器学习深度学习的同学]]></title>
      <url>/2018/03/18/%E5%86%99%E7%BB%99%E6%83%B3%E8%BD%AC%E8%A1%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%8C%E5%AD%A6/</url>
      <content type="html"><![CDATA[<p><img src="/2018/03/18/写给想转行机器学习深度学习的同学/1.jpg" alt=""></p>
<a id="more"></a>
<p>update 1:很多同学还是私信我，让我推荐或者提供一些电子书给他们，我这边也打包了一些我认为比较重要的，如果有需要的同学可以「邮箱」联系我。申明，我所发送的书个人均已购买正版实体书，建议大家也支持正版，谢谢。</p>
<hr>
<p>自从我毕业以来，先是火机器学习，然后火大数据，之后火深度学习，现在火人工智能这些算法领域。越来越多的朋友想从工业，金融等等行业转行到算法相关的行业，我一年前在知乎上写了一个答案<a href="https://www.zhihu.com/question/56050487/answer/148428497" target="_blank" rel="noopener">本科生怎样通过努力拿到较好的机器学习/数据挖掘相关的offer？</a>，当时拿了不少的赞，所以也一直有同学找我咨询相关的问题，确确实实也有相当一批人拿到了不错的offer。</p>
<p>我个人不是很喜欢更新非技术的文章，但是我还是觉得如果能帮助到一些人，其实也是另一种技术输出的展现，所以我就写下了下面这篇短文，希望对迷茫的人有所帮助。</p>
<h2 id="评估转行难度"><a href="#评估转行难度" class="headerlink" title="评估转行难度"></a>评估转行难度</h2><p>今天一大早，我在刷知乎的时候，刷到这个题目<a href="https://www.zhihu.com/question/265041005/answer/344257277" target="_blank" rel="noopener">非计算机专业学生如何转行AI，并找到算法offer?</a>，我看到这个叫做<a href="https://www.zhihu.com/people/wang-rui-meng-43/activities" target="_blank" rel="noopener">BrianRWang</a>的答主的一个“10问检验你的基础水平”，我觉得是至少我看来非常全面考验数学基础的，所以这边就和大家分享一下（答案我会在最后给出，有兴趣的最好自己做一下，括号里面的我个人觉得没有意义所以没有给出解释，有兴趣的却又解不出来的同学可以私信我）：</p>
<blockquote>
<p>1.什么是贝叶斯定理？请简述其公式？现分别有 A，B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器 A 的概率是多少?</p>
</blockquote>
<p>这题考了概率论的基础，虽然考了贝叶斯，但是后面的容器问题完全可以不用贝叶斯也可以算出来，算是一题数学敏感度的测试题，看看自己适不适合去努力切入这个方向。</p>
<blockquote>
<p> 2.请简述卡方分布和卡方检验的定义？(给你一个2*2的列表让你算卡方分布，你会怎么做？)</p>
</blockquote>
<p>这题考了梳理统计的基础，括号里面的我个人觉得没有意义，有兴趣的可以查表算一下。</p>
<blockquote>
<p>3.在概率统计学里，自由度是如何被定义的，又该怎样去应用？</p>
</blockquote>
<p>原作者BrianRWang认为这题比较偏，属于冷门题目。个人看法：其实我觉得如果是任何一个理工科的同学，这题都应该能答出来，大学的课程里，自由度的理解直接决定了统计科目大家的学习质量。</p>
<p>以上的三题考了概率论与数理统计的基础，在机器学习理论中，概率论和数理统计的基础是否扎实直接决定了能否很好的理解各个理论的前置条件，适用场景，提升方向等，着实重要。</p>
<blockquote>
<p>4.请简述什么是线性代数里的矩阵特征值和特征向量？(求矩阵:A=np.array([[1,2],[3,4]])的特征值，特征向量，写出其运算公式)</p>
</blockquote>
<p>线性代数题目，很简单给出对应的公式即可，我在SVD介绍的时候就完全讲过。如果换成，如何理解特征值及特征向量在空间中的实际意义，这题就会变得非常卡人。</p>
<blockquote>
<p>5.如何使用级数分解的方法求解e^x?(并给出在数值计算中可能遇到的问题。)</p>
</blockquote>
<p>数学分析的题目，一个公式。</p>
<p>以上的题目都是线性代数，数学分析的题目，都是比较考验大学的基本功，如果不记得也很正常，只要能说出大概的思想就行，比如空间选择啊，点导数展开。</p>
<blockquote>
<p>6.数据结构的定义是什么？运用数据结构的意义是什么？</p>
</blockquote>
<p>计算机题，这题应该是几个问答中最简单的了。</p>
<blockquote>
<p>7.请说明至少两种用于数据可视化（data visualization）的package。并且说明，在数据分析报告里用数据可视化的意义是什么？</p>
</blockquote>
<p>前一问如果主动接触过计算科学的人这题比较好答，如果是纯新手，这题就是无从下手的。后面一小问也是属于考察你的数据敏感度的，如果能够match到一些点，很加分。</p>
<blockquote>
<p>8.假如让你用编程方法，比如python，处理一个你没见过的数学问题，比如求解一个pde或者整快速傅里叶变换，你应该查什么东西，找哪一个package的参考资料？</p>
</blockquote>
<p>同上一条前一部分。</p>
<blockquote>
<p>9.请简述面向对象编程和函数式编程分别的定义，并举出其案例。</p>
</blockquote>
<p>计算机题，考了基础的编程的一些风格的了解程度，说实话，这题我第一次看到也很懵，还去Google了一下。</p>
<p>原作者还有一个第10题，不涉及技术，我就没放。以上四题更偏向coding的能力，虽然说算法工程师、数据挖掘工程师、NLP工程师，等等，都是挂着科研的title，但是过硬的coding能力是完全不能缺少的，要其他人把很复杂的数学理论用代码帮你实现出来的交流成本巨大，我觉得精通或者熟悉至少一门语言还是非常重要的。</p>
<p>原作者认为：</p>
<blockquote>
<p>以上提问如果能闭卷对7个及以上，证明一个学生的基础还是比较好的。只要聪明肯学，一定是有所裨益的。在7个，到3个之间，不妨提高一下自己的数学水平；努努力还是可以学会机器学习的。如果写对不了两个（“这都啥啊？”），郴州勃学院复读班欢迎你过去。</p>
</blockquote>
<p>其实我还是比较认同的，答对3个或者2.5个以上的同学，完全可以试一试转一转，我觉得不存在说入不了门的情况。能答对7个或者7.5以上的同学，我觉得可以投简历了，如果我收到你的简历，即便是你没有历史的工作经验，我很愿意让你试一试的。</p>
<h2 id="一些资料"><a href="#一些资料" class="headerlink" title="一些资料"></a>一些资料</h2><p>很多转行的朋友会问我，到底看什么书会比较好，我刚开始会推荐一堆，后来自己想了想发现，还是太天真，大家工作忙的要死，看一本就很难了，别说一堆。</p>
<p>我最后就浓缩了三本:：周志华老师的西瓜书（《机器学习》周志华 清华大学出版社），李航的带你玩转基础理论（《统计学习方法》李航 清华大学出版社），经典厕所读物（《数学之美》吴军 人民邮电出版社）。</p>
<p>确实是很经典很经典的书，我现在基本上每次必回答以上三本。</p>
<p>除此之外，在coursera上找吴恩达（Andrew Ng）教授的机器学习课程，他把要用到的数学知识也做了简单的讲解，机器学习方面的理论和算法讲的也很详细，而且很基础，肯定可以看懂。<a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Machine Learning | Coursera</a>，应该是最适合看的视频类的资料没有之一。</p>
<p>我不反对也不支持大家去参加几千几万的速成班，几十几百的live课程，但是我觉得你不妨先看完以上的书和视频再做决定，一定不会让你失望。之前我一直在给team做吴恩达（Andrew Ng）在线课程的分享，一直到最近我发现不如整理出来给team以外的大家一起看算了，所以在Gradient Checking(9-5)这节课之后的所有课程，如果有价值的地方，我都做了笔记后面会分享在我的GitHub中，希望给大家一些帮助。</p>
<p>最后，希望我们都不负自己的青春。</p>
<p><em>附录：</em><br><em>1.<a href="https://github.com/sladesha/machine_learning/blob/master/Knowledge%20Summary/Questions%20and%20Answers.md" target="_blank" rel="noopener">BrianRWang的十条问题的答案链接</a></em><br><em>2.<a href="https://github.com/sladesha/machine_learning/blob/master/Knowledge%20Summary/Notes%20of%20Andrew%20Ng&#39;s%20Lessons.md" target="_blank" rel="noopener">吴恩达（Andrew Ng）Gradient Checking(9-5)这节课之后的课程整理（持续更新中）</a></em></p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 公告 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[yoho!buy注册概率预估]]></title>
      <url>/2018/03/04/yoho!buy%E6%B3%A8%E5%86%8C%E6%A6%82%E7%8E%87%E9%A2%84%E4%BC%B0/</url>
      <content type="html"><![CDATA[<p><img src="/2018/03/04/yoho!buy注册概率预估/1.jpeg" alt=""></p>
<a id="more"></a>
<hr>
<p>GRU 部分的demo代码：<a href="https://github.com/sladesha/deep_learning/tree/master/RNN_applied_classification/src" target="_blank" rel="noopener">model.py</a>，注意，其中上传的lr_classification.py并没有做wide&amp;deep设计</p>
<hr>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文主要介绍yoho!buy大数据团队在深度学习传统应用方向上的一些实践和思考。传统用户行为预估方向上，如何根据用户的行为数据，对用户行为建模，进而预测用户的购买行为，点击行为，注册行为等等一直以来都受到工业界及学术界的关注。相对而言，就用户注册概率的预测受限数据获取的局限性、传统的计算模型的时效性等原因并没有很多可参考的研究案例。我们想和大家分享的「yoho!buy基于GRU+LR算法下的用户注册概率预估」，基于循环神经网络的框架，充分的利用了用户在app上的行为信息，保证了高效的结果反馈速度，兼备算法框架良好的延拓性能。</p>
<h1 id="注册概率预估定义"><a href="#注册概率预估定义" class="headerlink" title="注册概率预估定义"></a>注册概率预估定义</h1><p><strong>注册概率预估</strong>，即预估用户下载app后，浏览app过程中主动注册的可能性。通过识别出有注册倾向的人群，辅助以人为介入的方式（优惠、折扣，关怀等），可以提高用户实际注册的概率。</p>
<p>基本的注册概率预估算法设计的流程如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/2.png" alt=""></p>
<p>数据整理节点：在于收集用户行为信息，包括地理位置，当时的时间，用户来源的渠道，用户点击行为等等；模型计算节点：在于根据数据整理节点的结果判断用户的注册概率高低；计算结果推送节点：在于根据不同注册概率用户采取不同的营销策略，个性化引导用户注册。</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/3.jpeg" alt=""></p>
<p>目前注册概率预估主要有两大难点：</p>
<ul>
<li>传统模型难以实时预测：因为缺乏平台忠诚度，用户第一次也有可能是最后一次登陆app之前这段时间是相对暂短，如何压缩用户每一次步操作下的模型计算时间，提高反馈频率是需要考虑的重要问题，而传统模型在这方面的表现比较平庸。</li>
<li>传统模型特征加工复杂：因为用户可能是第一次接触app，在没有注册信息，历史行为信息，完成订单信息等等数据下，利用有限的数据进行的特征处理，如果想要有不错的效果相对而言特征加工过程的复杂程度和难度要比普通的项目更加具有挑战性。</li>
</ul>
<p>我们通过以Recurrent Neural Network 及 Logistic Regression为基模型，通过Stacking方式，针对性的尝试去解决以上两个注册预估中的难点。其中Recurrent Neural Network最本质的能够work的地方在于，实际上在没有过多特征的时候，对于新用户来说，他的浏览路径实际上就是反映了他对这个app的喜好。</p>
<h1 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h1><h2 id="Recurrent-Neural-Network基模型"><a href="#Recurrent-Neural-Network基模型" class="headerlink" title="Recurrent Neural Network基模型"></a>Recurrent Neural Network基模型</h2><h3 id="数据整理部分"><a href="#数据整理部分" class="headerlink" title="数据整理部分"></a>数据整理部分</h3><p>无论在Kaggle还是天池大赛上，数据特征工程是非常重要而且繁琐的一个过程：<br>关于预处理，通常我们会采取：</p>
<ul>
<li>数据检查，提出异常字符、乱码等数据</li>
<li>缺失值处理，剔除、填充、拟合构造等</li>
<li>方差衡量，剔除方差低的低贡献特征</li>
<li>共线性检查，提高泛化能力</li>
<li>异常检验，剔除错误异常数据</li>
<li>…</li>
</ul>
<p>在数据预处理结束之后，我们还会在更新完的数据集上进行特征筛选：</p>
<ul>
<li>基于自变量与因变量之间的交互熵</li>
<li>基于模型中的特征贡献程度（Xgboost里面的importance/Lasso、Ridge中的参数绝对大小）</li>
<li>基于预训练模型中的特征参数的显著性</li>
<li>基于自变量之间的相关性</li>
<li>…</li>
</ul>
<p>在数据特征筛选结束之后，我们还需要进行特征组合寻找最大方差下的新的特征，还会通过PCA/LDA/t-SNE/FM等寻找是否可以进行降维或者升维，交叉特征构造等等。通常无论是在离线训练还是线上预测中，对特征的加工处理过程都是非常耗时的，极有可能在用户已经离开app后，用户的注册概率还没有算出来。</p>
<p>我们利用Recurrent Neural Network来解决注册预估的时候，我们需要做的数据整理就非常的轻松，只需根据用户浏览的顺序，将用户浏览的页面编号Item_Page,同时记录用户浏览的先后顺序Time_Rank：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/4.jpg" alt=""></p>
<p>构造如下的数据形式：</p>
<table>
<thead>
<tr>
<th>User_Imme</th>
<th style="text-align:center">Item_Page</th>
<th>Time_Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td>012939003331092</td>
<td style="text-align:center">92374573284354</td>
<td>1518422132</td>
</tr>
<tr>
<td>012939003331092</td>
<td style="text-align:center">82374573771273</td>
<td>1518422142</td>
</tr>
<tr>
<td>012939003331092</td>
<td style="text-align:center">92374573284354</td>
<td>1518422147</td>
</tr>
<tr>
<td>078939002221093</td>
<td style="text-align:center">66774573284354</td>
<td>1518422247</td>
</tr>
<tr>
<td>078939002221093</td>
<td style="text-align:center">66774573284442</td>
<td>1518422249</td>
</tr>
<tr>
<td>…</td>
<td style="text-align:center">…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>数据处理过程，只需要按照用户的浏览先后顺序进行排序即可，大大的降低了耗时，对整体算法的实效性上不会产生任何影响。因此，我们甚至可以在用户每一次产生动作之后就对其的注册概率进行重新判定，得到用户的浏览流对应的注册概率波动情况。</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/5.png" alt=""></p>
<p>我们的用户可以大致可分为铁粉用户和普通用户两种。从用户的注册概率分布就可以很清晰的看出：铁粉用户浏览目的明确，寻找自己关注的商品，一旦找到立马注册下单，所以铁粉用户的时间流较短，注册概率呈现上升趋势，注册概率均值较高，需要我们运营手段干涉的情况较少；而普通用户属于无明确目的的浏览，所以注册概率的波动较大，注册概率均值较低，但是一旦察觉到用户有高概率注册的行为却未注册且注册概率持续下降时立刻进行营销引导，多次营销尝试后用户成功注册。</p>
<h3 id="模型计算部分"><a href="#模型计算部分" class="headerlink" title="模型计算部分"></a>模型计算部分</h3><p>整体的模型框架概览如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/6.png" alt=""></p>
<p>我们试图通过循环网络部分提取出用户的浏览行为的汇总信息，再通过Logistic Regression部分融合用户基础信息，以行为特点+基础特征猜测用户每次浏览是属于随便点点还是认真挑选。</p>
<h4 id="Recurrent-Neural-Network部分"><a href="#Recurrent-Neural-Network部分" class="headerlink" title="Recurrent Neural Network部分"></a>Recurrent Neural Network部分</h4><p><strong>1.循环单元结构</strong></p>
<p>在循环神经网络模块中，我们采取了Gated Recurrent Unit (GRU)代替普通RNN作为最小循环单元进行计算，以此来避免梯度消失等问题：</p>
<p>隐藏状态计算如下：<br><img src="/2018/03/04/yoho!buy注册概率预估/7.jpeg" alt=""><br><img src="/2018/03/04/yoho!buy注册概率预估/8.jpeg" alt=""><br><img src="/2018/03/04/yoho!buy注册概率预估/9.jpeg" alt=""><br><img src="/2018/03/04/yoho!buy注册概率预估/10.jpeg" alt=""></p>
<p>对比LSTM，GRU只用了两个gates，将LSTM中的输入门和遗忘门合并成了更新门。并且并不把线性自更新建立在额外的memory cell上，而是直接线性累积建立在隐藏状态上，并靠gates来调控，这样就可以大大的加快离线训练的速度，同时在RNN的官方论文中，我们看到了实测的效果如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/11.jpeg" alt=""></p>
<p>很明显的可以看到:虽然GRU减少了一个门的存在，但是效果与LSTM相当，但是几乎每次测试的test效果都要优秀于传统方法，同时GRU是真的肉眼可见的比LSTM快。综合考虑了计算速度及最后计算结果的准确程度，我们选择了多层GRU模型而并非是以输入门、输出门、遗忘门为信息传递的LSTM模型。</p>
<p><strong>2.循环网络结构</strong></p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/12.png" alt=""></p>
<p>整体上看，网络结构也是非常简单的。如上图，先将用户浏览的所有商品进行embedding操作，然后根据用户每个商品的浏览顺序构建时序序列，进行多层的GRU模型训练，最后再以前馈网络传递，softmax后得到用户下次点击每个商品的概率，再根据预测结果Output Second Items 和 Real Second Items修正多层GRU layer中的参数。</p>
<p>通过已经训练好的循环网络，我们根据新用户浏览商品的顺序，得到用户每次浏览的后一次浏览每个商品的概率（output scores）及用户前N次浏览信息的trend，seasonality的汇总（隐藏状态GRU States）。与此同时，我们可以通过控制GRU layer的层数及优化多次隐藏状态GRU States的拼接方式控制整体模型框架的复杂程度。</p>
<p><strong>3.用户数据构造</strong></p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/13.png" alt=""></p>
<p>用户数据训练过程常常采用如上图这样的最小批处理。每一个Session就可以看作是一个用户，每一个i可以看作是一个商品，商品i的下标代表着商品的浏览顺序。</p>
<p>input中，每一行表示的为多个用户正常的浏览顺序中的起始商品，比如Session1用户浏览顺序：i1.1–&gt;i1.2–&gt;i1.3–&gt;1.4中的i1.1，i1.2，i1.3；对应的，在output中，每一行代表多个用户在input位置的下一次浏览内容，比如Session1用户浏览顺序：i1.1–&gt;i1.2，i1.2–i1.3，i1.3–&gt;i1.4中的i1.2，i1.3，i1.4；当input中的一个的用户或者说是一个的Session的点击信息被全部使用后，追加一个新的用户或者说是Session的点击信息，同时通过控制同时计算的用户或者说Session的个数，直到所有的Session信息都被使用完一遍，这样就构造完成了一个由用户的上次点击结果预测用户的下次点击结果的循环神经网络。</p>
<p>这样设计避免了通常的神经网络构造在用户行为上应用中的两个问题：</p>
<ul>
<li>固定滑动窗口导致大量用户信息不能获取完整</li>
<li>拆分用户的浏览行为计算导致循环网络在信息理解上的歧义</li>
</ul>
<p><strong>4.损失函数选取</strong></p>
<p>关于损失函数的选取，我们这边主要推荐两种方案：基于贝叶斯后验优化(BayesianPersonalizedRanking,BPR)和第一准则(TOP1)：</p>
<ul>
<li>贝叶斯后验优化(BayesianPersonalizedRanking,BPR)是一个近似矩阵分解的方法，我们分别选取当前用户当前商品的下一个真实浏览的商品作为Positive Item，随机抽样的商品作为Negative Item，具体表达形如：<img src="/2018/03/04/yoho!buy注册概率预估/14.jpeg" alt=""><br>，其中Ns即为随机采样个数，<img src="/2018/03/04/yoho!buy注册概率预估/15.jpeg" alt=""><br>中k为i时对应Positive Item的真实计算得分，K为j时对应Negative Item，保持i=Positive Item不变，计算所有抽样出来的Negative Item作为j进行计算即可。</li>
<li>第一准则(TOP1)是我们自己设计的一个近似排序的方法，我们想要真实的Positive Item所计算出来的结果尽可能的接近1，Negative Item计算出来的结果尽可能的接近0，所以我们要保证采样出来的Negative Item比Positive Item在当前计算方式下的得分要低，所以，我们可以设计损失函数如下：<img src="/2018/03/04/yoho!buy注册概率预估/16.jpeg" alt=""><br>，最末项增加了个正则项修正拟合程度。</li>
</ul>
<p>除此之外，《SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS》中还提到了POP，S-POP，Item-KNN，BPR-MF等方法，可单独了解。</p>
<h4 id="Logistics-Regression部分"><a href="#Logistics-Regression部分" class="headerlink" title="Logistics Regression部分"></a>Logistics Regression部分</h4><p><strong>通过Logistics Regression部分提高融合Recurrent Neural Network的潜藏层和传统的用户基础特征，进行一次重排序的操作，个性化的提供用户的注册激励也是非常重要的一环。</strong><br>整个Recurrent Neural Network部分在一定程度上帮助我们获取到了用户的浏览操作行为中的trend和seasonality（隐藏状态GRU States），但是缺乏考虑外部信息，比如热点爆品，用户区域，用户性别，用户需求等等。最典型的一个例子就是，我们不能向一个浏览了多个中性黑色太阳眼镜的正在试图走酷雅风格的东北女性推送潮牌男性短裤优惠券作为注册激励，很多时候会起到相反的作用。</p>
<p>如何兼顾Recurrent Neural Network部分的潜藏数据与为数不多的用户基础特征数据，并加以融合快速反馈出结果，是需要多方面考虑的：<br><strong>数据应用</strong></p>
<table>
<thead>
<tr>
<th>数据类别</th>
<th style="text-align:center">数据详解</th>
</tr>
</thead>
<tbody>
<tr>
<td>基础用户画像</td>
<td style="text-align:center">人口属性，地点，性别，消费力等</td>
</tr>
<tr>
<td>主动行为数据</td>
<td style="text-align:center">品类偏好、品牌偏好、行为性别等</td>
</tr>
<tr>
<td>文本偏好数据</td>
<td style="text-align:center">浏览商品文字描述特征</td>
</tr>
<tr>
<td>反馈数据</td>
<td style="text-align:center">停留时长，复停留行为，当前时间段等</td>
</tr>
<tr>
<td>…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
<ul>
<li>基础用户画像&amp;主动行为数据：我们可以在用户原始日志中快速清洗出用户的地址，环境，设备等基础信息，结合用户浏览商品的性别+价格+品牌加权预估出用户的性别，消费力等价值属性，品类偏好、品牌偏好、行为性别等基础汇总属性。</li>
<li>文本偏好数据：根据用户的浏览商品，去匹配是否命中了我们预先提取出的注册用户浏览高频关键词，比如“鬼洗”，“典藏”等等及当前的一些热点词汇“小白鞋”，“华莱士”等。</li>
</ul>
<p><img src="/2018/03/04/yoho!buy注册概率预估/17.jpeg" alt=""></p>
<ul>
<li>反馈数据：在整个Recurrent Neural Network部分我们考虑的是用户的浏览顺序，但是忽略了用户的浏览质量，用户进入平台后的15s内，A商品重复浏览了3次，停留了9S，B商品重复浏览了1次，停留了1S，商品A的注册激励价值是远远高于商品B的。</li>
</ul>
<p>通过以上的方式获取到的“用户画像”好处在于快速，再扩充了用户基本属性的同时还能够在规定的时间内完成所有的重排序计算，但是缺点在于一定程度上降低了用户特征刻画的精度。</p>
<p><strong>数据融合</strong></p>
<p>在实际的应用过程中，我们发现，在一定程度上交叉部分高价值的用户特征有助于提高最后的预测结果的准确性，构造的框架图如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/18.jpeg" alt=""></p>
<p>这边借鉴但是没有完全采用wide&amp;deep的方法，借鉴了对原始用户特征需要通过embedding layer进行处理，比如通过简单的one hot encoding的形式，然后采取特征交叉的方式获取新的用户特征，最后再进行前向传播或者logistics regression；但是此处，在embedding layer的过程中会采取人为限制分箱逻辑去噪（剔除了比如地点归属000ex00f这样的错误数据），在交叉过程中只选取了部分对最后的用户注册影响较大的因素进行交叉，在提升了模型对用户拟合的能力的同时也保证了模型的实效性。</p>
<p><strong>数据流设计</strong></p>
<p>简单的数据执行流如下：<br><img src="/2018/03/04/yoho!buy注册概率预估/19.jpeg" alt=""></p>
<p>主要步骤如下：</p>
<ul>
<li>Kafka+Flume解析实时点击、搜索、浏览等用户操作日志流，在线进行用户操作数据的抽取</li>
<li>实时解析基础用户环境信息，获取环境特征：手机型号，网络，地址等，实时写入到线上Hadoop/Spark中的HDFS里</li>
<li>根据离线存储在HDFS中的用户操作数据、用户点击流数据和用户是否真实注册的结果离线更新循环网络GRU及LR模型参数</li>
<li>将新的模型参数应用于线上用户数据的预测</li>
</ul>
<p>最后可得到个人及全站的注册概率变化可视化如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/20.png" alt=""></p>
<h1 id="可优化方向"><a href="#可优化方向" class="headerlink" title="可优化方向"></a>可优化方向</h1><ul>
<li>GRU卷积神经网络的层数优化，由多层隐藏层替代单层隐藏层提高对用户行为的汇总效果</li>
<li>Logistics Regression部分可以由多模型bagging替换，降低过拟合的可能</li>
<li>反馈数据清洗，对于有强烈意愿注册的用户进行识别，避免干扰正样本池</li>
<li>推荐内容干扰，那些热门爆款更多的用户看到，而且“被看到”这个行为也加深了它接下去被接连看到的可能性</li>
<li>GRU卷积神经网络的构造中，修改上一次操作预测下一次操作为上一次操作预测<strong>目标行为</strong>（常停留时长的点击、收藏点击等等高价值的行为节点）</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>传统的机器学习方案给用户行为预估的项目一个基准水平线，而深度学习的出现，一定程度上使得这个上限有所提高，但是以数据为基础，用算法去雕琢，只有将二者有机结合，才会带来更好的效果提升。</p>
<p>以上是yoho!buy团队在实践中的一点总结，当然我们还有还多事情要做，keep learning！最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[提升有监督学习效果的实战解析]]></title>
      <url>/2018/01/20/%E6%8F%90%E5%8D%87%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%95%88%E6%9E%9C%E7%9A%84%E5%AE%9E%E6%88%98%E8%A7%A3%E6%9E%90/</url>
      <content type="html"><![CDATA[<p><img src="/2018/01/20/提升有监督学习效果的实战解析/1.png" alt=""><br><a id="more"></a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近很长时间没有和大家分享东西了，最近一直在忙公司的项目，先说一声抱歉。</p>
<p>之前写过<a href="http://shataowei.com/2017/11/01/交叉销售算法/" target="_blank" rel="noopener">销售预估算法</a>,但是被诸多大佬吐槽有监督学习部分毫无深度，其实我是想写给一些刚入门的朋友看的，这边我boss最近也想让我总结一些相对”上档次”的一点的东西，我做了一些稍微深入一点的总结，希望能够给新人朋友有稍微深入的方法介绍。</p>
<hr>
<p>去年年末的那段时间里，看了很多天池大赛里面得高分的选手的算法思路，大概总结了有监督学习中的一些核心流程及重要细节：</p>
<ul>
<li>feature processing tricks</li>
</ul>
<p>这个是老生常谈的问题，但是我还是看到了一些不错的点，比如<strong>根据high importance feature剔除高度缺失的cases</strong>这些等等</p>
<ul>
<li>single feature + crossing feature</li>
</ul>
<p><strong>交叉特征组合原始特征</strong>，可以显著的提升auc，提高命中的准确程度，这边除了FM，我们也可以在常规的算法中去实现这个trick</p>
<ul>
<li>有监督学习架构思路</li>
</ul>
<p>下面，我们来看看针对每个点，具体是如何实现的，及我们需要注意哪些相关的东西：</p>
<h2 id="feature-processing-tricks"><a href="#feature-processing-tricks" class="headerlink" title="feature processing tricks"></a>feature processing tricks</h2><h3 id="case-and-feature-selection"><a href="#case-and-feature-selection" class="headerlink" title="case and feature selection"></a>case and feature selection</h3><p>我们在做模型训练之前通常会对模型的feature做一些删减，比如共线性检验，去除掉相似度过高的连续feature；比如变异度检验，去除掉一些数据变化差异过小的feature等等。然而，在常规的样本处理中，我们通常只会根据初始的数据分布去看，比如用户在feature上缺失大于来某个阈值才回去剔除这个用户；其实，在深入的思考一下这个问题，会发现，如果用户在高重要性的feature缺失程度高去剔除才更合理一些，这样想可能不是很清晰，这边看下面这个feature flow：</p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/2.png" alt=""></p>
<p>针对uid来看，如果普通的统计的话，uid3的null的个数5个，uid5的null的个数4个，我们应该优先剔除uid3，再考虑剔除uid5，因为null过多的用户所能提供的信息量会相对的少，会增大泛化误差。</p>
<p>但如果我们提前知道，对于判断label的能力，feature3&gt;feature5&gt;feature6&gt;feature8&gt;其他，那么uid5在高重要性的缺失情况极度严重于uid3，所以我们应该优先剔除uid5，相对于上面一种情况，我们预先知道feature的重要性排序就显得很重要了。关于如何判断提供了几种简单的方法：</p>
<ul>
<li>方差膨胀系数：我们认为，在数据归一化之后，数据波动的更大的feature能够提供的信息量相对而言也是更大的，举个很明显的例子，如果feature1全都是1的话，它对我们判断用户是否下单这样结果毫无意义。</li>
<li>互信息：我一直认为，互信息是判断feature重要性的非常好的方法。方差膨胀系数只单纯了考虑feature本身的特征，而互信息在考虑feature的同时也考虑了label之间的关系，H(X,Y) = H(X) - H(X/Y)，这个信息量的公式很好的解释了这一点。</li>
<li>xgb’s importance：如果互信息是方差膨胀系数的进阶，那么xgb’s importance则是互信息的进阶，在考虑label与feature之间的关系的时候，同时还考虑了feature与feature之间的关系，这样得出来的重要性排序更加全面了一些。</li>
</ul>
<p>除此之外：</p>
<ul>
<li>Logistic regression的params的参数</li>
<li>Recursive feature elimination（递归参数选择方法）</li>
<li>Pearson Correlation</li>
<li>Distance correlation</li>
<li>Mean decrease impurity/Mean decrease accuracy</li>
<li>…</li>
</ul>
<p>诸如这样的方法很多，需要根据数据的形式，目标变量的形式，时间成本，效率等等综合考虑，这边只是给大家梳理一下常规的方法，至于实际使用的情况，需要大家累积项目经验。</p>
<h3 id="null-feature-treatment-method"><a href="#null-feature-treatment-method" class="headerlink" title="null-feature treatment method"></a>null-feature treatment method</h3><p>在空值或者异常值的处理上，基本上分为2个派别，要么剔除这个feature或者case，要么填充这个feature或者case，它们的缺点也显而易见，随意剔除会减少判断的信息，如果数据较少的时候，会降低模型的效果；填充的话会造成困惑，到底是众数？平均数？中位数？最大值？最小值？现在很多人的处理方法都是观察数据的分布，如果偏态分布就考虑分位数填充，如果是正态分布就考虑均值或者众数填充，相对而言，这样处理的时间成本会更高，而且很多时候解释的说服力不是很强。</p>
<p>我在看了17年3月份JD的订单预估赛，17年的天池工业赛等等的高分答案中，不得不说，有一个分箱的方法确实能够提高0.5-1.5的auc，我之前思考过，可能存在的原因：</p>
<ul>
<li>保存了原始的信息，没有以填充或者删除的方式改变真实的数据分布</li>
<li>让feature存在的形式更加合理，比如age这个字段，其实我们在乎的不是27或者28这样的差别，而是90后，80后这样的差别，如果不采取分箱的形式，一定程度上夸大了27与26之前的差异</li>
<li>在数据计算中，不仅仅加快了计算的速度而且消除了实际数据记录中的随机偏差，平滑了存储过程中可能出现的噪音</li>
</ul>
<p>这边就直接给大家分享一下我的梳理：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/3.png" alt=""><br>这边涉及到一个问题，连续数值特征是否一定要切为离散特征，建议综合考虑以下几个问题:a.所使用的算法是否为knn、svm这样的距离计算的算法b.是否在实际业务中依赖于离散判断c.连续数值特征的实际意义是否支持离散化。如果以上问题都没有问题的话，我建议优先考虑离散化连续特征，在一定程度上，离散完的feature有更好的解释意义。</p>
<h2 id="single-feature-crossing-feature"><a href="#single-feature-crossing-feature" class="headerlink" title="single feature + crossing feature"></a>single feature + crossing feature</h2><p>我们在之前的<a href="http://shataowei.com/2017/12/04/FM理论解析及应用/" target="_blank" rel="noopener">FM理论解析及应用</a>中提到过特征交叉这个概念，当时的文章中紧接着通过矩阵的计算技巧：</p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/4.png" alt=""></p>
<p>构造了全部feature的C(n,2)的形式，后面追加了线性模型，这样一定程度上可以提高分类算法的准确度。这是一个非常好的将低维特征向高维转化的方式，所以在我们其他算法的过程中也可以借鉴这种思路，但是假设我们初始的feature量特别多，比如我在日常的CTR预估或者feature梳理的过程中，很容易就整理500以上的feature集合，如果仅考虑C(n,2)的形式的话，就有250<em>499个feature的新增组合，这个是不可能接受的，所以回到我们上面一节feature processing tricks中提到的case and feature selection就是一个非常好的解决办法，我们可以先通过比如xgboost中的importance：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/5.png" alt=""><br>我这边实际的画出了我做下单概率预测时初始筛选完成后的417个feature经过xgboost初步分类后的importance，可以很明显看前37，前53，前94个feature对应了三次importance的拐点，我们可以在这些拐点中选择一个既能够涵盖绝大多数的信息量，又不会造成后续交叉特征个数过多的值，比如我这边选择的是60，那么我接下来会生成的新的的交叉feature就是30`</em><code>59个，比不做处理下的417</code>*`208要小很多倍，而且相对而言不会减少很多的信息量。</p>
<p>整体的流程我这边也画出来了，希望能够给大家一个比较清晰的认识：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/6.png" alt=""></p>
<p>可以看到，样本cases在经过了最初的空值筛选及第一轮高重要性feature后的空值筛选后，就保持不变了，而特征feature的筛选过程则贯穿了整个交叉特征生成流。</p>
<h2 id="bagging及stacking的思路架构"><a href="#bagging及stacking的思路架构" class="headerlink" title="bagging及stacking的思路架构"></a>bagging及stacking的思路架构</h2><p>我相信在读的各位，不论是机器学习从业者抑或是算法工程师甚至是其他研发工程师，一定看过类似如下的快速拖动的模块流：</p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/7.jpg" alt="机器学习工具Clementine截图"></p>
<p>它相当于把每个功能封装到一个固定的盒子中，当我们需要使用某个模块的时候，进行模块的操作，不需要的时候直接切断模块的流向即可，我们甚至可以空值每个模块的var及bias的偏向程度，在bagging和stacking的思路框架中，我非常常用的就是类似这样的思想：确定好我要进行的组合模块的组合方式（stacking还是bagging还是blending），再确定这次为想要做的子模块是什么，在根据组合形式及子模块细微调节每个子模块。</p>
<h3 id="首先，子模块可以有哪些？"><a href="#首先，子模块可以有哪些？" class="headerlink" title="首先，子模块可以有哪些？"></a><strong>首先，子模块可以有哪些？</strong></h3><ul>
<li>svm分类/回归</li>
<li>logistic分类/回归</li>
<li>神经网络分类/回归</li>
<li>xgboost分类/回归</li>
<li>gbdt分类/回归</li>
<li>xgboost叶子节点index</li>
<li>gbdt叶子节点index</li>
<li>randomforest分类/回归</li>
<li>elastic net</li>
</ul>
<p>除了这些，还有么？当然，如果你愿意的话，每一个你自己构造出来的分类或者回归的single model都可以成为你bagging或者stacking或者blending之前的子模块。</p>
<h3 id="如何训练子模块？"><a href="#如何训练子模块？" class="headerlink" title="如何训练子模块？"></a><strong>如何训练子模块？</strong></h3><p>这边的方法可谓是多种多样，百花齐放，很大程度上来讲，你在天池也好，kaggle也好，你能前十还是前十开外决定因素是你的feature处理的好坏，但是你能拿第一还是第十很大程度上就是依赖你的子模块构造及子模块组合上。这边给大家分享我最近看到的比较有意思的三个子模块形式：</p>
<p><strong>1.<a href="https://github.com/wepe" target="_blank" rel="noopener">wepon</a>的Large-Scale SVM</strong></p>
<p>读过我之前写的<a href="http://shataowei.com/2017/12/01/SVM理论解析及python实现/" target="_blank" rel="noopener">SVM理论解析及python实现</a>这篇文章的朋友应该还记得，我当时说过svm在10.7%的数据集中取得第一，算是传统的机器学习方法中非常值得一学的算法，但是实际应用中，在处理大规模数据问题时存在训练时间过长和内存空间需求过大的问题比较让人头疼，wepon同学采取的方法如下：<img src="/2018/01/20/提升有监督学习效果的实战解析/8.png" alt=""></p>
<p>这种方法看似增加了计算复杂度，实际上是却是减小的，假设原始训练数据大小是n，则在原始数据上训练的复杂度是o(n^2)，将数据集n分成p份，则每份数据量是(n/p)，每一份训练一个子svm，复杂度是o((n/p)^2)，全加起来o(n^2/p)，复杂度比在原始数据上训练减小了p倍。变向的解决了在量大的数据集合上使用svm，提高速度同时保证质量这个问题。论文支持建议参考<a href="http://jmlr.org/papers/volume15/claesen14a/claesen14a.pdf" target="_blank" rel="noopener">Ensemble SVM</a>。</p>
<p><strong>2.<a href="https://mp.weixin.qq.com/s/lUP2BehOh7KczR3WRnOqFw" target="_blank" rel="noopener">爱奇艺 Gbdts’ Node Leafs</a></strong></p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/9.png" alt=""></p>
<p>我们分别来解释一下左右的Dense features 和 Spare Features。</p>
<p>首先，左侧这块很好理解，在<a href="http://shataowei.com/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/" target="_blank" rel="noopener">上一次的文章中</a>,我们已经讲了如何利用xgboost或者gbdt获得用户的数据落在的每棵树上面的叶子节点的index值：<img src="/2018/01/20/提升有监督学习效果的实战解析/10.jpeg" alt=""><br>如果有不清楚的同学，请回顾一下上次讲的内容。</p>
<p>右侧这块分别写了user preference 和video content，当然这是因为它是视频公司的原因，在我实际的使用中，我用的是user preference 和 item content，这里的preference和content其实就是你个人信息及行为的向量化的形式。</p>
<p>最简单的表示就是把你的基本信息和item信息先onehotencoding，再首尾相接成一个超长的vector，这就是一个稀疏的Spare Features。</p>
<p>当然除了这种粗暴的办法，还有比如我们在若干天之前讲过的<a href="http://shataowei.com/2017/08/19/深度学习下的电商商品推荐/" target="_blank" rel="noopener">深度学习下的电商商品推荐</a>中的word2vec的技巧，先将所有的用户随机生成为我们需要的长度N维下一一对应的向量，在通过huffman编码的形式找到每个item对应的Huffman树子的唯一路径，再通过在每个节点上生成一个logsitic分类的办法，使得所有该路径成立的概率最高，以此来修正我们最初随便生成的N维向量，最后这个N维向量就可以看作是一个Spare Features。</p>
<p>还有么？当然，我私下问了我之前在该公司任职的同学，他们还有一种思路就是划分数据集到M个子集，每个子集上面生成一个xgboost，然后每个子集取xgboost的叶子节点，相当于把左侧的Dense features复制了M份Dense features放在了右边的Spare Features，最后会得到一个M+1个Dense features。实际使用起来的效果完全不比word2vec的结果差。</p>
<p><strong>3.基于GRU的潜藏层</strong></p>
<p>Domonkos Tikk和Alexandros Karatzoglou在《Session-based Recommendations with Recurrent Neural Networks》文章中提到了可以用循环神经网络RNN来预估用户的行为，如下图：</p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/11.png" alt=""></p>
<p>我们可以清晰的看到，针对每个用户Session1，他的行为由i1.1变化至i1.4其实是一个有序的过程，我们可以设计一个从i1.1—-&gt;i1.2,i1.2—-&gt;i1.3,i1.3—-&gt;i1.4这样的一个循环流程。同时在他的文章中还解释了这样的设计解决的两个问题：</p>
<ul>
<li>the length of sessions can be very different</li>
<li>breaking down into fragments</li>
</ul>
<p>一来通过了首尾相接，解决不同用户的session不同长度；二来通过了embedding layer，解决了不完整session下预测的可能。具体网络设计如下：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/12.png" alt=""></p>
<p>模型的更新流可以参考下面：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/13.png" alt=""></p>
<p>我们只需要拿到每个用户的item流下所对应的state即可，这state就包含了这个用前M次的操作潜藏信息，同时我们还可以随意定义这个信息向量的长度，这个就可以看作用户状态向量，作为子模块的输出。</p>
<p>这个思路的缺点就是，要预测的基础数据不存在时序性，效果极差。比如滴滴打车的下单过程，从登陆到打到车的时间最短在20s，最长在1分钟，否则用户就退出了app，这样的情况下，时序性质就显得格外薄弱，强行用这样的RNN获得的用户属性非常不存在代表性。</p>
<h3 id="如何组合子模块？"><a href="#如何组合子模块？" class="headerlink" title="如何组合子模块？"></a>如何组合子模块？</h3><h4 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h4><p><img src="/2018/01/20/提升有监督学习效果的实战解析/14.png" alt=""></p>
<p>这个是我们<a href="http://shataowei.com/2017/12/30/Kaggle-TianChi分类问题相关纯算法理论剖析/" target="_blank" rel="noopener">Kaggle&amp;TianChi分类问题相关纯算法理论剖析</a>就强调过的bagging的最简单的形式，在每个子模块的设计选择过程中要尽可能的保证：</p>
<ul>
<li>low biase </li>
<li>high var</li>
</ul>
<p>也就是说子模块可以适当的过拟合，增加子模型拟合准确程度，通过加权平均的时候可以降低泛化误差</p>
<h4 id="stacking"><a href="#stacking" class="headerlink" title="stacking"></a>stacking</h4><p><img src="/2018/01/20/提升有监督学习效果的实战解析/15.png" alt=""></p>
<p>这个是我们<a href="http://shataowei.com/2017/12/30/Kaggle-TianChi分类问题相关纯算法理论剖析/" target="_blank" rel="noopener">Kaggle&amp;TianChi分类问题相关纯算法理论剖析</a>就强调过的stacking的最简单的形式，在每个子模块1、子模块2的设计选择过程中要尽可能的保证：</p>
<ul>
<li>high biase </li>
<li>low var</li>
</ul>
<p>在子模块3的时候，要保证：</p>
<ul>
<li>low biase </li>
<li>high var</li>
</ul>
<p>也就是说，在子模块1，2的选择中，我们需要保证可稍欠拟合，在子模块3的拟合上再保证拟合的准确度及强度</p>
<h4 id="blending"><a href="#blending" class="headerlink" title="blending"></a>blending</h4><p>我们知道单个组合子模块的结果不够理想，如果想得到更好的结果，需要把很多单个子模块的结果融合在一起：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/16.png" alt=""><br>这种方法也可以提高我们最后的预测的效果。</p>
<p>关于有监督学习的方法大概就梳理到这边，最后希望能够给一些新人同学对有监督的理解和实战有一些帮助。</p>
<p>没啥广告要打，就这样吧。</p>
<hr>
<p><em>Reference:</em><br><em>[1] 周志华。《机器学习》，清华大学出版社，3.7，2016</em><br><em>[2] wepon。 《<a href="https://github.com/wepe/PPD_RiskControlCompetition" target="_blank" rel="noopener">PPD_RiskControlCompetition</a>》</em><br><em>[3] 爱奇艺技术产品团队。 《<a href="https://mp.weixin.qq.com/s/lUP2BehOh7KczR3WRnOqFw" target="_blank" rel="noopener">爱奇艺个性化推荐排序实践</a>》</em><br><em>[4] slade。 《<a href="http://shataowei.com/2017/12/30/Kaggle-TianChi分类问题相关纯算法理论剖析/" target="_blank" rel="noopener">Kaggle&amp;TianChi分类问题相关纯算法理论剖析</a>》</em><br><em>[5] E Cernadas，D Amorim。 《<a href="http://xueshu.baidu.com/s?wd=paperuri%3A%28491eb32b0997a8dde16c12fe69bf3eac%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2697065%26amp%3Bdl%3Dacm%26amp%3Bcoll%3Ddl%26amp%3Bpreflayout%3Dflat&amp;ie=utf-8&amp;sc_us=5730949819175219868" target="_blank" rel="noopener">Do we need hundreds of classifiers to solve real world classification problems?</a>》</em><br><em>[6] slade。 《<a href="http://shataowei.com/2017/08/19/深度学习下的电商商品推荐/" target="_blank" rel="noopener">深度学习下的电商商品推荐</a>》</em><br><em>[7] Domonkos Tikk，Alexandros Karatzoglo。 《Session-based Recommendations with Recurrent Neural Networks》</em><br><em>[8] slade. 《<a href="http://shataowei.com/2017/12/04/FM理论解析及应用/" target="_blank" rel="noopener">FM理论解析及应用</a>》</em></p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 模型设计 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kaggle&TianChi分类问题相关纯算法理论剖析]]></title>
      <url>/2017/12/28/Kaggle&amp;TianChi%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9B%B8%E5%85%B3%E7%BA%AF%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E5%89%96%E6%9E%90/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/1.png" alt=""></p>
<a id="more"></a>
<hr>
<p>17/12/30-update ：很多朋友私密我想要代码，甚至利用金钱诱惑我，好吧，我沦陷了。因为原始代码涉及到公司的特征工程及一些利益trick，所以我构造了一个数据集后复现了部分算法流程，需要看详细代码实现朋友可以移步<a href="https://github.com/sladesha/machine_learning/tree/master/Ensemble" target="_blank" rel="noopener">Ensemble_Github</a></p>
<hr>
<p>更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过stw386@sina.com联系我，知无不答。</p>
<hr>
<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><p>在<a href="http://shataowei.com/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/" target="_blank" rel="noopener">上一次的文章</a>中，我们讲了，如何快速的利用bagging、boosting、stacking、ensemble的形式实现一个分类算法，当时我们直接看了代码以及核心的理论注意点。如果需要有更加优异的结果表现，对整套算法的设计及相关的理论了解是必不可少的。本文将从数学、工程、领域经验的角度去剖析如何用好bagging、boosting、stacking、ensemble去训练一个相对完善的模型。</p>
<p>再次提醒，本文中的数据公式较多，抽象概念较多，需要一定的高等代数、泛函分析、机器学习基础作为前置条件。如果你只需要知道如何运行或者完成分类识别，请参考<a href="http://shataowei.com/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/" target="_blank" rel="noopener">Kaggle-TianChi分类问题相关算法快速实现</a>。如果需要更详尽的理论解析或者有哪些地方不明白的同学，建议私下联系我stw386@sina.com。如果你想skip read本文，请直接阅读最后一个小节：调参流程梳理。</p>
<p>那么接下来让我们开始正文，虽然本文写的很冗长，我依旧建议阅读完此文，即便是处于懵懂的状态，对后续模型调整的理解也是有一定益处的，而且我会尽可能的用通俗易懂的语言来讲，很多地方会存在解释不严谨的地方但是更易于理解。</p>
<h1 id="Bias-Variance-Tradeof"><a href="#Bias-Variance-Tradeof" class="headerlink" title="Bias-Variance-Tradeof"></a>Bias-Variance-Tradeof</h1><p>在上次的文章中，我们就提到了一个好的模型应该有着非常好的拟合能力，就是说我的偏差要尽可能的小；同时，也要保证方差尽可能的小，这样我们才能在泛化能力上有很不错的表现。</p>
<p>设样本容量为n的训练集为随机变量的集合(X1, X2, …, Xn)，那么模型是以这些随机变量为输入的随机变量函数（这边的F虽然上函数，但是也是随机变化的）：F(X1, X2, …, Xn)。抽样的随机性带来了模型的随机性。那如何定义一个模型的Bias和Variance呢？这边我们采取的是基模型的加权均值E(F)=E(∑(γ<code>*</code>fi))移动来替代bias、基模型的加权方差Var(F)=Var(∑(γ<code>*</code>fi))替代Variance，更详细的数学定义如下：<br><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/2.png" alt=""><br>这边在Variance推导过程中用到了这个性质：<br>Cov(X<code>*</code>Y) = E(X<code>*</code>Y) - E(X)<code>*</code>E(Y)，同时将方差拆分成协方差的形式。我们可以看到，组合后的模型的Bias和基模型的Bias的权重γ相关，组合后的模型的Var和基模型的权重γ、基模型个数m、基模型相关性ρ相关。<br><strong>请务必深刻的记得上述bias和var的数学式子，在后续无论是调参数还是模型设计，我们都是围绕着，降低bias和var的角度去做的。</strong></p>
<h2 id="Bagging-Bias-and-Variance"><a href="#Bagging-Bias-and-Variance" class="headerlink" title="Bagging Bias and Variance"></a>Bagging Bias and Variance</h2><p><strong>很多同学在没看这篇文章之前就知道，bagging算法和stacking算法是需要基模型保持强基模型（偏差低方差高）的</strong>，但是不知道大家有没有想过为什么？阿里15年校招的时候，我有幸回答过一个题目就是这个问题，下面让我们看看为什么是这样的？</p>
<p>对于bagging算法而言，每次的抽样都是以尽可能使得基模型相互独立为前提的，为了维持这样的假设，我们做了三件事：</p>
<ul>
<li>样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样</li>
<li>子抽样：整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量</li>
<li>弱抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）下的feature的抽样</li>
</ul>
<p>同时，由此我们由此也可以得到bias和var公式中的γ=1/m，基模型的权重一定程度是可以看作是相等的，<br>所以原来的E和Var公式就变成：<br><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/3.png" alt=""><br>组合模型F的bias和基模型fi的bias一致，这就是我们为什么要求基模型fi的bias要低的原因，因为组合模型F的拟合能力E(F)不随着基模型个数的增加而上升。</p>
<p>组合模型F的var与基模型fi的var、基模型fi的个数m、基模型的相关性ρ相关，很明显可以看出，随着基模型的个数上升Var(F)第二项是在下降的，所以基模型的个数上升会降低组合模型的方差，这就是为什么基模型的方差可以高一些。</p>
<p>除此之外，我们还可以看出，如果基模型相关性ρ越低，整体的方差是越小的，所以我们才去做样本抽样，子抽样，弱抽样等等行为。还有，基模型的个数上升一定程度上会降低组合模型F的方差，但是不是无限递减的，公式中它只能降低第二项的方差值，第一项的方差值不随基模型的个数而增减。</p>
<p>从这个角度看，是不是对Bagging算法的理解又深刻了一些？接下来让我们看看Boosting。</p>
<h2 id="Boosting-Bias-and-Variance"><a href="#Boosting-Bias-and-Variance" class="headerlink" title="Boosting Bias and Variance"></a>Boosting Bias and Variance</h2><p>依旧的是<strong>很多同学在没看这篇文章之前就知道，boosting算法是需要基模型保持弱基模型（偏差高方差低）的</strong>，让我们一探究竟。<br><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/4.png" alt=""><br>熟悉boosting算法的同学都知道，boosting算法的基模型的相关性几乎≈1的，后续模型强依赖于前模型，所以我们可以认为ρ=1，得到如上的简化式子。组合模型F的bias是基模型的bias的累加，基模型的准确度都不是很高（因为其在训练集上的准确度不高），随着基模型数的增多，整体模型的期望值增加，更接近真实值。而站在方差的角度，组合模型F的方差是随着基模型的fi个数上升而平方上升的，这就要求我们的基模型的方差不能太高，否则组合模型的F就会增长爆炸。这就是为什么我们在boosting模型设计的时候，需要基模型保持弱模型（偏差高方差低）的原因。</p>
<p>多说一句，大家也看到了，boosting的Var(F)是依赖于基模型的权重γ的，所以在后续的gbdt、xgboost，各位数据科学家选择了类似bagging的采样模式，降低模型的γ，控制方差，所以说，了解原理再去重新或者优化还是很重要的。</p>
<h2 id="Bagging、Boosting、Stacking-Bias-amp-Variance总结"><a href="#Bagging、Boosting、Stacking-Bias-amp-Variance总结" class="headerlink" title="Bagging、Boosting、Stacking Bias&amp;Variance总结"></a>Bagging、Boosting、Stacking Bias&amp;Variance总结</h2><p>纵观刚才说了的这么多，我们在Bagging、Boosting、Stacking的模型设计中所围绕的就是降低bias的同时，降低Variance。而我们所做的sklearn里面的参数调整就可以说用来：</p>
<ul>
<li>a.构建基模型，变化基模型的Bias和Variance </li>
<li>b.组合模型构建，控制基模型后，如何把这个基模型很好的组合成一个优秀的ensemble模型。</li>
</ul>
<h1 id="GBDT-理论剖析"><a href="#GBDT-理论剖析" class="headerlink" title="GBDT 理论剖析"></a>GBDT 理论剖析</h1><h2 id="模型过程推导"><a href="#模型过程推导" class="headerlink" title="模型过程推导"></a>模型过程推导</h2><p>其实random forest和gbdt、xgboost都是非常好的Bagging、Boosting、Stacking算法的优化升级版本，我个人用的gbdt稍多，所以就以gbdt为例子给大家梳理一遍，从理论，到调参数，到trick分享。</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/5.png" alt=""></p>
<p>我最讨厌很多博主贴个上面的伪代码就跑了，我念书的时候，老师讲题目也是，我靠，我要是看得懂还需要你贴？所以，我们选择一步一步的来看这个伪代码。</p>
<p>让我们先概览一下整个流程，gbdt有递归设计如下：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/6.jpeg" alt=""></p>
<p>如果y(i)代表着第i个基模型，第i+1个基模型其实是基于第i个基模型的结果而追加了一个New function去修正前i个基模型的误差。如果以F代替y，h代替f的话，我们可以得到下面这个递归函数：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/7.png" alt=""></p>
<p>第i个基模型是由前i-1个基模型中的h(x)累计得到的，我们最后想要得到的分类器即为：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/8.png" alt=""></p>
<p>每一轮迭代中，只要集中精力使得Fi(x)每次loss下降的最快即可：每次构建一个残差负梯度作为更新方向的New function(即hi(x))，就可以解决这个复杂的递归问题，从而得到F(x)的解析式。</p>
<p>接下来，我们再看看更加详细的做法：</p>
<ul>
<li>初始化部分，在这次梳理之前，我也一直认为是随机构造的，这边看完伪代码我才知道，在初始值设置的时候，考虑了直接使得损失函数极小化的常数值，它是只有一个根节点的树，即是一个c常数值。</li>
</ul>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/9.png" alt=""></p>
<ul>
<li>构建回归树，这边就稍许复杂，让我们拆开一步一步来看：</li>
</ul>
<p><strong>首先，先求整体损失函数的反向梯度。</strong>先举个mse的例子，如果现在我们考虑的是<strong>mse(着重注意，只有mse的情况下以下的梯度才是这样的)</strong>的形式，我们要做的就是在每一步的时候让我们的预测值Fi(x)与真实值y的损失函数：1/2<code>*</code>(y-Fi(x))^2最小(前面的1/2是为了方便求导计算加上去的)，如果对梯度下降有了解的朋友就知道，此刻要求它的最小就是去求偏导：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/10.png" alt=""></p>
<p>按照这个方向去更新Fi(x)，可以保证，组合模型F(x)的每次都是按照最优的方向去优化。</p>
<p>MSE的损失函数确实是残差形式，不代表所有的损失函数下更新的方向都满足这样的残差形式。<strong>kaggle master 在<a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/" target="_blank" rel="noopener">blog</a>里面提到Although we can minimize this function directly, gradient descent will let us minimize more complicated loss functions that we can’t minimize directly</strong>，我们就需要设计出一种更快速能提升泛化能力且不失一般性的解决方案，所以有大神提出了以梯度下降的值直接代替因变量y，也就是我每次预测不去比预测值与真实值y的差异，我们比较的是预测的梯度方向与真实的梯度方向。</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/11.jpeg" alt=""></p>
<p><strong>再基于已经预估出来的负梯度去计算最优的更新步长，使得预测值与真实值更加靠近</strong>，因为每层的负梯度的方向不固定，所以每层i的步长都是变化的。</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/12.png" alt=""></p>
<p>最后通过缩减率v（这边就是类似logistics里面的∂）控制速率。</p>
<p>综上，假设test集合第i轮预测中，根据训练集训练出来的负梯度拟合模型不妨记为fi(x)、最优步长γi、缩减率v，可得到最终的递归公式为：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/13.png" alt=""></p>
<h2 id="损失函数介绍"><a href="#损失函数介绍" class="headerlink" title="损失函数介绍"></a>损失函数介绍</h2><p>刚才上面我举了一个mse作为损失函数的例子，其实还有很多其他的，参考如下：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/14.png" alt=""></p>
<p>这边说个有意思的东西，就是如果有兴趣的朋友可以把exponential：指数损失函数计算一下，反向梯度为：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/15.png" alt=""></p>
<p>则有第i轮损失函数：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/16.png" alt=""></p>
<p>这货就是adaboost的第i轮损失函数的非归一化的结果，是不是很有趣，虽然知道了没啥用，但是起码得到了我们在用gbdt的时候，loss=’exponential’即为adaboost的结论啊，哈哈～所以说，我觉得去推推公式，还是很有意思的。</p>
<p>到此为止，gbdt是怎么构造得来的就讲完了，其实这个和bias&amp;variance关系不大，但是为了铺垫后续的GBDT实战剖析，我觉得还是非常有必要梳理一遍的，但就比起Bias-Variance-Tradeof这节的内容，我觉得各位还是着重理解Bias-Variance-Tradeof，这节可以看作是’甜品’缓解下气氛。</p>
<h1 id="GBDT-实战剖析"><a href="#GBDT-实战剖析" class="headerlink" title="GBDT 实战剖析"></a>GBDT 实战剖析</h1><p>我们以python下的sklearn.ensemble中的GradientBoosting及RandomForest为例子，实战分析一下，如何能够理性的调好参数而并非玄学的gridsearch。</p>
<p>在Bias-Variance-Tradeof我们提到了参数设置分为两块：a.构建基模型,b.构建组合模型，我们分GradientBoosting参数如下：</p>
<h2 id="构建组合模型："><a href="#构建组合模型：" class="headerlink" title="构建组合模型："></a>构建组合模型：</h2><p>1) n_estimators:<br>基模型的个数，对于gbdt来说，因为我们需要通过基模型的个数来提升准确率所以n_estimators一般都会大于random forest的n_estimators的个数，实际上RandomForestClassifier默认为10，GradientBoostingClassifier默认为100也证明了这点。</p>
<p>2）learning_rate：<br>步长，对于gbdt来说，步长依赖于n_estimators，100=2<code>*</code>50=5<code>*</code>20，就是这个道理。而random forest里面不存在步长这个概念。在gbdt里面，关于步长的优化一般是伴随着基模型的变化而变化的。</p>
<p>3）subsample：<br>子采样率，还记得我们上面对子采样的描述么？一般来说，降低“子采样率”（subsample），也会造成子模型间的关联度降低，整体模型的方差减小，但是当子采样率低到一定程度时，子模型的偏差增大，将引起整体模型的准确度降低。</p>
<p>4) init：初始化，更多见GBDT 理论剖析中，我们对初始化的描述。</p>
<p>5）loss：<br>对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。<br>分类模型不说了，刚才在GBDT 理论剖析中讲了，一般用”deviance”比较多；回归模型中，”ls”我们也在GBDT 理论剖析中讲了，异常点多的情况下”huber”,训练集分段预测的话用”quantile”，但是我个人建议异常点或者分段预测还是在数据已处理中完成。</p>
<p>6) alpha：这个参数只有Huber损失”huber”和分位数损失”quantile”下的GradientBoostingRegressor，alpha越小对噪声处理的力度越强，alpha越小分位数的值越小。</p>
<h2 id="构建基模型："><a href="#构建基模型：" class="headerlink" title="构建基模型："></a>构建基模型：</h2><p>1）max_features：<br>每次划分最大特征数，有log2，sqrt，None等等，默认的是sqrt，该值越小，我们每次能获得信息越少，造成偏差时变大的，同时方差是变小的，所以当我们模型拟合能力不足的时候，可以考虑提升该值。</p>
<p>2）max_depth:<br>基模型最大深度，深度越大，模型的拟合能力越强，bias越小。根据Bias-Variance-Tradeof我们对bagging和boosting里面的Var和Bias的描述可知，如果在boost（gbdt）采用了过深的基模型，组合模型的var会很大，在泛化能力会降低，造成训练集效果优秀，测试集差；如果在bagging（random forest）采取了过浅的基模型，组合模型的拟合能力会不足，我们可以考虑增加深度，甚至不控制生长。</p>
<p>3）min_samples_split：<br>内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。随着分裂所需的最小样本数的增加，子模型的结构变得越来越简单，极端情况下，方差减小导致整体模型的拟合能力不足。</p>
<p>4）min_weight_fraction_leaf：<br>叶节点最小权重总值，这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和其他子叶节点一起被剪枝，会使得模型变得简单，降低了方差，提高了偏差，如果正负样本不一致，需要考虑调整这个值。</p>
<p>5）max_leaf_nodes：<br>最大叶子节点数，通过限制最大叶子节点数，可以防止过拟合，会使得模型变得简单，降低了方差，提高了偏差。<strong>这边需要注意如果设置了max_leaf_nodes，会忽略max_depth参数值</strong>。</p>
<p>梳理完以上每个参数的对模型拟合的能力及对Vaild集合泛化能力的影响，我们可以根据项目训练中，实际模型的训练集拟合效果，检验集的泛化效果进行优化参数。</p>
<h2 id="调参流程梳理"><a href="#调参流程梳理" class="headerlink" title="调参流程梳理"></a>调参流程梳理</h2><p>ok，问题来了，很多同学看了之后说，你说了这么多参数作用，你还是没告诉我改如何调参数？我就喜欢这种很关注结果的同学，以下干货来自个人及个人朋友及我从知乎等网站”剽窃”来的观点，不负任何理论责任。</p>
<p>我第一任老大，现在在阿里做算法专家，他根据24个数据集合上以不同的调参流程去训练相同的测试集得出的效果对比，总结出以下一个流程：</p>
<ul>
<li>先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值</li>
<li>再确定合适的subsample</li>
<li>再调优最大树深度（max_depth）</li>
<li><strong>再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）</strong>,这边是不一定使用的</li>
<li>再调优最大叶节点数（max_leaf_nodes）</li>
<li>再组合优化分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf） </li>
<li>最后，优化分裂时考虑的最大特征数（max_features）</li>
<li>组合调整n_estimators和learning_rate</li>
</ul>
<p>但是，我今年在逛知乎的时候偶然看到一个帖子，里面讲的就是调参数的困扰，提到了一个点，就是先确定了max_depth=3后，无论怎么优化min_samples_split和min_samples_leaf对结果都没有任何影响了。当时我想了很久，最后是一位知友解答了这疑惑，其实这样的：</p>
<p>假设原始数据中正负样本比是1:1000，在做max_depth=3的时候，因为样本不均衡，已经可以通过非常简单的少量feature对正负样本进行区分，所以，在之后怎么调节分裂所需要最小样本树和子节点最小样本数都不能够影响到回归树的构造，然而该区分的回归树是没有泛化能力的。</p>
<p>要解决这个问题要么平衡数据，要么就是先确定回归决策树每个叶子结点最小的样本数(min_samples_leaf),再确定分裂所需最小样本数（min_samples_split），才能确定最大深度,这样就能保证不会出现某棵树通过一个feature将数量较少的的正类以较过拟合的简单浅层树拟合出来，而是优先保证了每一次我构造树都尽可能的平衡满足了数据量合理，数据具有样本具有代表性，不会过拟合这样的假设。所以，可以优化为：</p>
<ul>
<li>先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值</li>
<li>再确定合适的subsample</li>
<li>再组合调优最大树深度（max_depth）和叶节点最小样本数（min_samples_leaf） </li>
<li>再调优最大叶节点数（max_leaf_nodes）</li>
<li><strong>再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）</strong>,这边是不一定使用的</li>
<li>再组合优化分裂所需最小样本数（min_samples_split）</li>
<li>最后，优化分裂时考虑的最大特征数（max_features）</li>
<li>组合调整n_estimators和learning_rate</li>
</ul>
<p>去年Aarshay Jain大神总结的<a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/" target="_blank" rel="noopener">调参数整理</a>也给出了一种调优思路：</p>
<ul>
<li>优先，调整最大叶节点数和最大树深度</li>
<li>其次，分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf）及叶节点最小权重总值（min_weight_fraction_leaf）</li>
<li>然后，分裂时考虑的最大特征数（max_features）</li>
</ul>
<p>容我多嘴一句，我们思考了这么多，其实如果能在最开始做一个正负样本平衡就会避免很多问题，所以，再次强调数据预处理的重要性。</p>
<p>除此在外，很多人会选择在以上模型调优结束后再以10<code>*</code>learning_rate进行”鞍点逃逸”，以0.1<code>*</code>learning_rate进行”极限探索”。至于random forest及xgboost的更多调参数的细节与gbdt类似，我就不赘述了，有问题可以问我。</p>
<p>终于结束了，这篇文章真的是又繁琐又冗长，希望能够给一些同学对gbdt更深刻的理解。</p>
<p>没啥广告要打，就这样吧。</p>
<p><em>另求一个比较好的公式编辑器，鬼知道我现在在excel里面写完公式截图过来有多扯淡，而且图片质量超差，谢谢了。</em></p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 模型设计 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[风控用户识别方法]]></title>
      <url>/2017/12/09/%E9%A3%8E%E6%8E%A7%E7%94%A8%E6%88%B7%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/09/风控用户识别方法/1.jpg" alt=""><br><a id="more"></a></p>
<p><strong>update:<br>18.1.1 :<a href="https://github.com/sladesha/Frcwp" target="_blank" rel="noopener">Frcwp</a>已如期上线，满足本文中的所有方法，欢迎拍砖</strong></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>因为工作方向相关，之前我也尝试着在Google、arXiv、wikipedia等等地方搜一些风控识别的资料或者思路，但是事与愿违的是，绝大多数的与风控算法都毫无关系，基本上都是推销自己家的产品的，所以，我之前也尝试着写了一些方法的梳理，如:</p>
<ul>
<li><a href="http://shataowei.com/2017/12/01/多算法识别撞库刷券等异常用户/" target="_blank" rel="noopener">多算法识别撞库刷券等异常用户</a></li>
<li><a href="http://shataowei.com/2017/08/09/数据预处理-异常值识别/" target="_blank" rel="noopener">异常值识别与处理</a></li>
</ul>
<p>但是在我前几天再回过头去看自己写的这些东西的时候，作为一个老司机来说，我都不想去看一篇又一篇动则上千字的文章，理论交错，文笔粗陋，正巧现在公司内部也有一个风控的项目，所以，我准备做一个开源的项目<strong><a href="https://github.com/sladesha/Frcwp" target="_blank" rel="noopener">Frcwp</a></strong>，核心在于：</p>
<ul>
<li>简单操作，几乎不用多少调参，自动识别异常点</li>
<li>理论清晰，支持的方法多，兼容性好</li>
<li>集成数据预处理的过程，减轻前置工作量</li>
</ul>
<p>“纠结”了几个朋友的情况下，一期已经完工，主要是搭建了最简单的框架，我相信，这只是一个开始，欢迎大家试用，也欢迎每一个人来批评，更希望有想法的同学一起来做这个事情。</p>
<hr>
<h1 id="接下来，让我们来讲讲，一期我们做了什么？"><a href="#接下来，让我们来讲讲，一期我们做了什么？" class="headerlink" title="接下来，让我们来讲讲，一期我们做了什么？"></a>接下来，让我们来讲讲，一期我们做了什么？</h1><p>核心我们一期做的异常点识别中，核心是利用的14年周志华教授提出的isolation forest算法进行识别，详细的理论部分请参见：<strong><a href="http://shataowei.com/2017/08/09/数据预处理-异常值识别/" target="_blank" rel="noopener">Isolation Forest</a></strong>，重复说一个事情的意义也不大。这边需要解释几点：</p>
<ul>
<li>具体是怎么得到当前的算法流程的呢？</li>
<li>为什么用当前的算法进行识别而不用其他的识别算法？</li>
<li>当前的设计下存在哪些问题？</li>
<li>未来的方向会在哪边？</li>
</ul>
<p>让我们来一一来回答这些问题。</p>
<h1 id="为了用Isolation-Forest而不用其他的识别算法？"><a href="#为了用Isolation-Forest而不用其他的识别算法？" class="headerlink" title="为了用Isolation Forest而不用其他的识别算法？"></a>为了用Isolation Forest而不用其他的识别算法？</h1><p>在设计这套算法之前，我们其实是遇到了一个实际的业务问题，<strong>黑产撞库</strong>。相信大家毫不陌生这个词，无论是阿里、京东、滴滴还是腾讯，被撞库是一件普通了不能再普通的事情，“黑产”的人从第三方渠道，获取到你历史上的手机号和一些你曾经用的密码，重复的登陆，暴力的尝试，如果你的密码设置的比较简单，比如：“123456”，“qwerty”…非常容易被破解，然后再根据你历史下单的情况，进行假冒“客服”退款，进行诈骗，百度一搜就有一堆这样的新闻：</p>
<ul>
<li><a href="http://tech.huanqiu.com/gundong/2017-11/11369192.html" target="_blank" rel="noopener">频发假冒电商客服</a></li>
<li><a href="http://www.qhnews.com/newscenter/system/2016/05/24/012012151.shtml" target="_blank" rel="noopener">当心!近期有人冒充假客服行骗 几分钟骗走市民八九万元</a><br>…</li>
</ul>
<p>所以，我们需要阻止“黑产”人员进行这样的暴力破解，获取用户的资料，由此而引发了我们对这个问题的思考。我们在对这个问题分析的时候，巧妙的发现了如下的一些信息：<br><img src="/2017/12/09/风控用户识别方法/2.png" alt=""><br>因为涉及公司机密，这边隐去了具体坐标和值，很容易发现以下问题：</p>
<ul>
<li>正常扇面内数据分布密集，未知扇面内数据分布松散，异常扇面内数据分布稀疏</li>
<li>正常扇面内的数据量占全量数据的绝大多数</li>
<li>不存在明显的分割线，正常扇面和异常扇面存在过度地带</li>
</ul>
<p>这个给了我们一些启发，我们做了如下的分析：</p>
<ul>
<li>我们观察了异常扇面内的用户黑白比，如我们预计的黑白比为20:3，也就是说分布远离大量数据点的用户绝大多数存在问题</li>
<li>为止区域的用户黑白比为1:2，这说明在黑白用户之间不存在明显的界限，有交错地带</li>
<li>正常区域内也存在黑名单用户，比例在504:1，也就是说，我们划分有一定识别能力，但是还是不能做到全量识别</li>
</ul>
<p>综合上述这些预先的处理，我们要用算法完成三件事情：<br>1.切分全量用户，做到识别出<strong>正常，未知，异常用户</strong><br>2.识别出异常用户和正常用户之间的<strong>差异约束切割</strong><br>3.在异常用户+未知用户里面，找出利用差异约束切割出黑名单</p>
<h1 id="为什么用当前的算法进行识别而不用其他的识别算法？"><a href="#为什么用当前的算法进行识别而不用其他的识别算法？" class="headerlink" title="为什么用当前的算法进行识别而不用其他的识别算法？"></a>为什么用当前的算法进行识别而不用其他的识别算法？</h1><p>切分数据的时候，我们这边采用的是切比雪夫切割。非理工科的同学可能比较疑惑什么是切比雪夫切割，这边如果数据是正态下，箱式图的Q3+3/2xQI作为上top点进行切割，大家就应该很熟悉了，其实利用的就是数据出现的概率。<br><img src="/2017/12/09/风控用户识别方法/3.jpeg" alt="来源于百度百科"><br>上面这张图很好的解释了，在数据服从正态分布的情况下，出现数据值比均值+3x标准差要大的概率不足0.1%，所以，我们可以认为这些数据是异常点了。那现在出现了一个问题，日常数据分布都不一定是正态的，所以引出来了类似的切比雪夫理论，它用的是马氏距离距离中心点的程度，详细的马氏距离理论见<a href="http://shataowei.com/2017/08/09/数据预处理-异常值识别/" target="_blank" rel="noopener">马氏距离分布</a>。</p>
<p>切分完成数据之后，我们要做寻找差异约束切割逻辑。从最上面的扇面图，我们很容易发现，正常数据与异常数据之间的密度差异很明显，所以如何识别密度差异的算法就是我们需要的，这边我大概找了6、7种常见的切分方法，这边主要讲三种：isolation forest，lof，distance similarity。理论我之前也讲过，贴上地址，不废话了：<a href="http://shataowei.com/2017/12/01/多算法识别撞库刷券等异常用户/" target="_blank" rel="noopener">密度算法</a>。这边主要展示效果差异：<br><img src="/2017/12/09/风控用户识别方法/4.png" alt=""><br>通过68个数据集，很明显的可以看出LOF的识别出来的用户的异常用户异常程度是低于Isolation Forest和Distince Similarity的，起码在我们这些数据集样本中，Isolation Forest和Distince Similarity识别效果差异不大，所以，我们再考虑了另一个性能问题：<br><img src="/2017/12/09/风控用户识别方法/5.png" alt=""><br>我们用了CV=10的交叉检验，发现，平均下来，Isolation Forest识别速度是Distince Similarity的1/3以下。综合上述，还有一些其他因素，最后我们选择了Isoation Forest的方法。</p>
<h1 id="当前的设计下存在哪些问题？"><a href="#当前的设计下存在哪些问题？" class="headerlink" title="当前的设计下存在哪些问题？"></a>当前的设计下存在哪些问题？</h1><p>上面说的都是比较正面的问题，让我们看看，有哪些缺点。<br>首先，从头到尾，我们一直在围绕密度差异这个问题，但是就我平时做的一些小爬虫都知道，降低暴力获取的速度，慢慢搞，这时候就以上的方法就无法做到有效的识别。除此之外，因为我们用了切比雪夫不等式，所以对其有概念的同学知道，算马氏距离的时候需要算协方差矩阵，当数据量异常异常大(我测算的是12mx100)的时候计算资源紧张，可能算不出来；数据量异常异常小的时候feature严重共线性，也可能计算不出来。</p>
<h1 id="未来的方向会在哪边？"><a href="#未来的方向会在哪边？" class="headerlink" title="未来的方向会在哪边？"></a>未来的方向会在哪边？</h1><p>所以，后续我们会新增其他算法，支持过大过小情况下的识别方法。针对数据量过小的识别情况，我在V0.0.3版本下更新了一个简单识别的方法，之后会优化更好的算法替代掉的。只要数据量太大无法计算的问题，我之后会采取矩阵切割分块计算的方法，这个是后话了。</p>
<p>最后，我们以当前算法包的使用来结束整篇介绍：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装</span></span><br><span class="line">pip install Frcwp</span><br></pre></td></tr></table></figure></p>
<p>自动识别过程：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Frcwp <span class="keyword">import</span> Frcwp</span><br><span class="line">traindata = pd.read_table(<span class="string">'../路径'</span>)<span class="comment">#数据可以在https://github.com/sladesha/machine_learning/tree/master/data下的data_all.csv获取</span></span><br><span class="line">frc = Frcwp()</span><br><span class="line">traindata = frc.changeformat(traindata, index=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># You can define your own outlier size , the details of these params can be got from ../Frcwp/Frcwp.py:</span></span><br><span class="line">params = &#123;</span><br><span class="line"><span class="string">'na_rate'</span>: <span class="number">0.4</span>,</span><br><span class="line"><span class="string">'single_dealed'</span>: <span class="number">1</span>,</span><br><span class="line"><span class="string">'is_scale'</span>: <span class="number">0</span>,</span><br><span class="line"><span class="string">'distince_method'</span>: <span class="string">'Maha'</span>,</span><br><span class="line"><span class="string">'outlier_rate'</span>: <span class="number">0.05</span>,</span><br><span class="line"><span class="string">'strange_rate'</span>: <span class="number">0.15</span>,</span><br><span class="line"><span class="string">'nestimators'</span>: <span class="number">150</span>,</span><br><span class="line"><span class="string">'contamination'</span>: <span class="number">0.2</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># train the frc model</span></span><br><span class="line">frc.fit(traindata, **params)</span><br></pre></td></tr></table></figure></p>
<p>相关的结果显示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict outliers with the trained frc model</span></span><br><span class="line">predict_params = &#123;</span><br><span class="line"><span class="string">'output'</span>: <span class="number">20</span>,</span><br><span class="line"><span class="string">'is_whole'</span>: <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">frc.predict(frc.potentialdata_set, **predict_params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># if you want get the whole probability of your potential outliers</span></span><br><span class="line">frc.similarity_label</span><br></pre></td></tr></table></figure></p>
<p>以上部分内容截取自我的<a href="https://github.com/sladesha/Frcwp" target="_blank" rel="noopener">github</a>，希望对大家有一些帮助。</p>
<p>最后，谢谢大家的阅读，欢迎大家关注我的<a href="http://shataowei.com" target="_blank" rel="noopener">个人博客</a>。</p>
<p><strong>本文拒绝任何形式的转载，若要转载请联系stw386@sina.com</strong></p>
]]></content>
      
        <categories>
            
            <category> 开源项目 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 风控 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[FM理论解析及应用]]></title>
      <url>/2017/12/04/FM%E7%90%86%E8%AE%BA%E8%A7%A3%E6%9E%90%E5%8F%8A%E5%BA%94%E7%94%A8/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/04/FM理论解析及应用/1.jpg" alt=""><br><a id="more"></a></p>
<h1 id="FM的产生背景"><a href="#FM的产生背景" class="headerlink" title="FM的产生背景"></a>FM的产生背景</h1><p>我其实没有做过很多CTR预估的事情，但是我在工作中常常遇到CRM流失预估、订单预估这些依赖于特征工程的事情，其中就涉及到特征的组合问题。</p>
<h1 id="one-hot过程"><a href="#one-hot过程" class="headerlink" title="one-hot过程"></a>one-hot过程</h1><p>在feature选取过程中，不可避免的会出现，学历这种高中、大学、研究生等多分类的feature，在实际应用中，我们对单个feature需要进行一种one hot过程，就是将原来的学历拆解为 是否为高中，是否为大学，注意，可以不用加是否为研究生一列，因为是否为高中，是否为大学的两列已经可以推导这个用户是否为研究生，加上这一列有时候反而会共线性。但是这样做，看起来没什么问题，想想看要是100个这样的特征，每个特征有100个这样单独的feature value的话，整体数据将是一个非常庞大的稀疏矩阵，无论是计算还是分析都是会存在巨大的问题的，所以看看我们能不能组合一些特征降低维度。</p>
<h1 id="什么叫做组合问题"><a href="#什么叫做组合问题" class="headerlink" title="什么叫做组合问题"></a>什么叫做组合问题</h1><p>现在有一组数据，其中特征包含性别（男女），学历（高中，大学，研究生），想要判断这两个feature对是否对化妆品感谢兴趣。单独的观察性别这一栏，发现有一定相关性，但是比较弱，并不是所有的女性都对化妆品感兴趣；单独的观察学历这一栏也发现，学历与对化妆品感兴趣的程度并没有显著的相关性。其实，我们可以从自己的感知理解，首先，数据中女生可能比男生对化妆品更感兴趣，但是女生数据中存在大量的高中生，相对于高中生而言，大学生和研究生可能对化妆品更加感兴趣一点，所以原来的两个feature：性别，学历就组合成了是否为性别女+学历大于高中一个feature，这就是特征组合的过程。如果feature总个数少还可以，要是要有上千上万个，光两两组和就有n*(n-1)/2种可能，所以我们需要想一个其他办法。</p>
<h1 id="组合特征后的表达形式"><a href="#组合特征后的表达形式" class="headerlink" title="组合特征后的表达形式"></a>组合特征后的表达形式</h1><p>首先，我们都知道一般的线性模型为：<br><img src="/2017/12/04/FM理论解析及应用/2.jpeg" alt=""></p>
<p>为了考虑组合特征的作用，我们采用多项式来代表，形如特征xi与xj的组合用xixj表示，具体的表达式如下：<br><img src="/2017/12/04/FM理论解析及应用/3.jpeg" alt=""><br>其中，wij为组合特征xixj的权重，n表示样本的feature个数，xi为第i个feature。</p>
<h1 id="方程定义完成了，下面就要开始数学定义"><a href="#方程定义完成了，下面就要开始数学定义" class="headerlink" title="方程定义完成了，下面就要开始数学定义"></a>方程定义完成了，下面就要开始数学定义</h1><p>对每一个特征xi引入辅助向量Vi=(vi1,vi2,…vik),这边的k就是矩阵拆解的规模值，利用ViVj.T对交叉项的系数wij进行估计,<br>及<img src="/2017/12/04/FM理论解析及应用/4.jpeg" alt=""><br>则<br><img src="/2017/12/04/FM理论解析及应用/5.jpeg" alt=""><br><strong>这边需要注意一点，k理论上讲，越大越能强化拟合的能力，但是实际在运算过程中，一来受限于计算能力，二来受限于数据量，过大的k只会带来过拟合的问题。我实测了40w左右的数据，观察到k值在6-8左右，valid集合数据拟合效果最优，仅供参考</strong></p>
<p>很明显，上面这么多未知数：1+n是线性未知数个数，nxfeature是组合特征的未知数个数，常规求解的效率可想而知。但是看到xixj这样的形式，我们很容易联想到：2ab = (a+b)^2 -a^2 -b^2，所以在解决这个wij、xi、xj点积的问题上，我们采用了：1/2 * ( (a+b+c)^2  -  a^2  -  b^2  -  c^2)的方式<br><img src="/2017/12/04/FM理论解析及应用/6.jpeg" alt=""></p>
<h1 id="下面让我们来解这个式子"><a href="#下面让我们来解这个式子" class="headerlink" title="下面让我们来解这个式子"></a>下面让我们来解这个式子</h1><p>这边需要一点导数功底，我们先来看对w0也就是bias求导，这个毫无意外，梯度为1；再对wi求导，这个也很简单，xi即可，这个也很简单，少许繁琐的就是wij求导，让我来仔细看看：<br><img src="/2017/12/04/FM理论解析及应用/7.jpg" alt=""><br>ok，我知道我的字很丑，别说话，看问题，所以我们可以总结为下面这个网上到处都有的式子：<br><img src="/2017/12/04/FM理论解析及应用/8.jpeg" alt=""><br>这个式子就是上面这么来的。<br>把上面的那个点积形式代入求解及为：<br><img src="/2017/12/04/FM理论解析及应用/9.jpeg" alt=""></p>
<h1 id="引申一个FFM概念"><a href="#引申一个FFM概念" class="headerlink" title="引申一个FFM概念"></a>引申一个FFM概念</h1><p>在FM模型中，每一个特征会对应一个隐变量，但在FFM模型中，认为应该将特征分为多个field，每个特征对应每个field分别有一个隐变量。</p>
<p>举个例子，我们的样本有3种类型的字段：qualifications, age, gender，分别可以代表学历，年龄段，性别。其中qualifications有3种数据，age有5种数据，gender有男女2种，经过one-hot编码以后，每个样本有7个特征，其中只有3个特征非空。<br>如果使用FM模型，则7个特征，每个特征对应一个隐变量。<br>如果使用FFM模型，则7个特征，每个特征对应3个隐变量，即每个类型对应一个隐变量，及对应qualifications, age, gender各占一个。</p>
<p>我看了Yu-Chin Juan实现了一个C++版的FFM模型的源码，倒过来想他的表达式应该是这样的：<br><img src="/2017/12/04/FM理论解析及应用/10.jpeg" alt=""><br>其他模块都与fm差不多，主要看Vj1f2Vj2f1这个东西。我们假设j1特征属于f1这个field，j2特征属于f2这个feild，则Vj1f2表示j1这个特征对应j2所属的field的隐变量。很恶心的解释，通俗的来讲就是，性别为女与学历这个field的组合有个隐变量，性别女与年龄这个field的组合又有一个不一样的隐变量，而却不考虑到底是什么学历是啥，年龄具体到什么细节。<br>Yu-Chin Juan大神在实际写code的过程中，干掉来常数和一次项，可能是为了方便计算，保留的如下：<br><img src="/2017/12/04/FM理论解析及应用/11.jpeg" alt=""><br>整理的最优化损失函数如下：<br><img src="/2017/12/04/FM理论解析及应用/12.jpeg" alt=""><br>前面为l2正则，后面为交互熵形式，我们看到了y*Φ(V,x)这个及其类似hinge loss里面的1−t⋅y部分，所以注意这边的y属于{-1，1}<br>这边的求导，我算了一个小时都没搞出来，等哪天有空了，再仔细的去算一下，去翻了原论文，最后的迭代形式如下：<br><img src="/2017/12/04/FM理论解析及应用/13.jpeg" alt=""><br>η是常规的速率，V是初始均匀分布即可</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>我这边完成了FM的代码实现，详细见我的github：<a href="https://github.com/sladesha/machine_learning/tree/master/FM" target="_blank" rel="noopener">fm代码</a><br>为了方便不想看细节，只想撸代码的同学，我打包上传到了pypi，你只需要<code>pip install Fsfm</code>即可体验<br>至于ffm，我下午实在没写出来，对不起彭老师，丢脸了，后续看什么时候有空再研究一下。</p>
<p>最后，着重提示，本文很多思路很解析都参考的Yu-Chin Juan的源代码，附上<a href="https://github.com/guestwalk/libffm" target="_blank" rel="noopener">github地址</a>，欢迎去关注原作者的内容，感谢大神带路，谢谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 特征交叉 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python开发：特征工程代码模版(二)]]></title>
      <url>/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%BA%8C/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/01/python开发：特征工程代码模版-二/1.jpg" alt=""><br><a id="more"></a><br><strong>update:</strong><br><strong>17.12.21 : Mutual Information互信息中mic_entroy函数里的I应该是i，已修正</strong></p>
<hr>
<p><strong>转载请注明文章来源：<a href="http://www.jianshu.com/writer#/notebooks/14156301/notes/20406464/preview" target="_blank" rel="noopener">python开发：特征工程代码模版（二）</a>，你们免费转我文章，不标注来源就算了，现在还开始写“原创”，这就过分了～</strong></p>
<p>正题开始：<br>这篇文章是入门级的特征处理的打包解决方案的python实现汇总，如果想get一些新鲜血液的朋友可以叉了，只是方便玩数据的人进行数据<strong>特征筛选</strong>的代码集合，话不多说，让我们开始。</p>
<hr>
<p>首先，让我们看一张入门级别的数据预处理的基本操作图，网上有很多版本，这个是我自己日常干活的时候必操作的行为罗列，其中<a href="http://www.jianshu.com/p/1f2f887f0811" target="_blank" rel="noopener">数据整理部分</a>已经在上一篇文章中给出了，下面我们讲一起来看看特征筛选这块。<strong>此图请尊重一下我，别拿出去传播，纯属个人的方法论，大家看看就行，谢谢。</strong>网上有其他版本的，你们去传播那些就ok了～<br><img src="/2017/12/01/python开发：特征工程代码模版-二/2.png" alt="特征工程"></p>
<hr>
<h3 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">var_filter</span><span class="params">(data, k=None)</span>:</span></span><br><span class="line">var_data = data.var().sort_values()</span><br><span class="line"><span class="keyword">if</span> k <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">new_data = VarianceThreshold(threshold=k).fit_transform(data)</span><br><span class="line"><span class="keyword">return</span> var_data, new_data</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> var_data</span><br></pre></td></tr></table></figure>
<p>这个方法的思路很明确，我们筛掉方差过小的feature，也很好理解，一列值完全或者几乎完全一致的feature对于我们去训练最后的模型没有任何好处。熵理论也同样印证了这一点。</p>
<h3 id="线性相关系数衡量"><a href="#线性相关系数衡量" class="headerlink" title="线性相关系数衡量"></a>线性相关系数衡量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearson_value</span><span class="params">(data, label, k=None)</span>:</span></span><br><span class="line">label = str(label)</span><br><span class="line"><span class="comment"># k为想删除的feature个数</span></span><br><span class="line">Y = data[label]</span><br><span class="line">x = data[[x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]]</span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">data_res = np.c_[Y, x.iloc[:, i]].T</span><br><span class="line">cor_value = np.abs(np.corrcoef(data_res)[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">res.append([label, x.columns[i], cor_value])</span><br><span class="line">res = sorted(np.array(res), key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>])</span><br><span class="line"><span class="keyword">if</span> k <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line"><span class="keyword">if</span> k &lt; len(res):</span><br><span class="line">new_c = []  <span class="comment"># 保留的feature</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(res) - k):</span><br><span class="line">new_c.append(res[i][<span class="number">1</span>])</span><br><span class="line"><span class="keyword">return</span> res, new_c</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">'feature个数越界～'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>当你明确了自变量与因变量之间存在线性关系的时候，你就需要剔除掉一些关心比较弱的变量，奥卡姆剃刀原理告诉我们，在尽可能压缩feature个数大小的情况下去得到效果最优的模型才是合理模型。</p>
<h3 id="共线性检验"><a href="#共线性检验" class="headerlink" title="共线性检验"></a>共线性检验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vif_test</span><span class="params">(data, label, k=None)</span>:</span></span><br><span class="line">label = str(label)</span><br><span class="line"><span class="comment"># k为想删除的feature个数</span></span><br><span class="line">x = data[[x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]]</span><br><span class="line">res = np.abs(np.corrcoef(x.T))</span><br><span class="line">vif_value = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">0</span>]):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(res.shape[<span class="number">0</span>]):</span><br><span class="line"><span class="keyword">if</span> j &gt; I:</span><br><span class="line">vif_value.append([x.columns[i], x.columns[j], res[i, j]])</span><br><span class="line">vif_value = sorted(vif_value, key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>])</span><br><span class="line"><span class="keyword">if</span> k <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line"><span class="keyword">if</span> k &lt; len(vif_value):</span><br><span class="line">new_c = []  <span class="comment"># 保留的feature</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line"><span class="keyword">if</span> vif_value[-i][<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> new_c:</span><br><span class="line">new_c.append(vif_value[-i][<span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">new_c.append(vif_value[-i][<span class="number">0</span>])</span><br><span class="line"><span class="keyword">if</span> len(new_c) == k:</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">out = [x <span class="keyword">for</span> x <span class="keyword">in</span> x.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> new_c]</span><br><span class="line"><span class="keyword">return</span> vif_value, out</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">'feature个数越界～'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> vif_value</span><br></pre></td></tr></table></figure>
<p>2-3年前面试必考题，什么叫做共线性？如何解决共线性？答案之一就是共线性检验啊，判断feature之间的相关性，剔除相关性较高的feature，在R语言里面有个VIF函数可以直接求的。除此之外，采用非线性函数做特征拆解也是很好的方法。共线性严重的情况下，会导致泛化误差异常大，需着重注意～</p>
<h3 id="Mutual-Information互信息"><a href="#Mutual-Information互信息" class="headerlink" title="Mutual Information互信息"></a>Mutual Information互信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MI</span><span class="params">(X, Y)</span>:</span></span><br><span class="line"><span class="comment"># len(X) should be equal to len(Y)</span></span><br><span class="line"><span class="comment"># X,Y should be the class feature</span></span><br><span class="line">total = len(X)</span><br><span class="line">X_set = set(X)</span><br><span class="line">Y_set = set(Y)</span><br><span class="line"><span class="keyword">if</span> len(X_set) &gt; <span class="number">10</span>:</span><br><span class="line">print(<span class="string">'%s非分类变量，请检查后再输入'</span> % X_set)</span><br><span class="line">sys.exit()</span><br><span class="line"><span class="keyword">elif</span> len(Y_set) &gt; <span class="number">10</span>:</span><br><span class="line">print(<span class="string">'%s非分类变量，请检查后再输入'</span> % Y_set)</span><br><span class="line">sys.exit()</span><br><span class="line"><span class="comment"># Mutual information</span></span><br><span class="line">MI = <span class="number">0</span></span><br><span class="line">eps = <span class="number">1.4e-45</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> X_set:</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> Y_set:</span><br><span class="line">indexi = np.where(X == i)</span><br><span class="line">indexj = np.where(Y == j)</span><br><span class="line">ijinter = np.intersect1d(indexi, indexj)</span><br><span class="line">px = <span class="number">1.0</span> * len(indexi[<span class="number">0</span>]) / total</span><br><span class="line">py = <span class="number">1.0</span> * len(indexj[<span class="number">0</span>]) / total</span><br><span class="line">pxy = <span class="number">1.0</span> * len(ijinter) / total</span><br><span class="line">MI = MI + pxy * np.log2(pxy / (px * py) + eps)</span><br><span class="line"><span class="keyword">return</span> MI</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic_entroy</span><span class="params">(data, label)</span>:</span></span><br><span class="line"><span class="comment"># mic_value值越小，两者相关性越弱</span></span><br><span class="line">label = str(label)</span><br><span class="line"><span class="comment"># k为想删除的feature个数</span></span><br><span class="line">x = data[[x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]]</span><br><span class="line">Y = data[label]</span><br><span class="line">mic_value = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> len(set(x.iloc[:, i])) &lt;= <span class="number">10</span>:</span><br><span class="line">res = MI(Y, x.iloc[:, i])</span><br><span class="line">mic_value.append([x.columns[i], res])</span><br><span class="line">mic_value = sorted(mic_value, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">return</span> mic_value</span><br></pre></td></tr></table></figure>
<p>本来我想偷懒，直接<code>import minepy</code>然后就得了，发现真的是特么难装，各种报错，一怒之下自己写了，这边求大佬告知，为什么<code>pip install minepy</code>会有这样的问题：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun</span><br><span class="line">error: command <span class="string">'/usr/bin/clang'</span> failed <span class="keyword">with</span> exit status <span class="number">1</span></span><br><span class="line">----------------------------------------</span><br><span class="line">Command <span class="string">"/Users/slade/anaconda3/bin/python -u -c "</span></span><br><span class="line"><span class="keyword">import</span> setuptools, tokenize;__file__=<span class="string">'/private/var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-build-hr9ej0lw/minepy/setup.py'</span>;</span><br><span class="line">f=getattr(tokenize, <span class="string">'open'</span>, open)(__file__);</span><br><span class="line">code=f.read().replace(<span class="string">'\r\n'</span>, <span class="string">'\n'</span>);f.close();</span><br><span class="line">exec(compile(code, __file__, <span class="string">'exec'</span>))<span class="string">" install --record /var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-30cn7rbs-record/install-record.txt --single-version-externally-managed --compile"</span> failed <span class="keyword">with</span> error code <span class="number">1</span> <span class="keyword">in</span> /private/var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-build-hr9ej0lw/minepy/</span><br></pre></td></tr></table></figure></p>
<p>回到正题，互信息其实很简单，我们看个公式I(X;Y)=H(X)-H(X|Y)，看完是不是超级清晰了，其实就是X发生的概率中去掉Y发生后X发生的概率，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。<br>计算公式如下，你们也可以在上面的代码里找到影子。<br><img src="/2017/12/01/python开发：特征工程代码模版-二/3.jpeg" alt="特征工程"><br>最后还是吐槽下，这个minepy太难装了，为了个互信息，不至于不至于～</p>
<h3 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapper_way</span><span class="params">(data, label, k=<span class="number">3</span>)</span>:</span></span><br><span class="line"><span class="comment"># k 为要保留的数据feature个数</span></span><br><span class="line">label = str(label)</span><br><span class="line">label_data = data[label]</span><br><span class="line">col = [x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]</span><br><span class="line">train_data = data[col]</span><br><span class="line">res = pd.DataFrame(</span><br><span class="line">RFE(estimator=LogisticRegression(), n_features_to_select=k).fit_transform(train_data, label_data))</span><br><span class="line">res_c = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> (res.iloc[:, i] - data.iloc[:, j]).sum() == <span class="number">0</span>:</span><br><span class="line">res_c.append(data.columns[j])</span><br><span class="line">res.columns = res_c</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这边开始的代码就基本上是方法梳理了，没啥亮点，我就大概和大家聊聊，递归特征消除法，用R语言里面的step()函数是一毛一样的东西，都是循环sample特征，选一个对于当前模型，特征组合最好的结果。如果数据量大，你会有非一般的感觉，这边就有小trick了，以后有空可以和大家分享～</p>
<h3 id="l1-l2正则方法"><a href="#l1-l2正则方法" class="headerlink" title="l1/l2正则方法"></a>l1/l2正则方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedded_way</span><span class="params">(data, label, way=<span class="string">'l2'</span>, C_0=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">label = str(label)</span><br><span class="line">label_data = data[label]</span><br><span class="line">col = [x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]</span><br><span class="line">train_data = data[col]</span><br><span class="line">res = pd.DataFrame(SelectFromModel(LogisticRegression(penalty=way, C=C_0)).fit_transform(train_data, label_data))</span><br><span class="line">res_c = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> (res.iloc[:, i] - data.iloc[:, j]).sum() == <span class="number">0</span>:</span><br><span class="line">res_c.append(data.columns[j])</span><br><span class="line">res.columns = res_c</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>正则理论参考：<a href="http://www.jianshu.com/p/4f91f0dcba95" target="_blank" rel="noopener">总结：常见算法工程师面试题目整理(二)</a>，这边要提一点，并不是所有情况下都需要正则预处理的，很多算法自带正则，比如logistic啊，比如我们自己去写tensorflow神经网络啊，模型会针对性的解决问题，而这边单纯用的logstic方法来筛选，相对而言内嵌的效果会更好的。</p>
<h3 id="基于树模型特征选择"><a href="#基于树模型特征选择" class="headerlink" title="基于树模型特征选择"></a>基于树模型特征选择</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tree_way</span><span class="params">(data,label)</span>:</span></span><br><span class="line">label = str(label)</span><br><span class="line">label_data = data[label]</span><br><span class="line">col = [x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]</span><br><span class="line">train_data = data[col]</span><br><span class="line">res = pd.DataFrame(SelectFromModel(GradientBoostingClassifier()).fit_transform(train_data, label_data))</span><br><span class="line">res_c = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> (res.iloc[:, i] - data.iloc[:, j]).sum() == <span class="number">0</span>:</span><br><span class="line">res_c.append(data.columns[j])</span><br><span class="line">res.columns = res_c</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这边用的是决策树每次分支下，如果改变一列值为随机值，观察对整体数据效果的影响。举个通俗易懂的例子，看看你在公司的重要性，就去和你老板提离职，要是老板疯狂给你加工资做你的思想工作，代表你很重要；如果你的老板让你去财务结账，代表你没啥意义。这里你就是这个feature，你老板就是数据效果的检验指标，常见的就是oob之类的。</p>
<p>这边facebook有个非常好的拓展的思路，但是大家都吹的多实际应用很少，我最近在搞这事情，等下更完这边的特征工程和下面一个nlp的case后，我想专门聊聊这个事情，用的就是决策树的另一角度，以叶子结点代替原feature，做到了非线性的特征融入线性模型，虽然很老套，但是我稍稍做了测试，效果斐然：<br><img src="/2017/12/01/python开发：特征工程代码模版-二/4.png" alt="特征工程"></p>
<p>最后的最后，感谢大家阅读，希望能够给大家带来收获，谢谢～</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python开发：特征工程代码模版(一)]]></title>
      <url>/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%B8%80/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/01/python开发：特征工程代码模版-一/1.jpg" alt=""><br><a id="more"></a><br>作为一个算法工程师，我们接的业务需求不会比数据分析挖掘工程师少，作为一个爱偷懒的人，总机械重复的完成一样的预处理工作，我是不能忍的，所以在最近几天，我正在完善一些常规的、通用的预处理的code，方便我们以后在每次分析之前直接import快速搞定，省的每次都要去做一样的事情。</p>
<p><strong>如果大家有什么想实现但是懒得去弄的预处理的步骤也可以私信我，我相对而言闲暇还是有的（毕竟工资少工作也不多，摊手：《），我开发完成后直接贴出来，大家以后一起用就行了</strong></p>
<p>我们需要预加载这些包，而且接下来所有的操作均在dataframe格式下完成，所以我们需要将数据先处理成dataframe格式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'slade_sal'</span></span><br><span class="line">__time__ = <span class="string">'20171128'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_data_format</span><span class="params">(data)</span>:</span></span><br><span class="line"><span class="comment"># 以下预处理都是基于dataframe格式进行的</span></span><br><span class="line">data_new = pd.DataFrame(data)</span><br><span class="line"><span class="keyword">return</span> data_new</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="空值处理"><a href="#空值处理" class="headerlink" title="空值处理"></a>空值处理</h1><p>接下来就开始我们的正题了，首先，我们需要判断哪些列是空值过多的，当一列数据的空值占列数的40%以上（经验值），这列能够带给我们的信息就不多了，所以我们需要把某个阀值（rate_base）以上的空值个数的列干掉，如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 去除空值过多的feature</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_remove</span><span class="params">(data, rate_base=<span class="number">0.4</span>)</span>:</span></span><br><span class="line">all_cnt = data.shape[<span class="number">0</span>]</span><br><span class="line">avaiable_index = []</span><br><span class="line"><span class="comment"># 针对每一列feature统计nan的个数，个数大于全量样本的rate_base的认为是异常feature，进行剔除</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line">rate = np.isnan(np.array(data.iloc[:, i])).sum() / all_cnt</span><br><span class="line"><span class="keyword">if</span> rate &lt;= rate_base:</span><br><span class="line">avaiable_index.append(i)</span><br><span class="line">data_available = data.iloc[:, avaiable_index]</span><br><span class="line"><span class="keyword">return</span> data_available, avaiable_index</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="离群点处理"><a href="#离群点处理" class="headerlink" title="离群点处理"></a>离群点处理</h1><p>把空值过多的列去完之后，我们需要考虑将一些特别离群的点去掉，这边需要注意两点：</p>
<ul>
<li>异常值分析类的场景禁止使用这步，比如信用卡评分，爬虫识别等，你如果采取了这步，还怎么去分离出这些异常啊</li>
<li>容忍度高的算法不建议使用这步，比如svm里面已经有了支持向量机这个东西，你如果采取了这步的离群识别的操作会改变原分布而且svm里面决定超平面的核心与离群点无关，后接函数会引发意想不到的彩蛋～</li>
</ul>
<p>这边采取盖帽法与额定的分位点方法，建议组合使用，用changed_feature_box定义需要采用盖帽法的列的index_num，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 离群点盖帽</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outlier_remove</span><span class="params">(data, limit_value=<span class="number">10</span>, method=<span class="string">'box'</span>, percentile_limit_set=<span class="number">90</span>, changed_feature_box=[])</span>:</span></span><br><span class="line"><span class="comment"># limit_value是最小处理样本个数set，当独立样本大于limit_value我们认为非可onehot字段</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">feature_change = []</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'box'</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line"><span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">return</span> data, feature_change</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'self_def'</span>:</span><br><span class="line"><span class="comment"># 快速截断</span></span><br><span class="line"><span class="keyword">if</span> len(changed_feature_box) == <span class="number">0</span>:</span><br><span class="line"><span class="comment"># 当方法选择为自定义，且没有定义changed_feature_box则全量数据全部按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 如果定义了changed_feature_box，则将changed_feature_box里面的按照box方法，changed_feature_box的feature index按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">in</span> changed_feature_box:</span><br><span class="line">q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line"><span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">return</span> data, feature_change</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="空值填充"><a href="#空值填充" class="headerlink" title="空值填充"></a>空值填充</h1><p>在此之后，我们需要对空值进行填充，这边方法就很多很多了，我这边实现的是基本的，分了连续feature和分类feature，分别针对continuous feature采取mean,min,max方式，class feature采取one_hot_encoding的方式；除此之外还可以做分层填充，差分填充等等，那个比较定制化，如果有需要，我也可以搞一套，但是个人觉得意义不大。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 空feature填充</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_fill</span><span class="params">(data, limit_value=<span class="number">10</span>, countinuous_dealed_method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">normal_index = []</span><br><span class="line">continuous_feature_index = []</span><br><span class="line">class_feature_index = []</span><br><span class="line">continuous_feature_df = pd.DataFrame()</span><br><span class="line">class_feature_df = pd.DataFrame()</span><br><span class="line"><span class="comment"># 当存在空值且每个feature下独立的样本数小于limit_value，我们认为是class feature采取one_hot_encoding；</span></span><br><span class="line"><span class="comment"># 当存在空值且每个feature下独立的样本数大于limit_value，我们认为是continuous feature采取mean,min,max方式</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> np.isnan(np.array(data.iloc[:, i])).sum() &gt; <span class="number">0</span>:</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line"><span class="keyword">if</span> countinuous_dealed_method == <span class="string">'mean'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].mean())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'max'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].max())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'min'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].min())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt; <span class="number">0</span> <span class="keyword">and</span> len(</span><br><span class="line">pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">class_feature_df = pd.concat(</span><br><span class="line">[class_feature_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line">class_feature_index.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">normal_index.append(i)</span><br><span class="line">data_update = pd.concat([data.iloc[:, normal_index], continuous_feature_df, class_feature_df], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> data_update</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="one-hot-encoding过程"><a href="#one-hot-encoding过程" class="headerlink" title="one hot encoding过程"></a>one hot encoding过程</h1><p>分类feature的one hot encoding过程，常见操作，不多说<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># onehotencoding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ohe</span><span class="params">(data, limit_value=<span class="number">10</span>)</span>:</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">class_index = []</span><br><span class="line">class_df = pd.DataFrame()</span><br><span class="line">normal_index = []</span><br><span class="line"><span class="comment"># limit_value以下的均认为是class feature，进行ohe过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">class_index.append(i)</span><br><span class="line">class_df = pd.concat([class_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">normal_index.append(i)</span><br><span class="line">data_update = pd.concat([data.iloc[:, normal_index], class_df], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> data_update</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="Smote过程"><a href="#Smote过程" class="headerlink" title="Smote过程"></a>Smote过程</h1><p>正负样本不平衡的解决，这边我写的是smote，理论部分建议参考：<a href="http://www.jianshu.com/p/ecbc924860af" target="_blank" rel="noopener">Python：SMOTE算法</a>,其实简单的欠抽样和过抽样就可以解决，建议参考这边文章：<a href="http://www.jianshu.com/p/9a3b3104776e" target="_blank" rel="noopener">Python:数据抽样平衡方法重写</a>。都是一些老生常谈的问题了，不多说了，上代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'sladesal'</span></span><br><span class="line">__time__ = <span class="string">'20171110'</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">data : 原始数据</span></span><br><span class="line"><span class="string">tag_index : 因变量所在的列数，以0开始</span></span><br><span class="line"><span class="string">max_amount : 少类别类想要达到的数据量</span></span><br><span class="line"><span class="string">std_rate : 多类:少类想要达到的比例</span></span><br><span class="line"><span class="string">#如果max_amount和std_rate同时定义优先考虑max_amount的定义</span></span><br><span class="line"><span class="string">kneighbor : 生成数据依赖kneighbor个附近的同类点，建议不超过5个</span></span><br><span class="line"><span class="string">kdistinctvalue : 认为每列不同元素大于kdistinctvalue及为连续变量，否则为class变量</span></span><br><span class="line"><span class="string">method ： 生成方法</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smote</span><span class="params">(data, tag_index=None, max_amount=<span class="number">0</span>, std_rate=<span class="number">5</span>, kneighbor=<span class="number">5</span>, kdistinctvalue=<span class="number">10</span>, method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">data = pd.DataFrame(data)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line"><span class="keyword">raise</span> ValueError</span><br><span class="line">case_state = data.iloc[:, tag_index].groupby(data.iloc[:, tag_index]).count()</span><br><span class="line">case_rate = max(case_state) / min(case_state)</span><br><span class="line">location = []</span><br><span class="line"><span class="keyword">if</span> case_rate &lt; <span class="number">5</span>:</span><br><span class="line">print(<span class="string">'不需要smote过程'</span>)</span><br><span class="line"><span class="keyword">return</span> data</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 拆分不同大小的数据集合</span></span><br><span class="line">less_data = np.array(</span><br><span class="line">data[data.iloc[:, tag_index] == np.array(case_state[case_state == min(case_state)].index)[<span class="number">0</span>]])</span><br><span class="line">more_data = np.array(</span><br><span class="line">data[data.iloc[:, tag_index] == np.array(case_state[case_state == max(case_state)].index)[<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 找出每个少量数据中每条数据k个邻居</span></span><br><span class="line">neighbors = NearestNeighbors(n_neighbors=kneighbor).fit(less_data)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(less_data)):</span><br><span class="line">point = less_data[i, :]</span><br><span class="line">location_set = neighbors.kneighbors([less_data[i]], return_distance=<span class="keyword">False</span>)[<span class="number">0</span>]</span><br><span class="line">location.append(location_set)</span><br><span class="line"><span class="comment"># 确定需要将少量数据补充到上限额度</span></span><br><span class="line"><span class="comment"># 判断有没有设定生成数据个数，如果没有按照std_rate(预期正负样本比)比例生成</span></span><br><span class="line"><span class="keyword">if</span> max_amount &gt; <span class="number">0</span>:</span><br><span class="line">amount = max_amount</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">amount = int(max(case_state) / std_rate)</span><br><span class="line"><span class="comment"># 初始化，判断连续还是分类变量采取不同的生成逻辑</span></span><br><span class="line">times = <span class="number">0</span></span><br><span class="line">continue_index = []  <span class="comment"># 连续变量</span></span><br><span class="line">class_index = []  <span class="comment"># 分类变量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(less_data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; kdistinctvalue:</span><br><span class="line">continue_index.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">class_index.append(i)</span><br><span class="line">case_update = list()</span><br><span class="line">location_transform = np.array(location)</span><br><span class="line"><span class="keyword">while</span> times &lt; amount:</span><br><span class="line"><span class="comment"># 连续变量取附近k个点的重心，认为少数样本的附近也是少数样本</span></span><br><span class="line">new_case = []</span><br><span class="line">pool = np.random.permutation(len(location))[<span class="number">1</span>]</span><br><span class="line">neighbor_group = location_transform[pool]</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'mean'</span>:</span><br><span class="line">new_case1 = less_data[list(neighbor_group), :][:, continue_index].mean(axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 连续样本的附近点向量上的点也是异常点</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'random'</span>:</span><br><span class="line">away_index = np.random.permutation(len(neighbor_group) - <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">neighbor_group_removeorigin = neighbor_group[<span class="number">1</span>:][away_index]</span><br><span class="line">new_case1 = less_data[pool][continue_index] + np.random.rand() * (</span><br><span class="line">less_data[pool][continue_index] - less_data[neighbor_group_removeorigin][continue_index])</span><br><span class="line"><span class="comment"># 分类变量取mode</span></span><br><span class="line">new_case2 = np.array(pd.DataFrame(less_data[neighbor_group, :][:, class_index]).mode().iloc[<span class="number">0</span>, :])</span><br><span class="line">new_case = list(new_case1) + list(new_case2)</span><br><span class="line"><span class="keyword">if</span> times == <span class="number">0</span>:</span><br><span class="line">case_update = new_case</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">case_update = np.c_[case_update, new_case]</span><br><span class="line">print(<span class="string">'已经生成了%s条新数据，完成百分之%.2f'</span> % (times, times * <span class="number">100</span> / amount))</span><br><span class="line">times = times + <span class="number">1</span></span><br><span class="line">less_origin_data = np.hstack((less_data[:, continue_index], less_data[:, class_index]))</span><br><span class="line">more_origin_data = np.hstack((more_data[:, continue_index], more_data[:, class_index]))</span><br><span class="line">data_res = np.vstack((more_origin_data, less_origin_data, np.array(case_update.T)))</span><br><span class="line">label_columns = [<span class="number">0</span>] * more_origin_data.shape[<span class="number">0</span>] + [<span class="number">1</span>] * (</span><br><span class="line">less_origin_data.shape[<span class="number">0</span>] + np.array(case_update.T).shape[<span class="number">0</span>])</span><br><span class="line">data_res = pd.DataFrame(data_res)</span><br><span class="line"><span class="keyword">return</span> data_res</span><br></pre></td></tr></table></figure></p>
<h1 id="总体整合"><a href="#总体整合" class="headerlink" title="总体整合"></a>总体整合</h1><p>一期的内容就这样吧，我感觉也没有啥好说的，都是数据分析挖掘的一些基本操作，我只是为了以后能够复用模版化了，下面贴一个全量我做预处理的过程，没啥差异，整合了一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'slade_sal'</span></span><br><span class="line">__time__ = <span class="string">'20171128'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_data_format</span><span class="params">(data)</span>:</span></span><br><span class="line"><span class="comment"># 以下预处理都是基于dataframe格式进行的</span></span><br><span class="line">data_new = pd.DataFrame(data)</span><br><span class="line"><span class="keyword">return</span> data_new</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除空值过多的feature</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_remove</span><span class="params">(data, rate_base=<span class="number">0.4</span>)</span>:</span></span><br><span class="line">all_cnt = data.shape[<span class="number">0</span>]</span><br><span class="line">avaiable_index = []</span><br><span class="line"><span class="comment"># 针对每一列feature统计nan的个数，个数大于全量样本的rate_base的认为是异常feature，进行剔除</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line">rate = np.isnan(np.array(data.iloc[:, i])).sum() / all_cnt</span><br><span class="line"><span class="keyword">if</span> rate &lt;= rate_base:</span><br><span class="line">avaiable_index.append(i)</span><br><span class="line">data_available = data.iloc[:, avaiable_index]</span><br><span class="line"><span class="keyword">return</span> data_available, avaiable_index</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 离群点盖帽</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outlier_remove</span><span class="params">(data, limit_value=<span class="number">10</span>, method=<span class="string">'box'</span>, percentile_limit_set=<span class="number">90</span>, changed_feature_box=[])</span>:</span></span><br><span class="line"><span class="comment"># limit_value是最小处理样本个数set，当独立样本大于limit_value我们认为非可onehot字段</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">feature_change = []</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'box'</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line"><span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">return</span> data, feature_change</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'self_def'</span>:</span><br><span class="line"><span class="comment"># 快速截断</span></span><br><span class="line"><span class="keyword">if</span> len(changed_feature_box) == <span class="number">0</span>:</span><br><span class="line"><span class="comment"># 当方法选择为自定义，且没有定义changed_feature_box则全量数据全部按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 如果定义了changed_feature_box，则将changed_feature_box里面的按照box方法，changed_feature_box的feature index按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">in</span> changed_feature_box:</span><br><span class="line">q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line"><span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">return</span> data, feature_change</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 空feature填充</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_fill</span><span class="params">(data, limit_value=<span class="number">10</span>, countinuous_dealed_method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">normal_index = []</span><br><span class="line">continuous_feature_index = []</span><br><span class="line">class_feature_index = []</span><br><span class="line">continuous_feature_df = pd.DataFrame()</span><br><span class="line">class_feature_df = pd.DataFrame()</span><br><span class="line"><span class="comment"># 当存在空值且每个feature下独立的样本数小于limit_value，我们认为是class feature采取one_hot_encoding；</span></span><br><span class="line"><span class="comment"># 当存在空值且每个feature下独立的样本数大于limit_value，我们认为是continuous feature采取mean,min,max方式</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> np.isnan(np.array(data.iloc[:, i])).sum() &gt; <span class="number">0</span>:</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line"><span class="keyword">if</span> countinuous_dealed_method == <span class="string">'mean'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].mean())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'max'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].max())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'min'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].min())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt; <span class="number">0</span> <span class="keyword">and</span> len(</span><br><span class="line">pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">class_feature_df = pd.concat(</span><br><span class="line">[class_feature_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line">class_feature_index.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">normal_index.append(i)</span><br><span class="line">data_update = pd.concat([data.iloc[:, normal_index], continuous_feature_df, class_feature_df], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> data_update</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># onehotencoding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ohe</span><span class="params">(data, limit_value=<span class="number">10</span>)</span>:</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">class_index = []</span><br><span class="line">class_df = pd.DataFrame()</span><br><span class="line">normal_index = []</span><br><span class="line"><span class="comment"># limit_value以下的均认为是class feature，进行ohe过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">class_index.append(i)</span><br><span class="line">class_df = pd.concat([class_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">normal_index.append(i)</span><br><span class="line">data_update = pd.concat([data.iloc[:, normal_index], class_df], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> data_update</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'sladesal'</span></span><br><span class="line">__time__ = <span class="string">'20171110'</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">data : 原始数据</span></span><br><span class="line"><span class="string">tag_index : 因变量所在的列数，以0开始</span></span><br><span class="line"><span class="string">max_amount : 少类别类想要达到的数据量</span></span><br><span class="line"><span class="string">std_rate : 多类:少类想要达到的比例</span></span><br><span class="line"><span class="string">#如果max_amount和std_rate同时定义优先考虑max_amount的定义</span></span><br><span class="line"><span class="string">kneighbor : 生成数据依赖kneighbor个附近的同类点，建议不超过5个</span></span><br><span class="line"><span class="string">kdistinctvalue : 认为每列不同元素大于kdistinctvalue及为连续变量，否则为class变量</span></span><br><span class="line"><span class="string">method ： 生成方法</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smote</span><span class="params">(data, tag_index=None, max_amount=<span class="number">0</span>, std_rate=<span class="number">5</span>, kneighbor=<span class="number">5</span>, kdistinctvalue=<span class="number">10</span>, method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">data = pd.DataFrame(data)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line"><span class="keyword">raise</span> ValueError</span><br><span class="line">case_state = data.iloc[:, tag_index].groupby(data.iloc[:, tag_index]).count()</span><br><span class="line">case_rate = max(case_state) / min(case_state)</span><br><span class="line">location = []</span><br><span class="line"><span class="keyword">if</span> case_rate &lt; <span class="number">5</span>:</span><br><span class="line">print(<span class="string">'不需要smote过程'</span>)</span><br><span class="line"><span class="keyword">return</span> data</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 拆分不同大小的数据集合</span></span><br><span class="line">less_data = np.array(</span><br><span class="line">data[data.iloc[:, tag_index] == np.array(case_state[case_state == min(case_state)].index)[<span class="number">0</span>]])</span><br><span class="line">more_data = np.array(</span><br><span class="line">data[data.iloc[:, tag_index] == np.array(case_state[case_state == max(case_state)].index)[<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 找出每个少量数据中每条数据k个邻居</span></span><br><span class="line">neighbors = NearestNeighbors(n_neighbors=kneighbor).fit(less_data)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(less_data)):</span><br><span class="line">point = less_data[i, :]</span><br><span class="line">location_set = neighbors.kneighbors([less_data[i]], return_distance=<span class="keyword">False</span>)[<span class="number">0</span>]</span><br><span class="line">location.append(location_set)</span><br><span class="line"><span class="comment"># 确定需要将少量数据补充到上限额度</span></span><br><span class="line"><span class="comment"># 判断有没有设定生成数据个数，如果没有按照std_rate(预期正负样本比)比例生成</span></span><br><span class="line"><span class="keyword">if</span> max_amount &gt; <span class="number">0</span>:</span><br><span class="line">amount = max_amount</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">amount = int(max(case_state) / std_rate)</span><br><span class="line"><span class="comment"># 初始化，判断连续还是分类变量采取不同的生成逻辑</span></span><br><span class="line">times = <span class="number">0</span></span><br><span class="line">continue_index = []  <span class="comment"># 连续变量</span></span><br><span class="line">class_index = []  <span class="comment"># 分类变量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(less_data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; kdistinctvalue:</span><br><span class="line">continue_index.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">class_index.append(i)</span><br><span class="line">case_update = list()</span><br><span class="line">location_transform = np.array(location)</span><br><span class="line"><span class="keyword">while</span> times &lt; amount:</span><br><span class="line"><span class="comment"># 连续变量取附近k个点的重心，认为少数样本的附近也是少数样本</span></span><br><span class="line">new_case = []</span><br><span class="line">pool = np.random.permutation(len(location))[<span class="number">1</span>]</span><br><span class="line">neighbor_group = location_transform[pool]</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'mean'</span>:</span><br><span class="line">new_case1 = less_data[list(neighbor_group), :][:, continue_index].mean(axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 连续样本的附近点向量上的点也是异常点</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'random'</span>:</span><br><span class="line">away_index = np.random.permutation(len(neighbor_group) - <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">neighbor_group_removeorigin = neighbor_group[<span class="number">1</span>:][away_index]</span><br><span class="line">new_case1 = less_data[pool][continue_index] + np.random.rand() * (</span><br><span class="line">less_data[pool][continue_index] - less_data[neighbor_group_removeorigin][continue_index])</span><br><span class="line"><span class="comment"># 分类变量取mode</span></span><br><span class="line">new_case2 = np.array(pd.DataFrame(less_data[neighbor_group, :][:, class_index]).mode().iloc[<span class="number">0</span>, :])</span><br><span class="line">new_case = list(new_case1) + list(new_case2)</span><br><span class="line"><span class="keyword">if</span> times == <span class="number">0</span>:</span><br><span class="line">case_update = new_case</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">case_update = np.c_[case_update, new_case]</span><br><span class="line">print(<span class="string">'已经生成了%s条新数据，完成百分之%.2f'</span> % (times, times * <span class="number">100</span> / amount))</span><br><span class="line">times = times + <span class="number">1</span></span><br><span class="line">less_origin_data = np.hstack((less_data[:, continue_index], less_data[:, class_index]))</span><br><span class="line">more_origin_data = np.hstack((more_data[:, continue_index], more_data[:, class_index]))</span><br><span class="line">data_res = np.vstack((more_origin_data, less_origin_data, np.array(case_update.T)))</span><br><span class="line">label_columns = [<span class="number">0</span>] * more_origin_data.shape[<span class="number">0</span>] + [<span class="number">1</span>] * (</span><br><span class="line">less_origin_data.shape[<span class="number">0</span>] + np.array(case_update.T).shape[<span class="number">0</span>])</span><br><span class="line">data_res = pd.DataFrame(data_res)</span><br><span class="line"><span class="keyword">return</span> data_res</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reload</span><span class="params">(data)</span>:</span></span><br><span class="line">feature = pd.concat([data.iloc[:, :<span class="number">2</span>], data.iloc[:, <span class="number">4</span>:]], axis=<span class="number">1</span>)</span><br><span class="line">tag = data.iloc[:, <span class="number">3</span>]</span><br><span class="line"><span class="keyword">return</span> feature, tag</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据切割</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_data</span><span class="params">(feature, tag)</span>:</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(feature, tag, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="keyword">return</span> X_train, X_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">path = sys.argv[<span class="number">0</span>]</span><br><span class="line">data_all = pd.read_table(str(path))</span><br><span class="line">print(<span class="string">'数据读取完成！'</span>)</span><br><span class="line"><span class="comment"># 更改数据格式</span></span><br><span class="line">data_all = change_data_format(data_all)</span><br><span class="line"><span class="comment"># 删除电话号码列</span></span><br><span class="line">data_all = data_all.iloc[:, <span class="number">1</span>:]</span><br><span class="line">data_all, data_avaiable_index = nan_remove(data_all)</span><br><span class="line">print(<span class="string">'空值列处理完毕！'</span>)</span><br><span class="line">data_all, _ = outlier_remove(data_all)</span><br><span class="line">print(<span class="string">'异常点处理完成！'</span>)</span><br><span class="line">data_all = nan_fill(data_all)</span><br><span class="line">print(<span class="string">'空值填充完成！'</span>)</span><br><span class="line">data_all = ohe(data_all)</span><br><span class="line">print(<span class="string">'onehotencoding 完成！'</span>)</span><br><span class="line">data_all = smote(data_all,tag_index=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'smote过程完成！'</span>)</span><br><span class="line">feature, tag = reload(data_all)</span><br><span class="line">X_train, X_test, y_train, y_test = split_data(feature, tag)</span><br><span class="line">print(<span class="string">'数据预处理完成！'</span>)</span><br></pre></td></tr></table></figure></p>
<p>大家自取自用，这个也没啥好转载的，没啥干货，只是方便大家日常工作，就别转了，谢谢各位编辑大哥了。</p>
<p>最后，感谢大家阅读，谢谢。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[能够快速实现的协同推荐]]></title>
      <url>/2017/12/01/%E8%83%BD%E5%A4%9F%E5%BF%AB%E9%80%9F%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%8D%8F%E5%90%8C%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<p>对于中小型的公司，用户的数据量及公司产品的个数都是较小规模的，需要提供给用户的推荐系统实现的重心也从人性化变成了实现成本，协同推荐就是非常常见、有效且可以快速实现的方法，也是本文想介绍的。</p>
<p>常规的快速简单推荐系统实现方法不排除以下几种：</p>
<ul>
<li><p>热门推荐<br>所有人打开浏览的内容都一致，惊喜性会有所缺失，但是实现特别简单，稍加逻辑带给用户的体验感满足了基本需求。</p>
</li>
<li><p>SVD+推荐<br>之前也讨论过实现方法了，附上链接：<a href="http://www.jianshu.com/p/a3e633e396a0" target="_blank" rel="noopener">SVD及扩展的矩阵分解方法</a></p>
</li>
<li><p>基于模型推荐<br>这个比较偏向业务场景，可以说是经典的场景化模型，之前写过一篇基于用户特征的偏好推荐，可以参考一下：<a href="http://www.jianshu.com/p/fd245999ebfe" target="_blank" rel="noopener">苏宁易购的用户交叉推荐</a></p>
</li>
<li><p>协同推荐<br>这个也是几乎每个公司都会用的，也是非常非常常见有效的算法之一</p>
</li>
</ul>
<hr>
<h1 id="协同推荐介绍"><a href="#协同推荐介绍" class="headerlink" title="协同推荐介绍"></a><strong>协同推荐介绍</strong></h1><p>首先，我们先来了解一下什么叫做协同推荐。<br>基于用户的协同过滤推荐算法是最早诞生的，1992年提出并用于邮件过滤系统，两年后1994年被 GroupLens 用于新闻过滤。一直到2000年左右，该算法都是推荐系统领域最著名的算法。算是非常古董级别的算法之一了，但是古董归古董，它的效果以及实现的成本却奠定了它在每个公司不可取代的地位。</p>
<h2 id="基于用户的协同推荐"><a href="#基于用户的协同推荐" class="headerlink" title="基于用户的协同推荐"></a><strong>基于用户的协同推荐</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">用户u1喜欢的电影是A，B，C</span><br><span class="line">用户u2喜欢的电影是A, C, E, F</span><br><span class="line">用户u3喜欢的电影是B，D</span><br></pre></td></tr></table></figure>
<p>假设u1、u2、u3用户喜欢的电影分布如上，基于用户的协同推荐干了这么一件事情，它根据每个用户看的电影（A、B、C、…）相似程度，来计算用户之间的相似程度，将高相似的用户看过但是目标用户还没有看过的电影推荐给目标用户。</p>
<h2 id="基于商品的协同推荐"><a href="#基于商品的协同推荐" class="headerlink" title="基于商品的协同推荐"></a><strong>基于商品的协同推荐</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">电影A被u1，u2看过</span><br><span class="line">电影B被u1，u3看过</span><br><span class="line">电影C被u1，u2看过</span><br><span class="line">电影D被u3看过</span><br><span class="line">电影E被u2看过</span><br><span class="line">电影F被u2看过</span><br></pre></td></tr></table></figure>
<p>假设A～F电影被用户观影的分布如上，基于商品的协同推荐干了这么一件事情，它根据电影（A、B、C、…）被不同用户观看相似程度，来计算电影之间的相似程度，根据目标用户看过的电影的高相似度的电影推荐给目标用户。</p>
<p>看起来以上的逻辑是非常简单的，其实本来也是非常简单的，我看了下，网上关于以上的代码实现还是比较林散和有问题的，优化了python版本的code，并详细解释了每一步，希望，对初学者有所帮助。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#time  2017-09-17</span></span><br><span class="line"><span class="comment">#author：shataowei</span></span><br><span class="line"><span class="comment">#based-item</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#所要的基础包比较简单</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">startTime = time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据的过程</span></span><br><span class="line"><span class="comment">#/Users/slade/Desktop/machine learning/data/recommender/u1.base</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readdata</span><span class="params">(location)</span>:</span></span><br><span class="line">list2item = &#123;&#125;  <span class="comment">#商品对应的用户列表(1:[[1,2],[2,3]]代表商品1对应用户1的行为程度为2,商品1对应的用户2的行为程度为3)</span></span><br><span class="line">list2user = &#123;&#125;  <span class="comment">#用户对应的商品列表(1:[[1,2],[2,3]]代表用户1对应商品1的行为程度为2,用户1对应的商品2的行为程度为3)</span></span><br><span class="line">f = open(location,<span class="string">'r'</span>)</span><br><span class="line">data = f.readlines()</span><br><span class="line">data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line"><span class="keyword">if</span> int(i[<span class="number">1</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2item.keys():</span><br><span class="line">list2item[int(i[<span class="number">1</span>])] = [[int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">list2item[int(i[<span class="number">1</span>])].append([int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> int(i[<span class="number">0</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2user.keys():</span><br><span class="line">list2user[int(i[<span class="number">0</span>])] = [[int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">list2user[int(i[<span class="number">0</span>])].append([int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"><span class="keyword">return</span> list2item,list2user</span><br><span class="line"><span class="comment">#list2item,list2user=readdata('/Users/slade/Desktop/machine learning/data/recommender/u1.base')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 基于item的协同推荐</span></span><br><span class="line"><span class="comment">#0.将用户行为程度离散化：浏览：1，搜索：2，收藏：3，加车：4，下单未支付5</span></span><br><span class="line"><span class="comment">#1.计算item之间的相似度：item共同观看次数/单item次数连乘</span></span><br><span class="line"><span class="comment">#2.寻找目标用户观看过的item相关的其他item列表</span></span><br><span class="line"><span class="comment">#3.计算其他item的得分：相似度*用户行为程度，求和</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#0 hive操作</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.1统计各商品出现次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">itemcf_itemall</span><span class="params">(userlist = list2user)</span>:</span></span><br><span class="line">I=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> userlist:</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> userlist[key]:</span><br><span class="line"><span class="keyword">if</span> item[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> I.keys():</span><br><span class="line">I[item[<span class="number">0</span>]] = <span class="number">0</span></span><br><span class="line">I[item[<span class="number">0</span>]] = I[item[<span class="number">0</span>]] + <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> I</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.2计算相似矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">itemcf_matrix</span><span class="params">(userlist = list2user)</span>:</span></span><br><span class="line">C=defaultdict(defaultdict)</span><br><span class="line">W=defaultdict(defaultdict)</span><br><span class="line"><span class="comment">#根据用户的已购商品来形成对应相似度矩阵</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> userlist:</span><br><span class="line"><span class="keyword">for</span> item1 <span class="keyword">in</span> userlist[key]:</span><br><span class="line"><span class="keyword">for</span> item2 <span class="keyword">in</span> userlist[key]:</span><br><span class="line"><span class="keyword">if</span> item1[<span class="number">0</span>] == item2[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"><span class="keyword">if</span> item2 <span class="keyword">not</span> <span class="keyword">in</span> C[item1[<span class="number">0</span>]].keys():</span><br><span class="line">C[item1[<span class="number">0</span>]][item2[<span class="number">0</span>]] = <span class="number">0</span></span><br><span class="line">C[item1[<span class="number">0</span>]][item2[<span class="number">0</span>]] = C[item1[<span class="number">0</span>]][item2[<span class="number">0</span>]] + <span class="number">1</span></span><br><span class="line"><span class="comment">#计算相似度，并填充上面对应的相似度矩阵</span></span><br><span class="line"><span class="keyword">for</span> i , j <span class="keyword">in</span> C.items():</span><br><span class="line"><span class="keyword">for</span> z , k <span class="keyword">in</span> j.items():</span><br><span class="line">W[i][z] = k/math.sqrt(I[i]*I[z])</span><br><span class="line"><span class="comment">#k/math.sqrt(I[i]*I[z])计算相似度，其中k为不同商品交集，sqrt(I[i]*I[z])用来压缩那些热门商品必然有高交集的问题</span></span><br><span class="line"><span class="keyword">return</span> W</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.寻找用户观看的其他item</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommendation</span><span class="params">(userid,k)</span>:</span></span><br><span class="line">score_final = defaultdict(int)</span><br><span class="line">useriditem = []</span><br><span class="line"><span class="keyword">for</span> item,score <span class="keyword">in</span> list2user[userid]:</span><br><span class="line"><span class="comment">#3.计算用户的item得分，k来控制用多少个相似商品来计算最后的推荐商品</span></span><br><span class="line"><span class="keyword">for</span> i , smimilarity <span class="keyword">in</span> sorted(W[item].items() , key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>] ,reverse =<span class="keyword">True</span>)[<span class="number">0</span>:k]:</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> list2user[userid]:</span><br><span class="line">useriditem.append(j[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> useriditem:</span><br><span class="line">score_final[i] = score_final[i] + smimilarity * score</span><br><span class="line"><span class="comment">#累加每一个商品用户的评分与其它商品的相似度积的和作为衡量</span></span><br><span class="line"><span class="comment">#最后的10控制输出多少个推荐商品</span></span><br><span class="line">l = sorted(score_final.items() , key = <span class="keyword">lambda</span> x : x[<span class="number">1</span>] , reverse = <span class="keyword">True</span>)[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line"><span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"><span class="comment">#I = itemcf_itemall()</span></span><br><span class="line"><span class="comment">#W = itemcf_matrix()</span></span><br><span class="line"><span class="comment">#result_userid = recommendation(2,k=20)</span></span><br><span class="line"></span><br><span class="line">endTime = time.time()</span><br><span class="line"><span class="keyword">print</span> endTime-startTime</span><br></pre></td></tr></table></figure>
<p>python来实现基于item的协同推荐就完成了，核心的相似度计算可以根据实际问题进行修改，整体流程同上即可，当然数据量大的时候分布式去写也是可以的。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#time  2017-09-17</span></span><br><span class="line"><span class="comment">#author：shataowei</span></span><br><span class="line"><span class="comment">#based-user</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">startTime = time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line"><span class="comment">#/Users/slade/Desktop/machine learning/data/recommender/u1.base</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readdata</span><span class="params">(location)</span>:</span></span><br><span class="line">list2item = &#123;&#125;  <span class="comment">#商品对应的用户列表</span></span><br><span class="line">list2user = &#123;&#125;  <span class="comment">#用户对应的商品列表</span></span><br><span class="line">f = open(location,<span class="string">'r'</span>)</span><br><span class="line">data = f.readlines()</span><br><span class="line">data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line"><span class="keyword">if</span> int(i[<span class="number">1</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2item.keys():</span><br><span class="line">list2item[int(i[<span class="number">1</span>])] = [[int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">list2item[int(i[<span class="number">1</span>])].append([int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> int(i[<span class="number">0</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2user.keys():</span><br><span class="line">list2user[int(i[<span class="number">0</span>])] = [[int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">list2user[int(i[<span class="number">0</span>])].append([int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"><span class="keyword">return</span> list2item,list2user</span><br><span class="line"><span class="comment">#list2item,list2user=readdata('/Users/slade/Desktop/machine learning/data/recommender/u1.base')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#基于用户的协同推荐</span></span><br><span class="line"><span class="comment">#0.先通过hive求出近一段时间（根据业务频率定义），用户商品的对应表</span></span><br><span class="line"><span class="comment">#1.求出目标用户的邻居，并计算目标用户与邻居之间的相似度</span></span><br><span class="line"><span class="comment">#2.列出邻居所以购买的商品列表</span></span><br><span class="line"><span class="comment">#3.针对第二步求出了商品列表，累加所对应的用户相似度，并排序求top</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#0.hive操作</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.1求出目标用户的邻居，及对应的相关程度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neighbour</span><span class="params">(userid,user_group = list2user,item_group = list2item)</span>:</span></span><br><span class="line">neighbours = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list2user[userid]:</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> list2item[item[<span class="number">0</span>]]:</span><br><span class="line"><span class="keyword">if</span> user[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> neighbours.keys():</span><br><span class="line">neighbours[user[<span class="number">0</span>]] = <span class="number">0</span></span><br><span class="line">neighbours[user[<span class="number">0</span>]] = neighbours[user[<span class="number">0</span>]] + <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> neighbors</span><br><span class="line"><span class="comment">#通常来说，基于item的推荐对于商品量较大的业务会构成一个巨大的商品矩阵，这时候如果用户人均购买量较低的时候，可以考虑使用基于user的推荐，它在每次计算的时候会只考虑相关用户，也就是这边的neighbours(有点支持向量基的意思)，大大的降低了计算量。</span></span><br><span class="line"><span class="comment">#neighbours = neighbour(userid=2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.2就算用户直接的相似程度,这边用的余弦相似度：点积/模的连乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similarity</span><span class="params">(user1,user2)</span>:</span></span><br><span class="line">x=<span class="number">0</span></span><br><span class="line">y=<span class="number">0</span></span><br><span class="line">z=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> item1 <span class="keyword">in</span> list2user[user1]:</span><br><span class="line"><span class="keyword">for</span> item2 <span class="keyword">in</span> list2user[user2]:</span><br><span class="line"><span class="keyword">if</span> item1[<span class="number">0</span>]==item2[<span class="number">0</span>]:</span><br><span class="line">x1 = item1[<span class="number">1</span>]*item1[<span class="number">1</span>]</span><br><span class="line">y1 = item2[<span class="number">1</span>]*item2[<span class="number">1</span>]</span><br><span class="line">z1 = item1[<span class="number">1</span>]*item2[<span class="number">1</span>]</span><br><span class="line">x = x + x1</span><br><span class="line">y = y + y1</span><br><span class="line">z = z + z1</span><br><span class="line"><span class="comment">#避免分母为0</span></span><br><span class="line"><span class="keyword">if</span> x * y == <span class="number">0</span> :</span><br><span class="line">simi = <span class="number">0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">simi = z / math.sqrt(x * y)</span><br><span class="line"><span class="keyword">return</span> simi</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.3计算目标用户与邻居之间的相似度：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">N_neighbour</span><span class="params">(userid,neighbours,k)</span>:</span></span><br><span class="line">neighbour = neighbours.keys()</span><br><span class="line">M = []</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> neighbour:</span><br><span class="line">simi = similarity(userid,user)</span><br><span class="line">M.append((user,simi))</span><br><span class="line">M = sorted(M,key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>] ,reverse = <span class="keyword">True</span>)[<span class="number">0</span>:k]</span><br><span class="line"><span class="keyword">return</span> M</span><br><span class="line"></span><br><span class="line"><span class="comment">#M = N_neighbour(userid,neighbours,k=200)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.列出邻居所购买过的商品并计算商品对应的推荐指数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neighbour_item</span><span class="params">(M=M)</span>:</span></span><br><span class="line">R = &#123;&#125;</span><br><span class="line">M1 = dict(M)</span><br><span class="line"><span class="keyword">for</span> neighbour <span class="keyword">in</span> M1:</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list2user[neighbour]:</span><br><span class="line"><span class="keyword">if</span> item[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> R.keys():</span><br><span class="line">R[item[<span class="number">0</span>]] = M1[neighbour] * item[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">R[item[<span class="number">0</span>]] = R[item[<span class="number">0</span>]] + M1[neighbour] * item[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#根据邻居买过什么及与邻居的相似度，计算邻居买过商品的推荐度</span></span><br><span class="line"><span class="keyword">return</span> R</span><br><span class="line"><span class="comment"># R = neighbour_item(M)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.排序得到推荐商品</span></span><br><span class="line">Rank = sorted(R.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>],reverse = <span class="keyword">True</span>)[<span class="number">0</span>:<span class="number">50</span>]</span><br><span class="line"></span><br><span class="line">endTime = time.time()</span><br><span class="line"><span class="keyword">print</span> endTime-startTime</span><br></pre></td></tr></table></figure>
<p>python来实现基于user的协同推荐就完成了，核心的相似度计算可以根据实际问题进行修改，基于user的实现过程中，用了邻居这个概念，大大降低了计算量，我用了大概20万用户，2千的商品数，基于user的推荐实现速度大概为基于商品的10分之一，效果差异却相差不大。</p>
<p>协同推荐是非常简单的推荐入门算法之一，也是必须要手动快速代码实现的算法之一，希望能给大家一些帮助。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Tensorflow的神经网络解决用户流失概率问题]]></title>
      <url>/2017/12/01/%E5%9F%BA%E4%BA%8ETensorflow%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3%E7%94%A8%E6%88%B7%E6%B5%81%E5%A4%B1%E6%A6%82%E7%8E%87%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>用户流失一直都是公司非常重视的一个问题，也是AAARR中的Retention的核心问题，所以各大算法竞赛都很关注。比如最近的：<a href="https://www.kaggle.com/c/kkbox-churn-prediction-challenge" target="_blank" rel="noopener">KKBOX的会员流失预测算法竞赛</a>，如何能够搭建一个精准的模型成了大家探索的重要问题。<br>本文主要讲解神经网络、TensorFlow的概述、如何利用python基于TensorFlow神经网络对流失用户进行分类预测，及可能存在的一些常见问题，作为深度学习的入门阅读比较适合。</p>
<hr>
<h1 id="行业做法："><a href="#行业做法：" class="headerlink" title="行业做法："></a>行业做法：</h1><p>通常的行业预测用户流失大概分以下几种思路：</p>
<ul>
<li>利用线性模型(比如Logistic)＋非线性模型Xgboost判断用户是否回流逝</li>
</ul>
<p>这种方法有关是行业里面用的最多的，效果也被得意验证足够优秀且稳定的。核心点在于特征的预处理，Xgboost的参数挑优，拟合程度的控制，这个方法值得读者去仔细研究一边。问题也是很明显的，会有一个行业baseline，基本上达到上限之后，想有有提升会非常困难，对要求精准预测的需求会显得非常乏力。</p>
<ul>
<li>规则触发</li>
</ul>
<p>这种方法比较古老，但是任然有很多公司选择使用，实现成本较低而且非常快速。核心在于，先确定几条核心的流失指标(比如近7日登录时长)，然后动态的选择一个移动的窗口，不停根据已经流失的用户去更新流失指标的阈值。当新用户达到阈值的时候，触发流失预警。效果不如第一个方法，但是实现简单，老板也很容易懂。</p>
<ul>
<li>场景模型的预测</li>
</ul>
<p>这个方法比较依赖于公司业务的特征，如果公司业务有部分依赖于评论，可以做文本分析，比如我上次写的<a href="http://www.jianshu.com/p/413cff5b9f3a" target="_blank" rel="noopener">基于word2vec下的用户流失概率分析</a>。如果业务有部分依赖于登录打卡，可以做时间线上的频次预估。这些都是比较偏奇门易巧，不属于通用类别的，不过当第一种方法达到上线的时候，这种方法补充收益会非常的大。</p>
<p>其实还有很多其它方法，我这边也不一一列出了，这个领域的方法论还是很多的。</p>
<hr>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><ul>
<li><strong>核心</strong><br><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/1.png" alt="神经网络流程"><br>上面这张图片诠释了神经网络正向传播的流程，先通过线性变换(上图左侧)Σxw+b将线性可分的数据分离，再通过非线性变换(上图右侧)Sigmoid函数将非线性可分的数据分离，最后将输入空间投向另一个输出空间。</li>
</ul>
<p>根据上面所说，我们可以知道，通过增加左侧线性节点的个数，我们可以强化线性变换的力度；而通过增加层数，多做N次激活函数(比如上面提到的Sigmoid)可以增强非线性变换的能力。</p>
<p>通过矩阵的线性变换+激活矩阵的非线性变换，将原始不可分的数据，先映射到高纬度，再进行分离。但是这边左侧节点的个数，网络的层数选择是非常困难的课题，需要反复尝试。</p>
<ul>
<li><strong>参数训练</strong><br>刚才我们了解了整个训练的流程，但是如何训练好包括线性变换的矩阵系数是一个还没有解决的问题。</li>
</ul>
<p>我们来看下面的过程：<br>input ==&gt; Σxw+b(线性变换) ==&gt; f(Σxw+b)(激活函数) ==&gt; …(多层的话重复前面过程) ==&gt; output(到此为止，正向传播结束，反向修正矩阵weights开始) ==&gt; <strong>error=actual_output-output(计算预测值与正式值误差)==&gt;output处的梯度==&gt;调整后矩阵weight=当前矩阵weight+error<code>x</code>学习速率<code>x</code>output处的负梯度</strong><br>核心目的在于通过比较预测值和实际值来调整权重矩阵，将预测值与实际值的差值缩小。<br>比如：梯度下降的方法，通过计算当前的损失值的方向的负方向，控制学习速率来降低预测值与实际值间的误差。</p>
<p>利用一行代码来解释就是  <code>synaptic_weights += dot(inputs, (real_outputs - output) * output * (1 - output))*η</code><br>这边output * (1 - output))是在output处的Sigmoid的倒数形式，η是学习速率。</p>
<p><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/2.png" alt="weight的循环流程"></p>
<ul>
<li><strong>神经网络流程小结</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1数据集获取（有监督数据整理）</span><br><span class="line">2神经网络参数确定，有多少层，多少个节点，激活函数是什么，损失函数是什么</span><br><span class="line">3数据预处理，pca，标准化，中心化，特征压缩，异常值处理</span><br><span class="line">4初始化网络权重</span><br><span class="line">5网络训练</span><br><span class="line">5.1正向传播</span><br><span class="line">5.2计算loss</span><br><span class="line">5.3计算反向梯度</span><br><span class="line">5.4更新梯度</span><br><span class="line">5.5重新正向传播</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这边只是简单介绍了神经网络的基础知识，针对有一定基础的朋友唤醒记忆，如果纯小白用户，建议从头开始认真阅读理解一遍过程，避免我讲的有偏颇的地方对你进行误导。</p>
<hr>
<h1 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h1><p>理论上讲，TensorFlow工具可以单独写一本书，用法很多而且技巧性的东西也非常的复杂，这边我们主要作为工具进行使用，遇到新技巧会在code中解释，但不做全书的梳理，建议去买一本《TensorFlow实战Google深度学习框架》，简单易懂。</p>
<p>TensorFlow是谷歌于2015年11月9日正式开源的计算框架，由Jeff Dean领导的谷歌大脑团队改编的DistBelief得到的，在ImageNet2014、YouTube视频学习，语言识别错误率优化，街景识别，广告，电商等等都有了非常优秀的产出，是我个人非常喜欢的工具。</p>
<p>除此之外，我在列出一些其他的框架工具供读者使用：</p>
<p><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/3.png" alt=""></p>
<p><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/4.png" alt=""></p>
<p>接下来看一下最基本的语法，方便之后我们直接贴代码的时候可以轻松阅读。</p>
<ul>
<li><p>张量：可以理解为多维array 或者 list，time决定张量是什么<br>tf.placeholder(time,shape,name)</p>
</li>
<li><p>变量：同一时刻下的不变的数据<br>tf.Variable(value,name)</p>
</li>
<li><p>常量：永远不变的常值<br>tf.constant(value)</p>
</li>
<li><p>执行环境开启与关闭，在环境中才能运行TensorFlow语法<br>sess=tf.Session()<br>sess.close()<br>sess.run(op)</p>
</li>
<li><p>初始化所有权重：类似于变量申明<br>tf.initialize_all_variables()</p>
</li>
<li><p>更新权重：<br>tf.assign(variable_to_be_updated,new_value)</p>
</li>
<li><p>加值行为，利用feed_dict里面的值来训练[output]函数<br>sess.run([output],feed_dict={input1:value1,input2:value2})<br>利用input1，input2，来跑output的值</p>
</li>
<li><p>矩阵乘法，类似于dot<br>tf.matmul(input,layer1)</p>
</li>
<li><p>激活函数，relu<br>tf.nn.relu()，除此之外，还有tf.nn.sigmoid，tf.nn.tanh等等<br><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/5.png" alt=""></p>
</li>
</ul>
<hr>
<h1 id="用户流失分析"><a href="#用户流失分析" class="headerlink" title="用户流失分析"></a>用户流失分析</h1><p>说了那么多前置的铺垫，让我们来真实的面对我们需要解决的问题：</p>
<p>首先，我们拿到了用户是否流失的历史数据集20724条，流失与飞流失用户占比在1:4，这部分数据需要进行一下预处理，这边就不细讲预处理过程了，包含缺失值填充(分层填充)，异常值处理(isolation foest)，数据平衡(tomek link)，特征选择(xgboost importance)，特征变形(normalizing)，特征分布优化等等，工程技巧我之前的文章都有讲解过，不做本文重点。</p>
<p><strong>taiking is cheap,show me the code.</strong><br><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/6.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#author:shataowei</span><br><span class="line">#time:20170924</span><br><span class="line">#基础包加载</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import math</span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#数据处理</span><br><span class="line">data = pd.read_table(&apos;/Users/slade/Desktop/machine learning/data/data_all.txt&apos;)</span><br><span class="line">data = data.iloc[:,1:len(data.columns)]</span><br><span class="line">data1 = (data - data.mean())/data.std()</span><br><span class="line">labels = data[&apos;tag&apos;]</span><br><span class="line">items = data1.iloc[:,1:len(data1.columns)]</span><br><span class="line">all_data = pd.concat([pd.DataFrame(labels),items],axis = 1)</span><br><span class="line"></span><br><span class="line">#数据集切分成训练集和测试集，占比为0.8：0.2</span><br><span class="line">train_X,test_X,train_y,test_y = train_test_split(items,labels,test_size = 0.2,random_state = 0)</span><br><span class="line"></span><br><span class="line">#pandas读取进来是dataframe，转换为ndarray的形式</span><br><span class="line">train_X = np.array(train_X)</span><br><span class="line">test_X = np.array(test_X)</span><br><span class="line"></span><br><span class="line">#我将0或者1的预测结果转换成了[0,1]或者[1,0]的对应形式，读者可以不转</span><br><span class="line">train_Y = []</span><br><span class="line">for i in train_y:</span><br><span class="line">if i ==0:</span><br><span class="line">train_Y.append([0,1])</span><br><span class="line">else:</span><br><span class="line">train_Y.append([1,0])</span><br><span class="line">test_Y = []</span><br><span class="line">for i in test_y:</span><br><span class="line">if i ==0:</span><br><span class="line">test_Y.append([0,1])</span><br><span class="line">else:</span><br><span class="line">test_Y.append([1,0])</span><br></pre></td></tr></table></figure>
<p>下面我们就要开始正式开始训练神经网络了，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">input_node = 9 #输入的feature的个数，也就是input的维度</span><br><span class="line">output_node = 2 #输出的[0,1]或者[1,0]的维度</span><br><span class="line">layer1_node = 500 #隐藏层的节点个数，一般在255-1000之间，读者可以自行调整</span><br><span class="line">batch_size = 200 #批量训练的数据，batch_size越小训练时间越长，训练效果越准确（但会存在过拟合）</span><br><span class="line">learning_rate_base = 0.8 #训练weights的速率η</span><br><span class="line">regularzation_rate = 0.0001 #正则力度</span><br><span class="line">training_steps = 10000 #训练次数，这个指标需要类似grid_search进行搜索优化</span><br><span class="line"></span><br><span class="line">#设定之后想要被训练的x及对应的正式结果y_</span><br><span class="line">x = tf.placeholder(tf.float32,[None,input_node])</span><br><span class="line">y_ = tf.placeholder(tf.float32,[None,output_node])</span><br><span class="line"></span><br><span class="line">#input到layer1之间的线性矩阵weight</span><br><span class="line">weight1 = tf.Variable(tf.truncated_normal([input_node,layer1_node],stddev=0.1))</span><br><span class="line">#layer1到output之间的线性矩阵weight</span><br><span class="line">weight2 = tf.Variable(tf.truncated_normal([layer1_node,output_node],stddev=0.1))</span><br><span class="line">#input到layer1之间的线性矩阵的偏置</span><br><span class="line">biases1 = tf.Variable(tf.constant(0.1,shape = [layer1_node]))</span><br><span class="line">#layer1到output之间的线性矩阵的偏置</span><br><span class="line">biases2 = tf.Variable(tf.constant(0.1,shape=[output_node]))</span><br><span class="line"></span><br><span class="line">#正向传播的流程，线性计算及激活函数relu的非线性计算得到result</span><br><span class="line">def interence(input_tensor,weight1,weight2,biases1,biases2):</span><br><span class="line">layer1 = tf.nn.relu(tf.matmul(input_tensor,weight1)+biases1)</span><br><span class="line">result = tf.matmul(layer1,weight2)+biases2</span><br><span class="line">return result</span><br><span class="line">y = interence(x,weight1,weight2,biases1,biases2)</span><br></pre></td></tr></table></figure></p>
<p>正向传播完成后，我们要反向传播来修正weight<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0,trainable = False)</span><br><span class="line">#交叉熵，用来衡量两个分布之间的相似程度</span><br><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = y_,logits=y)</span><br><span class="line">cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line"></span><br><span class="line">#l2正则化，这部分的理论分析可以参考我之前写的：http://www.jianshu.com/p/4f91f0dcba95</span><br><span class="line">regularzer = tf.contrib.layers.l2_regularizer(regularzation_rate)</span><br><span class="line">regularzation = regularzer(weight1) + regularzer(weight2)</span><br><span class="line"></span><br><span class="line">#损失函数为交叉熵+正则化</span><br><span class="line">loss = cross_entropy_mean + regularzation</span><br><span class="line"></span><br><span class="line">#我们用learning_rate_base作为速率η，来训练梯度下降的loss函数解</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(learning_rate_base).minimize(loss,global_step = global_step)</span><br><span class="line"></span><br><span class="line">#y是我们的预测值，y_是真实值，我们来找到y_及y(比如[0.1，0.2])中最大值对应的index位置，判断y与y_是否一致</span><br><span class="line">correction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))</span><br><span class="line"></span><br><span class="line">#如果y与y_一致则为1，否则为0，mean正好为其准确率</span><br><span class="line">accurary = tf.reduce_mean(tf.cast(correction,tf.float32))</span><br></pre></td></tr></table></figure></p>
<p>模型训练结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#初始化环境，设置输入值，检验值</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line">validate_feed = &#123;x:train_X,y_:train_Y&#125;</span><br><span class="line">test_feed = &#123;x:test_X,y_:test_Y&#125;</span><br><span class="line"></span><br><span class="line">#模型训练，每到1000次汇报一次训练效果</span><br><span class="line">for i in range(training_steps):</span><br><span class="line">start = (i*batch_size)%len(train_X)</span><br><span class="line">end = min(start+batch_size,16579)</span><br><span class="line">xs = train_X[start:end]</span><br><span class="line">ys = train_Y[start:end]</span><br><span class="line">if i%1000 ==0:</span><br><span class="line">validate_accuary = sess.run(accurary,feed_dict = validate_feed)</span><br><span class="line">print &apos;the times of training is %d, and the accurary is %s&apos; %(i,validate_accuary)</span><br><span class="line">sess.run(train_op,feed_dict = &#123;x:xs,y_:ys&#125;)</span><br></pre></td></tr></table></figure></p>
<p>训练的结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2017-09-24 12:11:28.409585: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">2017-09-24 12:11:28.409620: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">2017-09-24 12:11:28.409628: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">2017-09-24 12:11:28.409635: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">the times of training is 0, and the accurary is 0.736775</span><br><span class="line">the times of training is 1000, and the accurary is 0.99246</span><br><span class="line">the times of training is 2000, and the accurary is 0.993003</span><br><span class="line">the times of training is 3000, and the accurary is 0.992943</span><br><span class="line">the times of training is 4000, and the accurary is 0.992943</span><br><span class="line">the times of training is 5000, and the accurary is 0.99234</span><br><span class="line">the times of training is 6000, and the accurary is 0.993124</span><br><span class="line">the times of training is 7000, and the accurary is 0.992943</span><br><span class="line">the times of training is 8000, and the accurary is 0.993124</span><br><span class="line">the times of training is 9000, and the accurary is 0.992943</span><br></pre></td></tr></table></figure></p>
<p>初步看出，在训练集合上，准确率在能够99%以上，让我们在看看测试集效果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_accuary = sess.run(accurary,feed_dict = test_feed)</span><br></pre></td></tr></table></figure></p>
<p><code>Out[5]: 0.99034983</code>,也是我们的测试数据集效果也是在99%附近，可以看出这个分类的效果还是比较高的。</p>
<p>初次之外，我们还可以得到每个值被预测出来的结果，也可以通过工程技巧转换为0-1的概率：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">result_y = sess.run(y,feed_dict=&#123;x:train_X&#125;)</span><br><span class="line">result_y_update=[]</span><br><span class="line">for i in result_y:</span><br><span class="line">if i[0]&gt;=i[1]:</span><br><span class="line">result_y_update.append([1,0])</span><br><span class="line">else:</span><br><span class="line">result_y_update.append([0,1])</span><br></pre></td></tr></table></figure></p>
<p>==&gt;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Out[7]:</span><br><span class="line">array([[-1.01412344,  1.21461654],</span><br><span class="line">[-3.66026735,  3.81834102],</span><br><span class="line">[-3.78952932,  3.79097509],</span><br><span class="line">...,</span><br><span class="line">[-3.71239662,  3.65721083],</span><br><span class="line">[-1.59250259,  1.89412308],</span><br><span class="line">[-3.35591984,  3.24001145]], dtype=float32)</span><br></pre></td></tr></table></figure></p>
<p>以上就实现了如果用TensorFlow里面的神经网络技巧去做一个分类问题，其实这并不TensorFlow的全部，传统的Bp神经网络，SVM也可以到达近似的效果，在接下来的文章中，我们将继续看到比如CNN图像识别，LSTM进行文本分类，RNN训练不均衡数据等复杂问题上面的优势。</p>
<p>##可能存在的问题<br>在刚做神经网络的训练前，要注意一下是否会犯以下的错误。</p>
<ul>
<li><p>数据是否规范化<br>模型计算的过程时间长度及模型最后的效果，均依赖于input的形式。大部分的神经网络训练过程都是以input为1的标准差，0的均值为前提的；除此之外，在算梯度算反向传播的时候，过大的值有可能会导致梯度消失等意想不到的情况，非常值得大家注意</p>
</li>
<li><p>batch的选择<br>在上面我也提了，过小的batch会增加模型过拟合的风险，且计算的时间大大增加。过大的batch会造成模型的拟合能力不足，可能会被局部最小值卡住等等，所以需要多次选择并计算尝试。</p>
</li>
<li><p>过拟合的问题<br>是否在计算过程中只考虑了损失函数比如交叉熵，有没有考虑l2正则、l2正则，或者有没有进行dropout行为，是否有必要加入噪声，在什么地方加入噪声（weight？input？），需不需要结合Bagging或者bayes方法等</p>
</li>
<li><p>激活函数的选择是否正确<br>比如relu只能产出&gt;=0的结果，是否符合最后的产出结果要求。比如Sigmoid的函数在数据离散且均大于+3的数据集合上会产生梯度消失的问题，等等</p>
</li>
</ul>
<hr>
<p>到这里，我觉得一篇用TensorFlow来训练分类模型来解决用户流失这个问题就基本上算是梳理完了。很多简单的知识点我没有提，上面这些算是比较重要的模块，希望对大家有所帮助，最后谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CRM预估 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于SSD下的图像内容识别（二）]]></title>
      <url>/2017/12/01/%E5%9F%BA%E4%BA%8ESSD%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>上一节粗略的描述了如何关于图像识别，抠图，分类的理论相关，本节主要用代码，来和大家一起分析每一步骤。<br>看完本节，希望你也能独立完成自己的图片、视频的内容实时定位。</p>
<p>首先，我们需要安装TensorFlow环境，建议利用<a href="https://www.anaconda.com/downloads" target="_blank" rel="noopener">conda</a>进行安装，配置，90%尝试单独安装的人最后都挂了。</p>
<p>其次，我们需要安装从git上下载训练好的模型，git clone <a href="https://github.com/balancap/SSD-Tensorflow" target="_blank" rel="noopener">https://github.com/balancap/SSD-Tensorflow</a><br>如果没有安装git的朋友，请自行百度安装。</p>
<p>最后找到你下载的位置进行解压，unzip ./SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt.zip<br><strong>这边务必注意，网上90%的教程这边就结束了，其实你这样是最后跑不通代码的，你需要把解压的文件进行移动到checkpoint的文件夹下面</strong>，这个问题git上这个同学解释了，详细的去看下<a href="https://github.com/balancap/SSD-Tensorflow/issues/150" target="_blank" rel="noopener">https://github.com/balancap/SSD-Tensorflow/issues/150</a></p>
<p>最后的最后，下载你需要检测的网路图片，就ok了</p>
<p>预处理步骤完成了，下面让我们看代码。<br><strong>加载相关的包：</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> mpcm</span><br><span class="line">sys.path.append(<span class="string">'./SSD-Tensorflow/'</span>)</span><br><span class="line"><span class="keyword">from</span> nets <span class="keyword">import</span> ssd_vgg_300, ssd_common, np_methods</span><br><span class="line"><span class="keyword">from</span> preprocessing <span class="keyword">import</span> ssd_vgg_preprocessing</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>配置相关TensorFlow环境</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpu_options = tf.GPUOptions(allow_growth=<span class="keyword">True</span>)</span><br><span class="line">config = tf.ConfigProto(log_device_placement=<span class="keyword">False</span>, gpu_options=gpu_options)</span><br><span class="line">isess = tf.InteractiveSession(config=config)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>做图片的格式的处理，使他满足input的条件</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们用的TensorFlow下的一个集成包slim，比tensor要更加轻便</span></span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line"><span class="comment">#训练数据中包含了一下已知的类别，也就是我们可以识别出以下的东西，不过后续我们将自己自己训练自己的模型，来识别自己想识别的东西</span></span><br><span class="line">l_VOC_CLASS = [</span><br><span class="line"><span class="string">'aeroplane'</span>,   <span class="string">'bicycle'</span>, <span class="string">'bird'</span>,  <span class="string">'boat'</span>,      <span class="string">'bottle'</span>,</span><br><span class="line"><span class="string">'bus'</span>,         <span class="string">'car'</span>,     <span class="string">'cat'</span>,   <span class="string">'chair'</span>,     <span class="string">'cow'</span>,</span><br><span class="line"><span class="string">'diningTable'</span>, <span class="string">'dog'</span>,     <span class="string">'horse'</span>, <span class="string">'motorbike'</span>, <span class="string">'person'</span>,</span><br><span class="line"><span class="string">'pottedPlant'</span>, <span class="string">'sheep'</span>,   <span class="string">'sofa'</span>,  <span class="string">'train'</span>,     <span class="string">'TV'</span></span><br><span class="line">]</span><br><span class="line"><span class="comment"># 定义数据格式</span></span><br><span class="line">net_shape = (<span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">data_format = <span class="string">'NHWC'</span>  <span class="comment"># [Number, height, width, color]，Tensorflow backend 的格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理将输入图片大小改成 300x300，作为下一步输入</span></span><br><span class="line">img_input = tf.placeholder(tf.uint8, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">3</span>))</span><br><span class="line">image_pre, labels_pre, bboxes_pre, bbox_img = ssd_vgg_preprocessing.preprocess_for_eval(</span><br><span class="line">img_input,</span><br><span class="line"><span class="keyword">None</span>,</span><br><span class="line"><span class="keyword">None</span>,</span><br><span class="line">net_shape,</span><br><span class="line">data_format,</span><br><span class="line">resize=ssd_vgg_preprocessing.Resize.WARP_RESIZE</span><br><span class="line">)</span><br><span class="line">image_4d = tf.expand_dims(image_pre, <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>下面我们来载入SSD作者已经搞定的模型</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 SSD 模型结构</span></span><br><span class="line">reuse = <span class="keyword">True</span> <span class="keyword">if</span> <span class="string">'ssd_net'</span> <span class="keyword">in</span> locals() <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line">ssd_net = ssd_vgg_300.SSDNet()</span><br><span class="line"><span class="keyword">with</span> slim.arg_scope(ssd_net.arg_scope(data_format=data_format)):</span><br><span class="line">predictions, localisations, _, _ = ssd_net.net(image_4d, is_training=<span class="keyword">False</span>, reuse=reuse)</span><br><span class="line"><span class="comment"># 导入官方给出的 SSD 模型参数</span></span><br><span class="line"><span class="comment">#这边修改成你自己的路径</span></span><br><span class="line">ckpt_filename = <span class="string">'/Users/slade/SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt'</span></span><br><span class="line">isess.run(tf.global_variables_initializer())</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.restore(isess, ckpt_filename)</span><br><span class="line">ssd_anchors = ssd_net.anchors(net_shape)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>下面让我们把SSD识别出来的结果在图片中表示出来</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不同类别，我们以不同的颜色表示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colors_subselect</span><span class="params">(colors, num_classes=<span class="number">21</span>)</span>:</span></span><br><span class="line">dt = len(colors) // num_classes</span><br><span class="line">sub_colors = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_classes):</span><br><span class="line">color = colors[i*dt]</span><br><span class="line"><span class="keyword">if</span> isinstance(color[<span class="number">0</span>], float):</span><br><span class="line">sub_colors.append([int(c * <span class="number">255</span>) <span class="keyword">for</span> c <span class="keyword">in</span> color])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">sub_colors.append([c <span class="keyword">for</span> c <span class="keyword">in</span> color])</span><br><span class="line"><span class="keyword">return</span> sub_colors</span><br><span class="line"><span class="comment">#画出在图中的位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bboxes_draw_on_img</span><span class="params">(img, classes, scores, bboxes, colors, thickness=<span class="number">5</span>)</span>:</span></span><br><span class="line">shape = img.shape</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(bboxes.shape[<span class="number">0</span>]):</span><br><span class="line">bbox = bboxes[i]</span><br><span class="line">color = colors[classes[i]]</span><br><span class="line"><span class="comment"># Draw bounding box...</span></span><br><span class="line">p1 = (int(bbox[<span class="number">0</span>] * shape[<span class="number">0</span>]), int(bbox[<span class="number">1</span>] * shape[<span class="number">1</span>]))</span><br><span class="line">p2 = (int(bbox[<span class="number">2</span>] * shape[<span class="number">0</span>]), int(bbox[<span class="number">3</span>] * shape[<span class="number">1</span>]))</span><br><span class="line">cv2.rectangle(img, p1[::<span class="number">-1</span>], p2[::<span class="number">-1</span>], color, thickness)</span><br><span class="line"><span class="comment"># Draw text...</span></span><br><span class="line">s = <span class="string">'%s:%.3f'</span> % ( l_VOC_CLASS[int(classes[i])<span class="number">-1</span>], scores[i])</span><br><span class="line">p1 = (p1[<span class="number">0</span>]<span class="number">-5</span>, p1[<span class="number">1</span>])</span><br><span class="line">cv2.putText(img, s, p1[::<span class="number">-1</span>], cv2.FONT_HERSHEY_SIMPLEX, <span class="number">1</span>, color, <span class="number">2</span>)</span><br><span class="line">colors_plasma = colors_subselect(mpcm.plasma.colors, num_classes=<span class="number">21</span>)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>让我们开始训练吧</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_image</span><span class="params">(img, select_threshold=<span class="number">0.3</span>, nms_threshold=<span class="number">.8</span>, net_shape=<span class="params">(<span class="number">300</span>, <span class="number">300</span>)</span>)</span>:</span></span><br><span class="line"><span class="comment">#先获取SSD网络的层相关的参数</span></span><br><span class="line">rimg, rpredictions, rlocalisations, rbbox_img = isess.run([image_4d, predictions, localisations, bbox_img],</span><br><span class="line">feed_dict=&#123;img_input: img&#125;)</span><br><span class="line"><span class="comment">#获取分类结果，位置</span></span><br><span class="line">rclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(</span><br><span class="line">rpredictions, rlocalisations, ssd_anchors,</span><br><span class="line">select_threshold=select_threshold, img_shape=net_shape, num_classes=<span class="number">21</span>, decode=<span class="keyword">True</span>)</span><br><span class="line">rbboxes = np_methods.bboxes_clip(rbbox_img, rbboxes)</span><br><span class="line">rclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=<span class="number">400</span>)</span><br><span class="line">rclasses, rscores, rbboxes = np_methods.bboxes_nms(rclasses, rscores, rbboxes, nms_threshold=nms_threshold)</span><br><span class="line"><span class="comment"># 让我们在图中画出来就行了</span></span><br><span class="line">rbboxes = np_methods.bboxes_resize(rbbox_img, rbboxes)</span><br><span class="line">bboxes_draw_on_img(img, rclasses, rscores, rbboxes, colors_plasma, thickness=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">return</span> img</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>预处理的函数都写完了，我们就可以执行了。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">img = cv2.imread(<span class="string">"/Users/slade/Documents/Yoho/picture_recognize/test7.jpg"</span>)</span><br><span class="line">img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">plt.imshow(process_image(img))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>img的数据形式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">In [8]: img</span><br><span class="line">Out[8]:</span><br><span class="line">array([[[ 35,  59,  43],</span><br><span class="line">[ 37,  60,  44],</span><br><span class="line">[ 38,  61,  45],</span><br><span class="line">...,</span><br><span class="line">[ 73,  99,  62],</span><br><span class="line">[ 74,  99,  60],</span><br><span class="line">[ 72,  97,  57]],</span><br><span class="line"></span><br><span class="line">[[ 37,  60,  44],</span><br><span class="line">[ 37,  60,  44],</span><br><span class="line">[ 37,  60,  44],</span><br><span class="line">...,</span><br><span class="line">[ 66,  92,  57],</span><br><span class="line">[ 67,  93,  56],</span><br><span class="line">[ 67,  92,  53]],</span><br><span class="line"></span><br><span class="line">[[ 37,  60,  44],</span><br><span class="line">[ 36,  59,  43],</span><br><span class="line">[ 37,  58,  43],</span><br><span class="line">...,</span><br><span class="line">[ 56,  83,  48],</span><br><span class="line">[ 60,  86,  51],</span><br><span class="line">[ 61,  87,  50]],</span><br><span class="line"></span><br><span class="line">...,</span><br><span class="line">[[ 96, 101,  95],</span><br><span class="line">[107, 109, 104],</span><br><span class="line">[ 98,  97,  95],</span><br><span class="line">...,</span><br><span class="line">[ 84, 126,  76],</span><br><span class="line">[ 72, 118,  72],</span><br><span class="line">[ 78, 126,  86]],</span><br><span class="line"></span><br><span class="line">[[ 98, 103,  96],</span><br><span class="line">[114, 116, 111],</span><br><span class="line">[112, 113, 108],</span><br><span class="line">...,</span><br><span class="line">[ 94, 137,  84],</span><br><span class="line">[ 87, 133,  86],</span><br><span class="line">[105, 153, 111]],</span><br><span class="line"></span><br><span class="line">[[ 99, 105,  95],</span><br><span class="line">[110, 113, 106],</span><br><span class="line">[134, 135, 129],</span><br><span class="line">...,</span><br><span class="line">[127, 170, 116],</span><br><span class="line">[121, 167, 118],</span><br><span class="line">[131, 180, 135]]], dtype=uint8)</span><br></pre></td></tr></table></figure></p>
<p>处理后的结果如下：<img src="/2017/12/01/基于SSD下的图像内容识别（二）/1.png" alt=""></p>
<p>是不是非常无脑，上面的代码直接复制就可以完成。</p>
<p>下面在拓展一下视频的处理方式，其实相关的内容是一致的。<br>利用moviepy.editor包里面的VideoFileClip的切片的功能，然后对每一次切片的结果进行process_image过程就可以了，这边就不贴代码了，需要的朋友私密我。</p>
<p>最后感谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 图像识别 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[动态最优化经典面试题]]></title>
      <url>/2017/12/01/%E5%8A%A8%E6%80%81%E6%9C%80%E4%BC%98%E5%8C%96%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>最近看到了一条史前的算法面试题，觉得挺有意思的，虽然网上已经有了很多完善的答案，但是我还是想自己整理一遍，强化印象，同时也和大家分享一下这道12年的Google题目：</p>
<p><strong>一幢 200 层的大楼，给你两个鸡蛋。如果在第 n 层扔下鸡蛋，鸡蛋不碎，那么从第 n-1 层扔鸡蛋，都不碎。这两只鸡蛋一模一样，不碎的话可以扔无数次。最高从哪层楼扔下时鸡蛋不会碎？</strong></p>
<p>先形象的理解一下这道题目，假设第一个蛋我们放在了i层，有两种case，碎或者不碎。<br>先看简单的结果，<br>case1:如果碎了，为了求出层数，那么我接下来的那颗蛋需要从第1层开始尝试i-1次，因为我们不允许冒第二次碎的风险了，这很好理解。<br>case2:如果没碎，我们得知一条新的信息，那就是我们要求的目标层在i层之上，但是我们依旧不知道是哪一层，假设是m层（m&gt;i）,那么同样的，和第i层一样，面临2个case，碎或者不碎。</p>
<p>这时候，我们的前提是在最恶劣的情况下，保证我们的每次的风险都尽可能的小，至少要少于上一次的风险。</p>
<p>我们可以让新的m的高度为i+i-1,其中，i是第一次我们放的层数，i-1是我们选择的风险若于第一次风险的层数高度，类推下去：i+i-1+i-2+i-3+…+1=200，得到i=20，就是我们第一次应该放的位置，同理第二次如果没有碎应该放的就是39…</p>
<p><strong>我个人对这道题目的理解中，其实就为了平分风险，让每次碎的高度都相等，也就是i-1 = m-i-1+1==&gt;m=2i-1</strong></p>
<p>这边的python代码网上也有很多，这边我罗列一个我写的，可能和别人的不一样，实现效率也可能较慢，建议大家在网上搜完善版本的，仅供大家熟悉上述的描述：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#n为层数，m为蛋数，f函数为求最优层数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(n, m)</span>:</span></span><br><span class="line"><span class="comment">#如果是0层的，返回最优层数为0</span></span><br><span class="line"><span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment">#如果只有一个鸡蛋，必须要从最低层开始试，所以为当前最安全层n</span></span><br><span class="line"><span class="keyword">if</span> m == <span class="number">1</span>:</span><br><span class="line"><span class="keyword">return</span> n</span><br><span class="line"><span class="comment">#这边我们来看，f(i - 1, m - 1)是如果i层碎了，我们需要计算i-1层下的情况，同时减少一颗蛋；</span></span><br><span class="line"><span class="comment">#f(n - i, m)是i层没碎，那相当于安全层从0变成了n，要计算的就是相当于有 f(n, m)变成了 f(n - i, m)</span></span><br><span class="line"><span class="comment">#最后在最大化风险下找出其中风险最小的层数即可</span></span><br><span class="line">best_floor = min([max([f(i - <span class="number">1</span>, m - <span class="number">1</span>), f(n - i, m)]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>)] + <span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> best_floor</span><br></pre></td></tr></table></figure></p>
<p>结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]:print(f(<span class="number">100</span>, <span class="number">2</span>))</span><br><span class="line"><span class="number">14</span></span><br></pre></td></tr></table></figure></p>
<p>这边f(200,2)实在没跑出来，时间太久了，所以跑了100，2的结果，迭代次数超多，具体我没有算过，建议优化一下计算的代码再执行。</p>
<p>最后谢谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[用户生命周期]]></title>
      <url>/2017/11/02/%E7%94%A8%E6%88%B7%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</url>
      <content type="html"><![CDATA[<blockquote>
<p>摘要：设计一套完整的用户生命周期策略，极大程度上会提高用户活跃，降低用户流失，反应用户留存，为平台运营的不可或缺的一环</p>
</blockquote>
<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>用户生命周期是指用户从加入平台开始，熟悉平台，参与平台，最终流失的整个过程。用户的生命周期相对于自身而言，是一种参与度的变化，参与度也可以称之为活跃度。</p>
<hr>
<p>###如何定义参与度？<br>以电商平台而言，冒泡（打开app），浏览，点击，搜索，收藏，加购物车，下单，评论等都是用户参与平台的主要行为，综合考虑（但不限于此）这些因素，</p>
<p><strong>活跃度：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">θ = α1* pv + α2 * click + α3* search + α4 * clc + α5* cart + α6* order + α7* comment + bubble</span><br></pre></td></tr></table></figure></p>
<p><em>其中，θ是活跃度，pv是用户浏览量，click是用户点击量，search是用户搜索量，clc是用户收藏量，cart是用户加购物车次，order为用户订单量，comment为用户评论量<br>α1 为全部用户冒泡次数 与 全部用户浏览量之比；<br>α2 为全部用户冒泡次数 与 全部用户点击量之比；<br>…
</em><br><strong>这样保证了，所有平台参与行为与用户活跃情况成正相关，同时动态变化的降低了操作成本低的变量的权重，也满足奥卡姆剃刀原理</strong><br>后续再利用活跃度来直接衡量生命周期状态。</p>
<h1 id="如何定义生命周期？"><a href="#如何定义生命周期？" class="headerlink" title="如何定义生命周期？"></a>如何定义生命周期？</h1><ul>
<li>以电商平台为例，考虑用户的行为，先来定义生命周期状态划分逻辑：<br>1.计算用户连续N(N&gt;3)个周期内的参与度组成特征向量<br>2.形成不同生命周期下的模式特征向量<br>3.分类用户的特征向量如下：</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">生命周期状态</th>
<th style="text-align:center">生命周期类型</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">初生期</td>
<td style="text-align:center">新增用户</td>
<td style="text-align:center">处于新生用户没有用户记录</td>
</tr>
<tr>
<td style="text-align:center">成长期</td>
<td style="text-align:center">显性成长</td>
<td style="text-align:center">最近三次生命周期状态都是成长期</td>
</tr>
<tr>
<td style="text-align:center">成长期</td>
<td style="text-align:center">隐性成长</td>
<td style="text-align:center">最近三次生命周期状态不全是成长期</td>
</tr>
<tr>
<td style="text-align:center">稳定期</td>
<td style="text-align:center">低稳定</td>
<td style="text-align:center">处于平稳期阶段，参与度低于1/4分位数</td>
</tr>
<tr>
<td style="text-align:center">稳定期</td>
<td style="text-align:center">中稳定</td>
<td style="text-align:center">处于平稳期阶段，参与度介于1/4-3/4分位数</td>
</tr>
<tr>
<td style="text-align:center">稳定期</td>
<td style="text-align:center">高稳定</td>
<td style="text-align:center">处于平稳期阶段，参与度高于3/4分位数</td>
</tr>
<tr>
<td style="text-align:center">衰退期</td>
<td style="text-align:center">轻微衰退</td>
<td style="text-align:center">连续x个周期进入衰退期或流失期</td>
</tr>
<tr>
<td style="text-align:center">衰退期</td>
<td style="text-align:center">重度微衰退</td>
<td style="text-align:center">连续x个周期进入衰退期或流失期</td>
</tr>
<tr>
<td style="text-align:center">流失期</td>
<td style="text-align:center">流失期</td>
<td style="text-align:center">刚进入流失期</td>
</tr>
<tr>
<td style="text-align:center">沉默期</td>
<td style="text-align:center">沉默期</td>
<td style="text-align:center">长期处于流失期</td>
</tr>
</tbody>
</table>
<ul>
<li>定义完整的用户生命周期状态后，再对用户的生命周期做session切分，根据聚类算法，将样本用户进行聚类，形成聚类中心，判断用户距离聚类中心距离，匹配用户所处的生命周期详细位置，反过来输出分位数，判断用户生命周期类型。</li>
</ul>
<h1 id="下面思考如何优化kmeans解决这个问题："><a href="#下面思考如何优化kmeans解决这个问题：" class="headerlink" title="下面思考如何优化kmeans解决这个问题："></a>下面思考如何优化kmeans解决这个问题：</h1><p>考虑到业务开发的效率等原因，常规的聚类算法中，kmeans常常为优先考虑的算法，但实际运用过程中，需要根据不同的问题有差异化的优化。</p>
<p>1.考虑用户的特征偏移<br>可能存在用户的活跃属性间断，比如用户外出出差一周，导致某个单位统计时间内平台参与度下降，用户的活跃属性下降，而实际用户为真实高活跃用户，只是出现异常间断点，影响用户活跃的最终判断，利用语义分析中的最佳路径计算方式解决这个问题。<br><img src="/2017/11/02/用户生命周期/1.png" alt=""><br>这三条线中，蓝色和青色线的分布走势类似，而红色线条的差异较大；计算蓝色–&gt;红色的欧式距离，蓝色–&gt;青色的欧式距离，发现蓝色–&gt;青色的欧式距离反而大于蓝色–&gt;红色的欧式距离，时间波动的情况下，欧式距离偏差较大。</p>
<p>所以，常规意义上的kmeans等基于欧式距离的算法这种情况下，使用较为局限。所以在整体思路不变的情况下，就距离计算，我们可以参考语音分析里面的DP（最佳路径规划算法），构造邻接矩阵，寻找最小最小路径和</p>
<p><img src="/2017/11/02/用户生命周期/2.png" alt=""><br>实际在计算蓝色曲线到青色曲线的距离的时候，同时计算AB（蓝色曲线当前位置A点到前一个时间段青色曲线位置B）、AC（蓝色曲线当前位置A点到当前时间段青色曲线位置C），AD（蓝色曲线当前位置A点到后一个时间段青色曲线位置D）的距离，综合判断一个点最短路径；再根据曲线上的每一个点，会形成一个矩阵，判断矩阵的每个点的最佳路径即可</p>
<p>可以用如下的公式表述：<br><img src="/2017/11/02/用户生命周期/3.png" alt=""><br>其中，<img src="/2017/11/02/用户生命周期/4.png" alt="">就是路径选择的过程</p>
<p>以上述的计算方式替换掉常规的kmeans中的欧式距离，提高了相似度的计算精度。</p>
<p>2.常规等距划分session不适用于生命周期</p>
<p>就用户平台活跃而言，不同用户可采用的用户时间窗口不同，新加入的用户可能可获取的时间长度较短；用户判断过程中的session与平台确定已知的生命周期session固定判断长度也是不相同的。同时，kmeans中的距离判断方法不能同时考虑到不同session下的距离计算问题</p>
<p><strong>最简单常规的计算方式：</strong><br>是补全较短的session的时间窗口，在相同的时间窗口之下，再去计算较短的时间窗口与较长的时间窗口下的生命周期的均值，这样会人为干涉过多，数据质量较低，图b即为数据补齐</p>
<p><strong>“STS距离”计算方式：</strong><br>在长时间窗口{r}集合中，寻找时间窗口长度子集，使得子集中的元长度与s曲线缺失的长度一致，在以s断点处开始向后寻找{r}子集合中的所有满足的元，再以均值时间序列替换原来的子集中的元作为r和s的拟合曲线，循环往复计算中心曲线2，如图c</p>
<p><img src="/2017/11/02/用户生命周期/5.png" alt=""></p>
<p>有了补齐长度下的中心曲线，再便可采用kmeans的常规方式，计算各时间长度窗口下的生命周期的距离</p>
<p>3.附加限制属性<br>再最后落地生命周期的长度的时候，考虑到商品平台的特殊属性，比如：</p>
<ul>
<li>商品周期性（奶粉用户周期购买等）</li>
<li>用户偏好属性(酒店用户品质偏好等）</li>
<li>平台的时间依赖情况(夏季冬季季节偏好等）</li>
<li>……<br>以上即为如何通过kmeans来确定一个用户所属的生命周期阶段</li>
</ul>
<p><em>本文参考文献如下：<br>1.<a href="http://www.doc88.com/p-0092455469704.html" target="_blank" rel="noopener">不等长时间序列下的滑窗相似度</a><br>2.<a href="http://www.jianshu.com/p/1417fcb06797" target="_blank" rel="noopener">kmeans距离计算方式剖析</a>
</em></p>
]]></content>
      
        <categories>
            
            <category> 特征刻画 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 生命周期 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[交叉销售算法]]></title>
      <url>/2017/11/01/%E4%BA%A4%E5%8F%89%E9%94%80%E5%94%AE%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p><img src="/2017/11/01/交叉销售算法/1.png" alt=""></p>
<p>最近做了一个交叉销售的项目，梳理了一些关键点，分享如下，希望对大家有所启发<br>核心目标：在有限资源下，尽可能的提供高转化率的用户群，辅助业务增长<br>初步效果：商家ROI值为50以上，用户日转化率提升10倍以上，用户日最低转化效果5pp以上<br>以下为正文：<br>数据准备：<br>1.商品相关性<br>存在商品A,B,C…，商品之间用户会存在行为信息的关联度，这边可以参考协调过滤算法中的Item-based，这边拓展为用户在不同商品之间的操作行为的差异性。</p>
<p><img src="/2017/11/01/交叉销售算法/2.png" alt=""></p>
<p>可以形成如下的特征矩阵：<br><img src="/2017/11/01/交叉销售算法/3.png" alt=""></p>
<p>这边相关的常见度量方式有以下几种：<br>a.距离衡量<br>包括浏览、点击、搜索等等各种行为的欧式、马氏、闵式、切比雪夫距离、汉明距离计算<br>b.相似度衡量<br>包括余弦相似度、杰卡德相似度衡量<br>c.复杂衡量<br>包括相关性衡量，熵值衡量，互信息量衡量，相关距离衡量<br>2.商品行为信息<br>探求商品及其对应行为信息的笛卡尔积的映射关系，得到一个商品+用户的行为魔方<br>商品集合：{商品A、商品B、…}<br>商品属性集合：{价格、是否打折、相比其他电商平台的比价、是否缺货…}<br>用户行为集合：{浏览次数、浏览时长、末次浏览间隔、搜索次数、末次搜索间隔…}<br>通过商品集合<em>商品属性集合</em>用户行为集合,形成高维的商品信息魔方，再通过探查算法，筛选优秀表现的特征，这里推荐的有pca，randomforest的importance，lasso变量压缩，相关性压缩，逐步回归压缩等方法，根据数据的属性特点可适当选取方法<br>最后，我们会得到如下一个待选特征组：<br><img src="/2017/11/01/交叉销售算法/4.png" alt=""></p>
<p>3.商品购买周期<br>针对每一件商品，都是有它自身的生命周期的，比如，在三个月内买过冰箱的用户，95%以上的用户是不会选择二次购买的；而在1个月的节点上，会有20%的用户会选择二次购买生活用纸。所以我们需要做的一件事情就是不断更新，平台上面每个类目下面的商品的自身生命周期。除此之外，考虑在过渡时间点，用户的需求变化情况，是否可以提前触发需求；这边利用，艾宾浩斯遗忘曲线和因子衰减规律拟合：</p>
<p><img src="/2017/11/01/交叉销售算法/5.png" alt="艾宾浩斯曲线"></p>
<p><img src="/2017/11/01/交叉销售算法/6.png" alt="衰减因子"></p>
<p>确定lamda和b，计算每个用户对应的每个类目，当前时间下的剩余价值：f(最高价值)<em>lamda</em>b</p>
<p><img src="/2017/11/01/交叉销售算法/7.png" alt="艾宾浩斯"></p>
<p><img src="/2017/11/01/交叉销售算法/8.png" alt="衰减因子公式"><br>4.商品挖掘特征，用户挖掘特征<br>业务运营过程中，通过数据常规可以得到1.基础结论，2.挖掘结论。基础结论就是统计结论，比如昨日订单量，昨日销售量 ，昨日用户量；挖掘结论就是深层结论，比如昨日活跃用户数，每日预估销售量，用户生命周期等<br>存在如下的探索形式，这是一个漫长而又非常有价值的过程：</p>
<p><img src="/2017/11/01/交叉销售算法/9.png" alt="特征分析"></p>
<p>模型整合<br>再确定以上四大类的数据特征之后，我们通过组合模型的方法，判断用户的交叉销售结果</p>
<hr>
<p>1.cart regression<br>确保非线性密度均匀数据拟合效果，针对存在非线性关系且数据可被网格切分的产业用户有高的预测能力<br>2.ridge regression<br>确保可线性拟合及特征繁多数据的效果，针对存在线性关系的产业用户有高的预测能力<br>3.Svm-liner<br>确保线性且存在不可忽视的异常点的数据拟合效果，针对存在异常用户较多的部分产业用户有高的预测能力<br>4.xgboost<br>确保数据复杂高维且无明显关系的数据拟合效果，针对存在维度高、数据杂乱、无模型规律的部分产业用户有高的预测能力<br>以上的组合模型并非固定，也并非一定全部使用，在确定自身产业的特点后，择优选择，然后采取投票、加权、分组等组合方式产出结果即可。</p>
<hr>
<p>附上推荐Rcode简述，<br><strong>cart regression：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fit &lt;- rpart(y~x, data=database, method=&quot;class&quot;,control=ct, parms = list(prior = c(0.7,0.3), split = &quot;information&quot;));</span><br><span class="line"># xval是n折交叉验证</span><br><span class="line"># minsplit是最小分支节点数，设置后达不到最小分支节点的话会继续分划下去</span><br><span class="line"># minbucket：叶子节点最小样本数</span><br><span class="line"># maxdepth：树的深度</span><br><span class="line"># cp全称为complexity parameter，指某个点的复杂度，对每一步拆分,模型的拟合优度必须提高的程度</span><br><span class="line"># kyphosis是rpart这个包自带的数据集</span><br><span class="line"># na.action：缺失数据的处理办法，默认为删除因变量缺失的观测而保留自变量缺失的观测。</span><br><span class="line"># method：树的末端数据类型选择相应的变量分割方法:</span><br><span class="line"># 连续性method=“anova”,离散型method=“class”,计数型method=“poisson”,生存分析型method=“exp”</span><br><span class="line">#parms用来设置三个参数:先验概率、损失矩阵、分类纯度的度量方法（gini和information）</span><br><span class="line"># cost我觉得是损失矩阵，在剪枝的时候，叶子节点的加权误差与父节点的误差进行比较，考虑损失矩阵的时候，从将“减少-误差”调整为“减少-损失”</span><br></pre></td></tr></table></figure></p>
<p><strong>ridge regression：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">library（glmnet）</span><br><span class="line">glmmod&lt;-glmnet(x,y,family = &apos;guassian&apos;,alpha = 0)</span><br><span class="line">最小惩罚：</span><br><span class="line">glmmod.min&lt;-glmnet(x,y,family = &apos;gaussian&apos;,alpha = 0,lambda = glmmod.cv$lambda.min)</span><br><span class="line">1个标准差下的最小惩罚：</span><br><span class="line">glmmod.1se&lt;-glmnet(x,y,family = &apos;gaussian&apos;,alpha = 0,lambda = glmmod.cv$lambda.1se)</span><br></pre></td></tr></table></figure></p>
<p><strong>Svm-liner ：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">svm(x, y, scale = TRUE, type = NULL, kernel = &quot;&quot;,degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),coef0 = 0, cost = 1, nu = 0.5, subset, na.action = na.omit)</span><br><span class="line">#type用于指定建立模型的类别:C-classification、nu-classification、one-classification、eps-regression和nu-regression</span><br><span class="line">#kernel是指在模型建立过程中使用的核函数</span><br><span class="line">#degree参数是指核函数多项式内积函数中的参数，其默认值为3</span><br><span class="line">#gamma参数给出了核函数中除线性内积函数以外的所有函数的参数，默认值为l</span><br><span class="line">#coef0参数是指核函数中多项式内积函数与sigmoid内积函数中的参数，默认值为0</span><br><span class="line">#参数cost就是软间隔模型中的离群点权重</span><br><span class="line">#参数nu是用于nu-regression、nu-classification和one-classification类型中的参数</span><br></pre></td></tr></table></figure></p>
<p><strong>xgboost:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(xgboost)</span><br><span class="line">xgb &lt;- xgboost(data = data.matrix(x[,-1]), label = y, eta = 0.1,max_depth = 15, nround=25, subsample = 0.5,colsample_bytree = 0.5,seed = 1,eval_metric = &quot;merror&quot;,objective = &quot;multi:softprob&quot;,num_class = 12, nthread = 3)</span><br><span class="line">#eta：默认值设置为0.3。步长，控制速度及拟合程度</span><br><span class="line">#gamma:默认值设置为0。子树叶节点个数</span><br><span class="line">#max_depth:默认值设置为6。树的最大深度</span><br><span class="line">#min_child_weight:默认值设置为1。控制子树的权重和</span><br><span class="line">#max_delta_step：默认值设置为0。控制每棵树的权重</span><br><span class="line">#subsample： 默认值设置为1。抽样训练占比</span><br><span class="line">#lambda and alpha：正则化</span><br></pre></td></tr></table></figure></p>
<p>最后通过组合算法的形式产出最终值：</p>
<p><img src="/2017/11/01/交叉销售算法/10.png" alt=""><br><strong>典型算法代表：randomforest,adaboost,gbdt</strong></p>
<p>之前写的没有用markdown，所以看起来很费力，还丢图，这次优化了一下视图，谢谢。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[随机森林-枝剪问题]]></title>
      <url>/2017/10/02/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%9E%9D%E5%89%AA%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>通常情况下， 随机森林不需要后剪枝。</p>
<p>剪枝的意义是：防止决策树生成过于庞大的子叶，避免实验预测结果过拟合，在实际生产中效果很差</p>
<p>剪枝通常有两种：</p>
<p>PrePrune：预剪枝，及早的停止树增长，在每个父节点分支的时候计算是否达到了限制值</p>
<p>PostPrune：后剪枝，基于完全生长（过拟合）的树上进行剪枝，砍掉一些对衡量函数影响不大的枝叶</p>
<p>剪枝的依据：</p>
<p>常见的有错误率校验（判断枝剪是降低了模型预测的正确率），统计学检验，熵值，代价复杂度等等</p>
<p>总结看来，枝剪的目的是担心全量数据在某棵树上的拟合过程中，过度判断了每个点及其对应类别的关系，有如以下这张图（以rule1&amp;rule2代替了rule3）：</p>
<hr>
<p>随机森林：</p>
<p>定义：它是一种模型组合（常见的Boosting，Bagging等，衍生的有gbdt），这些算法最终的结果是生成N(可能会有几百棵以上）棵树，组合判断最终结果。</p>
<p>如何组合判断？</p>
<p>1.通常我们会规定随机森林里面的每棵树的选参个数，常见的有log，sqrt等等，这样的选取是随机选则的，这样有一个好处，让每一棵树上都有了尽可能多的变量组合，降低过拟合程度</p>
<p>2.树的个数及树的节点的变量个数，通常的来说，最快捷的方式是先确定节点的变量个数为sqrt（变量的个数），然后在根据oob的准确率反过来看多个棵树时最优，确定了树的个数的时候再反过来确定mtry的个数，虽然有局限，但是也并不存在盲目性</p>
<p>3.我个人理解，随机森林中的每一棵树我们需要它在某一片的数据中有非常好的拟合性，它并不是一个全数据拟合，只需要在它负责那块上有最佳的拟合效果。每次遇到这些数据(特征)的时候，我们在最后汇总N棵树的结果的时候，给这些数据对应的那块模型以最高权重即可</p>
<p>最后总结一下，就是随机森林里面的每棵树的产生通过选特征参数选数据结构，都已经考虑了避免共线性避免过拟合，剩下的每棵树需要做的就是尽可能的在自己所对应的数据(特征)集情况下尽可能的做到最好的预测结果；如同，公司已经拆分好部门，你不需要考虑这样拆分是不是公司运营最好的一个组合方式，你需要做的就是当公司需要你的时候，尽可能的做好自己的事情，就酱。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 树枝剪问题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[大数据量下的划分聚类方法]]></title>
      <url>/2017/07/19/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%8F%E4%B8%8B%E7%9A%84%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>在常规聚类案例中，数据一般都是以iris集或者不足GB级的数据作为测试案例，实际商业运用中，数据量级要远远大于这些。比如滴滴出行15年日均单量就达到1000万单，出行轨迹的数据存储达到上百TB，常规的k均值聚类，二分聚类等无法完成如此量级的数据聚类，这边就提供一个以CLARANS为基础的算法思路。</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/1.png" alt=""></p>
<p>什么是聚类?<br>定义是这样的，把一个数据对象，划分成子集的过程，使得子集内相似度大，子集外相似度小。这样的一个过程叫做聚类。</p>
<p>大学课程老师以一个公式概括过这样的过程：<code>max(子集内相似度/子集间相似度)</code>，我觉得也很形象便于理解。</p>
<p>什么是划分聚类？<br>聚类方法有很多种，包括基于划分、基于密度、基于网格、基于层次、基于模型等等，这边主要介绍基于划分的聚类方法，剩余的方法会在后续的文章中持续更新[如果不鸽的话]。划分聚类一般是：<strong>采取互斥族（子集）划分</strong>，说的更直白一点就是<strong>每个点属于且仅属于一个族（子集）</strong>。</p>
<p>常见的划分聚类有哪些？<br>k均值划分：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">input：</span><br><span class="line">- k：族的个数</span><br><span class="line">- D：输入数据集合</span><br><span class="line">output：</span><br><span class="line">k个族（子集）的数据集合</span><br><span class="line">methods：</span><br><span class="line">1.在D中任选（常用的包库中都是这样做，但是建议自己写的同学以密度先分块，在密度块中任选）k个对象作为初始中心</span><br><span class="line">2.计算剩余对象到k对象的聚类，聚类远近分配到对应的族</span><br><span class="line">3.更新族均值作为新的族中心</span><br><span class="line">4.重复2-4直到中心不变化</span><br></pre></td></tr></table></figure></p>
<p>如图过程：</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/2.png" alt="kmeans"></p>
<p>以上为最简单的k均值，很容易看出，它存在几个问题，首先计算量非常的大，假设有m条数据，k个中心点，那距离计算的次数就是<code>o(mkt)=k*(m-k)*迭代次数t</code>，重复t次直到收敛的过程是非常大的计算过程；再而，如果数据均为‘男、女’，‘高、中、低’等，那距离定义就是非常不合理的，此外，初始k难确定，非凸数据，离群点等等都存在问题</p>
<p>围绕中心划分（PAM）：<br>刚才说到了异常点会影响k均值，那么我们看看为什么？<br>假设与点1、2、3、8、9、10、25，一眼就知道(1、2、3)，(8，9，10)为族，但如果由k均值的话，以k=2为例，((1，2，3)，(8，9，10，25))，mse=196；<br>((1，2，3，8)，(9，10，25))，mse=189.7；它重复以mse为损失函数，而不去考虑数据是否合理，所以针对的，我们有<strong>绝对误差标准</strong>：<br><img src="/2017/07/19/大数据量下的划分聚类方法/3.png" alt="绝对误差标准"></p>
<p>这样做，通过全局距离最小化，可以一定程度上避免异常点的问题，但是，思考一下计算量是什么？是o(n**2)，这意味着对数据量大的问题，这就是一个典型的NP问题（一定有解，但是不一定在有限时间资源内可以被解出来）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">input:</span><br><span class="line">- k：族的个数</span><br><span class="line">- D：输入数据集合</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">k个族（子集）的数据集合</span><br><span class="line"></span><br><span class="line">methods:</span><br><span class="line">1.D中任选k个对象最为初始种子</span><br><span class="line">2.仿照k均值分配剩余对象</span><br><span class="line">3.随机选取非种子对象O</span><br><span class="line">4.计算若是以O为中心下的总损失函数代价S=原始种子下的绝对误差E-新的对象O下的绝对误差E</span><br><span class="line">5.如果S&gt;0,则以新对象O替换旧的种子对象，否则不变化</span><br><span class="line">6.重复2-5，直到收敛</span><br></pre></td></tr></table></figure></p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/4.png" alt="绝对误差标准"><br>我们看这个图好理解一点，就是存在族（集合）中任一点p，当前的初始种子为Q1，随机选取剩余其他对象为族中心Qrandom1，计算PQ1的距离与PQrandom1的距离，图中dist(PQ1)<dist(pqrandom1)的距离，所以p仍属于q1为中心的族，若存在族中心qrandom2，使得dist(pq1)>dist(PQrandom2)，则更新族中心为Qrandom2,此时绝对误差E会变化，计算是否降低了绝对误差E以确定是否更好族中心。<br>如何解决大数据量下的聚类问题？<br>其实看了以上两个算法，大同小异，但是都不可避免有一个弱点，就是计算量上都是随着初始数据量的增大而几何增长的，所以这边需要对数据量进行控制。</dist(pqrandom1)的距离，所以p仍属于q1为中心的族，若存在族中心qrandom2，使得dist(pq1)></p>
<p>大家回想一下，同样的对数据量进行控制的算法有哪些给我们有启发？<br><a href="http://www.jianshu.com/writer#/notebooks/14156301/notes/14303651" target="_blank" rel="noopener">数据平衡算法</a><br>这种方法好像可以减少数据量，哪有没有历史成功案例支持呢？<br><a href="http://www.jianshu.com/writer#/notebooks/13640327/notes/13715869" target="_blank" rel="noopener">基于决策树引申出的集成算法</a><br>貌似存在一个叫做adaboost、randomforest这类的算法，好像就用了<strong>数据平衡的算法</strong>。</p>
<p>那么，我们是否可以用在聚类里面呢？答案是可以的，我们现在看一个由上述思路得到的CLARANS算法，实际开发中，我们team对其进行了优化，内部称之为’CLARANS+’<br>在理解CLARANS+之前，我们先理解CLARA:</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/5.png" alt=""></p>
<p>从这张图上，我们可以很清晰的看出，CLARA首先通过类似randomforest里面的随机抽样的方法，将原始数据集随机抽样成若干个子数据集sample data，理论上采样的子集分布应该与原分布近似，所以样本中心点必然与原分布中心近似。</p>
<hr>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/6.png" alt="确定中心"></p>
<p>在数据量较少的子集上，我们可以重复确定每个子集的中心Medoid，<strong>这边计算中心的方法有很多，包括上述讲到的K均值，PAM，也可以参考相似度比如常见的余弦相似，likelihood rate，高斯核相似等等</strong></p>
<p>最后采取随机抽取，或者投票加权等方法确定原始样本的中心即可。</p>
<p>CLARA的有效性依赖于样本的大小，分布及质量，所以该算法一定程度上会依赖于初始抽样的质量。除此之外，每一个随机样本的计算负责度为O（ks*s+k（n-k）），s为样本的大小，k为族数，n为总对象数，若抽取样本子集过少，其简化计算的程度也越低。</p>
<p>说到这里，CLARA的算法是确定了中心后不在改变，这就有一定的运气成分，假设确定的k个钟均离最佳中心很远的情况下，CLARA最后无论如何去选已知中心，都得不到最优秀的聚类中心。</p>
<p>所以，<strong>我们来看看可以提高CLARA的聚类质量及可伸缩性的CLARANS算法</strong></p>
<p>上述思路不变，但在CLARA确定中心之后，我们新增了一步，就是按照PAM中的方法一样，我们在子集上选取一个与当前中心x(Medoid)不一样的对象y(New Medoid)，计算用y(New Medoid)替换x(Medoid)后绝对误差是否下降，下降则替换否则不变，重复l次之后，我们可以认为此时的中心点为局部中心最优解；整体数据集所有子集均重复m次后，得出的中心点为全局局部最优解。如下图：</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/7.png" alt="新增New Medoid"></p>
<p>实际上，我们可以做的还很多<br>理论上讲，以上的算法结果已经尽可能的保证了数据的合理压缩，压缩后的数据集内的中心点足够鲁棒，但是实际运用过程中，我们没有尽可能的考虑到开头说的那句：<br><strong>什么是聚类?</strong><br>定义是这样的，把一个数据对象，划分成子集的过程，使得子集内相似度大，子集外相似度小。这样的一个过程叫做聚类。</p>
<p>所以，我们尝试性的做了CLARANS+，我们把CLARANS里面确定出来的每个sample data子集里面最优秀的top k个New Medoids映射回同一个空间：<br>以绿色和天蓝色数据集为例子：</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/8.png" alt="CLARANS"><br>橘色方框内为CLARANS最后确定中心后做的随机或者加权投票后采纳的被橘黄色框框住的天蓝色数据与绿色数据的中心点，很明显可以看出，这样导致的结果违背了“子集外相似度最小的原则”。</p>
<p>我们，仿照Lasso对应lambda.1se的方式，考虑除了最优点外，在其可接受的范围附近，认为他们同样属于最优点，也就是top k个New Medoids重新选择距离最远的点作为最优中心，也就是如下图中的紫色方框中的点：</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/9.png" alt="最远距离点"></p>
<p>通过实际的业务测试，我们建议top k个点中的个默认为2-3比较好（数据分布差异大选择2，否则选择3）,如果不能确定，就默认为3。</p>
<p>以上理论方法就解释了如何在大量数据量下，简单快速的寻找到最优中心点的过程，谢谢大家。</p>
<p>参考文献：<br>[1] Jiawei Han.[数据挖掘概念与技术]2001，8<br>[2] 毛国君等.数据挖掘原理与算法[M].北京:清华大学出版社,2005.<br>[3] <a href="http://www.jianshu.com/writer#/notebooks/14156301/notes/14303651" target="_blank" rel="noopener">数据平衡算法</a><br>[4] <a href="http://www.jianshu.com/writer#/notebooks/13640327/notes/13715869" target="_blank" rel="noopener">基于决策树引申出的集成算法</a><br>[5] <a href="http://scikit-learn.org/stable/modules/clustering.html#clustering" target="_blank" rel="noopener">http://scikit-learn.org/stable/modules/clustering.html#clustering</a><br>[6] <a href="https://en.wikipedia.org/wiki/Clustering" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Clustering</a></p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据抽样平衡方法重写]]></title>
      <url>/2017/07/13/%E6%95%B0%E6%8D%AE%E6%8A%BD%E6%A0%B7%E5%B9%B3%E8%A1%A1%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99/</url>
      <content type="html"><![CDATA[<p>之前在R里面可以通过调用Rose这个package调用数据平衡函数，这边用python改写了一下，也算是自我学习了。</p>
<p>R：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定工作目录</span></span><br><span class="line">setwd(path)</span><br><span class="line"><span class="comment"># 安装包</span></span><br><span class="line">install.packages(<span class="string">"ROSE"</span>)</span><br><span class="line"><span class="keyword">library</span>(ROSE)</span><br><span class="line"><span class="comment"># 检查数据</span></span><br><span class="line">data(hacide)</span><br><span class="line">table(hacide.train$cls)</span><br><span class="line"><span class="number">0</span>     <span class="number">1</span></span><br><span class="line"><span class="number">980</span>    <span class="number">20</span></span><br></pre></td></tr></table></figure></p>
<hr>
<p>过抽样实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_balanced_over &lt;- ovun.sample(cls ~ ., data = hacide.train, method = &quot;over&quot;,N = 1960)$data</span><br><span class="line">table(data_balanced_over$cls)</span><br><span class="line">0    1</span><br><span class="line">980 980</span><br></pre></td></tr></table></figure></p>
<p><strong>这边需要注意是<code>ovun</code>不是<code>over</code></strong></p>
<hr>
<p>欠采样实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_balanced_under &lt;- ovun.sample(cls ~ ., data = hacide.train, method = &quot;under&quot;, N = 40, seed = 1)$data</span><br><span class="line">table(data_balanced_under$cls)</span><br><span class="line">0    1</span><br><span class="line">20  20</span><br></pre></td></tr></table></figure></p>
<p>这边需要注意的是欠采样是不放回采样，同时对数据信息的损失也是极大的</p>
<hr>
<p>组合采样实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_balanced_both &lt;- ovun.sample(cls ~ ., data = hacide.train, method = &quot;both&quot;, p=0.5, N=1000, seed = 1)$data</span><br><span class="line">table(data_balanced_both$cls)</span><br><span class="line">0    1</span><br><span class="line">520 480</span><br></pre></td></tr></table></figure></p>
<p><code>method</code>的不同值代表着不同的采样方法，p这边是控制正类的占比，seed保证抽取样本的固定，也就是种子值。</p>
<hr>
<hr>
<p>在python上，我也没有发现有现成的package可以import，所以就参考了R的实现逻辑重写了一遍，新增了一个分层抽样<code>group_sample</code>,删除了过采样，重写了组合抽样<code>combine_sample</code>,欠抽样<code>under_sample</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math <span class="keyword">as</span> ma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sample_s</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">''''this is my pleasure'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_sample</span><span class="params">(self, data_set, label, percent=<span class="number">0.1</span>)</span>:</span></span><br><span class="line"><span class="comment"># 分层抽样</span></span><br><span class="line"><span class="comment"># data_set:数据集</span></span><br><span class="line"><span class="comment"># label:分层变量</span></span><br><span class="line"><span class="comment"># percent:抽样占比</span></span><br><span class="line"><span class="comment"># q:每次抽取是否随机,null为随机</span></span><br><span class="line"><span class="comment"># 抽样根据目标列分层，自动将样本数较多的样本分层按percent抽样，得到目标列样本较多的特征欠抽样数据</span></span><br><span class="line">x = data_set</span><br><span class="line">y = label</span><br><span class="line">z = percent</span><br><span class="line">diff_case = pd.DataFrame(x[y]).drop_duplicates([y])</span><br><span class="line">result = []</span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(diff_case)):</span><br><span class="line">k = np.array(diff_case)[i]</span><br><span class="line">data_set = x[x[y] == k[<span class="number">0</span>]]</span><br><span class="line">nrow_nb = data_set.iloc[:, <span class="number">0</span>].count()</span><br><span class="line">data_set.index = range(nrow_nb)</span><br><span class="line">index_id = rd.sample(range(nrow_nb), int(nrow_nb * z))</span><br><span class="line">result = pd.concat([result, data_set.iloc[index_id, :]], axis=<span class="number">0</span>)</span><br><span class="line">new_data = pd.Series(result[<span class="string">'label'</span>]).value_counts()</span><br><span class="line">new_data = pd.DataFrame(new_data)</span><br><span class="line">new_data.columns = [<span class="string">'cnt'</span>]</span><br><span class="line">k1 = pd.DataFrame(new_data.index)</span><br><span class="line">k2 = new_data[<span class="string">'cnt'</span>]</span><br><span class="line">new_data = pd.concat([k1, k2], axis=<span class="number">1</span>)</span><br><span class="line">new_data.columns = [<span class="string">'id'</span>, <span class="string">'cnt'</span>]</span><br><span class="line">max_cnt = max(new_data[<span class="string">'cnt'</span>])</span><br><span class="line">k3 = new_data[new_data[<span class="string">'cnt'</span>] == max_cnt][<span class="string">'id'</span>]</span><br><span class="line">result = result[result[y] == k3[<span class="number">0</span>]]</span><br><span class="line"><span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">under_sample</span><span class="params">(self, data_set, label, percent=<span class="number">0.1</span>, q=<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="comment"># 欠抽样</span></span><br><span class="line"><span class="comment"># data_set:数据集</span></span><br><span class="line"><span class="comment"># label:抽样标签</span></span><br><span class="line"><span class="comment"># percent:抽样占比</span></span><br><span class="line"><span class="comment"># q:每次抽取是否随机</span></span><br><span class="line"><span class="comment"># 抽样根据目标列分层，自动将样本数较多的样本按percent抽样，得到目标列样本较多特征的欠抽样数据</span></span><br><span class="line">x = data_set</span><br><span class="line">y = label</span><br><span class="line">z = percent</span><br><span class="line">diff_case = pd.DataFrame(pd.Series(x[y]).value_counts())</span><br><span class="line">diff_case.columns = [<span class="string">'cnt'</span>]</span><br><span class="line">k1 = pd.DataFrame(diff_case.index)</span><br><span class="line">k2 = diff_case[<span class="string">'cnt'</span>]</span><br><span class="line">diff_case = pd.concat([k1, k2], axis=<span class="number">1</span>)</span><br><span class="line">diff_case.columns = [<span class="string">'id'</span>, <span class="string">'cnt'</span>]</span><br><span class="line">max_cnt = max(diff_case[<span class="string">'cnt'</span>])</span><br><span class="line">k3 = diff_case[diff_case[<span class="string">'cnt'</span>] == max_cnt][<span class="string">'id'</span>]</span><br><span class="line">new_data = x[x[y] == k3[<span class="number">0</span>]].sample(frac=z, random_state=q, axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> new_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_sample</span><span class="params">(self, data_set, label, number, percent=<span class="number">0.35</span>, q=<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="comment"># 组合抽样</span></span><br><span class="line"><span class="comment"># data_set:数据集</span></span><br><span class="line"><span class="comment"># label:目标列</span></span><br><span class="line"><span class="comment"># number:计划抽取多类及少类样本和</span></span><br><span class="line"><span class="comment"># percent：少类样本占比</span></span><br><span class="line"><span class="comment"># q:每次抽取是否随机</span></span><br><span class="line"><span class="comment"># 设定总的期待样本数量，及少类样本占比，采取多类样本欠抽样，少类样本过抽样的组合形式</span></span><br><span class="line">x = data_set</span><br><span class="line">y = label</span><br><span class="line">n = number</span><br><span class="line">p = percent</span><br><span class="line">diff_case = pd.DataFrame(pd.Series(x[y]).value_counts())</span><br><span class="line">diff_case.columns = [<span class="string">'cnt'</span>]</span><br><span class="line">k1 = pd.DataFrame(diff_case.index)</span><br><span class="line">k2 = diff_case[<span class="string">'cnt'</span>]</span><br><span class="line">diff_case = pd.concat([k1, k2], axis=<span class="number">1</span>)</span><br><span class="line">diff_case.columns = [<span class="string">'id'</span>, <span class="string">'cnt'</span>]</span><br><span class="line">max_cnt = max(diff_case[<span class="string">'cnt'</span>])</span><br><span class="line">k3 = diff_case[diff_case[<span class="string">'cnt'</span>] == max_cnt][<span class="string">'id'</span>]</span><br><span class="line">k4 = diff_case[diff_case[<span class="string">'cnt'</span>] != max_cnt][<span class="string">'id'</span>]</span><br><span class="line">n1 = p * n</span><br><span class="line">n2 = n - n1</span><br><span class="line">fre1 = n2 / float(x[x[y] == k3[<span class="number">0</span>]][<span class="string">'label'</span>].count())</span><br><span class="line">fre2 = n1 / float(x[x[y] == k4[<span class="number">1</span>]][<span class="string">'label'</span>].count())</span><br><span class="line">fre3 = ma.modf(fre2)</span><br><span class="line">new_data1 = x[x[y] == k3[<span class="number">0</span>]].sample(frac=fre1, random_state=q, axis=<span class="number">0</span>)</span><br><span class="line">new_data2 = x[x[y] == k4[<span class="number">1</span>]].sample(frac=fre3[<span class="number">0</span>], random_state=q, axis=<span class="number">0</span>)</span><br><span class="line">test_data = pd.DataFrame([])</span><br><span class="line"><span class="keyword">if</span> int(fre3[<span class="number">1</span>]) &gt; <span class="number">0</span>:</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; (int(fre3[<span class="number">1</span>])):</span><br><span class="line">data = x[x[y] == k4[<span class="number">1</span>]]</span><br><span class="line">test_data = pd.concat([test_data, data], axis=<span class="number">0</span>)</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">result = pd.concat([new_data1, new_data2, test_data], axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p>
<p>后续使用，只需要复制上述code，存成<code>.py</code>的文件，后续使用的时候：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载函数</span></span><br><span class="line"><span class="keyword">import</span> sample_s <span class="keyword">as</span> sa</span><br><span class="line"><span class="comment">#这边可以选择你需要的分层抽样、欠抽样、组合抽样的函数</span></span><br><span class="line">sample = sa.group_sample()</span><br><span class="line"><span class="comment">#直接调用函数即可</span></span><br><span class="line">new_data3 = sample.combine_sample(data_train, <span class="string">'label'</span>, <span class="number">60000</span>, <span class="number">0.4</span>)</span><br><span class="line"><span class="comment">#将data_train里面的label保持正样本（少类样本）达到0.4的占比下，总数抽取到60000个样本</span></span><br></pre></td></tr></table></figure></p>
<p>其实不是很难的一个过程，只是强化自己对python及R语言的书写方式的记忆，谢谢。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据平衡 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[订单需求预估]]></title>
      <url>/2017/07/12/%E8%AE%A2%E5%8D%95%E9%9C%80%E6%B1%82%E9%A2%84%E4%BC%B0/</url>
      <content type="html"><![CDATA[<p>之前写了一篇以基于elastic的需求预估的文章，只不过用的是R语言开发的，最近在学python，就仿照逻辑写了一篇python的，主要修改点如下：</p>
<ul>
<li>用决策树替换了elastic算法</li>
<li>用分层抽样替换了组合抽样</li>
</ul>
<p><strong>需要看详细理论及思考过程参考链接：<a href="http://www.jianshu.com/p/e932b9744da6" target="_blank" rel="noopener">商品需求预估</a></strong></p>
<p>python code如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import random as rd</span><br><span class="line">from sklearn import tree</span><br><span class="line"></span><br><span class="line"># 读取数据</span><br><span class="line">data_orgin = pd.read_table(&quot;C:/Users/17031877/Desktop/supermarket_second_hair_washing_train.txt&quot;)</span><br><span class="line">data_deal_1 = data_orgin.drop([&apos;aimed_date&apos;, &apos;member_id&apos;, &apos;age&apos;, &apos;gender&apos;, &apos;diff_rgst&apos;], axis=1)</span><br></pre></td></tr></table></figure></p>
<p>这边是常规的数据读取，删除了不必要的列</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#因变量单列</span><br><span class="line">label = data_deal_1[&apos;label&apos;]</span><br><span class="line"></span><br><span class="line"># 用户分量级</span><br><span class="line">value00 = [&apos;max_date_diff&apos;, &apos;aimed_max_date_diff&apos;]</span><br><span class="line">data00 = data_deal_1[value00]</span><br><span class="line"></span><br><span class="line">value01 = [&apos;max_pay&apos;, &apos;per_pay&apos;, &apos;six_month_max_pay&apos;, &apos;six_month_per_pay&apos;, &apos;three_month_max_pay&apos;, &apos;three_month_per_pay&apos;,</span><br><span class="line">&apos;one_month_max_pay&apos;, &apos;one_month_per_pay&apos;, &apos;fifteen_day_max_pay&apos;, &apos;fifteen_day_per_pay&apos;, &apos;aimed_max_pay&apos;,</span><br><span class="line">&apos;aimed_per_pay&apos;, &apos;aimed_six_month_max_pay&apos;, &apos;aimed_six_month_per_pay&apos;, &apos;aimed_three_month_max_pay&apos;,</span><br><span class="line">&apos;aimed_three_month_per_pay&apos;, &apos;aimed_one_month_max_pay&apos;, &apos;aimed_one_month_per_pay&apos;,</span><br><span class="line">&apos;aimed_fifteen_day_max_pay&apos;, &apos;aimed_fifteen_day_per_pay&apos;, &apos;qty_drtn_seven&apos;, &apos;qty_drtn_fourteen&apos;]</span><br><span class="line">data01 = data_deal_1[value01]</span><br><span class="line"></span><br><span class="line">value02 = [&apos;cnt_time&apos;, &apos;six_month_cnt_time&apos;, &apos;three_month_cnt_time&apos;, &apos;one_month_cnt_time&apos;, &apos;fifteen_day_cnt_time&apos;,</span><br><span class="line">&apos;aimed_cnt_time&apos;, &apos;aimed_six_month_cnt_time&apos;, &apos;aimed_three_month_cnt_time&apos;, &apos;aimed_one_month_cnt_time&apos;,</span><br><span class="line">&apos;aimed_fifteen_day_cnt_time&apos;, &apos;pv_times_seven&apos;, &apos;pv_times_fourteen&apos;, &apos;search_times_seven&apos;,</span><br><span class="line">&apos;search_times_fourteen&apos;, &apos;clc_times_seven&apos;, &apos;clc_times_fourteen&apos;, &apos;cart2_times_seven&apos;,</span><br><span class="line">&apos;cart2_times_fourteen&apos;, &apos;cart1_times_seven&apos;, &apos;cart1_times_fourteen&apos;, &apos;unpay_times_seven&apos;,</span><br><span class="line">&apos;unpay_times_fourteen&apos;]</span><br><span class="line">data02 = data_deal_1[value02]</span><br><span class="line"></span><br><span class="line">value03 = [&apos;pv_visit_last_period&apos;, &apos;search_last_period&apos;, &apos;clc_last_period&apos;, &apos;cart2_last_period&apos;, &apos;cart1_last_period&apos;,</span><br><span class="line">&apos;unpay_last_period&apos;]</span><br><span class="line">data03 = data_deal_1[value03]</span><br></pre></td></tr></table></figure>
<p>因为不同量级的数据之后做异常点处理的时候截断位置不同，所有需要分割数据处理</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def test_function_one(x, l):</span><br><span class="line">k = x.dropna(how=&apos;any&apos;)</span><br><span class="line">y = k.quantile(l)</span><br><span class="line">z = k.max()</span><br><span class="line">x[x &gt; y] = y</span><br><span class="line">x = x.fillna(value=z)</span><br><span class="line">return x</span><br><span class="line">for i in range(len(data00.columns)):</span><br><span class="line">data00.iloc[:, i] = test_function_one(data00.iloc[:, i], 0.98)</span><br><span class="line"></span><br><span class="line">def test_function_two(x, l):</span><br><span class="line">k = x.dropna(how=&apos;any&apos;)</span><br><span class="line">y = k.quantile(l)</span><br><span class="line">z = 0</span><br><span class="line">x[x &gt; y] = y</span><br><span class="line">x = x.fillna(value=z)</span><br><span class="line">return x</span><br><span class="line">for i in range(len(data01.columns)):</span><br><span class="line">data01.iloc[:, i] = test_function_two(data01.iloc[:, i], 0.95)</span><br><span class="line">for i in range(len(data02.columns)):</span><br><span class="line">data02.iloc[:, i] = test_function_two(data02.iloc[:, i], 0.99)</span><br><span class="line"></span><br><span class="line">def test_function_three(x):</span><br><span class="line">z = 14</span><br><span class="line">x[x &gt; z] = z</span><br><span class="line">x = x.fillna(value=z)</span><br><span class="line">return x</span><br><span class="line">for i in range(len(data03.columns)):</span><br><span class="line">data03.iloc[:, i] = test_function_three(data03.iloc[:, i])</span><br><span class="line"># 数据合并</span><br><span class="line">data_train = pd.concat([label, data00, data01, data02, data03], axis=1)</span><br></pre></td></tr></table></figure>
<p>根据数据量的不同做数据分割，跑上面写完的code函数就可以</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#数量级对比</span><br><span class="line">zero_case = data_train[data_train[&apos;label&apos;] == 0][&apos;label&apos;].count()</span><br><span class="line">print &apos;负样本数：%d&apos; % zero_case</span><br><span class="line">one_case = data_train[data_train[&apos;label&apos;] == 1][&apos;label&apos;].count()</span><br><span class="line">print &apos;正样本数: %d&apos; % (one_case)</span><br><span class="line"></span><br><span class="line">负样本数：292936</span><br><span class="line">正样本数: 3973</span><br><span class="line">Backend TkAgg is interactive backend. Turning interactive mode on.</span><br></pre></td></tr></table></figure>
<p>实际看下来，正负样本的差异的确还是很大，这个其实做多了就有经验，常规的来看，潜在的浏览、搜索到最后的成单，普遍自然转化不到1%，也正是这么低的转化，才需要一些算法来做信息抓去。</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def case_sample(x, y, z):</span><br><span class="line">diff_case = pd.DataFrame(x[y]).drop_duplicates([y])</span><br><span class="line">result = []</span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line">for i in range(len(diff_case)):</span><br><span class="line">k = np.array(diff_case)[i]</span><br><span class="line">data_set = x[x[y] == k[0]]</span><br><span class="line">nrow_nb = data_set.iloc[:, 0].count()</span><br><span class="line">data_set.index = range(nrow_nb)</span><br><span class="line">index_id = rd.sample(range(nrow_nb), int(nrow_nb * z))</span><br><span class="line">result = pd.concat([result, data_set.iloc[index_id, :]], axis=0)</span><br><span class="line">return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">zero_case = data_train[data_train[&apos;label&apos;] == 0]</span><br><span class="line">one_case = data_train[data_train[&apos;label&apos;] == 1]</span><br><span class="line"># 开始分层抽样</span><br><span class="line">new_zero_case = case_sample(zero_case, &apos;unpay_last_period&apos;, 0.1)</span><br><span class="line"># 新数量级对比</span><br><span class="line">new_zero_case_count = new_zero_case[new_zero_case[&apos;label&apos;] == 0][&apos;label&apos;].count()</span><br><span class="line"># 数据集合并</span><br><span class="line">new_data_train = pd.concat([new_zero_case, one_case], axis=0)</span><br></pre></td></tr></table></figure>
<p><code>case_sample</code>是一个简单的分层抽样的小函数，<code>x</code>是数据集，<code>y</code>是分层变量，<code>z</code>是抽样占比；新的样本<code>new_data_train</code>中正负样本比例在1:10左右，这边的样本比是我自己设置的，不一定是最合理的；且此处也不一定要求一定用分层抽样，只是我用来练练手的；推荐还是遵从奥卡姆原理，在未知的情况下，尽可能简单的解决问题，比如组合抽样就是很不错的方法。</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#函数设置</span><br><span class="line">clf = tree.DecisionTreeRegressor(criterion=&apos;mse&apos;, max_features=&apos;log2&apos;, random_state=1)</span><br><span class="line"></span><br><span class="line">#函数拟合</span><br><span class="line">y = new_data_train[&apos;label&apos;]</span><br><span class="line">x = new_data_train.drop(&apos;label&apos;, 1)</span><br><span class="line">clf.fit(x, y)</span><br><span class="line"></span><br><span class="line">#数据预测</span><br><span class="line">y_predict = clf.predict(x)</span><br><span class="line"></span><br><span class="line"># 结果对比</span><br><span class="line">y.index = range(len(y))</span><br><span class="line">combined_date = pd.concat([y, pd.DataFrame(y_predict)], axis=1)</span><br><span class="line">combined_date.columns = [&apos;actual&apos;, &apos;predict&apos;]</span><br></pre></td></tr></table></figure>
<p>这边稍微讲解一下，我认为的<code>sklearn</code>中<code>DecisionTreeRegressor</code>中比较终于的参数设置，<code>criterion</code>这边为模型优化的标准，常规的有<code>mse</code>和<code>mae</code>，建议在数据量差异不大的时候多考虑<code>mse</code>；<code>max_features</code>是每次训练用的特征个数，综合特征量级考虑，一般有<code>log2</code>，<code>sqrt</code>，尽可能是抽取比例在70%；<code>max_depth</code>刚开始可以默认，第一类模型出来后，可在结果附近迭代，寻找<code>out of bag</code>最小的error下的值；<strong>另外，我没有发现有weight设置，可能是我不熟悉，但是如果sklearn这边不提供weight的化，我们在做数据预处理的时候一定要平衡数据，不然当数据集过偏的时候最后的结果会以“牺牲”少类的判断正确率去完善整体正确率。</strong></p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># case 1</span><br><span class="line">x = []</span><br><span class="line">y = []</span><br><span class="line">for i in range(1, 10):</span><br><span class="line">test_data = combined_date</span><br><span class="line">i = i / float(10)</span><br><span class="line">for j in range(combined_date[&apos;actual&apos;].count()):</span><br><span class="line">if test_data.iloc[j, 1] &gt; i:</span><br><span class="line">test_data.iloc[j, 1] = 1</span><br><span class="line">else:</span><br><span class="line">continue</span><br><span class="line">z = test_data[test_data[&apos;actual&apos;] == test_data[&apos;predict&apos;]][&apos;actual&apos;].count() / float(test_data[&apos;actual&apos;].count())</span><br><span class="line">x.append(i)</span><br><span class="line">y.append(z)</span><br></pre></td></tr></table></figure>
<p>这边写了检查函数，检查了分别0.1~1，以0.1为间隔的情况下的分割点，每个分割点下<strong>预测正确的数量/所有统计的样本数</strong>，也就是下面的<strong>accuracy</strong>.<br><img src="/2017/07/12/订单需求预估/1.png" alt=""></p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># case 2</span><br><span class="line">test_data = combined_date</span><br><span class="line">aimed_data = test_data[test_data[&apos;predict&apos;]&gt;0]</span><br><span class="line">k1=aimed_data[aimed_data[&apos;actual&apos;]==1][&apos;predict&apos;].count()</span><br><span class="line">k2=float(aimed_data[&apos;predict&apos;].count())</span><br><span class="line"></span><br><span class="line">print &apos;所有预测可能下单用户中真实下单用户数：%d&apos; %(k1)</span><br><span class="line">print &apos;所有预测可能下单用户数：%d&apos; %(k2)</span><br></pre></td></tr></table></figure>
<p>因为这边需要对用户营销，所以更关系topN的转化率，需要看一下实际正样本被覆盖了多数，以上即为code，这边的效果值为98.7%，还是比较高的，但是应该是过拟合了，所有一般不建议单纯使用决策树模型</p>
<hr>
<p>所有的python code到这里就结束了，后续我做项目的同时会同时更新R及python两种code的思考，和大家讨论分享学习，谢谢。</p>
<p><em>参考文献：</em></p>
<ul>
<li><em><a href="http://scikitlearn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" target="_blank" rel="noopener">sklearn.tree.DecisionTreeRegressor</a></em></li>
<li><em><a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">维基百科中对ROC的介绍</a></em></li>
<li><em><a href="https://www.zhihu.com/question/27205203/answer/148900663" target="_blank" rel="noopener">决策树常见问题及面试关键点介绍</a></em></li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 预测 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[推荐系统-威尔逊区间法]]></title>
      <url>/2017/06/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%A8%81%E5%B0%94%E9%80%8A%E5%8C%BA%E9%97%B4%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>我推荐一种之前在惠普做过一种排序方法：威尔逊区间法</p>
<p>我们先做如下设定：</p>
<p>（1）每个用户的打分都是独立事件。</p>
<p>（2）用户只有两个选择，要么投喜欢’1’，要么投不喜欢’0’。</p>
<p>（3）如果总人数为n，其中喜欢的为k，那么喜欢的比例p就等于k/n。</p>
<p>这是一种统计分布，叫做”二项分布”（binomial distribution）</p>
<p>理论上讲，p越大应该越好，但是n的不同，导致p的可信性有差异。100个人投票，50个人投喜欢；10个人投票，6个人喜欢，我们不能说后者比前者要好。</p>
<p>所以这边同时要考虑（p，n）</p>
<p>刚才说满足二项分布，这里p可以看作”二项分布”中某个事件的发生概率，因此我们可以计算出p的置信区间。</p>
<p>所谓”置信区间”，就是说，以某个概率而言，p会落在的那个区间。</p>
<p>置信区间展现的是这个参数的真实值有一定概率落在测量结果的周围的程度。置信区间给出的是被测量参数的测量值的可信程度，即前面所要求的“一个概率”，也就是结论的可信程度。</p>
<p>二项分布的置信区间有多种计算公式，最常见的是”正态区间”（Normal approximation interval）。但是，它只适用于样本较多的情况（np &gt; 5 且 n(1 − p) &gt; 5），对于小样本，它的准确性很差。</p>
<p>这边，我推荐用t检验来衡量小样本的数据，可以解决数据过少准确率不高的问题。</p>
<p>这样一来，排名算法就比较清晰了：</p>
<p>第一步，计算每个case的p（好评率）。</p>
<p>第二步，计算每个”好评率”的置信区间（参考z Test或者t Test，以95%的概率来处理）。</p>
<p>第三步，根据置信区间的下限值，进行排名。这个值越大，排名就越高。</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/1.png" alt=""></p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/2.png" alt=""></p>
<p>解释一下，n为评价数，p为好评率，z为对应检验对应概率区间下的统计量</p>
<p>比如t-分布：</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/3.png" alt=""></p>
<p>可以看到，当n的值足够大时，这个下限值会趋向p，如果n非常小，这个下限值会大大小于p，更加符合实际。</p>
<p>Reddit的评论排名，目前就使用这个算法。国内的化，滴滴也有部分业务涉及，效果也不错。</p>
<hr>
<p>更新一下，没想到这个话题还是有高达9个人关注，所以这边我再说一些更细化的过程吧</p>
<p>在计算排名的时候，我们通常会考虑三个事情</p>
<p>1.上文讲到的，次数+好评率的分布，次数越多好评率越可靠，好评率越高该项越值得推荐</p>
<p>2.时间因素，如果一个项目是10天前推送的，一个项目是昨天推送的，很明显前者的次数远大于后者</p>
<p>3.影响权重，你这边只考虑了喜欢和不喜欢，其实所有的排序不可能只以1个维度考虑，通常会考虑多个维度，比如浏览次数，搜索次数等，你需要考虑每个的重要性或者说权重大小</p>
<p>1这里就不讲了，其他方法也有很多，比如贝叶斯平均的优化版本、再比如经典的Hacker公式：</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/4.png" alt=""></p>
<p>2.时间因素：</p>
<p>时间越久，代表之前的投票结果对当前的影响越小，这边有很多不同的影响方式，举几个例子：</p>
<p>比如艾宾浩斯遗忘规律：</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/5.png" alt=""></p>
<p>这里的c、k决定下降速度，业务运用过程中，c值一般在[1,2],k值一般在[1.5,2.5]</p>
<p>比如时效衰减：</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/6.png" alt=""></p>
<p>这里就是比较常见的移动窗口式的，永远只看近期某一段时间，而且时间内呈线性下降，不过可以改变变化方式</p>
<p>3.不同种的属性对于结果的影响自然不同</p>
<p>举个例子，用户主动搜索和用户浏览相比，用户主动搜索的情况下，用户的需求更为强烈</p>
<p>通常需要判断这些强烈程度都是通过：</p>
<p>相关性：看因变量与自变量之间的相关系数，如：cor函数</p>
<p>importance：看删除或者修改自变量，对应变量的判断影响大小，如：randomForest的重要性</p>
<p>离散程度：看自变量的数据分布是否足够分散，是否具有判断依据，如：变异系数或者pca</p>
<p>等等</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[协同过滤推荐]]></title>
      <url>/2017/06/21/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<p>set.seed ( 1234 )</p>
<h1 id="加载数据包"><a href="#加载数据包" class="headerlink" title="加载数据包"></a>加载数据包</h1><p>library ( “recommenderlab” )</p>
<h1 id="构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as-函数转为raringMatrix类型。"><a href="#构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as-函数转为raringMatrix类型。" class="headerlink" title="构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as()函数转为raringMatrix类型。"></a>构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as()函数转为raringMatrix类型。</h1><p>val1&lt;- matrix ( sample ( c ( as.numeric ( 0 : 5 ) ,NA ) ,50 ,replace = TRUE ,prob = c ( rep ( .4 / 6 , 6 ) , .6 ) ) ,ncol = 10 , dimnames = list ( user = paste ( “u” ,1 : 5 ,sep = ‘’ ) ,item = paste ( “i” ,1 : 10 ,sep = ‘’ ) ) )</p>
<p>val2 &lt;- as ( val1, “realRatingMatrix” )</p>
<p>数据转换</p>
<p>val3&lt;- normalize ( val2 )</p>
<h1 id="二元分类转换，normalize-函数进行标准化处理，标准化的目的是为了去除用户评分的偏差"><a href="#二元分类转换，normalize-函数进行标准化处理，标准化的目的是为了去除用户评分的偏差" class="headerlink" title="二元分类转换，normalize()函数进行标准化处理，标准化的目的是为了去除用户评分的偏差"></a>二元分类转换，normalize()函数进行标准化处理，标准化的目的是为了去除用户评分的偏差</h1><p>val4 &lt;- binarize ( val3 , minRating = 4 )</p>
<p>val5 &lt;- as ( val4 , “matrix” )</p>
<p>数据可视化</p>
<p>接下来，我们采用MovieLense数据集，</p>
<p>data ( MovieLense )</p>
<p>key1 &lt;- sample ( MovieLense , 943 , replace = F )<br>image ( MovieLense )</p>
<p>hist ( getRatings ( normalize ( MovieLense ) ) , breaks = 100 )</p>
<p>hist ( rowCounts ( key1 ) , breaks = 50 )</p>
<p>建立模型</p>
<p>对于realRatingMatrix有六种方法：IBCF(基于物品的推荐)、UBCF（基于用户的推荐）、PCA（主成分分析）、RANDOM（随机推荐）、SVD（矩阵因子化）、POPULAR（基于流行度的推荐）</p>
<p>建立协同过滤推荐算法模型，主要运用recommender(data=ratingMatrix,method,parameter=NULL)函数，getModel()可查看模型参数</p>
<p>key1_recom &lt;- Recommender (key1 , method = “IBCF” )</p>
<p>key1_popul &lt;- Recommender ( key1, method = “POPULAR” )</p>
<h1 id="查看模型方法"><a href="#查看模型方法" class="headerlink" title="查看模型方法"></a>查看模型方法</h1><p>names ( getModel ( key1_recom ) )</p>
<p>模型预测</p>
<p>TOP-N预测</p>
<p>对模型预测可运用predict()函数，在此分别以TOP-N预测及评分预测为例，预测第940-943位观影者的评分情况。n表示最终为TOP-N的列表推荐，参数type = “ratings”表示运用评分预测观影者对电影评分，模型结果均需转为list或矩阵表示</p>
<p>pred &lt;- predict ( key1_popul ,key1 [ 940 : 943,] , n = 5 )</p>
<p>as ( pred , “list” )</p>
<h1 id="top-N为有序列表，抽取最优推荐子集"><a href="#top-N为有序列表，抽取最优推荐子集" class="headerlink" title="top-N为有序列表，抽取最优推荐子集"></a>top-N为有序列表，抽取最优推荐子集</h1><p>pred3 &lt;- bestN ( pred , n = 3 )</p>
<p>as ( pred3 , “list” )</p>
<h1 id="评分预测"><a href="#评分预测" class="headerlink" title="评分预测"></a>评分预测</h1><p>rate &lt;- predict ( key1_popul , key1 [ 940 : 943 ] , type = “ratings” )</p>
<p>as ( rate , “matrix” ) [ , 1 : 5 ]</p>
<p>预测模型评价</p>
<p>评分预测模型评价</p>
<p>eva &lt;- evaluationScheme (key1 [ 1 : 800 ] , method = “split” , train = 0.9,given = 15)<br>method=”split”&amp;train=0.9为按90%划分训练测试集合,given为评价的类目数</p>
<p>r_eva1&lt;- Recommender ( getData ( eva , “train” ) , “UBCF” )</p>
<p>p_eva1&lt;- predict ( r_eva1 , getData ( eva, “known” ) , type = “ratings” )</p>
<p>r_eva2 &lt;- Recommender ( getData ( eva, “train” ) , “IBCF” )</p>
<p>p_eva2 &lt;- predict ( r_eva2 , getData ( eva, “known” ) , type = “ratings” )<br>c_eva1 &lt;- calcPredictionAccuracy ( p_eva1 , getData ( eva , “unknown” ) )</p>
<p>c_eva2 &lt;- calcPredictionAccuracy ( p_eva2 , getData ( eva , “unknown” ) )</p>
<p>error &lt;- rbind ( c_eva1 , c_eva2 )</p>
<p>rownames ( error ) &lt;- c ( “UBCF” , “IBCF” )<br>计算预测模型的准确度</p>
<p>TOP-N预测模型评价</p>
<p>通过4-fold交叉验证方法分割数据集，运用evaluate()进行TOP-N预测模型评价,评价结果可通过ROC曲线及准确率-召回率曲线展示:</p>
<h1 id="4-fold交叉验证"><a href="#4-fold交叉验证" class="headerlink" title="4-fold交叉验证"></a>4-fold交叉验证</h1><p>tops &lt;- evaluationScheme ( key1 [ 1 : 800 ] , method = “cross” , k = 4 , given = 3 ,goodRating = 5 )</p>
<p>results &lt;- evaluate ( tops , method = “POPULAR” , type = “topNList” ,n = c ( 1 , 3 , 5 , 10 ) )</p>
<h1 id="获得混淆矩阵"><a href="#获得混淆矩阵" class="headerlink" title="获得混淆矩阵"></a>获得混淆矩阵</h1><p>getConfusionMatrix ( results ) [ [ 1 ] ]</p>
<p>avg ( results )</p>
<p>推荐算法的比较</p>
<p>除了对预测模型进行评价，还可以对不同推荐算法进行比较。可首先构建一个推荐算法列表，通过ROC曲线、、准确率-召回率曲线或RMSE直方图进行比较</p>
<p>TOP-N算法比较</p>
<p>set.seed ( 2016 )</p>
<p>scheme &lt;- evaluationScheme ( key1 , method = “split” , train = 0.9 , k = 1 , given = 10 , goodRating = 5 )</p>
<h1 id="构建不同算法模型"><a href="#构建不同算法模型" class="headerlink" title="构建不同算法模型"></a>构建不同算法模型</h1><p>results &lt;- evaluate ( scheme ,test_data ,n = c ( 1 ,3 ,5 ,10 ,15 ,20 ) )</p>
<h1 id="模型比较-ROC曲线"><a href="#模型比较-ROC曲线" class="headerlink" title="模型比较#ROC曲线"></a>模型比较#ROC曲线</h1><p>plot ( results , annotate = c ( 1 , 3 ) , legend = “bottomright” )</p>
<h1 id="准确率-召回率曲线"><a href="#准确率-召回率曲线" class="headerlink" title="准确率-召回率曲线"></a>准确率-召回率曲线</h1><p>plot ( results , “prec/rec” , annotate = c ( 2 , 3 , 4 ) , legend = “topleft” )</p>
<p>预测评分算法比较</p>
<p>results2 &lt;- evaluate ( scheme , algorithms , type = “ratings” )</p>
<p>plot ( results2 , ylim = c ( 0 , 20 ) )</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[聚类算法思路总结]]></title>
      <url>/2017/06/20/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>1.cost function</p>
<p>1.1 距离</p>
<p>常见的为欧式距离（L1 norm）&amp;&amp;p=2，拓展的可以有闵可夫斯基距离（L2 norm）&amp;&amp;p=1：</p>
<p>当p趋向于无穷的时候，切比雪夫距离（Chebyshev distance）：</p>
<p>红色的时候为切比雪夫距离，蓝色为闵可夫斯基距离，绿色为欧式距离。</p>
<p>1.2相似系数</p>
<p>夹角余弦及相关系数，相关系数不受线性变换的影响，但是计算速度远慢于距离计算。</p>
<p>1.3dynamic time warping动态时间规整</p>
<p>举例子：</p>
<p>序列A：1,1,1,10,2,3，序列B：1,1,1,2,10,3</p>
<p>欧式距离：distance[i][j]=(b[j]-a[i])*(b[j]-a[i])来计算的话，总的距离和应该是128</p>
<p>应该说这个距离是非常大的，而实际上这个序列的图像是十分相似的。因为序列A中的10对应得是B中的2，A中的2对应的B中的10，导致计算膨胀，现在将A中的10对应B中的10，A中的1对应B中的2再计算，膨胀因素会小很多（时间前推一步）。</p>
<p>2.聚类算法</p>
<p>2.1分层聚类：</p>
<p>自上而下：所有点先聚为一类，然后分层次的一步一步筛出与当前类别差异最大的点</p>
<p>自下而上：所有点先各自为一类，组合成n个类的集合，然后寻找出最靠近的两者聚为新的一类，循环往复</p>
<p>数值类分类：（适用于计算量巨大或者数据量巨大的时候）</p>
<p>BIRCH算法，层次平衡迭代规约和聚类，</p>
<p>主要参数包含：聚类特征和聚类特征树：</p>
<p>聚类特征：</p>
<p>给定N个d维的数据点{x1,x2,….,xn}，CF定义如下：CF=（N，LS，SS）,其中，N为子类中的节点的个数，LS是子类中的N个节点的线性和，SS是N个节点的平方和</p>
<p>存在计算定义：CF1+CF2=（n1+n2, LS1+LS2, SS1+SS2）</p>
<p>假设簇C1中有三个数据点：（2,3），（4,5），（5,6），则CF1={3，（2+4+5,3+5+6），（2^2+4^2+5^2,3^2+5^2+6^2）}={3，（11,14），（45,70）}</p>
<p>假设一个簇中，存在质心C和半径R，若有xi，i=1…n个点属于该簇，质心为：C=(X1+X2+…+Xn)/n，R=(|X1-C|^2+|X2-C|^2+…+|Xn-C|^2)/n</p>
<p>其中，簇半径表示簇中所有点到簇质心的平均距离。当有一个新点加入的时候，属性会变成CF=（N，LS，SS）的统计值，会压缩数据。</p>
<p>聚类特征树：</p>
<p>内节点的平衡因子B，子节点的平衡因子L，簇半径T。</p>
<p>B=6，深度为3，T为每个子节点中簇的范围最大不能超过的值，T越大簇越少，T越小簇越多。</p>
<p>名义分类：</p>
<p>ROCK算法：凝聚型的层次聚类算法</p>
<p>1.如果两个样本点的相似度达到了阈值（θ），这两个样本点就是邻居。阈值（θ）有用户指定，相似度也是通过用户指定的相似度函数计算。常用的分类属性的相似度计算方法有：Jaccard系数，余弦相似度</p>
<p>Jaccard系数：J=|A∩B|/|A∪B|，一般用于分类变量之间的相似度</p>
<p>余弦相似度：【-1，1】之间，越趋近于0的时候，方向越一致，越趋向同一。</p>
<p>2.目标函数（criterion function）：最终簇之间的链接总数最小，而簇内的链接总数最大</p>
<p>3.相似度合并：遵循最终簇之间的链接总数最小，而簇内的链接总数最大的规则计算所有对象的两两相似度，将相似性最高的两个对象合并。通过该相似性度量不断的凝聚对象至k个簇，最终计算上面目标函数值必然是最大的。</p>
<p>load(‘country.RData’)</p>
<p>d&lt;-dist(countries[,-1])</p>
<p>x&lt;-as.matrix(d)</p>
<p>library(cba)</p>
<p>rc &lt;-rockCluster(x, n=4, theta=0.2, debug=TRUE)</p>
<p>KNN算法：</p>
<p>先确定K的大小，计算出每个点之外的所有点到这个目标点的距离，选出K个最近的作为一类。一般类别之间的归类的话，投票和加权为常用的，投票及少数服从多数，投票的及越靠近的点赋予越大的权重值。</p>
<p>2.2分隔聚类：</p>
<p>需要先确定分成的类数，在根据类内的点都足够近，类间的点都足够远的目标去做迭代。</p>
<p>常用的有K-means，K-medoids，K-modes等，只能针对数值类的分类，且只能对中等量级数据划分，只能对凸函数进行聚类，凹函数效果很差。</p>
<p>2.3密度聚类：</p>
<p>有效的避免了对分隔聚类下对凹函数聚类效果不好的情况，有效的判别入参主要有1:单点外的半径2：单点外半径内包含的点的个数</p>
<p>DBSCAN为主要常见的算法，可优化的角度是现在密度较高的地方进行聚类，再往密度较低的地方衍生，优化算法：OPTICS。</p>
<p>2.4网格聚类：</p>
<p>将n个点映射到n维上，在不同的网格中，计算点的密度，将点更加密集的网格归为一类。</p>
<p>优点是：超快，超级快，不论多少数据，计算速度只和维度相关。</p>
<p>缺点：n维的n难取，受分布影响较大（部分行业数据分布及其不规则）</p>
<p>2.5模型聚类：</p>
<p>基于概率和神经网络聚类，常见的为GMM，高斯混合模型。缺点为，计算量较大，效率较低。</p>
<p>GMM：每个点出现的概率：将k个高斯模型混合在一起，每个点出现的概率是几个高斯混合的结果</p>
<p>假设有K个高斯分布，每个高斯对data points的影响因子为πk，数据点为x，高斯参数为theta，则：</p>
<p>利用极大似然的方法去求解均值Uk，协方差矩阵（Σk），影响因子πk，但是普通的梯度下降的方法在这里求解会很麻烦，这边就以EM算法代替估计求解。</p>
<p>3.优化数据结构：</p>
<p>1.数据变换：</p>
<p>logit处理，对所有数据进行log变换</p>
<p>傅里叶变换</p>
<p>小波变换</p>
<p>2.降维：</p>
<p>PCA：</p>
<p>利用降维（线性变换)的思想，整体方差最大的情况下（在损失很少信息的前提下），把多个指标转化为几个不相关的综合指标（主成分),将变量线性组合代替原变量，保持代替后的数据信息量最大（方差最大）。</p>
<p>LLE：</p>
<p>(1) 寻找每个样本点的k个近邻点；</p>
<p>(2)由每个样本点的近邻点计算出该样本点的局部重建权值矩阵；</p>
<p>(3)由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值。</p>
<p>(换句话说，就是由周围N个点构成改点的一个向量矩阵表示）</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 聚类 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[常用R语言包介绍]]></title>
      <url>/2017/06/19/%E5%B8%B8%E7%94%A8R%E8%AF%AD%E8%A8%80%E5%8C%85%E4%BB%8B%E7%BB%8D/</url>
      <content type="html"><![CDATA[<p>r与python差异比较大的一个地方就是，python的机器学习算法集中程度比较高，比如sklearn，就集成了很多的算法，而R语言更多时候需要一个包一个包去了解，比较费时费力，对于python转过来的朋友非常不友好，抽空整理了工作中常用的R包如下：</p>
<p>常用检验函数：</p>
<p>基本上分布中常见的都罗列了：</p>
<p>常用作图函数包：</p>
<p>ggplot2：万能，基本上excel能画的图它都能画</p>
<p>rattle：fancyRpartPlot函数，决策树画图函数</p>
<p>基础包函数：barplot、pie、dotchart、hist、densityplot、boxplot、contour等等</p>
<p>正态检验：qqplot、qqline、qqnorm</p>
<p>连续分类回归模型：</p>
<p>stats包 lm函数，实现多元线性回归；glm函数，实现广义线性回归；nls函数，实现非线性最小二乘回归；knn函数，k最近邻算法</p>
<p>rpart包 rpart函数，基于CART算法的分类回归树模型</p>
<p>randomForest包 randomForest函数，基于rpart算法的集成算法</p>
<p>e1071包 svm函数，支持向量机算法</p>
<p>kernlab包 ksvm函数，基于核函数的支持向量机</p>
<p>nnet包 nnet函数，单隐藏层的神经网络算法</p>
<p>neuralnet包 neuralnet函数，多隐藏层多节点的神经网络算法</p>
<p>RSNNS包 mlp函数，多层感知器神经网络；rbf函数，基于径向基函数的神经网络</p>
<p>离散分类回归模型：</p>
<p>stats包 glm函数，实现Logistic回归，选择logit连接函数</p>
<p>kknn包 kknn函数，加权的k最近邻算法</p>
<p>rpart包 rpart函数，基于CART算法的分类回归树模型</p>
<p>adabag包bagging函数，基于rpart算法的集成算法；boosting函数，基于rpart算法的集成算法</p>
<p>party包ctree函数，条件分类树算法</p>
<p>RWeka包OneR函数，一维的学习规则算法；JPip函数，多维的学习规则算法；J48函数，基于C4.5算法的决策树</p>
<p>C50包C5.0函数，基于C5.0算法的决策树</p>
<p>e1071包naiveBayes函数，贝叶斯分类器算法</p>
<p>klaR包NaiveBayes函数，贝叶斯分类器算分</p>
<p>MASS包lda函数，线性判别分析；qda函数，二次判别分析</p>
<p>聚类：Nbclust包Nbclust函数可以确定应该聚为几类</p>
<p>stats包kmeans函数，k均值聚类算法；hclust函数，层次聚类算法</p>
<p>cluster包pam函数，k中心点聚类算法</p>
<p>fpc包dbscan函数，密度聚类算法；kmeansruns函数，相比于kmeans函数更加稳定，而且还可以估计聚为几类；pamk函数，相比于pam函数，可以给出参考的聚类个数</p>
<p>mclust包Mclust函数，期望最大（EM）算法</p>
<p>关联规则：arules包apriori函数</p>
<p>Apriori关联规则算法</p>
<p>recommenderlab协调过滤</p>
<p>DRM：重复关联</p>
<p>ECLAT算法： 采用等价类，RST深度搜索和集合的交集： eclat</p>
<p>降维算法：</p>
<p>psych包prcomp函数、factanal函数</p>
<p>时序分析：</p>
<p>ts时序构建函数</p>
<p>timsac包时序分析</p>
<p>holtwinter包时序分析</p>
<p>decomp、tsr、stl成分分解</p>
<p>zoo 时间序列数据的预处理</p>
<p>统计及预处理：</p>
<p>常用的包 Base R, nlme</p>
<p>aov, anova 方差分析</p>
<p>density 密度分析</p>
<p>t.test, prop.test, anova, aov:假设检验</p>
<p>rootSolve非线性求根</p>
<p>reshape2数据预处理</p>
<p>plyr及dplyr数据预处理大杀器</p>
<p>最后剩下常用的就是读入和写出了：</p>
<p>RODBC 连接ODBC数据库接口</p>
<p>jsonlite 读写json文件</p>
<p>yaml 读写yaml文件</p>
<p>rmakdown写文档</p>
<p>knitr自动文档生成</p>
<p>一般业务中使用比较多的就是上面这些了，当然R里面有很多冷门的包，也很好用滴~</p>
]]></content>
      
        <categories>
            
            <category> 工具 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> R语言工具 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[决策树及衍射指标]]></title>
      <url>/2017/06/02/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%8A%E8%A1%8D%E5%B0%84%E6%8C%87%E6%A0%87/</url>
      <content type="html"><![CDATA[<p>一、常用的决策树节点枝剪的衡量指标：</p>
<p>熵：</p>
<p>如果一件事有k种可的结果，每种结果的概率为 pi（i＝1…k）</p>
<p>该事情的信息量：</p>
<p>熵越大，随机变量的不确定性越大。</p>
<p>信息增益：</p>
<p>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下的经验条件熵H(D|A)之差</p>
<p>换句话说，就是原信息集下的信息量－在A特征条件下的信息集的信息量</p>
<p>信息增益越大，信息增多，不确定性减小</p>
<p>信息增益率：</p>
<p>信息增益率定义:特征A对训练数据集D的信息增益比定义为其信息增益与训练数据D关于特征A的值的熵HA(D)之比</p>
<p>注：p：每个唯独上，每个变量的个数／总变量个数</p>
<p>二、常用的决策树介绍：</p>
<p>ID3算法：</p>
<p>ID3算法的核心是在决策树各个子节点上应用信息增益准则选择特征，递归的构建决策树，具体方法是:从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。</p>
<p>解释：在做每次选择差分枝的时候，以不确定性最小点作为loss fuction，直到无法细分</p>
<p>缺点：</p>
<p>1.ID3算法只有树的生成，所以该算法生成的树容易产生过拟合，分得太细，考虑条件太多。</p>
<p>2.不能处理连续属性</p>
<p>3.选择具有较多分枝的属性，而分枝多的属性不一定是最优的选择。</p>
<p>4.局部最优化，整体熵值最小，贪心算法算子节点的分支</p>
<p>C4.5算法：</p>
<p>基于ID3算法，用信息增益比来选择属性，对非离散数据也能处理，能够对不完整数据进行处理。</p>
<p>采用增益率（GainRate）来选择分裂属性。计算方式如下：</p>
<p>CART算法：</p>
<p>CART算法选择分裂属性的方式是比较有意思的，首先计算不纯度，然后利用不纯度计算Gini指标。</p>
<p>计算每个子集最小的Gini指标作为分裂指标。</p>
<p>不纯度的计算方式为：</p>
<p>pi表示按某个变量划分中，目标变量不同类别的概率。</p>
<p>某个自变量的Gini指标的计算方式如下：</p>
<p>计算出每个每个子集的Gini指标，选取其中最小的Gini指标作为树的分支（Gini（D）越小，则数据集D的纯度越高）。连续型变量的离散方式与信息增益中的离散方式相同。</p>
<p>三、基于决策树的一些集成算法：</p>
<p>随机森林：</p>
<p>随机生成n颗树，树之间不存在关联，取结果的时候，以众数衡量分类结果；除了分类，变量分析，无监督学习，离群点分析也可以。</p>
<p>生成过程：</p>
<p>1.n个样本，随机选择n个样本（有放回），训练一颗树</p>
<p>从原始训练数据集中,应用bootstrap方法有放回地随机抽取 K个新的自助样本集,并由此构建 K棵分类回归树,每次未被抽到的样本组成了 K个袋外数据(Out-of-bag,OOB)</p>
<p>2.每个样本有M个属性，随机选m个，采取校验函数（比如信息增益、熵啊之类的），选择最佳分类点</p>
<p>3.注意，每个树不存在枝剪</p>
<p>4.将生成的多棵树组成随机森林,用随机森林对新的数据进行分类,分类结果按树分类器的投票多少而定</p>
<p>树的个数随机选取，一般500，看三个误差函数是否收敛；变量的个数一般取均方作为mtry</p>
<p>GBDT：</p>
<p>DT步骤：</p>
<p>GBDT里面的树是回归树！</p>
<p>GBDT做每个节点上的分支的时候，都会以最小均方误差作为衡量（真实值－预测值）的平方和／N，换句话说，就是存在真实线l1，预测线l2，两条线之间的间距越小越好。</p>
<p>BT步骤：</p>
<p>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。</p>
<p>换句话说，就是第一次预测的差值记为下一次预测的初始值，一直到某一次计算出的差值为0，把前n次的结果相加，就是一个真实预测。</p>
<p>Adaboost：</p>
<p>步骤：</p>
<p>1.初始化所有训练样例的权重为1 / N,其中N是样本数</p>
<p>2.对其中第1~m个样本:</p>
<p>a.训练m个弱分类器，使其最小化bias：</p>
<p>b.接下来计算该弱分类器的权重α，降低错判的分类器的权重，：</p>
<p>c.更新权重：</p>
<p>3.最后得到组合分类器：</p>
<p>核心的思想如下图：</p>
<p>全量数据集在若干次训练后，降低训练正确的样本的权重，提高训练错误样本的权重，得到若干个Y对应的分类器，在组合投票得到最终的分类器</p>
<p>四、惠普实验室-集成并行化的随机森林：</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 树划分问题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[因子分析原理剖析]]></title>
      <url>/2017/06/02/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>因子分析概述：</p>
<p>因子分析分为Q型和R型，我们对R型进行如下研究：</p>
<p>一.因子分析步骤：</p>
<p>1.确认是是否适合做因子分析</p>
<p>2.构造因子变量</p>
<p>3.旋转方法解释</p>
<p>4.计算因子变量得分</p>
<p>二.因子分析的计算过程：</p>
<p>1.将原始数据标准化</p>
<p>目的：消除数量级量纲不同</p>
<p>2.求标准化数据的相关矩阵</p>
<p>3.求相关矩阵的特征值和特征向量</p>
<p>4.计算方差贡献率和累计方差贡献率</p>
<p>5.确定因子</p>
<p>F1,F2,F3…为前m个因子包含数据总量（累计贡献率）不低于80%。可取前m各因子来反映原评价</p>
<p>6.因子旋转</p>
<p>当所得因子不足以明显确定或不易理解时选择此方法</p>
<p>7.原指标的线性组合求各因子的得分</p>
<p>两种方法：回归估计和barlett估计法</p>
<p>8.综合得分：以各因子的方差贡献率为权，各因子的线性组合得到各综合评价指标函数</p>
<p>F=（λ1F1+…λmFm）/(λ1+…λm)</p>
<p>=W1F1+…WmFm</p>
<p>9.得分排序</p>
<p>因子分析详解：</p>
<p>因子分析模型，又名正交因子模型</p>
<p>X=AF+ɛ</p>
<p>其中：</p>
<p>X=[X1,X2,X3…XP]‘</p>
<p>A=</p>
<p>F=[F1,F2…Fm]’</p>
<p>ɛ=[ɛ1,ɛ2…ɛp]’</p>
<p>以上满足：</p>
<p>（1）m小于等于p</p>
<p>（2）cov(F,ɛ)=0</p>
<p>(3)Var(F)=Im</p>
<p>D(ɛ)=Var(ɛ)=</p>
<p>ɛ1,ɛ2…ɛp不相关，且方差不同</p>
<p>我们把F成为X公共因子，A为荷载矩阵，ɛ为X特殊因子</p>
<p>A=(aij)</p>
<p>数学上证明：aij就是i个变量与第j个因子的相关系数，参见层次分析法aij定义。</p>
<p><1>荷载矩阵</1></p>
<p>就荷载矩阵的估计和解释方法有主因子和极大似然估计，我们就主因子分析而言：（是主因子不是主成份）</p>
<p>设随机向量X的协方差阵为Ʃ</p>
<p>λ1,λ2,λ3..&gt;0为Ʃ的特征根</p>
<p>μ1，μ2，μ3…为对应的标准正交向量</p>
<p>我们大一学过线代或者高代，里面有个东西叫谱分析：</p>
<p>Ʃ=λ1μ1μ1’+……+λpμpμp’</p>
<p>=</p>
<p>当因子个数和变量个数一样多，特殊因子方差为0.</p>
<p>此时，模型为X=AF,其中Var(F)=Ip</p>
<p>于是，Var(X)=Var(AF)=AVar(F)A’=AA’</p>
<p>对照Ʃ分解式,A第j列应该是</p>
<p>也就是说，除了uj前面部分，第j列因子签好为第j个主成份的系数，所以为主成份法。</p>
<p>如果非要作死考虑ɛ</p>
<p>原来的协方差阵可以分解为：</p>
<p>Ʃ=AA’+D=</p>
<p>以上分析的目的；</p>
<p>1.因子分析模型是描述原变量X的协方差阵Ʃ的一种模型</p>
<p>2.主成份分析中每个主成份相应系数是唯一确定的，然而因子分析中的每个因子的相应系数不是唯一的，因而我们的因子荷载矩阵不是唯一的</p>
<p>(主成分分析是因子分析的特例，非常类似，有兴趣的可以去看看，这两者非常容易混淆)</p>
<p><2>共同度和方差贡献</2></p>
<p>无论是在spss或者R的因子分析中都围绕着贡献度，我们来看下，它到底是什么意思。</p>
<p>由因子分析模型，当仅有一个公因子F时，</p>
<p>Var(Xi)=Var(aiF)+Var(ɛi)</p>
<p>由于数据标准化，左端为1，右端分别为共性方差和个性方差</p>
<p>共性方差越大，说明共性因子作用越大。</p>
<p>因子载荷矩阵A中的第i行元素之平方和记为hi2</p>
<p>成为变量(Xi)共同度</p>
<p>它是公共因子对(Xi)的方差锁做出的贡献，反映了全部公共因子对变量(Xi)的影响。</p>
<p>hi2大表明第i个分量对F的每一个分量F1,F2,…Fm的共同依赖程度大</p>
<p>将因子载荷矩阵A的第j列的各元素的平方和记为gj2</p>
<p>成为公共因子Fj对x的方差贡献。</p>
<p>gj2表示第j个公共因子Fj对x的每一个分量Xi所提供的方差的总和，他就是衡量公共因子的相对重要行的指标。gj2越大，表明公共因子Fj对x的贡献越大，或者说对x的影响和作用就越大。</p>
<p>如果将载荷矩阵A的所有gj2都计算出来，按大小排列，就可以提炼最有影响力的公共因子。</p>
<p><3>因子旋转</3></p>
<p>这方面涉及较为简单，我就简单提一下</p>
<p>目的：建立因子分析模型不是只要找主因子，更加重要的是意义，以便对实际进行分析，因子旋转就是使所得结论更加清晰的表示。</p>
<p>方法：正交旋转，斜交旋转两大类，常用正交。</p>
<p>便于理解，我解释下旋转的意义，以平面直角坐标系为例，我们想得到的数据正好为：y=x和y=-x上的点，我们能解释的却在x=0和y=0上，这时候我们就可以旋转坐标系，却不影响结果。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 因子分析 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
