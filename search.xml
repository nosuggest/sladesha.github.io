<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[SMOTE算法]]></title>
      <url>/2017/12/01/SMOTE%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<hr>
<p>17.11.28更新一下：最近把这个算法集成到了数据预处理的python工程代码中了，不想看原理想直接用的，有简易版的<a href="http://www.jianshu.com/p/1f2f887f0811" target="_blank" rel="noopener">python开发：特征工程代码模版
</a>，进入页面后ctrl+F搜smote就行，请自取</p>
<hr>
<p>之前一直没有用过python，最近做了一些数量级比较大的项目，觉得有必要熟悉一下python，正好用到了smote，网上也没有搜到，所以就当做一个小练手来做一下。</p>
<p>首先，看下Smote算法之前，我们先看下当正负样本不均衡的时候，我们通常用的方法：</p>
<ul>
<li>抽样<br>常规的包含过抽样、欠抽样、组合抽样<br>过抽样：将样本较少的一类sample补齐<br>欠抽样：将样本较多的一类sample压缩<br>组合抽样：约定一个量级N，同时进行过抽样和欠抽样，使得正负样本量和等于约定量级N</li>
</ul>
<p>这种方法要么丢失数据信息，要么会导致较少样本共线性，存在明显缺陷</p>
<ul>
<li>权重调整<br>常规的包括算法中的weight，weight matrix<br>改变入参的权重比，比如boosting中的全量迭代方式、逻辑回归中的前置的权重设置</li>
</ul>
<p>这种方式的弊端在于无法控制合适的权重比，需要多次尝试</p>
<ul>
<li>核函数修正<br>通过核函数的改变，来抵消样本不平衡带来的问题</li>
</ul>
<p>这种使用场景局限，前置的知识学习代价高，核函数调整代价高，黑盒优化</p>
<ul>
<li>模型修正<br>通过现有的较少的样本类别的数据，用算法去探查数据之间的特征，判读数据是否满足一定的规律<br>比如，通过线性拟合，发现少类样本成线性关系，可以新增线性拟合模型下的新点</li>
</ul>
<p>实际规律比较难发现，难度较高</p>
<p><strong>SMOTE（Synthetic minoritye over-sampling technique,SMOTE）是Chawla在2002年提出的过抽样的算法，一定程度上可以避免以上的问题</strong></p>
<p>下面介绍一下这个算法：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-f498eba11b0aa678.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="正负样本分布"></p>
<p>很明显的可以看出，蓝色样本数量远远大于红色样本，在常规调用分类模型去判断的时候可能会导致之间忽视掉红色样本带了的影响，只强调蓝色样本的分类准确性，这边需要增加红色样本来平衡数据集</p>
<p>Smote算法的思想其实很简单，先随机选定n个少类的样本，如下图</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-5782835263511a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="找出初始扩展的少类样本"></p>
<p>再找出最靠近它的m个少类样本，如下图</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-9fa8ec63346c4684.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>再任选最临近的m个少类样本中的任意一点，</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-dad1a4b19048d4ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在这两点上任选一点，这点就是新增的数据样本</p>
<hr>
<p>R语言上的开发较为简单，有现成的包库，这边简单介绍一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rm(list=ls())</span><br><span class="line">install.packages(“DMwR”,dependencies=T)</span><br><span class="line">library(DMwR)#加载smote包</span><br><span class="line">newdata=SMOTE(formula,data,perc.over=,perc.under=)</span><br><span class="line">#formula:申明自变量因变量</span><br><span class="line">#perc.over：过采样次数</span><br><span class="line">#perc.under：欠采样次数</span><br></pre></td></tr></table></figure></p>
<p>效果对比：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-dc13c3ce230c1d0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>简单的看起来就好像是重复描绘了较少的类<br>这边的smote是封装好的，直接调用就行了，没有什么特别之处</p>
<hr>
<p>这边自己想拿刚学的python练练手，所有就拿python写了一下过程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from numpy import *</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">#读数据</span><br><span class="line">data = pd.read_table(&apos;C:/Users/17031877/Desktop/supermarket_second_man_clothes_train.txt&apos;, low_memory=False)</span><br><span class="line"></span><br><span class="line">#简单的预处理</span><br><span class="line">test_date = pd.concat([data[&apos;label&apos;], data.iloc[:, 7:10]], axis=1)</span><br><span class="line">test_date = test_date.dropna(how=&apos;any&apos;)</span><br></pre></td></tr></table></figure></p>
<p>数据大致如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">test_date.head()</span><br><span class="line">Out[25]:</span><br><span class="line">   label  max_date_diff  max_pay  cnt_time</span><br><span class="line">0      0           23.0  43068.0        15</span><br><span class="line">1      0           10.0   1899.0         2</span><br><span class="line">2      0          146.0   3299.0        21</span><br><span class="line">3      0           30.0  31959.0        35</span><br><span class="line">4      0            3.0  24165.0        98</span><br><span class="line">test_date[&apos;label&apos;][test_date[&apos;label&apos;]==0].count()/test_date[&apos;label&apos;][test_date[&apos;label&apos;]==1].count()</span><br><span class="line">Out[37]: 67</span><br></pre></td></tr></table></figure></p>
<p>label是样本类别判别标签，1:0=67:1，需要对label=1的数据进行扩充</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 筛选目标变量</span><br><span class="line">aimed_date = test_date[test_date[&apos;label&apos;] == 1]</span><br><span class="line"># 随机筛选少类扩充中心</span><br><span class="line">index = pd.DataFrame(aimed_date.index).sample(frac=0.1, random_state=1)</span><br><span class="line">index.columns = [&apos;id&apos;]</span><br><span class="line">number = len(index)</span><br><span class="line"># 生成array格式</span><br><span class="line">aimed_date_new = aimed_date.ix[index.values.ravel(), :]</span><br></pre></td></tr></table></figure>
<p>随机选取了全量少数样本的10%作为数据扩充的中心点</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 自变量标准化</span><br><span class="line">sc = StandardScaler().fit(aimed_date_new)</span><br><span class="line">aimed_date_new = pd.DataFrame(sc.transform(aimed_date_new))</span><br><span class="line">sc1 = StandardScaler().fit(aimed_date)</span><br><span class="line">aimed_date = pd.DataFrame(sc1.transform(aimed_date))</span><br><span class="line"></span><br><span class="line"># 定义欧式距离计算</span><br><span class="line">def dist(a, b):</span><br><span class="line">    a = array(a)</span><br><span class="line">    b = array(b)</span><br><span class="line">    d = ((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2 + (a[2] - b[2]) ** 2 + (a[3] - b[3]) ** 2) ** 0.5</span><br><span class="line">    return d</span><br></pre></td></tr></table></figure>
<p>下面定义距离计算的方式，所有算法中，涉及到距离的地方都需要标准化去除冈量，也同时加快了计算的速度<br>这边采取了欧式距离的方式，更多计算距离的方式参考：<br><a href="http://www.jianshu.com/p/1417fcb06797" target="_blank" rel="noopener">多种距离及相似度的计算理论介绍</a></p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 统计所有检验距离样本个数</span><br><span class="line">row_l1 = aimed_date_new.iloc[:, 0].count()</span><br><span class="line">row_l2 = aimed_date.iloc[:, 0].count()</span><br><span class="line">a = zeros((row_l1, row_l2))</span><br><span class="line">a = pd.DataFrame(a)</span><br><span class="line"># 计算距离矩阵</span><br><span class="line">for i in range(row_l1):</span><br><span class="line">    for j in range(row_l2):</span><br><span class="line">        d = dist(aimed_date_new.iloc[i, :], aimed_date.iloc[j, :])</span><br><span class="line">        a.ix[i, j] = d</span><br><span class="line">b = a.T.apply(lambda x: x.min())</span><br></pre></td></tr></table></figure>
<p>调用上面的计算距离的函数，形成一个距离矩阵</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 找到同类点位置</span><br><span class="line">h = []</span><br><span class="line">z = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    for j in range(len(a.iloc[i, :])):</span><br><span class="line">        ai = a.iloc[i, j]</span><br><span class="line">        bi = b[i]</span><br><span class="line">        if ai == bi:</span><br><span class="line">            h.append(i)</span><br><span class="line">            z.append(j)</span><br><span class="line">        else:</span><br><span class="line">            continue</span><br><span class="line">new_point = [0, 0, 0, 0]</span><br><span class="line">new_point = pd.DataFrame(new_point)</span><br><span class="line">for i in range(len(h)):</span><br><span class="line">    index_a = z[i]</span><br><span class="line">    new = aimed_date.iloc[index_a, :]</span><br><span class="line">    new_point = pd.concat([new, new_point], axis=1)</span><br><span class="line"></span><br><span class="line">new_point = new_point.iloc[:, range(len(new_point.columns) - 1)]</span><br></pre></td></tr></table></figure>
<p>再找到位置的情况下，再去原始的数据集中根据位置查找具体的数据</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">r1 = []</span><br><span class="line">for i in range(len(new_point.columns)):</span><br><span class="line">    r1.append(random.uniform(0, 1))</span><br><span class="line">new_point_last = []</span><br><span class="line">new_point_last = pd.DataFrame(new_point_last)</span><br><span class="line"># 求新点 new_x=old_x+rand()*(append_x-old_x)</span><br><span class="line">for i in range(len(new_point.columns)):</span><br><span class="line">    new_x = (new_point.iloc[1:4, i] - aimed_date_new.iloc[number - 1 - i, 1:4]) * r1[i] + aimed_date_new.iloc[</span><br><span class="line">                                                                                          number - 1 - i, 1:4]</span><br><span class="line">    new_point_last = pd.concat([new_point_last, new_x], axis=1)</span><br><span class="line">print new_point_last</span><br></pre></td></tr></table></figure>
<p>最后，再根据smote的计算公式<code>new_x=old_x+rand()*(append_x-old_x)</code>，计算出新的点即可，python练手到此就结束了</p>
<p>其实，在这个结果上，我们可以综合Tomek link做一个集成的数据扩充的算法，思路如下：<br>假设，我们利用上述的算法产生了两个青色方框的新数据点：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-051da4d8b859bbb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br><strong>我们认为，对于新产生的青色数据点与其他非青色样本点距离最近的点，构成一对Tomek link</strong>，如下图框中的青蓝两点</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-b603de85fc0748e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>我们可以定义规则：<br>当以新产生点为中心，Tomek link的距离为范围半径，去框定一个空间，空间内的<code>少数类的个数/多数类的个数</code>&lt;最低阀值的时候，认为新产生点为“垃圾点”，应该剔除或者再次进行smote训练；空间内的<code>少数类的个数/多数类的个数</code>&gt;=最低阀值的时候,在进行保留并纳入smote训练的初始少类样本集合中去抽样<br>所以，剔除左侧的青色新增点，只保留右边的新增数据如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-5a51fae31eb2d1ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>参考文献：</p>
<ul>
<li><a href="https://www.jair.org/media/953/live-953-2037-jair.pdf" target="_blank" rel="noopener">https://www.jair.org/media/953/live-953-2037-jair.pdf</a></li>
<li><a href="https://github.com/fmfn/UnbalancedDataset" target="_blank" rel="noopener">https://github.com/fmfn/UnbalancedDataset</a></li>
<li>Batista, G. E., Bazzan, A. L., &amp; Monard, M. C. (2003, December). Balancing Training Data for Automated Annotation of Keywords: a Case Study. In WOB (pp. 10-18).</li>
<li>Batista, G. E., Prati, R. C., &amp; Monard, M. C. (2004). A study of the behavior of several methods for balancing machine learning training data. ACM Sigkdd Explorations Newsletter, 6(1), 20-29.</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python开发：特征工程代码模版(二)]]></title>
      <url>/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%BA%8C/</url>
      <content type="html"><![CDATA[<p><strong>转载请注明文章来源：<a href="http://www.jianshu.com/writer#/notebooks/14156301/notes/20406464/preview" target="_blank" rel="noopener">python开发：特征工程代码模版（二）</a>，你们免费转我文章，不标注来源就算了，现在还开始写“原创”，这就过分了～</strong></p>
<p>正题开始：<br>这篇文章是入门级的特征处理的打包解决方案的python实现汇总，如果想get一些新鲜血液的朋友可以叉了，只是方便玩数据的人进行数据<strong>特征筛选</strong>的代码集合，话不多说，让我们开始。</p>
<hr>
<p>首先，让我们看一张入门级别的数据预处理的基本操作图，网上有很多版本，这个是我自己日常干活的时候必操作的行为罗列，其中<a href="http://www.jianshu.com/p/1f2f887f0811" target="_blank" rel="noopener">数据整理部分</a>已经在上一篇文章中给出了，下面我们讲一起来看看特征筛选这块。<strong>此图请尊重一下我，别拿出去传播，纯属个人的方法论，大家看看就行，谢谢。</strong>网上有其他版本的，你们去传播那些就ok了～<br><img src="http://upload-images.jianshu.io/upload_images/1129359-bee94e391963d055.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="特征工程.png"></p>
<hr>
<h3 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">var_filter</span><span class="params">(data, k=None)</span>:</span></span><br><span class="line">    var_data = data.var().sort_values()</span><br><span class="line">    <span class="keyword">if</span> k <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        new_data = VarianceThreshold(threshold=k).fit_transform(data)</span><br><span class="line">        <span class="keyword">return</span> var_data, new_data</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> var_data</span><br></pre></td></tr></table></figure>
<p>这个方法的思路很明确，我们筛掉方差过小的feature，也很好理解，一列值完全或者几乎完全一致的feature对于我们去训练最后的模型没有任何好处。熵理论也同样印证了这一点。</p>
<h3 id="线性相关系数衡量"><a href="#线性相关系数衡量" class="headerlink" title="线性相关系数衡量"></a>线性相关系数衡量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearson_value</span><span class="params">(data, label, k=None)</span>:</span></span><br><span class="line">    label = str(label)</span><br><span class="line">    <span class="comment"># k为想删除的feature个数</span></span><br><span class="line">    Y = data[label]</span><br><span class="line">    x = data[[x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]]</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">        data_res = np.c_[Y, x.iloc[:, i]].T</span><br><span class="line">        cor_value = np.abs(np.corrcoef(data_res)[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        res.append([label, x.columns[i], cor_value])</span><br><span class="line">    res = sorted(np.array(res), key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">if</span> k <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> k &lt; len(res):</span><br><span class="line">            new_c = []  <span class="comment"># 保留的feature</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(res) - k):</span><br><span class="line">                new_c.append(res[i][<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">return</span> res, new_c</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'feature个数越界～'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>当你明确了自变量与因变量之间存在线性关系的时候，你就需要剔除掉一些关心比较弱的变量，奥卡姆剃刀原理告诉我们，在尽可能压缩feature个数大小的情况下去得到效果最优的模型才是合理模型。</p>
<h3 id="共线性检验"><a href="#共线性检验" class="headerlink" title="共线性检验"></a>共线性检验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vif_test</span><span class="params">(data, label, k=None)</span>:</span></span><br><span class="line">    label = str(label)</span><br><span class="line">    <span class="comment"># k为想删除的feature个数</span></span><br><span class="line">    x = data[[x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]]</span><br><span class="line">    res = np.abs(np.corrcoef(x.T))</span><br><span class="line">    vif_value = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(res.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">if</span> j &gt; I:</span><br><span class="line">                vif_value.append([x.columns[i], x.columns[j], res[i, j]])</span><br><span class="line">    vif_value = sorted(vif_value, key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">if</span> k <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> k &lt; len(vif_value):</span><br><span class="line">            new_c = []  <span class="comment"># 保留的feature</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">                <span class="keyword">if</span> vif_value[-i][<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> new_c:</span><br><span class="line">                    new_c.append(vif_value[-i][<span class="number">1</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_c.append(vif_value[-i][<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> len(new_c) == k:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            out = [x <span class="keyword">for</span> x <span class="keyword">in</span> x.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> new_c]</span><br><span class="line">            <span class="keyword">return</span> vif_value, out</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'feature个数越界～'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> vif_value</span><br></pre></td></tr></table></figure>
<p>2-3年前面试必考题，什么叫做共线性？如何解决共线性？答案之一就是共线性检验啊，判断feature之间的相关性，剔除相关性较高的feature，在R语言里面有个VIF函数可以直接求的。除此之外，采用非线性函数做特征拆解也是很好的方法。共线性严重的情况下，会导致泛化误差异常大，需着重注意～</p>
<h3 id="Mutual-Information互信息"><a href="#Mutual-Information互信息" class="headerlink" title="Mutual Information互信息"></a>Mutual Information互信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MI</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># len(X) should be equal to len(Y)</span></span><br><span class="line">    <span class="comment"># X,Y should be the class feature</span></span><br><span class="line">    total = len(X)</span><br><span class="line">    X_set = set(X)</span><br><span class="line">    Y_set = set(Y)</span><br><span class="line">    <span class="keyword">if</span> len(X_set) &gt; <span class="number">10</span>:</span><br><span class="line">        print(<span class="string">'%s非分类变量，请检查后再输入'</span> % X_set)</span><br><span class="line">        sys.exit()</span><br><span class="line">    <span class="keyword">elif</span> len(Y_set) &gt; <span class="number">10</span>:</span><br><span class="line">        print(<span class="string">'%s非分类变量，请检查后再输入'</span> % Y_set)</span><br><span class="line">        sys.exit()</span><br><span class="line">    <span class="comment"># Mutual information</span></span><br><span class="line">    MI = <span class="number">0</span></span><br><span class="line">    eps = <span class="number">1.4e-45</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> X_set:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> Y_set:</span><br><span class="line">            indexi = np.where(X == i)</span><br><span class="line">            indexj = np.where(Y == j)</span><br><span class="line">            ijinter = np.intersect1d(indexi, indexj)</span><br><span class="line">            px = <span class="number">1.0</span> * len(indexi[<span class="number">0</span>]) / total</span><br><span class="line">            py = <span class="number">1.0</span> * len(indexj[<span class="number">0</span>]) / total</span><br><span class="line">            pxy = <span class="number">1.0</span> * len(ijinter) / total</span><br><span class="line">            MI = MI + pxy * np.log2(pxy / (px * py) + eps)</span><br><span class="line">    <span class="keyword">return</span> MI</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic_entroy</span><span class="params">(data, label, k=None)</span>:</span></span><br><span class="line">    <span class="comment"># mic_value值越小，两者相关性越弱</span></span><br><span class="line">    label = str(label)</span><br><span class="line">    <span class="comment"># k为想删除的feature个数</span></span><br><span class="line">    x = data[[x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]]</span><br><span class="line">    Y = data[label]</span><br><span class="line">    mic_value = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> len(set(x.iloc[:, i])) &lt;= <span class="number">10</span>:</span><br><span class="line">            res = MI(Y, x.iloc[:, I])</span><br><span class="line">            mic_value.append([x.columns[i], res])</span><br><span class="line">        mic_value = sorted(mic_value, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> mic_value</span><br></pre></td></tr></table></figure>
<p>本来我想偷懒，直接<code>import minepy</code>然后就得了，发现真的是特么难装，各种报错，一怒之下自己写了，这边求大佬告知，为什么<code>pip install minepy</code>会有这样的问题：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun</span><br><span class="line">error: command <span class="string">'/usr/bin/clang'</span> failed <span class="keyword">with</span> exit status <span class="number">1</span></span><br><span class="line"> ----------------------------------------</span><br><span class="line">Command <span class="string">"/Users/slade/anaconda3/bin/python -u -c "</span></span><br><span class="line"><span class="keyword">import</span> setuptools, tokenize;__file__=<span class="string">'/private/var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-build-hr9ej0lw/minepy/setup.py'</span>;</span><br><span class="line">f=getattr(tokenize, <span class="string">'open'</span>, open)(__file__);</span><br><span class="line">code=f.read().replace(<span class="string">'\r\n'</span>, <span class="string">'\n'</span>);f.close();</span><br><span class="line">exec(compile(code, __file__, <span class="string">'exec'</span>))<span class="string">" install --record /var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-30cn7rbs-record/install-record.txt --single-version-externally-managed --compile"</span> failed <span class="keyword">with</span> error code <span class="number">1</span> <span class="keyword">in</span> /private/var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-build-hr9ej0lw/minepy/</span><br></pre></td></tr></table></figure></p>
<p>回到正题，互信息其实很简单，我们看个公式I(X;Y)=H(X)-H(X|Y)，看完是不是超级清晰了，其实就是X发生的概率中去掉Y发生后X发生的概率，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。<br>计算公式如下，你们也可以在上面的代码里找到影子。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-7d2c84374876a9ba.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>最后还是吐槽下，这个minepy太难装了，为了个互信息，不至于不至于～</p>
<h3 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapper_way</span><span class="params">(data, label, k=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="comment"># k 为要保留的数据feature个数</span></span><br><span class="line">    label = str(label)</span><br><span class="line">    label_data = data[label]</span><br><span class="line">    col = [x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]</span><br><span class="line">    train_data = data[col]</span><br><span class="line">    res = pd.DataFrame(</span><br><span class="line">        RFE(estimator=LogisticRegression(), n_features_to_select=k).fit_transform(train_data, label_data))</span><br><span class="line">    res_c = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> (res.iloc[:, i] - data.iloc[:, j]).sum() == <span class="number">0</span>:</span><br><span class="line">                res_c.append(data.columns[j])</span><br><span class="line">    res.columns = res_c</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这边开始的代码就基本上是方法梳理了，没啥亮点，我就大概和大家聊聊，递归特征消除法，用R语言里面的step()函数是一毛一样的东西，都是循环sample特征，选一个对于当前模型，特征组合最好的结果。如果数据量大，你会有非一般的感觉，这边就有小trick了，以后有空可以和大家分享～</p>
<h3 id="l1-l2正则方法"><a href="#l1-l2正则方法" class="headerlink" title="l1/l2正则方法"></a>l1/l2正则方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedded_way</span><span class="params">(data, label, way=<span class="string">'l2'</span>, C_0=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    label = str(label)</span><br><span class="line">    label_data = data[label]</span><br><span class="line">    col = [x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]</span><br><span class="line">    train_data = data[col]</span><br><span class="line">    res = pd.DataFrame(SelectFromModel(LogisticRegression(penalty=way, C=C_0)).fit_transform(train_data, label_data))</span><br><span class="line">    res_c = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> (res.iloc[:, i] - data.iloc[:, j]).sum() == <span class="number">0</span>:</span><br><span class="line">                res_c.append(data.columns[j])</span><br><span class="line">    res.columns = res_c</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>正则理论参考：<a href="http://www.jianshu.com/p/4f91f0dcba95" target="_blank" rel="noopener">总结：常见算法工程师面试题目整理(二)</a>，这边要提一点，并不是所有情况下都需要正则预处理的，很多算法自带正则，比如logistic啊，比如我们自己去写tensorflow神经网络啊，模型会针对性的解决问题，而这边单纯用的logstic方法来筛选，相对而言内嵌的效果会更好的。</p>
<h3 id="基于树模型特征选择"><a href="#基于树模型特征选择" class="headerlink" title="基于树模型特征选择"></a>基于树模型特征选择</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tree_way</span><span class="params">(data,label)</span>:</span></span><br><span class="line">    label = str(label)</span><br><span class="line">    label_data = data[label]</span><br><span class="line">    col = [x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]</span><br><span class="line">    train_data = data[col]</span><br><span class="line">    res = pd.DataFrame(SelectFromModel(GradientBoostingClassifier()).fit_transform(train_data, label_data))</span><br><span class="line">    res_c = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> (res.iloc[:, i] - data.iloc[:, j]).sum() == <span class="number">0</span>:</span><br><span class="line">                res_c.append(data.columns[j])</span><br><span class="line">    res.columns = res_c</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这边用的是决策树每次分支下，如果改变一列值为随机值，观察对整体数据效果的影响。举个通俗易懂的例子，看看你在公司的重要性，就去和你老板提离职，要是老板疯狂给你加工资做你的思想工作，代表你很重要；如果你的老板让你去财务结账，代表你没啥意义。这里你就是这个feature，你老板就是数据效果的检验指标，常见的就是oob之类的。</p>
<p>这边facebook有个非常好的拓展的思路，但是大家都吹的多实际应用很少，我最近在搞这事情，等下更完这边的特征工程和下面一个nlp的case后，我想专门聊聊这个事情，用的就是决策树的另一角度，以叶子结点代替原feature，做到了非线性的特征融入线性模型，虽然很老套，但是我稍稍做了测试，效果斐然：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-2fdb0c0cdefc876f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>最后的最后，感谢大家阅读，希望能够给大家带来收获，谢谢～</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[多算法识别撞库刷券等异常用户]]></title>
      <url>/2017/12/01/%E5%A4%9A%E7%AE%97%E6%B3%95%E8%AF%86%E5%88%AB%E6%92%9E%E5%BA%93%E5%88%B7%E5%88%B8%E7%AD%89%E5%BC%82%E5%B8%B8%E7%94%A8%E6%88%B7/</url>
      <content type="html"><![CDATA[<p><img src="http://upload-images.jianshu.io/upload_images/1129359-001d854dc165f48e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>在运营业务中，绝大多数公司会面临恶意注册，恶意刷接口，恶意刷券等流量问题，此类问题的常规解决方案都是拍定单位时间内的ip访问上限次数、qps上限次数等等，会存在误伤、频繁修改阀值等问题。</p>
<h1 id="问题剖析："><a href="#问题剖析：" class="headerlink" title="问题剖析："></a><strong>问题剖析：</strong></h1><p>此类问题的关键在识别出与正常数据集群差异较大的离群点。所以，存在两个难点：</p>
<ul>
<li>1.难以找到一个很清晰的边界，界定什么是正常用户，什么是异常用户</li>
<li>2.维数灾难及交叉指标计算之间的高频计算性能瓶颈</li>
</ul>
<p><strong>算法概述：</strong></p>
<ul>
<li>1.图形位置分布</li>
<li>2.统计方法检测</li>
<li>3.距离位置检测</li>
<li>4.密度位置检测</li>
<li>5.无监督模型识别</li>
</ul>
<h1 id="算法详述："><a href="#算法详述：" class="headerlink" title="算法详述："></a><strong>算法详述：</strong></h1><h2 id="图形位置分布"><a href="#图形位置分布" class="headerlink" title="图形位置分布"></a><strong>图形位置分布</strong></h2><p>当我们不需要长期监控异常用户，只需要少数几次识别异常用户，且精度要求不高的时候，我们可以采取简单方便的图形识别方式，例如：箱式图。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-f32b17059e0e776b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>箱式图判断中，一般我们只需要锁定25%(Q1)分位点的用户特征值，75%(Q3)分位点的用户特征值，Q3与Q1之间的位差即为IQR，一般认定Q3+1.5个IQR外的点即为异常点，对应的用户即为异常用户。这种方法也叫做“盖帽法”，不必人为设定上限阀值，随着用户的数据变化而变化上界，避免了高频修改的问题，只是精度欠缺且绝大多数情况下识别出的异常用户较少。</p>
<p>方法比较简单，也不多加解释了。</p>
<h2 id="统计方法检测"><a href="#统计方法检测" class="headerlink" title="统计方法检测"></a><strong>统计方法检测</strong></h2><p>方法也比较简单，上线开发简单。一直是分两步：</p>
<ul>
<li>先假设全量数据服从一定的分布，比如常见的正太分布，泊松分布等</li>
<li>在计算每个点属于这个分布的概率，也就是大家常用的以平均值和方差定密度函数的问题</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-0d68807163ff6089.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因为这边，我们前期无法知道数据服从什么样的分布，所以，我们这边可以用切比雪夫不等式来代替确定的分布形式。除此之外，也就是同时用了马氏距离来衡量了每个具体的点在整体数据集中的位置。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-c60ac94e5cc813c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>核心代码就是下面这个协方差矩阵及矩阵相乘：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#两个维度之间协方差矩阵</span></span><br><span class="line">S=np.cov(X)</span><br><span class="line"><span class="comment">#协方差矩阵的逆矩阵</span></span><br><span class="line">SI = np.linalg.inv(S)</span><br><span class="line"><span class="comment">#第一次计算全量用户的维度重心</span></span><br><span class="line">XTmean = XT.mean(axis=<span class="number">0</span>)</span><br><span class="line">d1=[]</span><br><span class="line">n  = XT.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">	delta = XT[i] - XTmean</span><br><span class="line">	d = np.sqrt(np.dot(np.dot(delta,SI),delta.T))</span><br><span class="line">	d1.append(d)</span><br></pre></td></tr></table></figure></p>
<p>这个方法的最核心的优点就是对全量数据进行了分块，可以理解为将1拆分成了必定有问题的1/m用户，可能有问题的1/n用户，必定没问题的1/w用户（1/m+1/n+1/w=1），这也奠定了后续更好的方法的基础。<br>但是问题也是很明显的，对于1/m，1/n的大小确定无法非常的精准，多了则影响正常用户，少了则无法准确拦截，还是一个划分的算法，并不能给出每个人的好坏程度。</p>
<h2 id="距离位置检测"><a href="#距离位置检测" class="headerlink" title="距离位置检测"></a><strong>距离位置检测</strong></h2><p>距离探测的方法有一个非常强的假设，正常的用户都比较集中，有较多的邻居，而异常用户都特立独行。<br>在常见的业务问题中都是满足的，比如对爬虫ip的识别，撞库的识别，这些一看那些高频访问的就不是正常用户，但是对于特别稀疏的业务场景，比如企业融资，高深度的敏感页面访问，均不是很适用，它们的频次较低无法构成一个邻居的概念。</p>
<p>这边非常常用的有2种，一个是连续特征间的欧式距离（标准化下的欧式距离（马氏距离）），另一个是名义变量下的余弦相似度。</p>
<p>这边只讨论第一种情况，连续特征下如何衡量数据是否为异常数据。前面我们也说到了，切比雪夫不等式的方法能够有效的划分出三个类别正常用户，异常用户，未知用户。所以，相应的，我们只需要在未知用户的集群里面去寻找与正常用户更不相似的，或者和异常用户更相似的用户就可以了。</p>
<p>对于单变量衡量：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-d406583407427fcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于多变量衡量：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-26048e0fbf3736fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>核心计算相似度的方式就是以上两个公式，会有一些细节处理的问题及注意点，大家可自行研究。</p>
<h2 id="密度位置检测"><a href="#密度位置检测" class="headerlink" title="密度位置检测"></a><strong>密度位置检测</strong></h2><p>这边先等下谈原理，较为冗长，先说结论，其实，在能够使用距离位置检测的情况下，优先使用距离位置检测的方法。密度方法的前提几乎与位置方法的前提一致，但是在计算量级上而言，存在较大的差异差别。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-12075db91cef6c5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>上述的图片是Fei Tony Liu, Kai Ming Ting, Zhi-Hua Zhou的一篇论文里面对比的常见的iForest,ORCA,LOF(也就是密度位置检测),RF方法的准确率和耗时情况，也清晰的可以看出，同为距离衡量的ORCA的耗时较大，但是LOF的耗时更高，甚至部分情况下都无法计算出结果。</p>
<p>下面让我们看下理论先，密度位置检测的方法之一，LOF：<br>概念定义：<br>1) d(p,o)：两点p和o之间的距离；<br>2) k-distance：第k距离<br>　　　　对于点p的第k距离dk(p)定义如下：<br>　　　　dk(p)=d(p,o)，并且满足：<br>　　　　　　a) 在集合中至少有不包括p在内的k个点o,∈C{x≠p}， 满足d(p,o,)≤d(p,o) ；<br>　　　　　　b) 在集合中最多有不包括p在内的k−1个点o,∈C{x≠p}，满足d(p,o,)&lt;d(p,o) ；</p>
<p>上面两个条件总结起来就是1.距离范围内至少满足一定数量的点数，2.最多允许有一个距离最大的非p点<br>形象的看，距离就是p的第k距离，也就是距离p第k远的点的距离，不包括p，如下图箭头的路径长度。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-f0ec3716c8511597.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>3) k-distance neighborhood of p：第k距离邻域<br>　　　　点p的第k距离邻域Nk(p)，就是p的第k距离即以内的所有点，包括第k距离。<br>　　　　因此p的第k邻域点的个数记为 |Nk(p)|，且|Nk(p)|≥k</p>
<p>我们在定义一些衡量指标，那么LOF就算是完成了：<br>1、可达距离（reach-distance）<br>点o到点p的第k可达距离定义为：<br>reach-distancek(p,o)=max{k−distance(o),d(p,o)}</p>
<p>2、局部可达密度（local reachablility density）<br>点p处的局部可达密度为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-4b18fea49c38f840.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，|Nk(p)|为p的第k领域点的个数，∑o∈Nk(p)reach-distk(p,o)计算的是p的k领域内的点到p的可达距离，也就是1中涉及的计算方式。</p>
<p>3、局部离群因子（local outlier factor）<br>点p的局部离群因子为：<br>LOF（p） = （∑o∈Nk(p)lrdk(o)/lrdk(p)）/|Nk(p)|<br>其中，lrdk(o)/lrdk(p)比值衡量了p点与附近的点之间的密切差异情况，LOF值=1时，代表p与p附近的点密度一致；LOF值<1时，代表p点的密度大于p附近点的密度；lof值>1时，代表p点的密度小于p附近点的密度，也是非常符合我们的前提假设的，异常点总是比较稀疏，正常点总是比较稠密的。</1时，代表p点的密度大于p附近点的密度；lof值></p>
<p>到此位置LOF的数学理论就完成了，让我们回顾一下它的思想。它其实就是找数据集合中的每一个点及其邻居的点，计算它和它的邻居的密度，当它的密度大于等于它邻居的密度的时候，则认为它是稠密中心，是正常用户数据；否则异常。<br>但是要计算每个点及对应的邻居的LOF值，计算成本也是非常的高的，最初我们也指出了这一点。</p>
<p>核心代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">k_distance</span><span class="params">(k, instance, instances, distance_function=distance_euclidean)</span>:</span></span><br><span class="line">    distances = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> instance2 <span class="keyword">in</span> instances:</span><br><span class="line">        distance_value = distance_function(instance, instance2)</span><br><span class="line">        <span class="keyword">if</span> distance_value <span class="keyword">in</span> distances:</span><br><span class="line">            distances[distance_value].append(instance2)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            distances[distance_value] = [instance2]</span><br><span class="line">    distances = sorted(distances.items())</span><br><span class="line">    neighbours = []</span><br><span class="line">    k_sero = <span class="number">0</span></span><br><span class="line">    k_dist = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">for</span> dist <span class="keyword">in</span> distances:</span><br><span class="line">        k_sero += len(dist[<span class="number">1</span>])</span><br><span class="line">        neighbours.extend(dist[<span class="number">1</span>])</span><br><span class="line">        k_dist = dist[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> k_sero &gt;= k:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> k_dist, neighbours</span><br></pre></td></tr></table></figure></p>
<h2 id="无监督模型识别"><a href="#无监督模型识别" class="headerlink" title="无监督模型识别"></a><strong>无监督模型识别</strong></h2><p>其实这边说完全的无监督，我觉得不是很准确，我觉得叫“半监督”可能更好一些。<br>这边方法很多，我只介绍两种：<br>1.Iforest<br>2.RNN</p>
<p>先让我们看下Iforest：<br>算法的关键在于:对于一个有若干维的数据集合，对于其中的任一维度，如果该维度是连续属性的话，在若干次随机二分类后，边界稀疏点最容易优先达到子叶节点,如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-039151615db4b747.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>算法实现详细的过程为：<br>假设数据集有N条数据，构建一颗iTree时，从N条数据中均匀抽样(一般是无放回抽样)出m(通常为256)个样本出来，作为这颗树的训练样本。在样本中，随机选一个特征，并在这个特征的所有值范围内(最小值与最大值之间)随机选一个值，对样本进行二叉划分，将样本中小于该值的划分到节点的左边，大于等于该值的划分到节点的右边,重复以上划分步骤，直到达到划分层数上限log(m)或者节点内只有一个样本，一棵树Itree的结果往往是不可信的，所以我们可以训练100-255棵树，最后整合所以树的结果取平均的深度作为输出深度，也叫做Isolation Forest。</p>
<p>有了算法逻辑，再看衡量指标：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-f30bdc606e81ad19.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，h(x)为x对应的节点深度，c(n)为样本可信度，s(x,n)~[0,1]，正常数据来讲s(x,n)小于0.8，s(x,n)越靠近1，数据异常的可能性越大。（这边需要注意，在sklearn中的Isolation是取得相反的逻辑，score越小数据异常的可能性越大。）</p>
<p>这边也贴上核心代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None, sample_weight=None)</span>:</span></span><br><span class="line">       X = check_array(X, accept_sparse=[<span class="string">'csc'</span>], ensure_2d=<span class="keyword">False</span>)</span><br><span class="line">       <span class="keyword">if</span> issparse(X):</span><br><span class="line">           <span class="comment"># Pre-sort indices to avoid that each individual tree of the</span></span><br><span class="line">           <span class="comment"># ensemble sorts the indices.</span></span><br><span class="line">           X.sort_indices()</span><br><span class="line"></span><br><span class="line">       rnd = check_random_state(self.random_state)</span><br><span class="line">       y = rnd.uniform(size=X.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">       <span class="comment"># ensure that max_sample is in [1, n_samples]:</span></span><br><span class="line">       n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">       <span class="keyword">if</span> <span class="keyword">not</span> (self.max_samples &lt;= n_samples):</span><br><span class="line">           warn(<span class="string">"max_samples is larger than the total number of samples"</span></span><br><span class="line">                <span class="string">" n_samples. Corrected as max_samples=n_samples"</span>)</span><br><span class="line">           self.max_samples = n_samples</span><br><span class="line">       <span class="keyword">if</span> <span class="keyword">not</span> (<span class="number">0</span> &lt; self.max_samples):</span><br><span class="line">           <span class="keyword">raise</span> ValueError(<span class="string">"max_samples has to be positive"</span>)</span><br><span class="line"></span><br><span class="line">       super(IsolationForest, self).fit(X, y, sample_weight=sample_weight)</span><br><span class="line">       <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">_cost</span><span class="params">(self, n)</span>:</span></span><br><span class="line">       <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">           <span class="keyword">return</span> <span class="number">1.</span></span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           harmonic_number = np.log(n) + <span class="number">0.5772156649</span></span><br><span class="line">           <span class="keyword">return</span> <span class="number">2.</span> * harmonic_number - <span class="number">2.</span> * (n - <span class="number">1.</span>) / n</span><br></pre></td></tr></table></figure></p>
<p>2.RNN<br>通常我们会以5层的卷积神经网络作为训练网络。我在这边处理之前将切比雪夫不等式划分出来的正常用户作为0-output，异常用户作为1-output，然后尽可能的降低损失函数的误差即可。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-9cbadf3ff359e091.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>第一层是常规层，将不同的input做线性组合：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-dbe05c6368f7f71f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>第二层、第四层是做数据非线性变化：<br>这边选用的是tanh函数</p>
<p>第三层是做梯度分层下的非线性变化，抹平相似特征间的ouput：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-cc49e1a8115d8a1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>其中，k为3，N为想要分的梯度的个数，a3为一个阶梯跳跃到另一个阶梯的转换效率，形如：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-7a4d7306a3ff518d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>第五层，也就是最后一层通过sigmoid进行0-1之间的压缩。</p>
<p>这边的损失函数用的是常见的mse：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-77ca366109e16fc1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>当通过测试数据训练完成后，再将未知数据进行模型训练，观察得到结果的大小，越靠近1，越有可能为异常用户。</p>
<hr>
<p>以上就是5种常见的只基于数据下的异常用户的识别，更偏方法技术一点，但是无论是算法实现还是业务应用中，同样需要注意输入特征的问题。由于大家运用方向不同，就不细节赘述。</p>
<p>详细实现demo可以私信我要，最后谢谢大家阅读了。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 风控 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python开发：特征工程代码模版(一)]]></title>
      <url>/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%B8%80/</url>
      <content type="html"><![CDATA[<p>作为一个算法工程师，我们接的业务需求不会比数据分析挖掘工程师少，作为一个爱偷懒的人，总机械重复的完成一样的预处理工作，我是不能忍的，所以在最近几天，我正在完善一些常规的、通用的预处理的code，方便我们以后在每次分析之前直接import快速搞定，省的每次都要去做一样的事情。</p>
<p><strong>如果大家有什么想实现但是懒得去弄的预处理的步骤也可以私信我，我相对而言闲暇还是有的（毕竟工资少工作也不多，摊手：《），我开发完成后直接贴出来，大家以后一起用就行了</strong></p>
<p>我们需要预加载这些包，而且接下来所有的操作均在dataframe格式下完成，所以我们需要将数据先处理成dataframe格式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'slade_sal'</span></span><br><span class="line">__time__ = <span class="string">'20171128'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_data_format</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># 以下预处理都是基于dataframe格式进行的</span></span><br><span class="line">    data_new = pd.DataFrame(data)</span><br><span class="line">    <span class="keyword">return</span> data_new</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="空值处理"><a href="#空值处理" class="headerlink" title="空值处理"></a>空值处理</h1><p>接下来就开始我们的正题了，首先，我们需要判断哪些列是空值过多的，当一列数据的空值占列数的40%以上（经验值），这列能够带给我们的信息就不多了，所以我们需要把某个阀值（rate_base）以上的空值个数的列干掉，如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 去除空值过多的feature</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_remove</span><span class="params">(data, rate_base=<span class="number">0.4</span>)</span>:</span></span><br><span class="line">    all_cnt = data.shape[<span class="number">0</span>]</span><br><span class="line">    avaiable_index = []</span><br><span class="line">    <span class="comment"># 针对每一列feature统计nan的个数，个数大于全量样本的rate_base的认为是异常feature，进行剔除</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line">        rate = np.isnan(np.array(data.iloc[:, i])).sum() / all_cnt</span><br><span class="line">        <span class="keyword">if</span> rate &lt;= rate_base:</span><br><span class="line">            avaiable_index.append(i)</span><br><span class="line">    data_available = data.iloc[:, avaiable_index]</span><br><span class="line">    <span class="keyword">return</span> data_available, avaiable_index</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="离群点处理"><a href="#离群点处理" class="headerlink" title="离群点处理"></a>离群点处理</h1><p>把空值过多的列去完之后，我们需要考虑将一些特别离群的点去掉，这边需要注意两点：</p>
<ul>
<li>异常值分析类的场景禁止使用这步，比如信用卡评分，爬虫识别等，你如果采取了这步，还怎么去分离出这些异常啊</li>
<li>容忍度高的算法不建议使用这步，比如svm里面已经有了支持向量机这个东西，你如果采取了这步的离群识别的操作会改变原分布而且svm里面决定超平面的核心与离群点无关，后接函数会引发意想不到的彩蛋～</li>
</ul>
<p>这边采取盖帽法与额定的分位点方法，建议组合使用，用changed_feature_box定义需要采用盖帽法的列的index_num，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 离群点盖帽</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outlier_remove</span><span class="params">(data, limit_value=<span class="number">10</span>, method=<span class="string">'box'</span>, percentile_limit_set=<span class="number">90</span>, changed_feature_box=[])</span>:</span></span><br><span class="line">    <span class="comment"># limit_value是最小处理样本个数set，当独立样本大于limit_value我们认为非可onehot字段</span></span><br><span class="line">    feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">    feature_change = []</span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">'box'</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">            <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">                q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">                q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line">                <span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">                top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">                data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">                feature_change.append(i)</span><br><span class="line">        <span class="keyword">return</span> data, feature_change</span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">'self_def'</span>:</span><br><span class="line">        <span class="comment"># 快速截断</span></span><br><span class="line">        <span class="keyword">if</span> len(changed_feature_box) == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 当方法选择为自定义，且没有定义changed_feature_box则全量数据全部按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">                <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">                    q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">                    data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">                    feature_change.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果定义了changed_feature_box，则将changed_feature_box里面的按照box方法，changed_feature_box的feature index按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">                <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">                    <span class="keyword">if</span> i <span class="keyword">in</span> changed_feature_box:</span><br><span class="line">                        q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">                        q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line">                        <span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">                        top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">                        data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">                        feature_change.append(i)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">                        data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">                        feature_change.append(i)</span><br><span class="line">            <span class="keyword">return</span> data, feature_change</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="空值填充"><a href="#空值填充" class="headerlink" title="空值填充"></a>空值填充</h1><p>在此之后，我们需要对空值进行填充，这边方法就很多很多了，我这边实现的是基本的，分了连续feature和分类feature，分别针对continuous feature采取mean,min,max方式，class feature采取one_hot_encoding的方式；除此之外还可以做分层填充，差分填充等等，那个比较定制化，如果有需要，我也可以搞一套，但是个人觉得意义不大。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 空feature填充</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_fill</span><span class="params">(data, limit_value=<span class="number">10</span>, countinuous_dealed_method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">    feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">    normal_index = []</span><br><span class="line">    continuous_feature_index = []</span><br><span class="line">    class_feature_index = []</span><br><span class="line">    continuous_feature_df = pd.DataFrame()</span><br><span class="line">    class_feature_df = pd.DataFrame()</span><br><span class="line">    <span class="comment"># 当存在空值且每个feature下独立的样本数小于limit_value，我们认为是class feature采取one_hot_encoding；</span></span><br><span class="line">    <span class="comment"># 当存在空值且每个feature下独立的样本数大于limit_value，我们认为是continuous feature采取mean,min,max方式</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">        <span class="keyword">if</span> np.isnan(np.array(data.iloc[:, i])).sum() &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">                <span class="keyword">if</span> countinuous_dealed_method == <span class="string">'mean'</span>:</span><br><span class="line">                    continuous_feature_df = pd.concat(</span><br><span class="line">                        [continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].mean())], axis=<span class="number">1</span>)</span><br><span class="line">                    continuous_feature_index.append(i)</span><br><span class="line">                <span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'max'</span>:</span><br><span class="line">                    continuous_feature_df = pd.concat(</span><br><span class="line">                        [continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].max())], axis=<span class="number">1</span>)</span><br><span class="line">                    continuous_feature_index.append(i)</span><br><span class="line">                <span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'min'</span>:</span><br><span class="line">                    continuous_feature_df = pd.concat(</span><br><span class="line">                        [continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].min())], axis=<span class="number">1</span>)</span><br><span class="line">                    continuous_feature_index.append(i)</span><br><span class="line">            <span class="keyword">elif</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt; <span class="number">0</span> <span class="keyword">and</span> len(</span><br><span class="line">                    pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">                class_feature_df = pd.concat(</span><br><span class="line">                    [class_feature_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line">                class_feature_index.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            normal_index.append(i)</span><br><span class="line">    data_update = pd.concat([data.iloc[:, normal_index], continuous_feature_df, class_feature_df], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data_update</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="one-hot-encoding过程"><a href="#one-hot-encoding过程" class="headerlink" title="one hot encoding过程"></a>one hot encoding过程</h1><p>分类feature的one hot encoding过程，常见操作，不多说<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># onehotencoding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ohe</span><span class="params">(data, limit_value=<span class="number">10</span>)</span>:</span></span><br><span class="line">    feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">    class_index = []</span><br><span class="line">    class_df = pd.DataFrame()</span><br><span class="line">    normal_index = []</span><br><span class="line">    <span class="comment"># limit_value以下的均认为是class feature，进行ohe过程</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">        <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">            class_index.append(i)</span><br><span class="line">            class_df = pd.concat([class_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            normal_index.append(i)</span><br><span class="line">    data_update = pd.concat([data.iloc[:, normal_index], class_df], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data_update</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="Smote过程"><a href="#Smote过程" class="headerlink" title="Smote过程"></a>Smote过程</h1><p>正负样本不平衡的解决，这边我写的是smote，理论部分建议参考：<a href="http://www.jianshu.com/p/ecbc924860af" target="_blank" rel="noopener">Python：SMOTE算法</a>,其实简单的欠抽样和过抽样就可以解决，建议参考这边文章：<a href="http://www.jianshu.com/p/9a3b3104776e" target="_blank" rel="noopener">Python:数据抽样平衡方法重写</a>。都是一些老生常谈的问题了，不多说了，上代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smote</span><span class="params">(data, tag_label=<span class="string">'tag_1'</span>, amount_personal=<span class="number">0</span>, std_rate=<span class="number">5</span>, k=<span class="number">5</span>,method = <span class="string">'mean'</span>)</span>:</span></span><br><span class="line">    cnt = data[tag_label].groupby(data[tag_label]).count()</span><br><span class="line">    rate = max(cnt) / min(cnt)</span><br><span class="line">    location = []</span><br><span class="line">    <span class="keyword">if</span> rate &lt; <span class="number">5</span>:</span><br><span class="line">        print(<span class="string">'不需要smote过程'</span>)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 拆分不同大小的数据集合</span></span><br><span class="line">        less_data = np.array(data[data[tag_label] == np.array(cnt[cnt == min(cnt)].index)[<span class="number">0</span>]])</span><br><span class="line">        more_data = np.array(data[data[tag_label] == np.array(cnt[cnt == max(cnt)].index)[<span class="number">0</span>]])</span><br><span class="line">        <span class="comment"># 找出每个少量数据中每条数据k个邻居</span></span><br><span class="line">        neighbors = NearestNeighbors(n_neighbors=k).fit(less_data)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(less_data)):</span><br><span class="line">            point = less_data[i, :]</span><br><span class="line">            location_set = neighbors.kneighbors([less_data[i]], return_distance=<span class="keyword">False</span>)[<span class="number">0</span>]</span><br><span class="line">            location.append(location_set)</span><br><span class="line">        <span class="comment"># 确定需要将少量数据补充到上限额度</span></span><br><span class="line">        <span class="comment"># 判断有没有设定生成数据个数，如果没有按照std_rate(预期正负样本比)比例生成</span></span><br><span class="line">        <span class="keyword">if</span> amount_personal &gt; <span class="number">0</span>:</span><br><span class="line">            amount = amount_personal</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            amount = int(max(cnt) / std_rate)</span><br><span class="line">        <span class="comment"># 初始化，判断连续还是分类变量采取不同的生成逻辑</span></span><br><span class="line">        times = <span class="number">0</span></span><br><span class="line">        continue_index = []  <span class="comment"># 连续变量</span></span><br><span class="line">        class_index = []  <span class="comment"># 分类变量</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(less_data.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; <span class="number">10</span>:</span><br><span class="line">                continue_index.append(i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                class_index.append(i)</span><br><span class="line">        case_update = pd.DataFrame()</span><br><span class="line">        <span class="keyword">while</span> times &lt; amount:</span><br><span class="line">            <span class="comment"># 连续变量取附近k个点的重心，认为少数样本的附近也是少数样本</span></span><br><span class="line">            new_case = []</span><br><span class="line">            pool = np.random.permutation(len(location))[<span class="number">0</span>]</span><br><span class="line">            neighbor_group = less_data[location[pool], :]</span><br><span class="line">            <span class="keyword">if</span> method == <span class="string">'mean'</span>:</span><br><span class="line">                new_case1 = neighbor_group[:, continue_index].mean(axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 连续样本的附近点向量上的点也是异常点</span></span><br><span class="line">            <span class="keyword">if</span> method ==<span class="string">'random'</span>:</span><br><span class="line">                new_case1 =less_data[pool][continue_index]  + np.random.rand()*(less_data[pool][continue_index]-neighbor_group[<span class="number">0</span>][continue_index])</span><br><span class="line">            <span class="comment"># 分类变量取mode</span></span><br><span class="line">            new_case2 = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> class_index:</span><br><span class="line">                L = pd.DataFrame(neighbor_group[:, i])</span><br><span class="line">                new_case2.append(np.array(L.mode()[<span class="number">0</span>])[<span class="number">0</span>])</span><br><span class="line">            new_case.extend(new_case1)</span><br><span class="line">            new_case.extend(new_case2)</span><br><span class="line">            case_update = pd.concat([case_update, pd.DataFrame(new_case)], axis=<span class="number">1</span>)</span><br><span class="line">            print(<span class="string">'已经生成了%s条新数据，完成百分之%.2f'</span> % (times, times * <span class="number">100</span> / amount))</span><br><span class="line">            times = times + <span class="number">1</span></span><br><span class="line">        data_res = np.vstack((more_data, np.array(case_update.T)))</span><br><span class="line">        data_res = pd.DataFrame(data_res)</span><br><span class="line">        data_res.columns = data.columns</span><br><span class="line">    <span class="keyword">return</span> data_res</span><br></pre></td></tr></table></figure></p>
<h1 id="总体整合"><a href="#总体整合" class="headerlink" title="总体整合"></a>总体整合</h1><p>一期的内容就这样吧，我感觉也没有啥好说的，都是数据分析挖掘的一些基本操作，我只是为了以后能够复用模版化了，下面贴一个全量我做预处理的过程，没啥差异，整合了一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'slade_sal'</span></span><br><span class="line">__time__ = <span class="string">'20171128'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_data_format</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># 以下预处理都是基于dataframe格式进行的</span></span><br><span class="line">    data_new = pd.DataFrame(data)</span><br><span class="line">    <span class="keyword">return</span> data_new</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除空值过多的feature</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_remove</span><span class="params">(data, rate_base=<span class="number">0.4</span>)</span>:</span></span><br><span class="line">    all_cnt = data.shape[<span class="number">0</span>]</span><br><span class="line">    avaiable_index = []</span><br><span class="line">    <span class="comment"># 针对每一列feature统计nan的个数，个数大于全量样本的rate_base的认为是异常feature，进行剔除</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line">        rate = np.isnan(np.array(data.iloc[:, i])).sum() / all_cnt</span><br><span class="line">        <span class="keyword">if</span> rate &lt;= rate_base:</span><br><span class="line">            avaiable_index.append(i)</span><br><span class="line">    data_available = data.iloc[:, avaiable_index]</span><br><span class="line">    <span class="keyword">return</span> data_available, avaiable_index</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 离群点盖帽</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outlier_remove</span><span class="params">(data, limit_value=<span class="number">10</span>, method=<span class="string">'box'</span>, percentile_limit_set=<span class="number">90</span>, changed_feature_box=[])</span>:</span></span><br><span class="line">    <span class="comment"># limit_value是最小处理样本个数set，当独立样本大于limit_value我们认为非可onehot字段</span></span><br><span class="line">    feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">    feature_change = []</span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">'box'</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">            <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">                q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">                q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line">                <span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">                top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">                data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">                feature_change.append(i)</span><br><span class="line">        <span class="keyword">return</span> data, feature_change</span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">'self_def'</span>:</span><br><span class="line">        <span class="comment"># 快速截断</span></span><br><span class="line">        <span class="keyword">if</span> len(changed_feature_box) == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 当方法选择为自定义，且没有定义changed_feature_box则全量数据全部按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">                <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">                    q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">                    data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">                    feature_change.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果定义了changed_feature_box，则将changed_feature_box里面的按照box方法，changed_feature_box的feature index按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">                <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">                    <span class="keyword">if</span> i <span class="keyword">in</span> changed_feature_box:</span><br><span class="line">                        q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">                        q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line">                        <span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">                        top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">                        data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">                        feature_change.append(i)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">                        data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">                        feature_change.append(i)</span><br><span class="line">            <span class="keyword">return</span> data, feature_change</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 空feature填充</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_fill</span><span class="params">(data, limit_value=<span class="number">10</span>, countinuous_dealed_method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">    feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">    normal_index = []</span><br><span class="line">    continuous_feature_index = []</span><br><span class="line">    class_feature_index = []</span><br><span class="line">    continuous_feature_df = pd.DataFrame()</span><br><span class="line">    class_feature_df = pd.DataFrame()</span><br><span class="line">    <span class="comment"># 当存在空值且每个feature下独立的样本数小于limit_value，我们认为是class feature采取one_hot_encoding；</span></span><br><span class="line">    <span class="comment"># 当存在空值且每个feature下独立的样本数大于limit_value，我们认为是continuous feature采取mean,min,max方式</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">        <span class="keyword">if</span> np.isnan(np.array(data.iloc[:, i])).sum() &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">                <span class="keyword">if</span> countinuous_dealed_method == <span class="string">'mean'</span>:</span><br><span class="line">                    continuous_feature_df = pd.concat(</span><br><span class="line">                        [continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].mean())], axis=<span class="number">1</span>)</span><br><span class="line">                    continuous_feature_index.append(i)</span><br><span class="line">                <span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'max'</span>:</span><br><span class="line">                    continuous_feature_df = pd.concat(</span><br><span class="line">                        [continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].max())], axis=<span class="number">1</span>)</span><br><span class="line">                    continuous_feature_index.append(i)</span><br><span class="line">                <span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'min'</span>:</span><br><span class="line">                    continuous_feature_df = pd.concat(</span><br><span class="line">                        [continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].min())], axis=<span class="number">1</span>)</span><br><span class="line">                    continuous_feature_index.append(i)</span><br><span class="line">            <span class="keyword">elif</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt; <span class="number">0</span> <span class="keyword">and</span> len(</span><br><span class="line">                    pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">                class_feature_df = pd.concat(</span><br><span class="line">                    [class_feature_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line">                class_feature_index.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            normal_index.append(i)</span><br><span class="line">    data_update = pd.concat([data.iloc[:, normal_index], continuous_feature_df, class_feature_df], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data_update</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># onehotencoding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ohe</span><span class="params">(data, limit_value=<span class="number">10</span>)</span>:</span></span><br><span class="line">    feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">    class_index = []</span><br><span class="line">    class_df = pd.DataFrame()</span><br><span class="line">    normal_index = []</span><br><span class="line">    <span class="comment"># limit_value以下的均认为是class feature，进行ohe过程</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line">        <span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">            class_index.append(i)</span><br><span class="line">            class_df = pd.concat([class_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            normal_index.append(i)</span><br><span class="line">    data_update = pd.concat([data.iloc[:, normal_index], class_df], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data_update</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smote</span><span class="params">(data, tag_label=<span class="string">'tag_1'</span>, amount_personal=<span class="number">0</span>, std_rate=<span class="number">5</span>, k=<span class="number">5</span>,method = <span class="string">'mean'</span>)</span>:</span></span><br><span class="line">    cnt = data[tag_label].groupby(data[tag_label]).count()</span><br><span class="line">    rate = max(cnt) / min(cnt)</span><br><span class="line">    location = []</span><br><span class="line">    <span class="keyword">if</span> rate &lt; <span class="number">5</span>:</span><br><span class="line">        print(<span class="string">'不需要smote过程'</span>)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 拆分不同大小的数据集合</span></span><br><span class="line">        less_data = np.array(data[data[tag_label] == np.array(cnt[cnt == min(cnt)].index)[<span class="number">0</span>]])</span><br><span class="line">        more_data = np.array(data[data[tag_label] == np.array(cnt[cnt == max(cnt)].index)[<span class="number">0</span>]])</span><br><span class="line">        <span class="comment"># 找出每个少量数据中每条数据k个邻居</span></span><br><span class="line">        neighbors = NearestNeighbors(n_neighbors=k).fit(less_data)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(less_data)):</span><br><span class="line">            point = less_data[i, :]</span><br><span class="line">            location_set = neighbors.kneighbors([less_data[i]], return_distance=<span class="keyword">False</span>)[<span class="number">0</span>]</span><br><span class="line">            location.append(location_set)</span><br><span class="line">        <span class="comment"># 确定需要将少量数据补充到上限额度</span></span><br><span class="line">        <span class="comment"># 判断有没有设定生成数据个数，如果没有按照std_rate(预期正负样本比)比例生成</span></span><br><span class="line">        <span class="keyword">if</span> amount_personal &gt; <span class="number">0</span>:</span><br><span class="line">            amount = amount_personal</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            amount = int(max(cnt) / std_rate)</span><br><span class="line">        <span class="comment"># 初始化，判断连续还是分类变量采取不同的生成逻辑</span></span><br><span class="line">        times = <span class="number">0</span></span><br><span class="line">        continue_index = []  <span class="comment"># 连续变量</span></span><br><span class="line">        class_index = []  <span class="comment"># 分类变量</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(less_data.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; <span class="number">10</span>:</span><br><span class="line">                continue_index.append(i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                class_index.append(i)</span><br><span class="line">        case_update = pd.DataFrame()</span><br><span class="line">        <span class="keyword">while</span> times &lt; amount:</span><br><span class="line">            <span class="comment"># 连续变量取附近k个点的重心，认为少数样本的附近也是少数样本</span></span><br><span class="line">            new_case = []</span><br><span class="line">            pool = np.random.permutation(len(location))[<span class="number">0</span>]</span><br><span class="line">            neighbor_group = less_data[location[pool], :]</span><br><span class="line">            <span class="keyword">if</span> method == <span class="string">'mean'</span>:</span><br><span class="line">                new_case1 = neighbor_group[:, continue_index].mean(axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 连续样本的附近点向量上的点也是异常点</span></span><br><span class="line">            <span class="keyword">if</span> method ==<span class="string">'random'</span>:</span><br><span class="line">                new_case1 =less_data[pool][continue_index]  + np.random.rand()*(less_data[pool][continue_index]-neighbor_group[<span class="number">0</span>][continue_index])</span><br><span class="line">            <span class="comment"># 分类变量取mode</span></span><br><span class="line">            new_case2 = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> class_index:</span><br><span class="line">                L = pd.DataFrame(neighbor_group[:, i])</span><br><span class="line">                new_case2.append(np.array(L.mode()[<span class="number">0</span>])[<span class="number">0</span>])</span><br><span class="line">            new_case.extend(new_case1)</span><br><span class="line">            new_case.extend(new_case2)</span><br><span class="line">            case_update = pd.concat([case_update, pd.DataFrame(new_case)], axis=<span class="number">1</span>)</span><br><span class="line">            print(<span class="string">'已经生成了%s条新数据，完成百分之%.2f'</span> % (times, times * <span class="number">100</span> / amount))</span><br><span class="line">            times = times + <span class="number">1</span></span><br><span class="line">        data_res = np.vstack((more_data, np.array(case_update.T)))</span><br><span class="line">        data_res = pd.DataFrame(data_res)</span><br><span class="line">        data_res.columns = data.columns</span><br><span class="line">    <span class="keyword">return</span> data_res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reload</span><span class="params">(data)</span>:</span></span><br><span class="line">    feature = pd.concat([data.iloc[:, :<span class="number">2</span>], data.iloc[:, <span class="number">4</span>:]], axis=<span class="number">1</span>)</span><br><span class="line">    tag = data.iloc[:, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> feature, tag</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据切割</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_data</span><span class="params">(feature, tag)</span>:</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(feature, tag, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    path = sys.argv[<span class="number">0</span>]</span><br><span class="line">    data_all = pd.read_table(str(path))</span><br><span class="line">    print(<span class="string">'数据读取完成！'</span>)</span><br><span class="line">    <span class="comment"># 更改数据格式</span></span><br><span class="line">    data_all = change_data_format(data_all)</span><br><span class="line">    <span class="comment"># 删除电话号码列</span></span><br><span class="line">    data_all = data_all.iloc[:, <span class="number">1</span>:]</span><br><span class="line">    data_all, data_avaiable_index = nan_remove(data_all)</span><br><span class="line">    print(<span class="string">'空值列处理完毕！'</span>)</span><br><span class="line">    data_all, _ = outlier_remove(data_all)</span><br><span class="line">    print(<span class="string">'异常点处理完成！'</span>)</span><br><span class="line">    data_all = nan_fill(data_all)</span><br><span class="line">    print(<span class="string">'空值填充完成！'</span>)</span><br><span class="line">    data_all = ohe(data_all)</span><br><span class="line">    print(<span class="string">'onehotencoding 完成！'</span>)</span><br><span class="line">    data_all = smote(data_all)</span><br><span class="line">    print(<span class="string">'smote过程完成！'</span>)</span><br><span class="line">    feature, tag = reload(data_all)</span><br><span class="line">    X_train, X_test, y_train, y_test = split_data(feature, tag)</span><br><span class="line">    print(<span class="string">'数据预处理完成！'</span>)</span><br></pre></td></tr></table></figure></p>
<p>大家自取自用，这个也没啥好转载的，没啥干货，只是方便大家日常工作，就别转了，谢谢各位编辑大哥了。</p>
<p>最后，感谢大家阅读，谢谢。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[常见算法工程师面试题目整理(二)]]></title>
      <url>/2017/12/01/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E6%95%B4%E7%90%86-%E4%BA%8C/</url>
      <content type="html"><![CDATA[<p>接着上回写的《<a href="http://www.jianshu.com/p/c3c921dca07b" target="_blank" rel="noopener">总结：常见算法工程师面试题目整理(1)</a>》,继续填接下来的坑。</p>
<h1 id="boost算法的思路是什么样的？讲一下你对adaboost-和-gbdt的了解？"><a href="#boost算法的思路是什么样的？讲一下你对adaboost-和-gbdt的了解？" class="headerlink" title="boost算法的思路是什么样的？讲一下你对adaboost 和 gbdt的了解？"></a>boost算法的思路是什么样的？讲一下你对adaboost 和 gbdt的了解？</h1><p>答：<br>boost的核心思想不同于bagging，<strong>它在基于样本预测结果对照与真实值得差距，进行修正，再预测再修正，逐步靠近正确值。</strong></p>
<p>我对adaboost和gbdt了解的也不算很全面：大概的梳理如下：<br>不足：<br>1.adaboost存在异常点敏感的问题<br>2.gbdt一定程度上优化了adaboost异常点敏感的问题，但是存在难以并行的缺点<br>3.两者的目标都是优化bias，必然导致训练出来的数据var的不稳定</p>
<p>亮点：<br>1.发现非线性的特征关系，网格化的切分feature<br>2.拟合的效果相较于其他分类器更加精准，且训练参数较少</p>
<h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost:"></a>Adaboost:</h2><p><img src="http://upload-images.jianshu.io/upload_images/1129359-101787a9248e8b84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>adaboost初始数据权重都是1/M，然后通过训练每一个弱分类器Classifier使得在每一次y_pred误差最小，得到每一个弱Classifier的权重方法对：（αi，yi）然后提高错分了的数据的权重，降低正确分类的数据权重，循环训练，最后组合最后若干次的训练弱Classifier对，得到强分类器。<br>其中，αi由误差决定：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-db72ac347e04a8ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>该弱分类器分类错误率em越大，则该若分类器作用越小。<br><strong>1.剖析了原理之后，我们发现，这样做对异常点非常敏感，异常点非常容易错分，会影响后续若干个弱分类器</strong></p>
<h2 id="gbdt"><a href="#gbdt" class="headerlink" title="gbdt:"></a>gbdt:</h2><p>gbdt的核心在于下面这个公式：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-f0506e7f16596d85.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>L（y，y_pred）：预测值与实际值间的误差<br>F(x):前若干个弱分类器的组合<br>关键的在于当前预测结果=对前若干个弱分类器+当前弱分类器修正，所以对前若干个分类器组合求偏导的方向进行梯度处理，保证L（x）出来的值最小。<br>这边结果在于你选取什么样的误差函数：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-f8b8d6a6a931db42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>Loss即为损失函数，Derivative即为导数<br>除此之外，在每一步弱分类器构建的同时，它还考虑了正则化：<br><strong>Ω=入T+μ<code>*</code>linalg.norm(f(xi))</strong><br>T为子叶节点数目，同时也对预测结果得分的值进行了l1或者l2压缩，避免过拟合。</p>
<p>我个人更喜欢用xgboost，在求解速度上，对异常值处理上面都要比gbdt要快，而且基于R、python版本都有package。</p>
<h1 id="听说你做过用户关系，你用的什么方法？社群算法有了解，讲讲什么叫做Modularity-Q？"><a href="#听说你做过用户关系，你用的什么方法？社群算法有了解，讲讲什么叫做Modularity-Q？" class="headerlink" title="听说你做过用户关系，你用的什么方法？社群算法有了解，讲讲什么叫做Modularity Q？"></a>听说你做过用户关系，你用的什么方法？社群算法有了解，讲讲什么叫做Modularity Q？</h1><p>1.我用的是Jaccard相关。<br>比如，用户1一共收过150个红包，发了100个红包，其中20个被用户2抢过<br>用户2一共收过100个红包，发了50个红包，其中30个被用户1抢过<br>similarity(user1=&gt;user2)=(30+20)/(150+100)<br>similarity(user2=&gt;user1)=(30+20)/(50+100)<br>similarity(user2=&gt;user1)=(30+20+30+20)/(150+100+50+100)</p>
<p>2.社区算法主要是用来衡量用户关系网中，不同用户、链接、信息之间的相似程度。<br>本来这边我准备讲pagerank的，结果被打断了，说需要讲内部结构相关的，其实我觉得PageRank这边来描述更加合适。不过，无所谓，我这边谈的是一个很基本的叫做：Kernighan-Lin算法（后面简称了KL算法）<br>KL算法中，先随机切分原数据集群，得到不同社区集，随机交换不同社区集内的不同点，观察优化值得变化程度是否为正向，循环即可。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-bde455f1e1c70512.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>共需执行次数：循环次数x集群A内点的个数x集群B内点的个数</p>
<p>感觉这边答的不行，被嫌弃了，有知道的大神可以自行去研究一下相关的社区算法，我这边只了解PageRank和LK。</p>
<p>3.Q-modularity：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-1d7ee9f81d493838.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这个简单，E：关系点连接线之间的个数，I：关系点连接线两端都在社群内的数量，O：关系点连接线有至少一端在社群外的连接线的数量</p>
<p>这个指标是用来衡量社群划分的稳定性的，讲真我也没用过，只是在周志华的算法的书上看过。</p>
<h1 id="如果让你设计一套推荐算法，请说出你的思路？"><a href="#如果让你设计一套推荐算法，请说出你的思路？" class="headerlink" title="如果让你设计一套推荐算法，请说出你的思路？"></a>如果让你设计一套推荐算法，请说出你的思路？</h1><p><img src="http://upload-images.jianshu.io/upload_images/1129359-e76e3b1badb3efac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>讲真，这个点，我起码说了有25分种，对面的面试管也很耐心的听完了，并且还给予了很多点的反馈，个人觉得非常受到尊重，我下面细节梳理一下。<br>首先，我个人非常赞同阿里现在的推荐算法这边的设计思路：<br><strong>推荐＝人＋场景＋物</strong><br>其中，<br><strong>人＝新用户＋老用户＋综合特征＋…</strong><br><strong>场景＝属性偏好＋周期属性＋黏度偏好＋…</strong><br><strong>物＝相关性＋物品价值＋特殊属性＋…</strong><br>接下来，我简单的剖析三个最常见也最重要的问题：</p>
<ul>
<li>冷启动<br>很多人有一种错觉，只要业务上线时间长了就不存在所谓的冷启动问题，实则不是，新用户是持续进入的、流失用户也是在增长的、很多盲目用户（没有有价值行为）等等都可以归纳为冷启动问题，这类问题的核心在于你可用的数据很少，甚至没有，我这边采取的是热门推荐的方法。<br>然而在热门推荐的算法中，我这边推荐一些方法：<br><strong>威尔逊区间法</strong>：综合考虑总的行为用户中，支持率与支持总数的平衡<br><strong>hacker new排序</strong>：综合考虑时间对支持率的影响<br><strong>pagerank排序</strong>：考虑用户流向下的页面权重排序<br><strong>梯度效率排序</strong>：考虑商品增速下的支持率的影响<br>…<br>方法很多，但是核心的一点是热门推荐是冷启动及实时推荐必不可少的一环，优化好实时推荐的算法是占到一个好的推荐算法的30%以上的权重的，<strong>切忌0推荐</strong>。</li>
</ul>
<ul>
<li><p>不同种算法产生的推荐内容互不冲突<br><img src="http://upload-images.jianshu.io/upload_images/1129359-0a59be6dc6d80e47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这个是苏宁易购的首页推荐位，1、2、3分别是三个推荐位，我们在做算法的时候常常会特别注意，不能用太多相关性比较高的变量，会产生共线性，但在推荐内容上，“58同城”的算法推荐团队之前有一份研究证明，同一个页面上由不同算法产出的推荐结果不存在相互影响。<br>所以，我非常赞同不同的算法产出不同的结果同时展示，因为我们不知道对目标用户是概率模型、距离模型、线性模型等不同模型中哪个产出的结果更加合适。<br>关于常用的推荐算法，我之前梳理过，这边也不再多加重复，需要仔细研究的可看我上面的图，或者看我之前的文章：《<a href="http://www.jianshu.com/p/00a28141521f" target="_blank" rel="noopener">深度学习下的电商商品推荐</a>》、《<a href="http://www.jianshu.com/p/a3e633e396a0" target="_blank" rel="noopener">偏RSVD及扩展的矩阵分解方法</a>》等等</p>
</li>
<li><p>你的对象是用户，不是冰冷的数字<br>我在苏宁呆的时间不长，但是我有个感觉，身边算法工程师很容易把自己陷入数字陷阱，近乎疯狂去用各种算法去拟合当前的用户数据，以求得得到高的ctr或者转化率。<br>不同的推荐场景需要使用不同的用户行为。举例假设存在经典的关系：买了炸鸡和番茄酱的用户，接下来的一周有35%的用户会来买汽水。所以，很多工程师会选择只要买了炸鸡和番茄酱的用户，就弹窗汽水，因为就35%的百分比而言，是非常高的支持度了。其实只要有用户画像的支持就会发现，这35%的用户中，80%的都是年龄在青少年，如果在推送之前做一个简单的逻辑判断只针对所有青少年进行推送汽水的话，35%轻而易举的上升到了70%，这是任何算都无法比拟的。</p>
</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-9fc5d1bcf3320e27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>最上方的橙黄色的横条中，橙色代表原始的目标用户，黄色代表非目标用户，假设我们知道黑色方框所框选的用户的转化率达到最小置信度的时候，我们可以通过特征映射、非线性分解、用户画像刻画等不同方法得到左右完全不同的新的用户分布，在同样的用户框选占比下，效果也是完全不同的。<br>真实推荐中，比如针对用户冬装推荐，我不仅仅以用户近期的搜索、浏览、购买商品等行为判断用户的偏好，我也根据他夏天的购买风格款式、他的年龄、生理性别、浏览性别等综合判断他可能会买什么。推荐算法才不会是冷漠的。</p>
<p>至于想要了解具体实现算法及创新的一些想法可以看上方的脑图，但是我觉得那并不是最重要。</p>
<h1 id="什么是P、NP、NP-Hard、NP-Complete问题？"><a href="#什么是P、NP、NP-Hard、NP-Complete问题？" class="headerlink" title="什么是P、NP、NP-Hard、NP-Complete问题？"></a>什么是P、NP、NP-Hard、NP-Complete问题？</h1><p>P：很快可以得出解的问题<br>NP：一定有解，但可很快速验证答案的问题</p>
<p>后面两个我没答出来，网上搜了下，分享下：<br>NP-Hard：比所有的NP问题都难的问题<br>NP-Complete：满足两点：</p>
<ol>
<li>是NP-Hard的问题</li>
<li>是NP问题</li>
</ol>
<p>个人不喜欢这种问题。</p>
<h1 id="常见的防止过拟合的方法是什么？为什么l1、l2正则会防止过拟合？"><a href="#常见的防止过拟合的方法是什么？为什么l1、l2正则会防止过拟合？" class="headerlink" title="常见的防止过拟合的方法是什么？为什么l1、l2正则会防止过拟合？"></a>常见的防止过拟合的方法是什么？为什么l1、l2正则会防止过拟合？</h1><p>当被问了第一个问题的时候，我愣了下，因为我觉得挺简单的，为什么要问这个，我感觉接下来有坑。<br>我回答的是：<br>先甩出了下面的图解释了一波欠拟合、正常、过拟合：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-1643201efb9b1212.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>然后举了几个例子：</p>
<ul>
<li>针对error递归的问题，l1，l2正则化</li>
<li>扩充数据量，使得数据更加符合真实分布</li>
<li>bagging等算法技巧</li>
</ul>
<p>当问到为什么的时候，我觉得自己回答的不好，有点蛋疼：<br>我说的是，l1以：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-f963d01cf9b23217.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>l2以：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-3aaae3d52f6fbf68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>l1中函数与约束点偏向相切于四个端点，端点处压缩了某个特征＝0；l2中函数与约束点偏向相切于圆，会造成某些特征值压缩的接近于0；<br>根据奥卡姆剃刀原理，<strong>当两个假说具有完全相同的解释力和预测力时，我们以那个较为简单的假说作为讨论依据</strong>，而通常过拟合的模型的特征维度过高过多，所以一定程度可以缓解过拟合。</p>
<p>面试管以一种奇怪的眼神看着我，然后表示他其实想让我通过先验概率解释，不过我这样说仿佛也有道理。我回来之后就研究了一下，比如l2，大致如下：<br>首先，我们确定两点：<br>l2，其实就给了系数w一个期望是0，协方差矩阵是 1/alpha的先验分布。l1对应的是Laplace先验。</p>
<p>我们相当于是给模型参数w设置一个协方差为1/alpha 的零均值高斯分布先验。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-18726e7041c3c6ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>根据贝叶斯定律：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-5ea3175e0b14244b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这一步我没看懂，我计算了半天也没由最大似然估计算出下面这个式子，有会的朋友可以私信我一下。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-a20815f21ca9231b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有了上面的式子就很简单了，alpha在0-正无穷之间，如果a接近0的话，左侧及为正常的MSE也就是没有做任何的惩罚。如果alpha越大的话，说明预测值越接近真实值的同时，协方差也控制的很小，模型越平稳，Var也越小，所以整体的模型更加有效，避免了过拟合导致训练数据拟合效果很差的问题。</p>
<p>到这里，我觉得常见的算法题目都讲完了，很多简单的知识点我没有提，上面这些算是比较经典的，我没答出来的，希望对大家有所帮助，最后谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[常见算法工程师面试题目整理(一)]]></title>
      <url>/2017/12/01/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E6%95%B4%E7%90%86-%E4%B8%80/</url>
      <content type="html"><![CDATA[<p>最近抽风，出去面试了不少公司，和不少算法工程师招聘的朋友有所交流，整理了相关比较有意思的题目，供大家参考：</p>
<p>附：每题视情况给出答案或答案简介，如有疑问，欢迎私信</p>
<h1 id="基于每日用户搜索内容，假设只有少量已知商品的情况下，如何根据用户搜索内容获取平台内没有的新商品？"><a href="#基于每日用户搜索内容，假设只有少量已知商品的情况下，如何根据用户搜索内容获取平台内没有的新商品？" class="headerlink" title="基于每日用户搜索内容，假设只有少量已知商品的情况下，如何根据用户搜索内容获取平台内没有的新商品？"></a>基于每日用户搜索内容，假设只有少量已知商品的情况下，如何根据用户搜索内容获取平台内没有的新商品？</h1><p><img src="http://upload-images.jianshu.io/upload_images/1129359-d9c7e96118574b73.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-6d189620bd41db14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>答案：<strong>这是一条类似于分词“新词获取问题”，答案是基于信息熵＋聚合度。</strong></p>
<p>这边需要考虑排除，首先做stop词库，先去除形容词等。<br>信息熵：比如用户搜索“曲面显示屏 白色”，假设现在我们的商品库中没有显示屏这个商品，我们需要判断“显示屏”是否是潜在的商品，我们需要考虑“显示屏”左词、右词出现的可能。换句话说，如果大家都在搜索“显示屏”商品的话，会出现大量的“便宜显示屏”、“可旋转显示屏”、“显示屏 黑色”等搜索短语，根据信息熵计算公式<code>-p∑logp</code>，“显示屏”前后出现的词语类别越多，信息熵越大，代表用户搜索的需求越旺盛，“显示屏”越有可能是没有的商品。</p>
<p>聚合度：根据信息熵的理论也会出现“显示”等高频出现的干扰词，再用聚合度，比如先计算出p(“显示”)、p(“屏”)、或p(“显”)、p(“示屏”)的概率，如果“显示”是一个高频合理的搜索词的话，p(“显示”)*p(“屏”)应该远远大于p(“显示屏”)，p(“显”)＊p(“示屏”)应该远远大于p(“显示屏”)的概率，而实际电商搜索中，用户连贯搜索“显示屏”的概率才是远超其它。</p>
<hr>
<h1 id="为什么logistic回归的要用sigmoid函数？优缺点？"><a href="#为什么logistic回归的要用sigmoid函数？优缺点？" class="headerlink" title="为什么logistic回归的要用sigmoid函数？优缺点？"></a>为什么logistic回归的要用sigmoid函数？优缺点？</h1><p>答案：优点：<br>1.数据压缩能力，将数据规约在［0，1］之间<br>2.导数形式优秀，方便计算<br>缺点：<br>1.容易梯度消失，x稍大的情况下就趋近一条水平线<br>2.非0中心化，在神经网络算法等情况下，造成反向传播时权重的全正全负的情况。</p>
<p>为什么要用？<br>答案1:<strong>logistic是基于Bernoulli分布的假设，也就是y|X~Bernoulli分布，而Bernoulli分布的指数族的形式就是1/(1+exp(-z))</strong><br>其实还有一个答案二，我当时没想起来，如就是：<br>对于logistic多分类而言，<br>x1、x2、…、xn，属于k类的概率正比于：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-dac233bce2e5a88b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们回到2类：<br>x1、x2、…xn属于1的概率是：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-ad46fbe25e77697a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>分子分母同除以分子极为1/(1+exp(-z))，z＝w11-w01，个人觉得这样的证明才有说服力</p>
<hr>
<h1 id="对比牛顿法、梯度下降法的关系"><a href="#对比牛顿法、梯度下降法的关系" class="headerlink" title="对比牛顿法、梯度下降法的关系"></a>对比牛顿法、梯度下降法的关系</h1><p>讲真，大学学完牛顿法就丢了，一时没回答出来，回来整理如下：<br>答案：<strong>牛顿法快于梯度下降法，且是梯度下降法的极限。</strong></p>
<p>首先，我们有展开式：<br>f′(x+Δx)=f′(x)+f″(x)∗Δx<br>Δx=−μ∗f′(x)<br>合并两个式子，有：<br>f′(x+Δx)=f′(x)+f″(x)∗(−μ∗f′(x))<br>令f′(x+Δx)＝0，<br>μ＝1/f″(x)，极为牛顿法在随机梯度下降中的μ</p>
<hr>
<h1 id="两个盒子，50个红球，50个白球，问如何放球，抽到红球的概率最高？（每个盒子必须有球）"><a href="#两个盒子，50个红球，50个白球，问如何放球，抽到红球的概率最高？（每个盒子必须有球）" class="headerlink" title="两个盒子，50个红球，50个白球，问如何放球，抽到红球的概率最高？（每个盒子必须有球）"></a>两个盒子，50个红球，50个白球，问如何放球，抽到红球的概率最高？（每个盒子必须有球）</h1><p><strong>答案：一个盒子1个红球，另外一个盒子剩余的99个球</strong></p>
<p>先假设第一个盒子放x个红球，y个白球，另外的一个盒子里面就有50-x红球，50-y个白球.<br>求的目标函数：p＝1/2<em>(x/(x+y))+1/2</em>((50-x)/(100-x-y))<br>subject to. x+y&gt;0 &amp; 100-x-y&gt;0</p>
<p>常规解法如上，被坑了一手的是，面试的说没有常规解，我回来思考了半天，可能是盒子里面的排练顺序有差异，上层的抽取概率&gt;下层的抽取概率，所以需要通过EM算法，先得到若干次抽取的结果下，每层的最大概率密度函数，再结合上述的结果去回答。</p>
<hr>
<h1 id="常见的正则化有是么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？"><a href="#常见的正则化有是么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？" class="headerlink" title="常见的正则化有是么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？"></a>常见的正则化有是么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？</h1><p><strong>答案：<br>(1)l1,l2正则化<br>l1对应python里面numpy.linalg.norm(ord=1)<br>形如|w1|+|w2|+|w3|+…<br>l2对应python里面numpy.linalg.norm(ord=2)<br>形如w1^2+w2^2+w3^2+…</strong></p>
<p><strong>(2)防止过拟合<br>其它防止过拟合的方法还有：<br>1.增加数据量<br>2.采取bagging算法，抽样训练数据
</strong><br>(3)画图解决**</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-ffe4f3f34f33f524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br><strong>左边的l1，右边的l2，</strong><br><strong>l1在作图只要不是特殊情况下与正方形的边相切，一定是与某个顶点优先相交，那必然存在横纵坐标轴中的一个系数为0，起到对变量的筛选的作用。</strong><br><strong>l2的时候，其实就可以看作是上面这个蓝色的圆，在这个圆的限制下，点可以是圆上的任意一点，所以q＝2的时候也叫做岭回归，岭回归是起不到压缩变量的作用的，在这个图里也是可以看出来的。</strong></p>
<hr>
<h1 id="分类模型如何选择？如何判断效果？如何计算AUC？你最熟悉的ensemble-Classification-model是什么？"><a href="#分类模型如何选择？如何判断效果？如何计算AUC？你最熟悉的ensemble-Classification-model是什么？" class="headerlink" title="分类模型如何选择？如何判断效果？如何计算AUC？你最熟悉的ensemble Classification model是什么？"></a>分类模型如何选择？如何判断效果？如何计算AUC？你最熟悉的ensemble Classification model是什么？</h1><p>我这边参考了《Do we Need Hundreds of Classifiers to Solve Real World Classification Problems》里面的结论，有兴趣的自行去搜<br>答案：<br><strong>整体上讲：数据量越大，神经网络越好；维度越多，bagging算法越优秀；数据量上不多不少的情况下，SVM效果最好；<br>常用判断：roc、auc、ks、f1值、recall等；<br>AUC计算方法：roc曲线下方的面积积分即可，或者大数定律的投点实验</strong></p>
<p>最熟悉的集成分类模型，我说的是randomforest，详述了原理及实际应用的注意点，后来我问了面试管，主要在这块想了解的是实际解决的相关项目的真实性：<br>1.randomforest是由若干颗cart树构成的，每棵树尽情生长不枝剪，最后采取加权投票或者均值的方式确定输出值<br>2.每棵树的数据是采取bagging式的随机抽取特征及数据样本，两颗树之间的数据有可能会重复<br>3.一般流程会先以sqrt(feature_number)作为每次输入的特征数，采取grid<em>search的方法观察tree的数量由0-500，oob的变化<br>这边被打断了，解释什么叫做oob，也就是out of bag，每次抽取的数据样本进行训练，没有被抽取到的数据作为检验样本，检验样本上的误差就叫做oob<br>4.根据实际要求的精度上后期可以跟进调整：每次输入的特征个数、每棵树的最大深度、每个节点的分支方式（GINI还是信息增益率）、子节点最少数据量、父节点最少数据量等等<br>这边又被打断了，问，什么叫做信息增益率？<br>首先熵的计算如下：<br>![](<a href="https://www.zhihu.com/equation?tex=E%28X%29%3D-%5Csum" target="_blank" rel="noopener">https://www.zhihu.com/equation?tex=E%28X%29%3D-%5Csum</a></em>%7Bi%3D1%7D%5E%7Bn%7D%7Bp<em>%7Bi%7Dlog</em>%7B2%7D%28p<em>%7Bi%7D+%29++%7D+)<br>信息增益如下：<br><img src="https://www.zhihu.com/equation?tex=IGain%28S%2CA%29%3DE%28S%29-E%28A%29" alt=""><br>![](<a href="https://www.zhihu.com/equation?tex=E%28A%29%3D%5Csum" target="_blank" rel="noopener">https://www.zhihu.com/equation?tex=E%28A%29%3D%5Csum</a></em>%7Bv%5Cin+value%28A%29%7D%7B%5Cfrac%7Bnum%28S<em>%7Bv%7D%29+%7D%7Bnum%28S%29%7DE%28S</em>%7Bv%7D+%29+%7D+)<br>比如14个人，好人5个坏人9个。这14个人被通过性别划分开，10个男性中3个坏人，7个好人；4个女性中2个坏人，2个好人。</p>
<p>信息增益就是:<br>IGain＝(-5/14)<em>log(5/14)+(-9/14)</em>log(9/14)-(10/14<em>(-3/10</em>log(3/10)-7/10<em>log(7/10))+4/14</em>(-1/2<em>log(1/2)-1/2</em>log(1/2)))<br>看到这样的计算方式，必然会存在问题，假设我们身份证为区分类别的化，每个身份证号码都是独一无二的，势必存在存在1/n*log(1)=0这样的最佳划分，但是这样的结果就是将所有的情况分别作为子节点，很明显没有意义，所以引出下面的信息增益率。</p>
<p>信息增益率就是:<br><img src="https://www.zhihu.com/equation?tex=Gain-ratio%3D%5Cfrac%7BIGain%28S%2CA%29%7D%7BInfo%7D+" alt=""><br><img src="https://www.zhihu.com/equation?tex=Info%3D-%5Csum_%7Bv%5Cin+value%28A%29%7D%7B%5Cfrac%7Bnum%28S_%7Bv%7D%29+%7D%7Bnum%28S%29%7Dlog_2%7B%5Cfrac%7Bnum%28S_%7Bv%7D+%29%7D%7Bnum%28S%29%7D+%7D%7D+" alt=""><br>比如上面分人的例子，Info＝-10/14log(10/14)-4/14log(4/14)<br>很明显也可以看出，当你划分的子类别越多，你的info会越大，Gain_ratio就越小，信息增益率就越低，惩罚了刚才身份证分类这种行为。</p>
<p>这也是id3和c4.5之间最大的差异，c4.5以信息增益率代替率id3里面的信息增益，除此之外，id3只能对分类变量处理而c4.5既可以分类变量也可以连续变量，还是很强的，同时他们都可以做多分类，而后续的cart等做多分类的成本会增加（叠加的方式）</p>
<p>其实，这些都很基础但是时间长了，真的很绕人，我也是先自己默默的在纸上画了挺久才和面试管聊，有点出乎我的意料。</p>
<hr>
<h1 id="循环神经网络中介绍一个你熟悉的？"><a href="#循环神经网络中介绍一个你熟悉的？" class="headerlink" title="循环神经网络中介绍一个你熟悉的？"></a>循环神经网络中介绍一个你熟悉的？</h1><p>我说的是LSTM。<br>首先，先跑出了循环的机制，同时点明了RNN潜在隐藏节点对output的影响，做了下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-7893355caf3ac2f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>及当前的预测结果，与input及上次的layer1节点下的结果相关。</p>
<p>正向循环：<br>节点1的值 = sigmoid(np.dot(输入参数,神经元1) + np.dot(上次节点1的值,潜在神经元))<br>输出值＝sigmoid(np.dot(节点1的值,神经元2))</p>
<p>误差计算：<br>真实y－输出值</p>
<p>delta：<br>节点2处的deltas=误差计算*sigmoid(np.dot(节点1的值,神经元2))／(1-sigmoid(np.dot(节点1的值,神经元2)))</p>
<p>反向修正神经元：<br>神经元2 += (节点1的值).T.dot(节点2处的delta)<br>潜藏神经元 += (上次的节点1的值).T.dot(节点1处的delta)<br>神经元1 += 输入值.T.dot(节点1处的delta)</p>
<p><strong>核心强调了：sigmoid(np.dot(输入参数,神经元1) + np.dot(上次节点1的值,潜在神经元))，输出值与输出值及上次节点1处的输入值有关。</strong><br>然后讲了简单的在语义识别的实际作用。</p>
<hr>
<h1 id="kmeans的原理及如何选择k？如何选择初始点？"><a href="#kmeans的原理及如何选择k？如何选择初始点？" class="headerlink" title="kmeans的原理及如何选择k？如何选择初始点？"></a>kmeans的原理及如何选择k？如何选择初始点？</h1><p>原理是送分题，<br>原理：在给定K值和K个初始类簇中心点的情况下,把每个点(亦即数据记录)分到离其最近的类簇中心点所代表的类簇中，优点在于易于理解和计算，缺点也是很明显，数据一多的情况计算量极大，且标签feature定义距离的难度大。</p>
<p>K的选择，我答的一般，欢迎大家补充，<br>1.根据具体的业务需求，实际需求确定最后聚成的类的个数<br>2.grid_search去试，看那种距离下，损失函数最小（其实这样回答不好，数据量大的情况下，机会不可能）<br>这边的损失函数类别较多，可能包括组内间距和／组外间距和等<br>3.随机抽样下的层次聚类作为预参考<br>理论上，随机采样的数据分布满足原来的数据集的分布，尤其是大量采样次数下的情况，针对每一个较小的数据集合采取层次聚类确定最后的聚类个数，再针对原始的数据集合进行kmeans聚类</p>
<p><strong>如何选取初始点？</strong><br>这个问题我被问过好多次，其实，不管是r或者python里面，或者大家日常使用中都是默认的随机选取，然后通过多次k-折等方法不断的去迭代，其实这样存在的问题就是如果初始点随机选取的有误，导致无论这么迭代都得不到最优的点，如：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-1cd98baed3ba431c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="随机初始点"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-68d2f0f5e4981734.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="修正初始点"><br>在随机初始点的情况下，红色区域的部分点被蓝色和绿色侵占为己点，修正初始点，也就是将随机初始点的聚类中心全部上移的情况下，蓝色点区收回了原属于自己的点区。<br>之前我恶补过一片论文：《K-means 初始聚类中心的选择算法》，里面提出了两个指标来衡量：<br>1.k-dist<br><img src="http://upload-images.jianshu.io/upload_images/1129359-98cadfdf14bd3d0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>某个点 p 到它的第k 个最近点的距离为点 p 的 k-dist 值。点的 k-dist 半径范围内至少包含k + 1 个点，理论上同一个聚类中改变k值不会引起k-dist值明显变化。将 k-dist 值由小到大排序，a、b、c表示平缓点，d，e，f为跃迁点。<br>2.DK图<br><img src="http://upload-images.jianshu.io/upload_images/1129359-999bc8fbac9d783e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>k-dist 图中相邻两点的 k-dist 值之差记为 DK。k-dist 图中相邻两点pm和pm－1的 k-dist的差为DKm=k-distm －k-distm－1 ( m ＞ 1) 。由于 k-distm 非递减，显然 DKm ＞ 0。DK 值接近的连续邻近点处于 k-dist 图的同一条平缓曲线上，即处于<br>同一个密度层次; DK 值大幅跳动的点处于密度转折曲线或噪声曲线上。<br>3.选择<br>对 DK 值从小到大排序，得到 DK 标准范围δ。依据 DK 标准范围内对应的数据点的分布情况，在 k-dist 图中找出 k’ 个平缓曲线，代表 k’ 个主要密度水平。选择每个密度水平的第一个点作为初始聚类中心。<br>重复若干次，得到若干组的优化聚类中心，在根据优化聚类中心组下的组内间距和／组外间距和判断那个点组为最优点组。</p>
<p>其实这样的开销也挺大的，目前也没有看到其它比较易理解的kmeans的初始点计算的方式。</p>
<hr>
<h1 id="大致讲解一下最优化中拉格朗日乘子法的思路？KKT是什么？"><a href="#大致讲解一下最优化中拉格朗日乘子法的思路？KKT是什么？" class="headerlink" title="大致讲解一下最优化中拉格朗日乘子法的思路？KKT是什么？"></a>大致讲解一下最优化中拉格朗日乘子法的思路？KKT是什么？</h1><p>当我们求解一个函数的最小值，且这个函数也被某些确定的限制条件限制的时候：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-85256fa5051078e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们可以将限制条件加入f(x)中一同进行后续的偏导计算：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-00b95ae21b53a94a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-1eddbd10f95ecba0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>至于KKT我了解的其实不多，也是回来之后恶补了一下，通过例子入手：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-32eaab95b96df5a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>求解上面这个问题的化，我们需要考虑构造两个约束变量a1，b1，使得<br>h1(x，a1)＝g1(x)＋a1<code>^</code>2＝a－x＋a1<code>^</code>2=0<br>h2(x，b1)＝g2(x)＋b1<code>^</code>2＝b－x＋b1<code>^</code>2=0<br>在根据普通拉格朗日乘子的方法对下面公式的每一项求偏导：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-b456f573b4d828a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-18dd18a6ef1ee3fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这个条件就是KKT条件<br>其实我觉得，<a href="http://www.cnblogs.com/zhangchaoyang/articles/2726873.html，这篇文章写的挺好的，想要详细了解的可以仔细参考一下。" target="_blank" rel="noopener">http://www.cnblogs.com/zhangchaoyang/articles/2726873.html，这篇文章写的挺好的，想要详细了解的可以仔细参考一下。</a></p>
<hr>
<h1 id="听说你做过风控，异常点检测你用过什么办法？"><a href="#听说你做过风控，异常点检测你用过什么办法？" class="headerlink" title="听说你做过风控，异常点检测你用过什么办法？"></a>听说你做过风控，异常点检测你用过什么办法？</h1><p>之前正好整理过，内心大喜：<br><strong>1.6个西格玛的原理</strong><br><strong>2.箱式图大于3/2QI＋Q3，小于Q1－3/2Qi</strong><br><strong>3.基于距离离群检测（聚类），包括欧式、马氏距离、街道距离</strong><br>这边被打断了，问了马氏距离的细节，好处：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-50ad069bb00fcb11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>追问了协方差Sigma怎么算：<br>Cov(X,Y)=E(XY)-E(X)E(Y)<br>追问了什么时候用马氏距离比较好：<br>举例很有名的曲线分布图，如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-a53692e2ec514816.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br><strong>4.pca的基于特征值压缩的方法</strong><br><strong>5.基于isolation forest识别的方法</strong><br>这边被追问了一次原理：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">method:</span><br><span class="line">1.从原始数据中随机选择一个属性feature；</span><br><span class="line">2.从原始数据中随机选择该属性的下的一个样本值value；</span><br><span class="line">3.根据feature下的value对每条记录进行分类，把小于value的记录放在左子集，把大于等于value的记录放在右子集；</span><br><span class="line">4.repeat 1-3 until：</span><br><span class="line">　　　　4.1.传入的数据集只有一条记录或者多条一样的记录；</span><br><span class="line">　　　　4.2.树的高度达到了限定高度；</span><br></pre></td></tr></table></figure></p>
<p>以s(x,n)为判断数据是否异常的衡量指标。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-d8d6f6e4dd3126fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br><img src="http://upload-images.jianshu.io/upload_images/1129359-5843710a633244c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>其中，h(x)为x对应的节点深度，c(n)为样本可信度，s(x,n)~[0,1]，正常数据来讲s(x,n)小于0.8，s(x,n)越靠近1，数据异常的可能性越大。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-afcfbf127c0820c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>详细的可以参见我的另一篇博客：<a href="http://www.jianshu.com/p/ac6418ee8e3f" target="_blank" rel="noopener">http://www.jianshu.com/p/ac6418ee8e3f</a></p>
<p>本来准备一次写完的，后来写着写着发现真的挺多，准备写个系列，最后谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SVM理论解析及python实现]]></title>
      <url>/2017/12/01/SVM%E7%90%86%E8%AE%BA%E8%A7%A3%E6%9E%90%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p><strong>关于常见的分类算法在不同数据集上的分类效果，在<a href="http://xueshu.baidu.com/s?wd=paperuri%3A%28491eb32b0997a8dde16c12fe69bf3eac%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2697065%26amp%3Bdl%3Dacm%26amp%3Bcoll%3Ddl%26amp%3Bpreflayout%3Dflat&amp;ie=utf-8&amp;sc_us=5730949819175219868" target="_blank" rel="noopener">《Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?》</a>这个篇论文上有比较完善的总结，因为文章内容比较长，这边我总结了下我认为比较关键的一些结论：</strong></p>
<p><strong>仅仅参考论文评价我们常用的：</strong></p>
<ul>
<li>神经网络的效果最好，13.2%的数据集中取得第一</li>
<li>SVM的效果其次，10.7%的数据集中取得第一</li>
<li>Bagging和Boost紧随其后，9%~10%左右的数据集取得第一</li>
<li>Elastic Net等的线性算法效果普通，5%-7%的数据集上取得第一</li>
</ul>
<p><strong>附加一些我个人平时调参的经验及感悟：</strong></p>
<ul>
<li>神经网络拟合的效果好是基于大量的数据量上，如果数据集较小，训练结果通常不如上述其他算法；除此之外，神经网络的训练成本高，关于控制非线性变换的隐藏层层数，控制线性力度的节点个数的设置需要大量的历史经验，相对成本非常高。</li>
<li>SVM对数据集合量以及维度没有很高的要求，而且可以解决线性问题（kernel=linear），非线性问题（kernel=RBF等），而且相对来说效果优秀。但是SVM的核心是计算最大分隔的间隔，如果全都是分类变量，效果会受一定的影响，而且需要额外的操作才能获取概率结果。</li>
<li>Bagging和Boost，能够解决非线性问题，Bagging基于抽样抽特征，控制Var的情况下降低Bias；Boost基于N个弱分类器的强化组合，控制Bias的情况下降低Var，对数据格式的要求也很低，实现上比较友好。缺点可能就是太一般，没有专业领域的亮点。</li>
<li>Elastic Net等线性回归算法，对数据量数据维度没有什么要求，部分算法会自己压缩feature，简单易操作，相比于上述任何一个算法都好实现，除此之外，还可以得到概率结果。缺点就是效果较差，如果在feature和label没有线性关系的时候无法得到理想结果。</li>
</ul>
<p>除了上述的方法，还有比如KNN、线性判别分析、Naive Bayes等方法，每个都有自己适用的场景，也但是通常不做首要考虑的分类算法。</p>
<p>针对其中的SVM，本文接下来和大家解析三个方面：<br>1.感知机、线性感知机、核感知机的理论概览<br>2.如何利用python中的sklearn快速的实现svm分类<br>3.SMO方法的核心功能实现</p>
<p>如果你只是想快速了解分类算法的概览，方便面试或者日常“交流”，到此就可以不用往下看了。<br>如果你是数据分析师或者软件工程师，只是想快速了解如果使用，直接跳到2。<br>如果你是机器学习工程师，需要对整个算法有个了解，贯穿整个SVM过程，直接看1，2。<br>如果你是算法工程师，需要重构算法，或在当前解决核函数计算瓶颈的，请全文阅读，并阅读推荐书籍。</p>
<p>让我们开始正文：</p>
<h1 id="感知机、线性感知机、核感知机的理论概览"><a href="#感知机、线性感知机、核感知机的理论概览" class="headerlink" title="感知机、线性感知机、核感知机的理论概览"></a>感知机、线性感知机、核感知机的理论概览</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a><strong>感知机</strong></h2><p>我们日常说的SVM其实只是一个感知机，也就是没有任何的核函数的情况。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-e30e898a5dbcfc62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面图中，对于二维数据来说，平面π1，π2，π3都可以将红色和蓝色的点给划分开。对于多维数据来说，这边的平面就可以引申为超平面，wx+b=0。</p>
<p>所以，我们可以说，对于数据集:<br><img src="http://upload-images.jianshu.io/upload_images/1129359-061576141287415d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>如果我们可以得到超平面wx+b=0，使得y=1的点集合与y=-1的点的集合分隔在平面两边（如上图所示），那我们就说原始数据集D线性可分，wx+b=0为其超平面。</p>
<p>首先，我们定义损失函数为：<strong>L=max(-y(wx+b),0)，</strong>我们来看下，如果我们预测正确，y=1，我们预测的为wx+b&gt;0或者，y=-1，我们预测的为wx+b<0,y(wx+b)>0,则不贡献梯度；否则，我们可以取-y(wx+b)为梯度，也就得到了上述的梯度公式。<br>接下来的就是常规意义上的对w和b的偏导，然后梯度下降求极值。</0,y(wx+b)></p>
<h2 id="线性感知机"><a href="#线性感知机" class="headerlink" title="线性感知机"></a><strong>线性感知机</strong></h2><p>现在我们思考两个问题：<br>a.上述的π1、π2、π3都是感知机，如何选取最优？<br>b.下面这张图线性不可分，如何解决？</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-be8f5dd3ffb8d7f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="线性不可分"></p>
<p>我们先来看，平面π1外任意点到平面的距离如何计算：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-924592ff1b2dea00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>假设任意点为X(x，y)，垂点X1(x1，y1)，垂点在π1平面(wx+b=0)上，所以，我们有X-X1=ρw，w为平面π1的法向量。所以，我们有：<br><code>||X-X1||**2=ρw(X-X1)=ρ(wx+b-(wx1+b))=ρ(wx+b)=ρ(wx+b)</code><br>在计算距离的时候，我们需要去归一化，无量纲化。<br>不难看出，距离的计算方式为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d611c966e17936b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>所以，我们在超平面选取的时候，需要考虑两点：<br><strong>(1)所以的分类结果要保持正确：</strong></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d4cb93b4eb7beae0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>(2)保证决策面离正负样本都极可能的远：</strong></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-5d6545e5e2b38433.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>里面的min的作用是计算所有的点到平面π的最小距离，外面的max的作用是尽可能的让最小距离最大，保证决策面离正负样本都尽可能的远。</p>
<p>假设(x1,y1)到决策平面的距离最近，所有y1(wx1+b)&gt;=1,所以目标函数：max(1/||w||)，可以优化为min(||w||^2/2)。<br>但是如果发生1.2节最开始的<strong>线性不可分的问题</strong>的时候，y1(wx1+b)&gt;=1就无法实现，所有我们需要加∆的容忍度，也就是变成了y1(wx1+b)&gt;=1-∆。既然加了∆，我们也需要对∆进行控制：∆=1-y1(wx1+b)，有更新后的目标函数：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-68f213e49cc0cba0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这边的<code>[]+</code>记为神经网络中常用的ReLU函数。<br>有了这个目标函数，接下来就是正常的梯度下降，偏导后求解的过程。</p>
<h2 id="核函数下的感知机"><a href="#核函数下的感知机" class="headerlink" title="核函数下的感知机"></a><strong>核函数下的感知机</strong></h2><p>上面考虑的问题均是线性可分的问题，假设数据分布如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-4de6cc9c295ef8ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>无论通过平面π1、π2、还是π3均无法做到线性的分割。<br>而核函数的目的就是通过内积的形式，将低维度的数据映射到高维度，通常采取的方式是w=∑αx的形式，带回到原来的损失函数：<br>比如普通感知机的：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-c0156fcd233cb69e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>比如线性感知机</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d857f274b114b7e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>K(xi,xj)常用的有：<br>多项式核函数：<br>(xi+xj+1)^p</p>
<p>径向基核函数：<br>exp(-ρ||xi-xj||^2)</p>
<p>至于之后的计算，还是可以和之前一致，将上述选择的核函数代入损失函数后采取梯度下降的方法，高效计算方式SMO算法在第三模块会简单的梳理一遍。</p>
<p>以上我们就大概的了解了感知机，linear svm，kernel svm的损失函数的来源及构造细节等等，接下来我们来看下如何快速的使用。</p>
<h1 id="如何利用python中的sklearn快速的实现svm分类"><a href="#如何利用python中的sklearn快速的实现svm分类" class="headerlink" title="如何利用python中的sklearn快速的实现svm分类"></a>如何利用python中的sklearn快速的实现svm分类</h1><p>在python的sklearn包中，有SVM的包，其中SVC是分类，SVR是回归，可以快速简单的上手，下面上code，并在注释中解释：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment">#data add,数据读取</span></span><br><span class="line">risk_data=pd.read_table(<span class="string">'/Users/slade/Desktop/Python File/data/data_all.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#data check，删除无用的列</span></span><br><span class="line">risk_data = risk_data.drop(<span class="string">'Iphone'</span>,axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#data scale，数据归一化（必备的操作），上述理论中也体现归一化后的距离计算的原因</span></span><br><span class="line">risk_data_mm = risk_data.max()-risk_data.min()</span><br><span class="line">risk_data_scale = pd.DataFrame([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(risk_data.columns)):</span><br><span class="line">    new_columns = (risk_data.iloc[:,i]-risk_data.iloc[:,i].min())/risk_data_mm[i]</span><br><span class="line">    risk_data_scale = pd.concat([risk_data_scale,pd.DataFrame(new_columns)],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#split data（将数据分割成训练集和测试集）</span></span><br><span class="line">train_data,test_data = train_test_split(risk_data_scale,test_size = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#update_train,update_test（因为我的数据集是非常不平衡的，这边我采取了欠采样的方法）</span></span><br><span class="line">train_badcase = train_data[train_data[<span class="string">'tag'</span>]==<span class="number">1</span>]</span><br><span class="line">train_goodcase = train_data[train_data[<span class="string">'tag'</span>]!=<span class="number">1</span>]</span><br><span class="line">sample_value=(<span class="number">10</span>*train_badcase.count()[<span class="number">0</span>])</span><br><span class="line">train_goodcase_sample = train_goodcase.sample(n=sample_value)</span><br><span class="line">train_data_update = pd.concat([train_badcase,train_goodcase_sample],axis = <span class="number">0</span>)</span><br><span class="line">y=train_data_update[<span class="string">'tag'</span>]</span><br><span class="line">x=train_data_update.iloc[:,<span class="number">1</span>:len(train_data_update.columns)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#svm（linear、rbf、sigmoid为核的SVM）</span></span><br><span class="line">clf_linear = svm.SVC(kernel=<span class="string">'linear'</span>).fit(x,y)</span><br><span class="line">clf_rbf = svm.SVC(kernel=<span class="string">'rbf'</span>).fit(x,y)</span><br><span class="line">clf_sigmoid = svm.SVC(kernel=<span class="string">'sigmoid'</span>).fit(x,y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#test，训练数据处理</span></span><br><span class="line">y_test = test_data[[<span class="string">'tag'</span>]]</span><br><span class="line">x_test = test_data.iloc[:,<span class="number">1</span>:len(test_data.columns)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型效果对比</span></span><br><span class="line"><span class="comment">#linear</span></span><br><span class="line">test_pred=clf_linear.predict(x_test)</span><br><span class="line">y_test.index = range(y_test.count())</span><br><span class="line">union_actual_pred = pd.concat([y_test,pd.DataFrame(test_pred)],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#show the result</span></span><br><span class="line">recall = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()/union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>].count()</span><br><span class="line">percison = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count() / union_actual_pred[union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()</span><br><span class="line">correction = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==union_actual_pred.iloc[:,<span class="number">1</span>]].count()/union_actual_pred.iloc[:,<span class="number">0</span>].count()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the recall is %s'</span> %recall</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the percison is %s'</span> %percison</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the correction is %s'</span> %correction</span><br><span class="line"></span><br><span class="line"><span class="comment">#rbf</span></span><br><span class="line">test_pred=clf_rbf.predict(x_test)</span><br><span class="line">y_test.index = range(y_test.count())</span><br><span class="line">union_actual_pred = pd.concat([y_test,pd.DataFrame(test_pred)],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#show the result</span></span><br><span class="line">recall = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()/union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>].count()</span><br><span class="line">percison = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count() / union_actual_pred[union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()</span><br><span class="line">correction = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==union_actual_pred.iloc[:,<span class="number">1</span>]].count()/union_actual_pred.iloc[:,<span class="number">0</span>].count()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the recall is %s'</span> %recall</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the percison is %s'</span> %percison</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the correction is %s'</span> %correction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#sigmoid</span></span><br><span class="line">test_pred=clf_sigmoid.predict(x_test)</span><br><span class="line">y_test.index = range(y_test.count())</span><br><span class="line">union_actual_pred = pd.concat([y_test,pd.DataFrame(test_pred)],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#show the result</span></span><br><span class="line">recall = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()/union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>].count()</span><br><span class="line">percison = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count() / union_actual_pred[union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()</span><br><span class="line">correction = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==union_actual_pred.iloc[:,<span class="number">1</span>]].count()/union_actual_pred.iloc[:,<span class="number">0</span>].count()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the recall is %s'</span> %recall</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the percison is %s'</span> %percison</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the correction is %s'</span> %correction</span><br></pre></td></tr></table></figure></p>
<p>上述粗略的给出了如何快速的通过svm进行一次训练，现在就svc中的参数进行剖析：<br><code>C</code>:惩罚力度，C越大代表惩罚程度越大，越不能容忍有点集交错的问题<br><code>kernel</code>:核函数，常规的有‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ ，默认的是rbf<br><code>degree</code>:当poly为核函数时启动，默认是3<br><code>gamma</code>:当‘rbf’, ‘poly’ 和 ‘sigmoid’为核函数时的参数设置，默认的是特征个数的倒数<br><code>probability</code>：是否输出概率值<br><code>shrinking</code>:是否自启动<br><code>tol</code>:停止训练的容忍度<br><code>max_iter</code>：最大的训练次数<br><code>class_weight</code>:因变量的权重<br><code>decision_function_shape</code>:因变量的形式：ovo一对一, ovr一对多, 默认’ovr’<br>根据自己的需求，对上述的参数进行grid_search即可完成快速训练任务。</p>
<h1 id="3-SMO方法的核心功能实现"><a href="#3-SMO方法的核心功能实现" class="headerlink" title="3.SMO方法的核心功能实现"></a>3.SMO方法的核心功能实现</h1><p>首先，我们需要明确，SVM学习过程可以转化为以下问题：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-e3de453c32a66b39.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>至于什么是KKT条件，请参照<a href="http://www.jianshu.com/p/4f91f0dcba95" target="_blank" rel="noopener">总结：常见算法工程师面试题目整理(二)</a>中的回答。<br>求解的方式也是比较复杂，这主要以梳理流程为主，我们的目的就是找到一组αi满足上述的约束，然后再根据该组的αi求解到w和b即可。</p>
<p>求解αi的过程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.选择两个拉格朗日乘子αi和αj</span><br><span class="line">2.固定其他拉格朗日乘子αk(k不等于i和j)，只对αi和αj优化w(α)</span><br><span class="line">3.根据优化后的αi和αj，更新截距b的值</span><br><span class="line">4.充分1-3直到收敛</span><br></pre></td></tr></table></figure></p>
<p>针对上面的过程存在2个问题<br>a.为什么要选择两个乘子？而不是更加方便计算的一个？<br>在原始的约束条件中，存在：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-b44c37b52864adfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>如果只选择一个为变化乘子的化，根据其他确定的乘子，该变化乘子也无法变化。</p>
<p>b.如何选择两个乘子αi和αj？</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-1af405394b75336f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>检验样本是否满足KKT条件也就是检验样本是否满足以下条件：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-bd24343c283a50ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>第一个参数αi优先检验0&lt;αj&lt;C也就是π3和π1平面上的点是否满足条件，如果全部满足条件，再检验全部数据集是否满足条件。</p>
<p>第二个参数αj则可以简单地随机选取，虽然这不是特别好，但效果已然不错。也可以通过最大化αi的误差与αj的误差之差的绝对值去判断，但是计算量会变大，因为又做了一次全量数据循环。</p>
<p>当αi和αj有了之后再去对b进行修正：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-9647f7dc0f6fba09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>即可。</p>
<p>这边的代码比较复杂，我就不贴了，百度上很多实现了的版本。</p>
<p>总的来说，我们对svm的过程有了个浅尝辄止的了解，部分算法工程师需要要深入的了解其深刻完整的含义，仍需完整完善的学习，<a href="https://www.amazon.cn/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA/dp/B007TSFMTA" target="_blank" rel="noopener">《统计学习方法 》</a>一书讲的深入透彻，建议可以研读一下。</p>
<p>部分软件工程师在运用中可能需要各种版本的svm，这边也贴出地址，供参考<a href="http://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html" target="_blank" rel="noopener">Support Vector Machine for Complex Outputs</a></p>
<p>最后，谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[能够快速实现的协同推荐]]></title>
      <url>/2017/12/01/%E8%83%BD%E5%A4%9F%E5%BF%AB%E9%80%9F%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%8D%8F%E5%90%8C%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<p>对于中小型的公司，用户的数据量及公司产品的个数都是较小规模的，需要提供给用户的推荐系统实现的重心也从人性化变成了实现成本，协同推荐就是非常常见、有效且可以快速实现的方法，也是本文想介绍的。</p>
<p>常规的快速简单推荐系统实现方法不排除以下几种：</p>
<ul>
<li><p>热门推荐<br>所有人打开浏览的内容都一致，惊喜性会有所缺失，但是实现特别简单，稍加逻辑带给用户的体验感满足了基本需求。</p>
</li>
<li><p>SVD+推荐<br>之前也讨论过实现方法了，附上链接：<a href="http://www.jianshu.com/p/a3e633e396a0" target="_blank" rel="noopener">SVD及扩展的矩阵分解方法</a></p>
</li>
<li><p>基于模型推荐<br>这个比较偏向业务场景，可以说是经典的场景化模型，之前写过一篇基于用户特征的偏好推荐，可以参考一下：<a href="http://www.jianshu.com/p/fd245999ebfe" target="_blank" rel="noopener">苏宁易购的用户交叉推荐</a></p>
</li>
<li><p>协同推荐<br>这个也是几乎每个公司都会用的，也是非常非常常见有效的算法之一</p>
</li>
</ul>
<hr>
<h1 id="协同推荐介绍"><a href="#协同推荐介绍" class="headerlink" title="协同推荐介绍"></a><strong>协同推荐介绍</strong></h1><p>首先，我们先来了解一下什么叫做协同推荐。<br>基于用户的协同过滤推荐算法是最早诞生的，1992年提出并用于邮件过滤系统，两年后1994年被 GroupLens 用于新闻过滤。一直到2000年左右，该算法都是推荐系统领域最著名的算法。算是非常古董级别的算法之一了，但是古董归古董，它的效果以及实现的成本却奠定了它在每个公司不可取代的地位。</p>
<h2 id="基于用户的协同推荐"><a href="#基于用户的协同推荐" class="headerlink" title="基于用户的协同推荐"></a><strong>基于用户的协同推荐</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">用户u1喜欢的电影是A，B，C</span><br><span class="line">用户u2喜欢的电影是A, C, E, F</span><br><span class="line">用户u3喜欢的电影是B，D</span><br></pre></td></tr></table></figure>
<p>假设u1、u2、u3用户喜欢的电影分布如上，基于用户的协同推荐干了这么一件事情，它根据每个用户看的电影（A、B、C、…）相似程度，来计算用户之间的相似程度，将高相似的用户看过但是目标用户还没有看过的电影推荐给目标用户。</p>
<h2 id="基于商品的协同推荐"><a href="#基于商品的协同推荐" class="headerlink" title="基于商品的协同推荐"></a><strong>基于商品的协同推荐</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">电影A被u1，u2看过</span><br><span class="line">电影B被u1，u3看过</span><br><span class="line">电影C被u1，u2看过</span><br><span class="line">电影D被u3看过</span><br><span class="line">电影E被u2看过</span><br><span class="line">电影F被u2看过</span><br></pre></td></tr></table></figure>
<p>假设A～F电影被用户观影的分布如上，基于商品的协同推荐干了这么一件事情，它根据电影（A、B、C、…）被不同用户观看相似程度，来计算电影之间的相似程度，根据目标用户看过的电影的高相似度的电影推荐给目标用户。</p>
<p>看起来以上的逻辑是非常简单的，其实本来也是非常简单的，我看了下，网上关于以上的代码实现还是比较林散和有问题的，优化了python版本的code，并详细解释了每一步，希望，对初学者有所帮助。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#time  2017-09-17</span></span><br><span class="line"><span class="comment">#author：shataowei</span></span><br><span class="line"><span class="comment">#based-item</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#所要的基础包比较简单</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">startTime = time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据的过程</span></span><br><span class="line"><span class="comment">#/Users/slade/Desktop/machine learning/data/recommender/u1.base</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readdata</span><span class="params">(location)</span>:</span></span><br><span class="line">    list2item = &#123;&#125;  <span class="comment">#商品对应的用户列表(1:[[1,2],[2,3]]代表商品1对应用户1的行为程度为2,商品1对应的用户2的行为程度为3)</span></span><br><span class="line">    list2user = &#123;&#125;  <span class="comment">#用户对应的商品列表(1:[[1,2],[2,3]]代表用户1对应商品1的行为程度为2,用户1对应的商品2的行为程度为3)</span></span><br><span class="line">    f = open(location,<span class="string">'r'</span>)</span><br><span class="line">    data = f.readlines()</span><br><span class="line">    data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> int(i[<span class="number">1</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2item.keys():</span><br><span class="line">            list2item[int(i[<span class="number">1</span>])] = [[int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            list2item[int(i[<span class="number">1</span>])].append([int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> int(i[<span class="number">0</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2user.keys():</span><br><span class="line">            list2user[int(i[<span class="number">0</span>])] = [[int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            list2user[int(i[<span class="number">0</span>])].append([int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line">    <span class="keyword">return</span> list2item,list2user</span><br><span class="line"><span class="comment">#list2item,list2user=readdata('/Users/slade/Desktop/machine learning/data/recommender/u1.base')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 基于item的协同推荐</span></span><br><span class="line"><span class="comment">#0.将用户行为程度离散化：浏览：1，搜索：2，收藏：3，加车：4，下单未支付5</span></span><br><span class="line"><span class="comment">#1.计算item之间的相似度：item共同观看次数/单item次数连乘</span></span><br><span class="line"><span class="comment">#2.寻找目标用户观看过的item相关的其他item列表</span></span><br><span class="line"><span class="comment">#3.计算其他item的得分：相似度*用户行为程度，求和</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#0 hive操作</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.1统计各商品出现次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">itemcf_itemall</span><span class="params">(userlist = list2user)</span>:</span></span><br><span class="line">    I=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> userlist:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> userlist[key]:</span><br><span class="line">            <span class="keyword">if</span> item[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> I.keys():</span><br><span class="line">                I[item[<span class="number">0</span>]] = <span class="number">0</span></span><br><span class="line">            I[item[<span class="number">0</span>]] = I[item[<span class="number">0</span>]] + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> I</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.2计算相似矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">itemcf_matrix</span><span class="params">(userlist = list2user)</span>:</span></span><br><span class="line">    C=defaultdict(defaultdict)</span><br><span class="line">    W=defaultdict(defaultdict)</span><br><span class="line"><span class="comment">#根据用户的已购商品来形成对应相似度矩阵</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> userlist:</span><br><span class="line">        <span class="keyword">for</span> item1 <span class="keyword">in</span> userlist[key]:</span><br><span class="line">            <span class="keyword">for</span> item2 <span class="keyword">in</span> userlist[key]:</span><br><span class="line">                <span class="keyword">if</span> item1[<span class="number">0</span>] == item2[<span class="number">0</span>]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> item2 <span class="keyword">not</span> <span class="keyword">in</span> C[item1[<span class="number">0</span>]].keys():</span><br><span class="line">                    C[item1[<span class="number">0</span>]][item2[<span class="number">0</span>]] = <span class="number">0</span></span><br><span class="line">                C[item1[<span class="number">0</span>]][item2[<span class="number">0</span>]] = C[item1[<span class="number">0</span>]][item2[<span class="number">0</span>]] + <span class="number">1</span></span><br><span class="line"><span class="comment">#计算相似度，并填充上面对应的相似度矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i , j <span class="keyword">in</span> C.items():</span><br><span class="line">        <span class="keyword">for</span> z , k <span class="keyword">in</span> j.items():</span><br><span class="line">            W[i][z] = k/math.sqrt(I[i]*I[z])</span><br><span class="line"><span class="comment">#k/math.sqrt(I[i]*I[z])计算相似度，其中k为不同商品交集，sqrt(I[i]*I[z])用来压缩那些热门商品必然有高交集的问题</span></span><br><span class="line">    <span class="keyword">return</span> W</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.寻找用户观看的其他item</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommendation</span><span class="params">(userid,k)</span>:</span></span><br><span class="line">    score_final = defaultdict(int)</span><br><span class="line">    useriditem = []</span><br><span class="line">    <span class="keyword">for</span> item,score <span class="keyword">in</span> list2user[userid]:</span><br><span class="line"><span class="comment">#3.计算用户的item得分，k来控制用多少个相似商品来计算最后的推荐商品</span></span><br><span class="line">        <span class="keyword">for</span> i , smimilarity <span class="keyword">in</span> sorted(W[item].items() , key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>] ,reverse =<span class="keyword">True</span>)[<span class="number">0</span>:k]:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> list2user[userid]:</span><br><span class="line">                useriditem.append(j[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> useriditem:</span><br><span class="line">                score_final[i] = score_final[i] + smimilarity * score</span><br><span class="line"><span class="comment">#累加每一个商品用户的评分与其它商品的相似度积的和作为衡量</span></span><br><span class="line"><span class="comment">#最后的10控制输出多少个推荐商品</span></span><br><span class="line">    l = sorted(score_final.items() , key = <span class="keyword">lambda</span> x : x[<span class="number">1</span>] , reverse = <span class="keyword">True</span>)[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">    <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"><span class="comment">#I = itemcf_itemall()</span></span><br><span class="line"><span class="comment">#W = itemcf_matrix()</span></span><br><span class="line"><span class="comment">#result_userid = recommendation(2,k=20)</span></span><br><span class="line"></span><br><span class="line">endTime = time.time()</span><br><span class="line"><span class="keyword">print</span> endTime-startTime</span><br></pre></td></tr></table></figure>
<p>python来实现基于item的协同推荐就完成了，核心的相似度计算可以根据实际问题进行修改，整体流程同上即可，当然数据量大的时候分布式去写也是可以的。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#time  2017-09-17</span></span><br><span class="line"><span class="comment">#author：shataowei</span></span><br><span class="line"><span class="comment">#based-user</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">startTime = time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line"><span class="comment">#/Users/slade/Desktop/machine learning/data/recommender/u1.base</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readdata</span><span class="params">(location)</span>:</span></span><br><span class="line">    list2item = &#123;&#125;  <span class="comment">#商品对应的用户列表</span></span><br><span class="line">    list2user = &#123;&#125;  <span class="comment">#用户对应的商品列表</span></span><br><span class="line">    f = open(location,<span class="string">'r'</span>)</span><br><span class="line">    data = f.readlines()</span><br><span class="line">    data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> int(i[<span class="number">1</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2item.keys():</span><br><span class="line">            list2item[int(i[<span class="number">1</span>])] = [[int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            list2item[int(i[<span class="number">1</span>])].append([int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> int(i[<span class="number">0</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2user.keys():</span><br><span class="line">            list2user[int(i[<span class="number">0</span>])] = [[int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            list2user[int(i[<span class="number">0</span>])].append([int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line">    <span class="keyword">return</span> list2item,list2user</span><br><span class="line"><span class="comment">#list2item,list2user=readdata('/Users/slade/Desktop/machine learning/data/recommender/u1.base')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#基于用户的协同推荐</span></span><br><span class="line"><span class="comment">#0.先通过hive求出近一段时间（根据业务频率定义），用户商品的对应表</span></span><br><span class="line"><span class="comment">#1.求出目标用户的邻居，并计算目标用户与邻居之间的相似度</span></span><br><span class="line"><span class="comment">#2.列出邻居所以购买的商品列表</span></span><br><span class="line"><span class="comment">#3.针对第二步求出了商品列表，累加所对应的用户相似度，并排序求top</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#0.hive操作</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.1求出目标用户的邻居，及对应的相关程度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neighbour</span><span class="params">(userid,user_group = list2user,item_group = list2item)</span>:</span></span><br><span class="line">    neighbours = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> list2user[userid]:</span><br><span class="line">        <span class="keyword">for</span> user <span class="keyword">in</span> list2item[item[<span class="number">0</span>]]:</span><br><span class="line">            <span class="keyword">if</span> user[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> neighbours.keys():</span><br><span class="line">                neighbours[user[<span class="number">0</span>]] = <span class="number">0</span></span><br><span class="line">            neighbours[user[<span class="number">0</span>]] = neighbours[user[<span class="number">0</span>]] + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> neighbors</span><br><span class="line"><span class="comment">#通常来说，基于item的推荐对于商品量较大的业务会构成一个巨大的商品矩阵，这时候如果用户人均购买量较低的时候，可以考虑使用基于user的推荐，它在每次计算的时候会只考虑相关用户，也就是这边的neighbours(有点支持向量基的意思)，大大的降低了计算量。</span></span><br><span class="line"><span class="comment">#neighbours = neighbour(userid=2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.2就算用户直接的相似程度,这边用的余弦相似度：点积/模的连乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similarity</span><span class="params">(user1,user2)</span>:</span></span><br><span class="line">    x=<span class="number">0</span></span><br><span class="line">    y=<span class="number">0</span></span><br><span class="line">    z=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> item1 <span class="keyword">in</span> list2user[user1]:</span><br><span class="line">        <span class="keyword">for</span> item2 <span class="keyword">in</span> list2user[user2]:</span><br><span class="line">            <span class="keyword">if</span> item1[<span class="number">0</span>]==item2[<span class="number">0</span>]:</span><br><span class="line">                x1 = item1[<span class="number">1</span>]*item1[<span class="number">1</span>]</span><br><span class="line">                y1 = item2[<span class="number">1</span>]*item2[<span class="number">1</span>]</span><br><span class="line">                z1 = item1[<span class="number">1</span>]*item2[<span class="number">1</span>]</span><br><span class="line">                x = x + x1</span><br><span class="line">                y = y + y1</span><br><span class="line">                z = z + z1</span><br><span class="line"><span class="comment">#避免分母为0</span></span><br><span class="line">    <span class="keyword">if</span> x * y == <span class="number">0</span> :</span><br><span class="line">        simi = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        simi = z / math.sqrt(x * y)</span><br><span class="line">    <span class="keyword">return</span> simi</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.3计算目标用户与邻居之间的相似度：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">N_neighbour</span><span class="params">(userid,neighbours,k)</span>:</span></span><br><span class="line">    neighbour = neighbours.keys()</span><br><span class="line">    M = []</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> neighbour:</span><br><span class="line">        simi = similarity(userid,user)</span><br><span class="line">        M.append((user,simi))</span><br><span class="line">    M = sorted(M,key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>] ,reverse = <span class="keyword">True</span>)[<span class="number">0</span>:k]</span><br><span class="line">    <span class="keyword">return</span> M</span><br><span class="line"></span><br><span class="line"><span class="comment">#M = N_neighbour(userid,neighbours,k=200)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.列出邻居所购买过的商品并计算商品对应的推荐指数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neighbour_item</span><span class="params">(M=M)</span>:</span></span><br><span class="line">    R = &#123;&#125;</span><br><span class="line">    M1 = dict(M)</span><br><span class="line">    <span class="keyword">for</span> neighbour <span class="keyword">in</span> M1:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> list2user[neighbour]:</span><br><span class="line">            <span class="keyword">if</span> item[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> R.keys():</span><br><span class="line">                R[item[<span class="number">0</span>]] = M1[neighbour] * item[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                R[item[<span class="number">0</span>]] = R[item[<span class="number">0</span>]] + M1[neighbour] * item[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#根据邻居买过什么及与邻居的相似度，计算邻居买过商品的推荐度</span></span><br><span class="line">    <span class="keyword">return</span> R</span><br><span class="line"><span class="comment"># R = neighbour_item(M)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.排序得到推荐商品</span></span><br><span class="line">Rank = sorted(R.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>],reverse = <span class="keyword">True</span>)[<span class="number">0</span>:<span class="number">50</span>]</span><br><span class="line"></span><br><span class="line">endTime = time.time()</span><br><span class="line"><span class="keyword">print</span> endTime-startTime</span><br></pre></td></tr></table></figure>
<p>python来实现基于user的协同推荐就完成了，核心的相似度计算可以根据实际问题进行修改，基于user的实现过程中，用了邻居这个概念，大大降低了计算量，我用了大概20万用户，2千的商品数，基于user的推荐实现速度大概为基于商品的10分之一，效果差异却相差不大。</p>
<p>协同推荐是非常简单的推荐入门算法之一，也是必须要手动快速代码实现的算法之一，希望能给大家一些帮助。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Tensorflow的神经网络解决用户流失概率问题]]></title>
      <url>/2017/12/01/%E5%9F%BA%E4%BA%8ETensorflow%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3%E7%94%A8%E6%88%B7%E6%B5%81%E5%A4%B1%E6%A6%82%E7%8E%87%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>用户流失一直都是公司非常重视的一个问题，也是AAARR中的Retention的核心问题，所以各大算法竞赛都很关注。比如最近的：<a href="https://www.kaggle.com/c/kkbox-churn-prediction-challenge" target="_blank" rel="noopener">KKBOX的会员流失预测算法竞赛</a>，如何能够搭建一个精准的模型成了大家探索的重要问题。<br>本文主要讲解神经网络、TensorFlow的概述、如何利用python基于TensorFlow神经网络对流失用户进行分类预测，及可能存在的一些常见问题，作为深度学习的入门阅读比较适合。</p>
<hr>
<h1 id="行业做法："><a href="#行业做法：" class="headerlink" title="行业做法："></a>行业做法：</h1><p>通常的行业预测用户流失大概分以下几种思路：</p>
<ul>
<li>利用线性模型(比如Logistic)＋非线性模型Xgboost判断用户是否回流逝</li>
</ul>
<p>这种方法有关是行业里面用的最多的，效果也被得意验证足够优秀且稳定的。核心点在于特征的预处理，Xgboost的参数挑优，拟合程度的控制，这个方法值得读者去仔细研究一边。问题也是很明显的，会有一个行业baseline，基本上达到上限之后，想有有提升会非常困难，对要求精准预测的需求会显得非常乏力。</p>
<ul>
<li>规则触发</li>
</ul>
<p>这种方法比较古老，但是任然有很多公司选择使用，实现成本较低而且非常快速。核心在于，先确定几条核心的流失指标(比如近7日登录时长)，然后动态的选择一个移动的窗口，不停根据已经流失的用户去更新流失指标的阈值。当新用户达到阈值的时候，触发流失预警。效果不如第一个方法，但是实现简单，老板也很容易懂。</p>
<ul>
<li>场景模型的预测</li>
</ul>
<p>这个方法比较依赖于公司业务的特征，如果公司业务有部分依赖于评论，可以做文本分析，比如我上次写的<a href="http://www.jianshu.com/p/413cff5b9f3a" target="_blank" rel="noopener">基于word2vec下的用户流失概率分析</a>。如果业务有部分依赖于登录打卡，可以做时间线上的频次预估。这些都是比较偏奇门易巧，不属于通用类别的，不过当第一种方法达到上线的时候，这种方法补充收益会非常的大。</p>
<p>其实还有很多其它方法，我这边也不一一列出了，这个领域的方法论还是很多的。</p>
<hr>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><ul>
<li><strong>核心</strong><br><img src="http://upload-images.jianshu.io/upload_images/1129359-af497cd5850f8704.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="神经网络流程"><br>上面这张图片诠释了神经网络正向传播的流程，先通过线性变换(上图左侧)Σxw+b将线性可分的数据分离，再通过非线性变换(上图右侧)Sigmoid函数将非线性可分的数据分离，最后将输入空间投向另一个输出空间。</li>
</ul>
<p>根据上面所说，我们可以知道，通过增加左侧线性节点的个数，我们可以强化线性变换的力度；而通过增加层数，多做N次激活函数(比如上面提到的Sigmoid)可以增强非线性变换的能力。</p>
<p>通过矩阵的线性变换+激活矩阵的非线性变换，将原始不可分的数据，先映射到高纬度，再进行分离。但是这边左侧节点的个数，网络的层数选择是非常困难的课题，需要反复尝试。</p>
<ul>
<li><strong>参数训练</strong><br>刚才我们了解了整个训练的流程，但是如何训练好包括线性变换的矩阵系数是一个还没有解决的问题。</li>
</ul>
<p>我们来看下面的过程：<br>input ==&gt; Σxw+b(线性变换) ==&gt; f(Σxw+b)(激活函数) ==&gt; …(多层的话重复前面过程) ==&gt; output(到此为止，正向传播结束，反向修正矩阵weights开始) ==&gt; <strong>error=actual_output-output(计算预测值与正式值误差)==&gt;output处的梯度==&gt;调整后矩阵weight=当前矩阵weight+error<code>x</code>学习速率<code>x</code>output处的负梯度</strong><br>核心目的在于通过比较预测值和实际值来调整权重矩阵，将预测值与实际值的差值缩小。<br>比如：梯度下降的方法，通过计算当前的损失值的方向的负方向，控制学习速率来降低预测值与实际值间的误差。</p>
<p>利用一行代码来解释就是  <code>synaptic_weights += dot(inputs, (real_outputs - output) * output * (1 - output))*η</code><br>这边output * (1 - output))是在output处的Sigmoid的倒数形式，η是学习速率。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-1ef9c397a42a4bc1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="weight的循环流程"></p>
<ul>
<li><strong>神经网络流程小结</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1数据集获取（有监督数据整理）</span><br><span class="line">2神经网络参数确定，有多少层，多少个节点，激活函数是什么，损失函数是什么</span><br><span class="line">3数据预处理，pca，标准化，中心化，特征压缩，异常值处理</span><br><span class="line">4初始化网络权重</span><br><span class="line">5网络训练</span><br><span class="line">        5.1正向传播</span><br><span class="line">        5.2计算loss</span><br><span class="line">        5.3计算反向梯度</span><br><span class="line">        5.4更新梯度</span><br><span class="line">        5.5重新正向传播</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这边只是简单介绍了神经网络的基础知识，针对有一定基础的朋友唤醒记忆，如果纯小白用户，建议从头开始认真阅读理解一遍过程，避免我讲的有偏颇的地方对你进行误导。</p>
<hr>
<h1 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h1><p>理论上讲，TensorFlow工具可以单独写一本书，用法很多而且技巧性的东西也非常的复杂，这边我们主要作为工具进行使用，遇到新技巧会在code中解释，但不做全书的梳理，建议去买一本《TensorFlow实战Google深度学习框架》，简单易懂。</p>
<p>TensorFlow是谷歌于2015年11月9日正式开源的计算框架，由Jeff Dean领导的谷歌大脑团队改编的DistBelief得到的，在ImageNet2014、YouTube视频学习，语言识别错误率优化，街景识别，广告，电商等等都有了非常优秀的产出，是我个人非常喜欢的工具。</p>
<p>除此之外，我在列出一些其他的框架工具供读者使用：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-fbb0b53b95e3791e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d922490c0d06a489.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>接下来看一下最基本的语法，方便之后我们直接贴代码的时候可以轻松阅读。</p>
<ul>
<li><p>张量：可以理解为多维array 或者 list，time决定张量是什么<br>tf.placeholder(time,shape,name)</p>
</li>
<li><p>变量：同一时刻下的不变的数据<br>tf.Variable(value,name)</p>
</li>
<li><p>常量：永远不变的常值<br>tf.constant(value)</p>
</li>
<li><p>执行环境开启与关闭，在环境中才能运行TensorFlow语法<br>sess=tf.Session()<br>sess.close()<br>sess.run(op)</p>
</li>
<li><p>初始化所有权重：类似于变量申明<br>tf.initialize_all_variables()</p>
</li>
<li><p>更新权重：<br>tf.assign(variable_to_be_updated,new_value)</p>
</li>
<li><p>加值行为，利用feed_dict里面的值来训练[output]函数<br>sess.run([output],feed_dict={input1:value1,input2:value2})<br>利用input1，input2，来跑output的值</p>
</li>
<li><p>矩阵乘法，类似于dot<br>tf.matmul(input,layer1)</p>
</li>
<li><p>激活函数，relu<br>tf.nn.relu()，除此之外，还有tf.nn.sigmoid，tf.nn.tanh等等<br><img src="http://upload-images.jianshu.io/upload_images/1129359-f8b60db1e8df00b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
</li>
</ul>
<hr>
<h1 id="用户流失分析"><a href="#用户流失分析" class="headerlink" title="用户流失分析"></a>用户流失分析</h1><p>说了那么多前置的铺垫，让我们来真实的面对我们需要解决的问题：</p>
<p>首先，我们拿到了用户是否流失的历史数据集20724条，流失与飞流失用户占比在1:4，这部分数据需要进行一下预处理，这边就不细讲预处理过程了，包含缺失值填充(分层填充)，异常值处理(isolation foest)，数据平衡(tomek link)，特征选择(xgboost importance)，特征变形(normalizing)，特征分布优化等等，工程技巧我之前的文章都有讲解过，不做本文重点。</p>
<p><strong>taiking is cheap,show me the code.</strong><br><img src="http://upload-images.jianshu.io/upload_images/1129359-b533576543101323.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#author:shataowei</span><br><span class="line">#time:20170924</span><br><span class="line">#基础包加载</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import math</span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#数据处理</span><br><span class="line">data = pd.read_table(&apos;/Users/slade/Desktop/machine learning/data/data_all.txt&apos;)</span><br><span class="line">data = data.iloc[:,1:len(data.columns)]</span><br><span class="line">data1 = (data - data.mean())/data.std()</span><br><span class="line">labels = data[&apos;tag&apos;]</span><br><span class="line">items = data1.iloc[:,1:len(data1.columns)]</span><br><span class="line">all_data = pd.concat([pd.DataFrame(labels),items],axis = 1)</span><br><span class="line"></span><br><span class="line">#数据集切分成训练集和测试集，占比为0.8：0.2</span><br><span class="line">train_X,test_X,train_y,test_y = train_test_split(items,labels,test_size = 0.2,random_state = 0)</span><br><span class="line"></span><br><span class="line">#pandas读取进来是dataframe，转换为ndarray的形式</span><br><span class="line">train_X = np.array(train_X)</span><br><span class="line">test_X = np.array(test_X)</span><br><span class="line"></span><br><span class="line">#我将0或者1的预测结果转换成了[0,1]或者[1,0]的对应形式，读者可以不转</span><br><span class="line">train_Y = []</span><br><span class="line">for i in train_y:</span><br><span class="line">    if i ==0:</span><br><span class="line">        train_Y.append([0,1])</span><br><span class="line">    else:</span><br><span class="line">        train_Y.append([1,0])</span><br><span class="line">test_Y = []</span><br><span class="line">for i in test_y:</span><br><span class="line">    if i ==0:</span><br><span class="line">        test_Y.append([0,1])</span><br><span class="line">    else:</span><br><span class="line">        test_Y.append([1,0])</span><br></pre></td></tr></table></figure>
<p>下面我们就要开始正式开始训练神经网络了，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">input_node = 9 #输入的feature的个数，也就是input的维度</span><br><span class="line">output_node = 2 #输出的[0,1]或者[1,0]的维度</span><br><span class="line">layer1_node = 500 #隐藏层的节点个数，一般在255-1000之间，读者可以自行调整</span><br><span class="line">batch_size = 200 #批量训练的数据，batch_size越小训练时间越长，训练效果越准确（但会存在过拟合）</span><br><span class="line">learning_rate_base = 0.8 #训练weights的速率η</span><br><span class="line">regularzation_rate = 0.0001 #正则力度</span><br><span class="line">training_steps = 10000 #训练次数，这个指标需要类似grid_search进行搜索优化</span><br><span class="line"></span><br><span class="line">#设定之后想要被训练的x及对应的正式结果y_</span><br><span class="line">x = tf.placeholder(tf.float32,[None,input_node])</span><br><span class="line">y_ = tf.placeholder(tf.float32,[None,output_node])</span><br><span class="line"></span><br><span class="line">#input到layer1之间的线性矩阵weight</span><br><span class="line">weight1 = tf.Variable(tf.truncated_normal([input_node,layer1_node],stddev=0.1))</span><br><span class="line">#layer1到output之间的线性矩阵weight</span><br><span class="line">weight2 = tf.Variable(tf.truncated_normal([layer1_node,output_node],stddev=0.1))</span><br><span class="line">#input到layer1之间的线性矩阵的偏置</span><br><span class="line">biases1 = tf.Variable(tf.constant(0.1,shape = [layer1_node]))</span><br><span class="line">#layer1到output之间的线性矩阵的偏置</span><br><span class="line">biases2 = tf.Variable(tf.constant(0.1,shape=[output_node]))</span><br><span class="line"></span><br><span class="line">#正向传播的流程，线性计算及激活函数relu的非线性计算得到result</span><br><span class="line">def interence(input_tensor,weight1,weight2,biases1,biases2):</span><br><span class="line">    layer1 = tf.nn.relu(tf.matmul(input_tensor,weight1)+biases1)</span><br><span class="line">    result = tf.matmul(layer1,weight2)+biases2</span><br><span class="line">    return result</span><br><span class="line">y = interence(x,weight1,weight2,biases1,biases2)</span><br></pre></td></tr></table></figure></p>
<p>正向传播完成后，我们要反向传播来修正weight<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0,trainable = False)</span><br><span class="line">#交叉熵，用来衡量两个分布之间的相似程度</span><br><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = y_,logits=y)</span><br><span class="line">cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line"></span><br><span class="line">#l2正则化，这部分的理论分析可以参考我之前写的：http://www.jianshu.com/p/4f91f0dcba95</span><br><span class="line">regularzer = tf.contrib.layers.l2_regularizer(regularzation_rate)</span><br><span class="line">regularzation = regularzer(weight1) + regularzer(weight2)</span><br><span class="line"></span><br><span class="line">#损失函数为交叉熵+正则化</span><br><span class="line">loss = cross_entropy_mean + regularzation</span><br><span class="line"></span><br><span class="line">#我们用learning_rate_base作为速率η，来训练梯度下降的loss函数解</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(learning_rate_base).minimize(loss,global_step = global_step)</span><br><span class="line"></span><br><span class="line">#y是我们的预测值，y_是真实值，我们来找到y_及y(比如[0.1，0.2])中最大值对应的index位置，判断y与y_是否一致</span><br><span class="line">correction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))</span><br><span class="line"></span><br><span class="line">#如果y与y_一致则为1，否则为0，mean正好为其准确率</span><br><span class="line">accurary = tf.reduce_mean(tf.cast(correction,tf.float32))</span><br></pre></td></tr></table></figure></p>
<p>模型训练结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#初始化环境，设置输入值，检验值</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line">validate_feed = &#123;x:train_X,y_:train_Y&#125;</span><br><span class="line">test_feed = &#123;x:test_X,y_:test_Y&#125;</span><br><span class="line"></span><br><span class="line">#模型训练，每到1000次汇报一次训练效果</span><br><span class="line">for i in range(training_steps):</span><br><span class="line">    start = (i*batch_size)%len(train_X)</span><br><span class="line">    end = min(start+batch_size,16579)</span><br><span class="line">    xs = train_X[start:end]</span><br><span class="line">    ys = train_Y[start:end]</span><br><span class="line">    if i%1000 ==0:</span><br><span class="line">        validate_accuary = sess.run(accurary,feed_dict = validate_feed)</span><br><span class="line">        print &apos;the times of training is %d, and the accurary is %s&apos; %(i,validate_accuary)</span><br><span class="line">    sess.run(train_op,feed_dict = &#123;x:xs,y_:ys&#125;)</span><br></pre></td></tr></table></figure></p>
<p>训练的结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2017-09-24 12:11:28.409585: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">2017-09-24 12:11:28.409620: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">2017-09-24 12:11:28.409628: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">2017-09-24 12:11:28.409635: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">the times of training is 0, and the accurary is 0.736775</span><br><span class="line">the times of training is 1000, and the accurary is 0.99246</span><br><span class="line">the times of training is 2000, and the accurary is 0.993003</span><br><span class="line">the times of training is 3000, and the accurary is 0.992943</span><br><span class="line">the times of training is 4000, and the accurary is 0.992943</span><br><span class="line">the times of training is 5000, and the accurary is 0.99234</span><br><span class="line">the times of training is 6000, and the accurary is 0.993124</span><br><span class="line">the times of training is 7000, and the accurary is 0.992943</span><br><span class="line">the times of training is 8000, and the accurary is 0.993124</span><br><span class="line">the times of training is 9000, and the accurary is 0.992943</span><br></pre></td></tr></table></figure></p>
<p>初步看出，在训练集合上，准确率在能够99%以上，让我们在看看测试集效果<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_accuary = sess.run(accurary,feed_dict = test_feed)</span><br></pre></td></tr></table></figure></p>
<p><code>Out[5]: 0.99034983</code>,也是我们的测试数据集效果也是在99%附近，可以看出这个分类的效果还是比较高的。</p>
<p>初次之外，我们还可以得到每个值被预测出来的结果，也可以通过工程技巧转换为0-1的概率：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">result_y = sess.run(y,feed_dict=&#123;x:train_X&#125;)</span><br><span class="line">result_y_update=[]</span><br><span class="line">for i in result_y:</span><br><span class="line">    if i[0]&gt;=i[1]:</span><br><span class="line">        result_y_update.append([1,0])</span><br><span class="line">    else:</span><br><span class="line">        result_y_update.append([0,1])</span><br></pre></td></tr></table></figure></p>
<p>==&gt;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Out[7]:</span><br><span class="line">array([[-1.01412344,  1.21461654],</span><br><span class="line">       [-3.66026735,  3.81834102],</span><br><span class="line">       [-3.78952932,  3.79097509],</span><br><span class="line">       ...,</span><br><span class="line">       [-3.71239662,  3.65721083],</span><br><span class="line">       [-1.59250259,  1.89412308],</span><br><span class="line">       [-3.35591984,  3.24001145]], dtype=float32)</span><br></pre></td></tr></table></figure></p>
<p>以上就实现了如果用TensorFlow里面的神经网络技巧去做一个分类问题，其实这并不TensorFlow的全部，传统的Bp神经网络，SVM也可以到达近似的效果，在接下来的文章中，我们将继续看到比如CNN图像识别，LSTM进行文本分类，RNN训练不均衡数据等复杂问题上面的优势。</p>
<p>##可能存在的问题<br>在刚做神经网络的训练前，要注意一下是否会犯以下的错误。</p>
<ul>
<li><p>数据是否规范化<br>模型计算的过程时间长度及模型最后的效果，均依赖于input的形式。大部分的神经网络训练过程都是以input为1的标准差，0的均值为前提的；除此之外，在算梯度算反向传播的时候，过大的值有可能会导致梯度消失等意想不到的情况，非常值得大家注意</p>
</li>
<li><p>batch的选择<br>在上面我也提了，过小的batch会增加模型过拟合的风险，且计算的时间大大增加。过大的batch会造成模型的拟合能力不足，可能会被局部最小值卡住等等，所以需要多次选择并计算尝试。</p>
</li>
<li><p>过拟合的问题<br>是否在计算过程中只考虑了损失函数比如交叉熵，有没有考虑l2正则、l2正则，或者有没有进行dropout行为，是否有必要加入噪声，在什么地方加入噪声（weight？input？），需不需要结合Bagging或者bayes方法等</p>
</li>
<li><p>激活函数的选择是否正确<br>比如relu只能产出&gt;=0的结果，是否符合最后的产出结果要求。比如Sigmoid的函数在数据离散且均大于+3的数据集合上会产生梯度消失的问题，等等</p>
</li>
</ul>
<hr>
<p>到这里，我觉得一篇用TensorFlow来训练分类模型来解决用户流失这个问题就基本上算是梳理完了。很多简单的知识点我没有提，上面这些算是比较重要的模块，希望对大家有所帮助，最后谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CRM预估 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于SSD下的图像内容识别（二）]]></title>
      <url>/2017/12/01/%E5%9F%BA%E4%BA%8ESSD%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>上一节粗略的描述了如何关于图像识别，抠图，分类的理论相关，本节主要用代码，来和大家一起分析每一步骤。<br>看完本节，希望你也能独立完成自己的图片、视频的内容实时定位。</p>
<p>首先，我们需要安装TensorFlow环境，建议利用<a href="https://www.anaconda.com/downloads" target="_blank" rel="noopener">conda</a>进行安装，配置，90%尝试单独安装的人最后都挂了。</p>
<p>其次，我们需要安装从git上下载训练好的模型，git clone <a href="https://github.com/balancap/SSD-Tensorflow" target="_blank" rel="noopener">https://github.com/balancap/SSD-Tensorflow</a><br>如果没有安装git的朋友，请自行百度安装。</p>
<p>最后找到你下载的位置进行解压，unzip ./SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt.zip<br><strong>这边务必注意，网上90%的教程这边就结束了，其实你这样是最后跑不通代码的，你需要把解压的文件进行移动到checkpoint的文件夹下面</strong>，这个问题git上这个同学解释了，详细的去看下<a href="https://github.com/balancap/SSD-Tensorflow/issues/150" target="_blank" rel="noopener">https://github.com/balancap/SSD-Tensorflow/issues/150</a></p>
<p>最后的最后，下载你需要检测的网路图片，就ok了</p>
<p>预处理步骤完成了，下面让我们看代码。<br><strong>加载相关的包：</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> mpcm</span><br><span class="line">sys.path.append(<span class="string">'./SSD-Tensorflow/'</span>)</span><br><span class="line"><span class="keyword">from</span> nets <span class="keyword">import</span> ssd_vgg_300, ssd_common, np_methods</span><br><span class="line"><span class="keyword">from</span> preprocessing <span class="keyword">import</span> ssd_vgg_preprocessing</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>配置相关TensorFlow环境</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpu_options = tf.GPUOptions(allow_growth=<span class="keyword">True</span>)</span><br><span class="line">config = tf.ConfigProto(log_device_placement=<span class="keyword">False</span>, gpu_options=gpu_options)</span><br><span class="line">isess = tf.InteractiveSession(config=config)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>做图片的格式的处理，使他满足input的条件</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们用的TensorFlow下的一个集成包slim，比tensor要更加轻便</span></span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line"><span class="comment">#训练数据中包含了一下已知的类别，也就是我们可以识别出以下的东西，不过后续我们将自己自己训练自己的模型，来识别自己想识别的东西</span></span><br><span class="line">l_VOC_CLASS = [</span><br><span class="line">                <span class="string">'aeroplane'</span>,   <span class="string">'bicycle'</span>, <span class="string">'bird'</span>,  <span class="string">'boat'</span>,      <span class="string">'bottle'</span>,</span><br><span class="line">                <span class="string">'bus'</span>,         <span class="string">'car'</span>,     <span class="string">'cat'</span>,   <span class="string">'chair'</span>,     <span class="string">'cow'</span>,</span><br><span class="line">                <span class="string">'diningTable'</span>, <span class="string">'dog'</span>,     <span class="string">'horse'</span>, <span class="string">'motorbike'</span>, <span class="string">'person'</span>,</span><br><span class="line">                <span class="string">'pottedPlant'</span>, <span class="string">'sheep'</span>,   <span class="string">'sofa'</span>,  <span class="string">'train'</span>,     <span class="string">'TV'</span></span><br><span class="line">]</span><br><span class="line"><span class="comment"># 定义数据格式</span></span><br><span class="line">net_shape = (<span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">data_format = <span class="string">'NHWC'</span>  <span class="comment"># [Number, height, width, color]，Tensorflow backend 的格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理将输入图片大小改成 300x300，作为下一步输入</span></span><br><span class="line">img_input = tf.placeholder(tf.uint8, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">3</span>))</span><br><span class="line">image_pre, labels_pre, bboxes_pre, bbox_img = ssd_vgg_preprocessing.preprocess_for_eval(</span><br><span class="line">    img_input,</span><br><span class="line">    <span class="keyword">None</span>,</span><br><span class="line">    <span class="keyword">None</span>,</span><br><span class="line">    net_shape,</span><br><span class="line">    data_format,</span><br><span class="line">    resize=ssd_vgg_preprocessing.Resize.WARP_RESIZE</span><br><span class="line">)</span><br><span class="line">image_4d = tf.expand_dims(image_pre, <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>下面我们来载入SSD作者已经搞定的模型</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 SSD 模型结构</span></span><br><span class="line">reuse = <span class="keyword">True</span> <span class="keyword">if</span> <span class="string">'ssd_net'</span> <span class="keyword">in</span> locals() <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line">ssd_net = ssd_vgg_300.SSDNet()</span><br><span class="line"><span class="keyword">with</span> slim.arg_scope(ssd_net.arg_scope(data_format=data_format)):</span><br><span class="line">    predictions, localisations, _, _ = ssd_net.net(image_4d, is_training=<span class="keyword">False</span>, reuse=reuse)</span><br><span class="line"><span class="comment"># 导入官方给出的 SSD 模型参数</span></span><br><span class="line"><span class="comment">#这边修改成你自己的路径</span></span><br><span class="line">ckpt_filename = <span class="string">'/Users/slade/SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt'</span></span><br><span class="line">isess.run(tf.global_variables_initializer())</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.restore(isess, ckpt_filename)</span><br><span class="line">ssd_anchors = ssd_net.anchors(net_shape)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>下面让我们把SSD识别出来的结果在图片中表示出来</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不同类别，我们以不同的颜色表示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colors_subselect</span><span class="params">(colors, num_classes=<span class="number">21</span>)</span>:</span></span><br><span class="line">    dt = len(colors) // num_classes</span><br><span class="line">    sub_colors = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_classes):</span><br><span class="line">        color = colors[I*dt]</span><br><span class="line">        <span class="keyword">if</span> isinstance(color[<span class="number">0</span>], float):</span><br><span class="line">            sub_colors.append([int(c * <span class="number">255</span>) <span class="keyword">for</span> c <span class="keyword">in</span> color])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sub_colors.append([c <span class="keyword">for</span> c <span class="keyword">in</span> color])</span><br><span class="line">    <span class="keyword">return</span> sub_colors</span><br><span class="line"><span class="comment">#画出在图中的位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bboxes_draw_on_img</span><span class="params">(img, classes, scores, bboxes, colors, thickness=<span class="number">5</span>)</span>:</span></span><br><span class="line">    shape = img.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(bboxes.shape[<span class="number">0</span>]):</span><br><span class="line">        bbox = bboxes[I]</span><br><span class="line">        color = colors[classes[I]]</span><br><span class="line">        <span class="comment"># Draw bounding box...</span></span><br><span class="line">        p1 = (int(bbox[<span class="number">0</span>] * shape[<span class="number">0</span>]), int(bbox[<span class="number">1</span>] * shape[<span class="number">1</span>]))</span><br><span class="line">        p2 = (int(bbox[<span class="number">2</span>] * shape[<span class="number">0</span>]), int(bbox[<span class="number">3</span>] * shape[<span class="number">1</span>]))</span><br><span class="line">        cv2.rectangle(img, p1[::<span class="number">-1</span>], p2[::<span class="number">-1</span>], color, thickness)</span><br><span class="line">        <span class="comment"># Draw text...</span></span><br><span class="line">        s = <span class="string">'%s:%.3f'</span> % ( l_VOC_CLASS[int(classes[i])<span class="number">-1</span>], scores[I])</span><br><span class="line">        p1 = (p1[<span class="number">0</span>]<span class="number">-5</span>, p1[<span class="number">1</span>])</span><br><span class="line">        cv2.putText(img, s, p1[::<span class="number">-1</span>], cv2.FONT_HERSHEY_SIMPLEX, <span class="number">1</span>, color, <span class="number">2</span>)</span><br><span class="line">colors_plasma = colors_subselect(mpcm.plasma.colors, num_classes=<span class="number">21</span>)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>让我们开始训练吧</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_image</span><span class="params">(img, select_threshold=<span class="number">0.3</span>, nms_threshold=<span class="number">.8</span>, net_shape=<span class="params">(<span class="number">300</span>, <span class="number">300</span>)</span>)</span>:</span></span><br><span class="line">    <span class="comment">#先获取SSD网络的层相关的参数</span></span><br><span class="line">    rimg, rpredictions, rlocalisations, rbbox_img = isess.run([image_4d, predictions, localisations, bbox_img],</span><br><span class="line">                                                              feed_dict=&#123;img_input: img&#125;)</span><br><span class="line">    <span class="comment">#获取分类结果，位置</span></span><br><span class="line">    rclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(</span><br><span class="line">            rpredictions, rlocalisations, ssd_anchors,</span><br><span class="line">            select_threshold=select_threshold, img_shape=net_shape, num_classes=<span class="number">21</span>, decode=<span class="keyword">True</span>)</span><br><span class="line">    rbboxes = np_methods.bboxes_clip(rbbox_img, rbboxes)</span><br><span class="line">    rclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=<span class="number">400</span>)</span><br><span class="line">    rclasses, rscores, rbboxes = np_methods.bboxes_nms(rclasses, rscores, rbboxes, nms_threshold=nms_threshold)</span><br><span class="line">    <span class="comment"># 让我们在图中画出来就行了</span></span><br><span class="line">    rbboxes = np_methods.bboxes_resize(rbbox_img, rbboxes)</span><br><span class="line">    bboxes_draw_on_img(img, rclasses, rscores, rbboxes, colors_plasma, thickness=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>预处理的函数都写完了，我们就可以执行了。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">img = cv2.imread(<span class="string">"/Users/slade/Documents/Yoho/picture_recognize/test7.jpg"</span>)</span><br><span class="line">img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">plt.imshow(process_image(img))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>img的数据形式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">In [8]: img</span><br><span class="line">Out[8]:</span><br><span class="line">array([[[ 35,  59,  43],</span><br><span class="line">        [ 37,  60,  44],</span><br><span class="line">        [ 38,  61,  45],</span><br><span class="line">        ...,</span><br><span class="line">        [ 73,  99,  62],</span><br><span class="line">        [ 74,  99,  60],</span><br><span class="line">        [ 72,  97,  57]],</span><br><span class="line"></span><br><span class="line">       [[ 37,  60,  44],</span><br><span class="line">        [ 37,  60,  44],</span><br><span class="line">        [ 37,  60,  44],</span><br><span class="line">        ...,</span><br><span class="line">        [ 66,  92,  57],</span><br><span class="line">        [ 67,  93,  56],</span><br><span class="line">        [ 67,  92,  53]],</span><br><span class="line"></span><br><span class="line">       [[ 37,  60,  44],</span><br><span class="line">        [ 36,  59,  43],</span><br><span class="line">        [ 37,  58,  43],</span><br><span class="line">        ...,</span><br><span class="line">        [ 56,  83,  48],</span><br><span class="line">        [ 60,  86,  51],</span><br><span class="line">        [ 61,  87,  50]],</span><br><span class="line"></span><br><span class="line">       ...,</span><br><span class="line">       [[ 96, 101,  95],</span><br><span class="line">        [107, 109, 104],</span><br><span class="line">        [ 98,  97,  95],</span><br><span class="line">        ...,</span><br><span class="line">        [ 84, 126,  76],</span><br><span class="line">        [ 72, 118,  72],</span><br><span class="line">        [ 78, 126,  86]],</span><br><span class="line"></span><br><span class="line">       [[ 98, 103,  96],</span><br><span class="line">        [114, 116, 111],</span><br><span class="line">        [112, 113, 108],</span><br><span class="line">        ...,</span><br><span class="line">        [ 94, 137,  84],</span><br><span class="line">        [ 87, 133,  86],</span><br><span class="line">        [105, 153, 111]],</span><br><span class="line"></span><br><span class="line">       [[ 99, 105,  95],</span><br><span class="line">        [110, 113, 106],</span><br><span class="line">        [134, 135, 129],</span><br><span class="line">        ...,</span><br><span class="line">        [127, 170, 116],</span><br><span class="line">        [121, 167, 118],</span><br><span class="line">        [131, 180, 135]]], dtype=uint8)</span><br></pre></td></tr></table></figure></p>
<p>处理后的结果如下：<img src="http://upload-images.jianshu.io/upload_images/1129359-203d6cf8cdd0268f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>是不是非常无脑，上面的代码直接复制就可以完成。</p>
<p>下面在拓展一下视频的处理方式，其实相关的内容是一致的。<br>利用moviepy.editor包里面的VideoFileClip的切片的功能，然后对每一次切片的结果进行process_image过程就可以了，这边就不贴代码了，需要的朋友私密我。</p>
<p>最后感谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 图像识别 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于SSD下的图像内容识别（一）]]></title>
      <url>/2017/12/01/%E5%9F%BA%E4%BA%8ESSD%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      <content type="html"><![CDATA[<p><strong>鸽了将近有一个月的时间没有更新东西，真的不是因为我懒，主要在忙一些工作上的事情，然后就是被安装caffe环境折磨的死去活来。我本来用的上mba来搭caffe环境的，一直在报一个框架问题，索性一怒之下换了mbp，下面就将我在SSD学习过程中遇到的问题和大家一起分享一下。</strong></p>
<h1 id="首先，先看一下我们能达到什么样的效果："><a href="#首先，先看一下我们能达到什么样的效果：" class="headerlink" title="首先，先看一下我们能达到什么样的效果："></a>首先，先看一下我们能达到什么样的效果：</h1><p>比如,这样的：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-89e17d5f358de829.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>再比如这样的：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-7835ea2af73f2f3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>甚至还可以这样：<br><a href="https://v.qq.com/x/page/a0567wd27jz.html" target="_blank" rel="noopener">https://v.qq.com/x/page/a0567wd27jz.html</a><br><a href="https://v.qq.com/x/page/j05679xhryx.html" target="_blank" rel="noopener">https://v.qq.com/x/page/j05679xhryx.html</a><br>这边吐槽一下，简书makedown不支持上传视频，简直差评！</p>
<p>那问题来了，在真实的业务场景中，我们有哪些应用呢？<br><strong>比如天猫的拍照购：</strong><br><img src="http://upload-images.jianshu.io/upload_images/1129359-3925472b3a407991.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>有货的相似推荐：</strong><br><img src="http://upload-images.jianshu.io/upload_images/1129359-bc6c5db0baae11a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这些都是非常优秀的应用场景。</p>
<h1 id="我们需要做哪些基本的步骤："><a href="#我们需要做哪些基本的步骤：" class="headerlink" title="我们需要做哪些基本的步骤："></a>我们需要做哪些基本的步骤：</h1><h2 id="抠出图片中关键的人或者物"><a href="#抠出图片中关键的人或者物" class="headerlink" title="抠出图片中关键的人或者物"></a>抠出图片中关键的人或者物</h2><p>如果只需要抠出图片中的核心信息的话，其实只需要加载python里面的selectivesearch包就可以（这里多说一句，建议都是使用conda安装所有包和库，不然你会后悔的）。<br>我们先来看下效果：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-a9175cef945ce8f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这个是怎么实现的呢？<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  cv2</span><br><span class="line"><span class="keyword">import</span> selectivesearch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span>  plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span>  mpatches</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span>  np</span><br><span class="line">img = cv2.imread(<span class="string">'/Users/slade/Documents/Yoho/picture_recognize/heshen.jpg'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#图片识别分割</span></span><br><span class="line">img_lbl, regions =selectivesearch.selective_search(</span><br><span class="line">    img, scale=<span class="number">500</span>, sigma=<span class="number">0.9</span>, min_size=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#这边的regions里面就有一个个划分出来的box</span></span><br><span class="line">regions[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#Out[3]: &#123;'labels': [0.0], 'rect': (0, 0, 619, 620), 'size': 177325&#125;,其中‘rect’定位了box的位置，‘size’确定了box的大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来我们把窗口和图像打印出来，对它有个直观认识</span></span><br><span class="line">fig, ax = plt.subplots(ncols=<span class="number">1</span>, nrows=<span class="number">1</span>, figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line">ax.imshow(img)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> reg <span class="keyword">in</span> regions:</span><br><span class="line">    x, y, w, h = reg[<span class="string">'rect'</span>]</span><br><span class="line">    rect = mpatches.Rectangle(</span><br><span class="line">        (x, y), w, h, fill=<span class="keyword">False</span>, edgecolor=<span class="string">'red'</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">    ax.add_patch(rect)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>搜索完成后展示图：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-7ef04cafcafbb603.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>很明显，这里面的方框太多了，所以我们需要把一些过小的，过大的，不规则的全部去掉：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">candidates = []</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> regions:</span><br><span class="line">    <span class="comment"># 重复的不要</span></span><br><span class="line">    <span class="keyword">if</span> r[<span class="string">'rect'</span>] <span class="keyword">in</span> candidates:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># 太小和太大的不要</span></span><br><span class="line">    <span class="keyword">if</span> r[<span class="string">'size'</span>] &lt; <span class="number">200</span> <span class="keyword">or</span> r[<span class="string">'size'</span>]&gt;<span class="number">20000</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    x, y, w, h = r[<span class="string">'rect'</span>]</span><br><span class="line">    <span class="comment"># 太不方的不要</span></span><br><span class="line">    <span class="keyword">if</span> w / h &gt; <span class="number">1.8</span> <span class="keyword">or</span> h / w &gt; <span class="number">1.8</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    candidates.append((x,y,w,h))</span><br><span class="line"></span><br><span class="line"><span class="comment">#剔除大box内的小box</span></span><br><span class="line">candidates_sec = []</span><br><span class="line"><span class="keyword">for</span>  i  <span class="keyword">in</span>  candidates:</span><br><span class="line">    <span class="keyword">if</span> len(candidates_sec)==<span class="number">0</span>:</span><br><span class="line">        candidates_sec.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Flag=<span class="keyword">False</span></span><br><span class="line">        replace=<span class="number">-1</span></span><br><span class="line">        index=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j  <span class="keyword">in</span> candidates_sec:</span><br><span class="line">            <span class="comment">##新box在小圈 则删除</span></span><br><span class="line">            <span class="keyword">if</span> i[<span class="number">0</span>]&gt;=j[<span class="number">0</span>] <span class="keyword">and</span> i[<span class="number">0</span>]+i[<span class="number">2</span>]&lt;=j[<span class="number">0</span>]+j[<span class="number">2</span>]  <span class="keyword">and</span> i[<span class="number">1</span>]&gt;=j[<span class="number">1</span>] <span class="keyword">and</span> i[<span class="number">1</span>]+i[<span class="number">3</span>]&lt;=j[<span class="number">1</span>]+j[<span class="number">3</span>]:</span><br><span class="line">                Flag=<span class="keyword">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment">##新box不在小圈 而在老box外部 替换老box</span></span><br><span class="line">            <span class="keyword">elif</span> i[<span class="number">0</span>]&lt;=j[<span class="number">0</span>] <span class="keyword">and</span> i[<span class="number">0</span>]+i[<span class="number">2</span>]&gt;=j[<span class="number">0</span>]+j[<span class="number">2</span>] <span class="keyword">and</span> i[<span class="number">1</span>]&lt;=j[<span class="number">1</span>] <span class="keyword">and</span> i[<span class="number">1</span>]+i[<span class="number">3</span>]&gt;=j[<span class="number">1</span>]+j[<span class="number">3</span>]:</span><br><span class="line">                replace=index</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            index+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> Flag:</span><br><span class="line">            <span class="keyword">if</span> replace&gt;=<span class="number">0</span>:</span><br><span class="line">                candidates_sec[replace]=i</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                candidates_sec.append(i)</span><br></pre></td></tr></table></figure></p>
<p>然后我们看看更新完后的图片效果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))</span><br><span class="line">ax.imshow(img)</span><br><span class="line">for x, y, w, h in candidates_sec:</span><br><span class="line">    rect = mpatches.Rectangle(</span><br><span class="line">        (x, y), w, h, fill=False, edgecolor=&apos;red&apos;, linewidth=1)</span><br><span class="line">    ax.add_patch(rect)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-f30fdce8924c65b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在根据重复优化一下，就可以得到最初的那张图片，基本上来说，就可以完成抠图这个事情了。</p>
<h2 id="相关理论概述："><a href="#相关理论概述：" class="headerlink" title="相关理论概述："></a>相关理论概述：</h2><p>上面这样的识别从数学角度上是怎么样实现的呢？<br>这边先引入一篇文章：2014年CVPR上的经典paper：《Rich feature hierarchies for Accurate Object Detection and Segmentation》，这篇文章的算法思想又被称之为：R-CNN（Regions with Convolutional Neural Network Features），是物体检测领域曾经获得state-of-art精度的经典文献。<br>论文较为复杂冗长，我这边主要先看一下我们关系的抠图模块：</p>
<h3 id="抠若干个box过程："><a href="#抠若干个box过程：" class="headerlink" title="抠若干个box过程："></a>抠若干个box过程：</h3><p>先切分图片到若干子区域的集合S<br>1.在计算集合S中找出相似性最大的区域max_similarity{ri,rj}<br>2.合并S_new=ri∪rj<br>3.从S集合中，移走所有与ri,rj的子集<br>4.将新集合S_new与相邻区域的相似度<br>5.repeat step2<br>直到S集合为空<br>这边相似度的计算考虑了三个方面：颜色相似，纹理相似，空间交错相似，分别解释如下：<br><strong>颜色相似：</strong><br><img src="http://upload-images.jianshu.io/upload_images/1129359-b9bcde646bff86a1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>其中对每个区域，我们都可以得到一个一维的颜色分布直方图：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-68604dd713f5f562.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>假设两个直方图波峰和波谷高度重合，那么计算下来的值比较大；反之如果波峰和波谷错开的，那么累加的值一定比较小。</p>
<p><strong>纹理相似：</strong><br>这边会用到<a href="http://www.cnblogs.com/saintbird/archive/2008/08/20/1271943.html" target="_blank" rel="noopener">SIFI算法</a>，也是一个比较经典的算法。<br>selectivesearch论文采用方差为1的高斯分布在8个方向做梯度统计，然后将统计结果（尺寸与区域大小一致）以bins=10计算直方图。（这个我也没有仔细去看，只是skip learn了一下）</p>
<p><strong>空间相似：</strong><br>这个最简单，代码呈现了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sim_fill</span><span class="params">(r1, r2, imsize)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        calculate the fill_similarity over the image</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    bbsize = (</span><br><span class="line">        (max(r1[<span class="string">"max_x"</span>], r2[<span class="string">"max_x"</span>]) - min(r1[<span class="string">"min_x"</span>], r2[<span class="string">"min_x"</span>]))</span><br><span class="line">        * (max(r1[<span class="string">"max_y"</span>], r2[<span class="string">"max_y"</span>]) - min(r1[<span class="string">"min_y"</span>], r2[<span class="string">"min_y"</span>]))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> - (bbsize - r1[<span class="string">"size"</span>] - r2[<span class="string">"size"</span>]) / imsize</span><br></pre></td></tr></table></figure></p>
<h3 id="若干个box筛选的过程："><a href="#若干个box筛选的过程：" class="headerlink" title="若干个box筛选的过程："></a>若干个box筛选的过程：</h3><p>首先，我们定义：IOU为两个bounding box的重叠度，如下图所示：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-fd7926f20f8eaac2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>矩形框A、B的一个重合度IOU计算公式为：<br>IOU=(A∩B)/(A∪B)<br>就是矩形框A、B的重叠面积占A、B并集的面积比例:<br>IOU=SI/(SA+SB-SI)</p>
<p>再引入非极大值抑制（NMS）概念：抑制不是极大值的元素，搜索局部的极大值。<br>翻译一下就是：比如之前看的这张图片：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-f30fdce8924c65b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>很明显还是很多box是相互inner的，虽然没有被相互包含进去，我们可以先选择最大的box，看其他box与这个最大的box的IOU值，删除IOU值大于预先设定的阈值的box，重复这个过程就是一个方框删除的过程。真实的NMS还会涉及到canny detection等等的细节问题，这边只是让大家快速入门使用起来，如需详细了解，请自行百度。</p>
<h3 id="若干个box内容对应："><a href="#若干个box内容对应：" class="headerlink" title="若干个box内容对应："></a>若干个box内容对应：</h3><p>我们虽然识别出了方框内存在物体，但是我们仍需要将物体与标签对应起来，这边的方法就是很多了，RCNN里面的方法：SVM，还有现在非常热门的CNN都可以对识别出来的子图片进行识别分类：<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/index.html" target="_blank" rel="noopener">VOC物体检测任务</a>是一个非常入门的分类问题。你可以通过任何一种你觉得可以的分类方法进行识别训练。</p>
<p>简单的流程化的识别拆分讲解这边就结束了，主要讲了candidates_boxs的产生，candidates_boxs通过基本属性的初筛，candidates_boxs根据IOU原则下的NMS进行复选，再将复选出来的box根据你已经训练好的分类模型确定到底是啥？<br>可以用下面这个图概述一下：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-3eb665a372aa4089.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="To-do"><a href="#To-do" class="headerlink" title="To-do:"></a>To-do:</h1><p>我们还有很多没讲完的，后面会持续更新：<br>主要包括：<br>1.如何配置一个快速训练的环境？<br>2.如何实现（输入图片，产出结果）整套识别流程？<br>3.如何自己训练一个图片分类器？<br>4.如何做快速迁移一个自己需要的及时图片识别流？</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 图像识别 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[动态最优化经典面试题]]></title>
      <url>/2017/12/01/%E5%8A%A8%E6%80%81%E6%9C%80%E4%BC%98%E5%8C%96%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>最近看到了一条史前的算法面试题，觉得挺有意思的，虽然网上已经有了很多完善的答案，但是我还是想自己整理一遍，强化印象，同时也和大家分享一下这道12年的Google题目：</p>
<p><strong>一幢 200 层的大楼，给你两个鸡蛋。如果在第 n 层扔下鸡蛋，鸡蛋不碎，那么从第 n-1 层扔鸡蛋，都不碎。这两只鸡蛋一模一样，不碎的话可以扔无数次。最高从哪层楼扔下时鸡蛋不会碎？</strong></p>
<p>先形象的理解一下这道题目，假设第一个蛋我们放在了i层，有两种case，碎或者不碎。<br>先看简单的结果，<br>case1:如果碎了，为了求出层数，那么我接下来的那颗蛋需要从第1层开始尝试i-1次，因为我们不允许冒第二次碎的风险了，这很好理解。<br>case2:如果没碎，我们得知一条新的信息，那就是我们要求的目标层在i层之上，但是我们依旧不知道是哪一层，假设是m层（m&gt;i）,那么同样的，和第i层一样，面临2个case，碎或者不碎。</p>
<p>这时候，我们的前提是在最恶劣的情况下，保证我们的每次的风险都尽可能的小，至少要少于上一次的风险。</p>
<p>我们可以让新的m的高度为i+i-1,其中，i是第一次我们放的层数，i-1是我们选择的风险若于第一次风险的层数高度，类推下去：i+i-1+i-2+i-3+…+1=200，得到i=20，就是我们第一次应该放的位置，同理第二次如果没有碎应该放的就是39…</p>
<p><strong>我个人对这道题目的理解中，其实就为了平分风险，让每次碎的高度都相等，也就是i-1 = m-i-1+1==&gt;m=2i-1</strong></p>
<p>这边的python代码网上也有很多，这边我罗列一个我写的，可能和别人的不一样，实现效率也可能较慢，建议大家在网上搜完善版本的，仅供大家熟悉上述的描述：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#n为层数，m为蛋数，f函数为求最优层数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(n, m)</span>:</span></span><br><span class="line"><span class="comment">#如果是0层的，返回最优层数为0</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment">#如果只有一个鸡蛋，必须要从最低层开始试，所以为当前最安全层n</span></span><br><span class="line">    <span class="keyword">if</span> m == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line"><span class="comment">#这边我们来看，f(i - 1, m - 1)是如果i层碎了，我们需要计算i-1层下的情况，同时减少一颗蛋；</span></span><br><span class="line"><span class="comment">#f(n - i, m)是i层没碎，那相当于安全层从0变成了n，要计算的就是相当于有 f(n, m)变成了 f(n - i, m)</span></span><br><span class="line"><span class="comment">#最后在最大化风险下找出其中风险最小的层数即可</span></span><br><span class="line">    best_floor = min([max([f(i - <span class="number">1</span>, m - <span class="number">1</span>), f(n - i, m)]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>)] + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> best_floor</span><br></pre></td></tr></table></figure></p>
<p>结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]:print(f(<span class="number">100</span>, <span class="number">2</span>))</span><br><span class="line"><span class="number">14</span></span><br></pre></td></tr></table></figure></p>
<p>这边f(200,2)实在没跑出来，时间太久了，所以跑了100，2的结果，迭代次数超多，具体我没有算过，建议优化一下计算的代码再执行。</p>
<p>最后谢谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[用户生命周期]]></title>
      <url>/2017/11/02/%E7%94%A8%E6%88%B7%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</url>
      <content type="html"><![CDATA[<blockquote>
<p>摘要：设计一套完整的用户生命周期策略，极大程度上会提高用户活跃，降低用户流失，反应用户留存，为平台运营的不可或缺的一环</p>
</blockquote>
<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>用户生命周期是指用户从加入平台开始，熟悉平台，参与平台，最终流失的整个过程。用户的生命周期相对于自身而言，是一种参与度的变化，参与度也可以称之为活跃度。</p>
<hr>
<p>###如何定义参与度？<br>以电商平台而言，冒泡（打开app），浏览，点击，搜索，收藏，加购物车，下单，评论等都是用户参与平台的主要行为，综合考虑（但不限于此）这些因素，</p>
<p><strong>活跃度：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">θ = α1* pv + α2 * click + α3* search + α4 * clc + α5* cart + α6* order + α7* comment + bubble</span><br></pre></td></tr></table></figure></p>
<p> <em>其中，θ是活跃度，pv是用户浏览量，click是用户点击量，search是用户搜索量，clc是用户收藏量，cart是用户加购物车次，order为用户订单量，comment为用户评论量<br>α1 为全部用户冒泡次数 与 全部用户浏览量之比；<br>α2 为全部用户冒泡次数 与 全部用户点击量之比；<br>…
</em><br><strong>这样保证了，所有平台参与行为与用户活跃情况成正相关，同时动态变化的降低了操作成本低的变量的权重，也满足奥卡姆剃刀原理</strong><br>后续再利用活跃度来直接衡量生命周期状态。</p>
<h1 id="如何定义生命周期？"><a href="#如何定义生命周期？" class="headerlink" title="如何定义生命周期？"></a>如何定义生命周期？</h1><ul>
<li>以电商平台为例，考虑用户的行为，先来定义生命周期状态划分逻辑：<br>1.计算用户连续N(N&gt;3)个周期内的参与度组成特征向量<br>2.形成不同生命周期下的模式特征向量<br>3.分类用户的特征向量如下：</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">生命周期状态</th>
<th style="text-align:center">生命周期类型</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">初生期</td>
<td style="text-align:center">新增用户</td>
<td style="text-align:center">处于新生用户没有用户记录</td>
</tr>
<tr>
<td style="text-align:center">成长期</td>
<td style="text-align:center">显性成长</td>
<td style="text-align:center">最近三次生命周期状态都是成长期</td>
</tr>
<tr>
<td style="text-align:center">成长期</td>
<td style="text-align:center">隐性成长</td>
<td style="text-align:center">最近三次生命周期状态不全是成长期</td>
</tr>
<tr>
<td style="text-align:center">稳定期</td>
<td style="text-align:center">低稳定</td>
<td style="text-align:center">处于平稳期阶段，参与度低于1/4分位数</td>
</tr>
<tr>
<td style="text-align:center">稳定期</td>
<td style="text-align:center">中稳定</td>
<td style="text-align:center">处于平稳期阶段，参与度介于1/4-3/4分位数</td>
</tr>
<tr>
<td style="text-align:center">稳定期</td>
<td style="text-align:center">高稳定</td>
<td style="text-align:center">处于平稳期阶段，参与度高于3/4分位数</td>
</tr>
<tr>
<td style="text-align:center">衰退期</td>
<td style="text-align:center">轻微衰退</td>
<td style="text-align:center">连续x个周期进入衰退期或流失期</td>
</tr>
<tr>
<td style="text-align:center">衰退期</td>
<td style="text-align:center">重度微衰退</td>
<td style="text-align:center">连续x个周期进入衰退期或流失期</td>
</tr>
<tr>
<td style="text-align:center">流失期</td>
<td style="text-align:center">流失期</td>
<td style="text-align:center">刚进入流失期</td>
</tr>
<tr>
<td style="text-align:center">沉默期</td>
<td style="text-align:center">沉默期</td>
<td style="text-align:center">长期处于流失期</td>
</tr>
</tbody>
</table>
<ul>
<li>定义完整的用户生命周期状态后，再对用户的生命周期做session切分，根据聚类算法，将样本用户进行聚类，形成聚类中心，判断用户距离聚类中心距离，匹配用户所处的生命周期详细位置，反过来输出分位数，判断用户生命周期类型。</li>
</ul>
<h1 id="下面思考如何优化kmeans解决这个问题："><a href="#下面思考如何优化kmeans解决这个问题：" class="headerlink" title="下面思考如何优化kmeans解决这个问题："></a>下面思考如何优化kmeans解决这个问题：</h1><p>考虑到业务开发的效率等原因，常规的聚类算法中，kmeans常常为优先考虑的算法，但实际运用过程中，需要根据不同的问题有差异化的优化。</p>
<p>1.考虑用户的特征偏移<br>可能存在用户的活跃属性间断，比如用户外出出差一周，导致某个单位统计时间内平台参与度下降，用户的活跃属性下降，而实际用户为真实高活跃用户，只是出现异常间断点，影响用户活跃的最终判断，利用语义分析中的最佳路径计算方式解决这个问题。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-b66a4221b9680612.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这三条线中，蓝色和青色线的分布走势类似，而红色线条的差异较大；计算蓝色–&gt;红色的欧式距离，蓝色–&gt;青色的欧式距离，发现蓝色–&gt;青色的欧式距离反而大于蓝色–&gt;红色的欧式距离，时间波动的情况下，欧式距离偏差较大。</p>
<p>所以，常规意义上的kmeans等基于欧式距离的算法这种情况下，使用较为局限。所以在整体思路不变的情况下，就距离计算，我们可以参考语音分析里面的DP（最佳路径规划算法），构造邻接矩阵，寻找最小最小路径和</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-fdef42649ff582a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>实际在计算蓝色曲线到青色曲线的距离的时候，同时计算AB（蓝色曲线当前位置A点到前一个时间段青色曲线位置B）、AC（蓝色曲线当前位置A点到当前时间段青色曲线位置C），AD（蓝色曲线当前位置A点到后一个时间段青色曲线位置D）的距离，综合判断一个点最短路径；再根据曲线上的每一个点，会形成一个矩阵，判断矩阵的每个点的最佳路径即可</p>
<p>可以用如下的公式表述：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-f5ba88f80a316047.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>其中，<img src="http://upload-images.jianshu.io/upload_images/1129359-f5e126daccf8ed96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">就是路径选择的过程</p>
<p>以上述的计算方式替换掉常规的kmeans中的欧式距离，提高了相似度的计算精度。</p>
<p>2.常规等距划分session不适用于生命周期</p>
<p>就用户平台活跃而言，不同用户可采用的用户时间窗口不同，新加入的用户可能可获取的时间长度较短；用户判断过程中的session与平台确定已知的生命周期session固定判断长度也是不相同的。同时，kmeans中的距离判断方法不能同时考虑到不同session下的距离计算问题</p>
<p><strong>最简单常规的计算方式：</strong><br>是补全较短的session的时间窗口，在相同的时间窗口之下，再去计算较短的时间窗口与较长的时间窗口下的生命周期的均值，这样会人为干涉过多，数据质量较低，图b即为数据补齐</p>
<p><strong>“STS距离”计算方式：</strong><br>在长时间窗口{r}集合中，寻找时间窗口长度子集，使得子集中的元长度与s曲线缺失的长度一致，在以s断点处开始向后寻找{r}子集合中的所有满足的元，再以均值时间序列替换原来的子集中的元作为r和s的拟合曲线，循环往复计算中心曲线2，如图c</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-96dabb79daa143a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有了补齐长度下的中心曲线，再便可采用kmeans的常规方式，计算各时间长度窗口下的生命周期的距离</p>
<p>3.附加限制属性<br>再最后落地生命周期的长度的时候，考虑到商品平台的特殊属性，比如：</p>
<ul>
<li>商品周期性（奶粉用户周期购买等）</li>
<li>用户偏好属性(酒店用户品质偏好等）</li>
<li>平台的时间依赖情况(夏季冬季季节偏好等）</li>
<li>……<br>以上即为如何通过kmeans来确定一个用户所属的生命周期阶段</li>
</ul>
<p><em>本文参考文献如下：<br>1.<a href="http://www.doc88.com/p-0092455469704.html" target="_blank" rel="noopener">不等长时间序列下的滑窗相似度</a><br>2.<a href="http://www.jianshu.com/p/1417fcb06797" target="_blank" rel="noopener">kmeans距离计算方式剖析</a>
</em></p>
]]></content>
      
        <categories>
            
            <category> 特征刻画 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 生命周期 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[交叉销售算法]]></title>
      <url>/2017/11/01/%E4%BA%A4%E5%8F%89%E9%94%80%E5%94%AE%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p><img src="http://upload-images.jianshu.io/upload_images/1129359-b283d67baa29f175.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>最近做了一个交叉销售的项目，梳理了一些关键点，分享如下，希望对大家有所启发<br>核心目标：在有限资源下，尽可能的提供高转化率的用户群，辅助业务增长<br>初步效果：商家ROI值为50以上，用户日转化率提升10倍以上，用户日最低转化效果5pp以上<br>以下为正文：<br>数据准备：<br>1.商品相关性<br>存在商品A,B,C…，商品之间用户会存在行为信息的关联度，这边可以参考协调过滤算法中的Item-based，这边拓展为用户在不同商品之间的操作行为的差异性。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-89aab00daf12957b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以形成如下的特征矩阵：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-2bb3bfc9f61d9511.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这边相关的常见度量方式有以下几种：<br>a.距离衡量<br>包括浏览、点击、搜索等等各种行为的欧式、马氏、闵式、切比雪夫距离、汉明距离计算<br>b.相似度衡量<br>包括余弦相似度、杰卡德相似度衡量<br>c.复杂衡量<br>包括相关性衡量，熵值衡量，互信息量衡量，相关距离衡量<br>2.商品行为信息<br>探求商品及其对应行为信息的笛卡尔积的映射关系，得到一个商品+用户的行为魔方<br>商品集合：{商品A、商品B、…}<br>商品属性集合：{价格、是否打折、相比其他电商平台的比价、是否缺货…}<br>用户行为集合：{浏览次数、浏览时长、末次浏览间隔、搜索次数、末次搜索间隔…}<br>通过商品集合<em>商品属性集合</em>用户行为集合,形成高维的商品信息魔方，再通过探查算法，筛选优秀表现的特征，这里推荐的有pca，randomforest的importance，lasso变量压缩，相关性压缩，逐步回归压缩等方法，根据数据的属性特点可适当选取方法<br>最后，我们会得到如下一个待选特征组：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-3dc206ce811fd331.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>3.商品购买周期<br>针对每一件商品，都是有它自身的生命周期的，比如，在三个月内买过冰箱的用户，95%以上的用户是不会选择二次购买的；而在1个月的节点上，会有20%的用户会选择二次购买生活用纸。所以我们需要做的一件事情就是不断更新，平台上面每个类目下面的商品的自身生命周期。除此之外，考虑在过渡时间点，用户的需求变化情况，是否可以提前触发需求；这边利用，艾宾浩斯遗忘曲线和因子衰减规律拟合：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d85e9481b10fbed1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="艾宾浩斯曲线.png"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-1a793afc031f2358.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="衰减因子.png"></p>
<p>确定lamda和b，计算每个用户对应的每个类目，当前时间下的剩余价值：f(最高价值)<em>lamda</em>b</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-5ea000e1656570cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="艾宾浩斯.png"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-57ff2e44ea9c3e9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="衰减因子公式.png"></p>
<p>4.商品挖掘特征，用户挖掘特征<br>业务运营过程中，通过数据常规可以得到1.基础结论，2.挖掘结论。基础结论就是统计结论，比如昨日订单量，昨日销售量 ，昨日用户量；挖掘结论就是深层结论，比如昨日活跃用户数，每日预估销售量，用户生命周期等<br>存在如下的探索形式，这是一个漫长而又非常有价值的过程：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-b06aa864056a5ed1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="特征分析.png"></p>
<p>模型整合<br>再确定以上四大类的数据特征之后，我们通过组合模型的方法，判断用户的交叉销售结果</p>
<hr>
<p>1.cart regression<br>确保非线性密度均匀数据拟合效果，针对存在非线性关系且数据可被网格切分的产业用户有高的预测能力<br>2.ridge regression<br>确保可线性拟合及特征繁多数据的效果，针对存在线性关系的产业用户有高的预测能力<br>3.Svm-liner<br>确保线性且存在不可忽视的异常点的数据拟合效果，针对存在异常用户较多的部分产业用户有高的预测能力<br>4.xgboost<br>确保数据复杂高维且无明显关系的数据拟合效果，针对存在维度高、数据杂乱、无模型规律的部分产业用户有高的预测能力<br>以上的组合模型并非固定，也并非一定全部使用，在确定自身产业的特点后，择优选择，然后采取投票、加权、分组等组合方式产出结果即可。</p>
<hr>
<p>附上推荐Rcode简述，<br><strong>cart regression：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fit &lt;- rpart(y~x, data=database, method=&quot;class&quot;,control=ct, parms = list(prior = c(0.7,0.3), split = &quot;information&quot;));</span><br><span class="line"># xval是n折交叉验证</span><br><span class="line"># minsplit是最小分支节点数，设置后达不到最小分支节点的话会继续分划下去</span><br><span class="line"># minbucket：叶子节点最小样本数</span><br><span class="line"># maxdepth：树的深度</span><br><span class="line"># cp全称为complexity parameter，指某个点的复杂度，对每一步拆分,模型的拟合优度必须提高的程度</span><br><span class="line"># kyphosis是rpart这个包自带的数据集</span><br><span class="line"># na.action：缺失数据的处理办法，默认为删除因变量缺失的观测而保留自变量缺失的观测。</span><br><span class="line"># method：树的末端数据类型选择相应的变量分割方法:</span><br><span class="line"># 连续性method=“anova”,离散型method=“class”,计数型method=“poisson”,生存分析型method=“exp”</span><br><span class="line">#parms用来设置三个参数:先验概率、损失矩阵、分类纯度的度量方法（gini和information）</span><br><span class="line"># cost我觉得是损失矩阵，在剪枝的时候，叶子节点的加权误差与父节点的误差进行比较，考虑损失矩阵的时候，从将“减少-误差”调整为“减少-损失”</span><br></pre></td></tr></table></figure></p>
<p><strong>ridge regression：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">library（glmnet）</span><br><span class="line">glmmod&lt;-glmnet(x,y,family = &apos;guassian&apos;,alpha = 0)</span><br><span class="line">最小惩罚：</span><br><span class="line">glmmod.min&lt;-glmnet(x,y,family = &apos;gaussian&apos;,alpha = 0,lambda = glmmod.cv$lambda.min)</span><br><span class="line">1个标准差下的最小惩罚：</span><br><span class="line">glmmod.1se&lt;-glmnet(x,y,family = &apos;gaussian&apos;,alpha = 0,lambda = glmmod.cv$lambda.1se)</span><br></pre></td></tr></table></figure></p>
<p><strong>Svm-liner ：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">svm(x, y, scale = TRUE, type = NULL, kernel = &quot;&quot;,degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),coef0 = 0, cost = 1, nu = 0.5, subset, na.action = na.omit)</span><br><span class="line">#type用于指定建立模型的类别:C-classification、nu-classification、one-classification、eps-regression和nu-regression</span><br><span class="line">#kernel是指在模型建立过程中使用的核函数</span><br><span class="line">#degree参数是指核函数多项式内积函数中的参数，其默认值为3</span><br><span class="line">#gamma参数给出了核函数中除线性内积函数以外的所有函数的参数，默认值为l</span><br><span class="line">#coef0参数是指核函数中多项式内积函数与sigmoid内积函数中的参数，默认值为0</span><br><span class="line">#参数cost就是软间隔模型中的离群点权重</span><br><span class="line">#参数nu是用于nu-regression、nu-classification和one-classification类型中的参数</span><br></pre></td></tr></table></figure></p>
<p><strong>xgboost:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(xgboost)</span><br><span class="line">xgb &lt;- xgboost(data = data.matrix(x[,-1]), label = y, eta = 0.1,max_depth = 15, nround=25, subsample = 0.5,colsample_bytree = 0.5,seed = 1,eval_metric = &quot;merror&quot;,objective = &quot;multi:softprob&quot;,num_class = 12, nthread = 3)</span><br><span class="line">#eta：默认值设置为0.3。步长，控制速度及拟合程度</span><br><span class="line">#gamma:默认值设置为0。子树叶节点个数</span><br><span class="line">#max_depth:默认值设置为6。树的最大深度</span><br><span class="line">#min_child_weight:默认值设置为1。控制子树的权重和</span><br><span class="line">#max_delta_step：默认值设置为0。控制每棵树的权重</span><br><span class="line">#subsample： 默认值设置为1。抽样训练占比</span><br><span class="line">#lambda and alpha：正则化</span><br></pre></td></tr></table></figure></p>
<p>最后通过组合算法的形式产出最终值：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-0d98bdffde0383d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Ensemble Learning"></p>
<p><strong>典型算法代表：randomforest,adaboost,gbdt</strong></p>
<p>之前写的没有用markdown，所以看起来很费力，还丢图，这次优化了一下视图，谢谢。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[随机森林-枝剪问题]]></title>
      <url>/2017/10/02/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-%E6%9E%9D%E5%89%AA%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>通常情况下， 随机森林不需要后剪枝。</p>
<p>剪枝的意义是：防止决策树生成过于庞大的子叶，避免实验预测结果过拟合，在实际生产中效果很差</p>
<p>剪枝通常有两种：</p>
<p>PrePrune：预剪枝，及早的停止树增长，在每个父节点分支的时候计算是否达到了限制值</p>
<p>PostPrune：后剪枝，基于完全生长（过拟合）的树上进行剪枝，砍掉一些对衡量函数影响不大的枝叶</p>
<p>剪枝的依据：</p>
<p>常见的有错误率校验（判断枝剪是降低了模型预测的正确率），统计学检验，熵值，代价复杂度等等</p>
<p>总结看来，枝剪的目的是担心全量数据在某棵树上的拟合过程中，过度判断了每个点及其对应类别的关系，有如以下这张图（以rule1&amp;rule2代替了rule3）：</p>
<hr>
<p>随机森林：</p>
<p>定义：它是一种模型组合（常见的Boosting，Bagging等，衍生的有gbdt），这些算法最终的结果是生成N(可能会有几百棵以上）棵树，组合判断最终结果。</p>
<p>如何组合判断？</p>
<p>1.通常我们会规定随机森林里面的每棵树的选参个数，常见的有log，sqrt等等，这样的选取是随机选则的，这样有一个好处，让每一棵树上都有了尽可能多的变量组合，降低过拟合程度</p>
<p>2.树的个数及树的节点的变量个数，通常的来说，最快捷的方式是先确定节点的变量个数为sqrt（变量的个数），然后在根据oob的准确率反过来看多个棵树时最优，确定了树的个数的时候再反过来确定mtry的个数，虽然有局限，但是也并不存在盲目性</p>
<p>3.我个人理解，随机森林中的每一棵树我们需要它在某一片的数据中有非常好的拟合性，它并不是一个全数据拟合，只需要在它负责那块上有最佳的拟合效果。每次遇到这些数据(特征)的时候，我们在最后汇总N棵树的结果的时候，给这些数据对应的那块模型以最高权重即可</p>
<p>最后总结一下，就是随机森林里面的每棵树的产生通过选特征参数选数据结构，都已经考虑了避免共线性避免过拟合，剩下的每棵树需要做的就是尽可能的在自己所对应的数据(特征)集情况下尽可能的做到最好的预测结果；如同，公司已经拆分好部门，你不需要考虑这样拆分是不是公司运营最好的一个组合方式，你需要做的就是当公司需要你的时候，尽可能的做好自己的事情，就酱。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 树枝剪问题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[面试之常见决策树异同]]></title>
      <url>/2017/09/01/%E9%9D%A2%E8%AF%95%E4%B9%8B%E5%B8%B8%E8%A7%81%E5%86%B3%E7%AD%96%E6%A0%91%E5%BC%82%E5%90%8C/</url>
      <content type="html"><![CDATA[<p>历史回顾：1984年提出的cart，1986年提出的ID3，1993年提出的c4.5</p>
<p><strong>理论上</strong>总的来说，<br>C4.5是基于ID3优化后产出的算法，主要优化了关于节点分支的计算方式，优化后解决了ID3分支过程中总喜欢偏向取值较多的属性<br>ID3是信息增益分支：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-e712ccfe23449bd2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">而CART一般是GINI系数分支：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-a39ef858bf85cc82.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">C4.5一般是信息增益率分支：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-422628f7caf8e0b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br><strong>工程上</strong>总的来说：<br>CART和C4.5之间主要差异在于分类结果上，<strong>CART可以回归分析也可以分类，C4.5只能做分类；C4.5子节点是可以多分的，而CART是无数个二叉子节点</strong>；<br>以此拓展出以CART为基础的“树群”random forest ， 以回归树为基础的“树群”GBDT</p>
<p>样本数据的差异：<br>ID3只能对分类变量进行处理，C4.5和CART可以处理连续和分类两种自变量<br>ID3对缺失值敏感，而C4.5和CART对缺失值可以进行多种方式的处理<br>只从样本量考虑，<em>小样本建议考虑c4.5、大样本建议考虑cart</em>。c4.5处理过程中需对数据集进行多次排序，处理成本耗时较高，而cart本身是一种大样本的统计方法，小样本处理下泛化误差较大</p>
<p>目标因变量的差异：<br>ID3和C4.5只能做分类，CART（分类回归树）不仅可以做分类（0/1）还可以做回归（0-1）<br>ID3和C4.5节点上可以产出多叉（低、中、高），而CART节点上永远是二叉（低、非低）</p>
<p>样本特征上的差异：<br>特征变量的使用中，多分的分类变量ID3和C4.5层级之间只单次使用，CART可多次重复使用</p>
<p>决策树产生过程中的优化差异：<br>C4.5是通过枝剪来修正树的准确性，而CART是直接利用全部数据发现所有树的结构进行对比</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SVD及扩展的矩阵分解方法]]></title>
      <url>/2017/08/27/SVD%E5%8F%8A%E6%89%A9%E5%B1%95%E7%9A%84%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>svd是现在比较常见的算法之一，也是数据挖掘工程师、算法工程师必备的技能之一，这边就来看一下svd的思想，svd的重写，svd的应用。<br>这边着重的看一下推荐算法中的使用，其实在图片压缩，特征压缩的工程中，svd也有着非常不凡的作用。</p>
<h1 id="svd的思想"><a href="#svd的思想" class="headerlink" title="svd的思想"></a>svd的思想</h1><h2 id="矩阵因子模型（潜在因子模型）"><a href="#矩阵因子模型（潜在因子模型）" class="headerlink" title="矩阵因子模型（潜在因子模型）"></a>矩阵因子模型（潜在因子模型）</h2><p>假设，我们现在有一个用户u对商品i的程度矩阵，浏览是1，搜索是2，加车是3，下单是4，付款是5:<br><img src="http://upload-images.jianshu.io/upload_images/1129359-4f9064ac5c4b12a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="用户商品矩阵"><br>实际情况下，用户不可能什么商品都买，所以，该矩阵必然是一个稀疏矩阵，任意一个矩阵必然可以被分解成2个矩阵的乘积：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-a7a3301f98cd391a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>k就是潜在因子的个数，举个例子，你去买衣服，你可能买了裙子，露背装，我去买衣服，我买了牛仔裤，潮牌T恤，影响我们购买商品的差异的原因可能有很多点，但是必然有些原因占比重要些，比如性别，收入，有一些可能不那么重要比如天气，心情。<strong>而拆分成的Pu矩阵表示了这些潜在因子对我或者你的影响程度，Qi矩阵表示了各种商品对这些潜在因子的影响程度。</strong><br>当我们尽可能的通过拆分矩阵的形式，目标使得拆分后的两个矩阵的乘积最匹配最上方的用户商品矩阵的已知的数据值，从而可以通过这两个矩阵的乘积填补掉空缺的值。</p>
<h2 id="Baseline-Predictors"><a href="#Baseline-Predictors" class="headerlink" title="Baseline Predictors"></a>Baseline Predictors</h2><p>这个是08年的，Koren在NetFlix大赛的一个思路，后续也延伸了svd多种变种，比如Asvd，有偏的Rsvd，对偶算法下的Svd++，这些算法的核心在于解决了Svd上面我们提到的那个矩阵庞大稀疏的问题，后续我们再看。</p>
<p>Baseline Predictors使用向量bi表示电影i的评分相对于平均评分的偏差，向量bu表示用户u做出的评分相对于平均评分的偏差，将平均评分记做μ。</p>
<p><strong>新的得分计算方式如下：Rui＝μ+bi+bu</strong><br>准备引入了商品及用户的实际分布的情况，有效的降低在测试数据上面的效果。</p>
<h2 id="svd数学原理"><a href="#svd数学原理" class="headerlink" title="svd数学原理"></a>svd数学原理</h2><p>首先，线代或者高等代数里面告诉我们：一个向量可以通过左乘一个矩阵的方式来进行拉伸，旋转，或者同时拉伸旋转。<br>所以，无论什么矩阵M，我们都可以找到一组正交基v1、v2，使得Mv1、Mv2也是正交的，不妨记其方向为为μ1、μ2。<br><strong>Mv1=δ1u1；<br>Mv2=δ2u2；</strong><br>存在向量x，在v1、v2空间里的表示为：x=（x·v1）v1+（x·v2）v2，<br>所以有，Mx有：<br>Mx=（x·v1）Mv1+（x·v2）Mv2<br>Mx=（x·v1）δ1u1+（x·v2）δ2u2<br>Mx=δ1u1(v1.T)x+δ2u2(v2.T)x<br>M=δ1u1(v1.T)+δ2u2(v2.T)<br>所以就有了那个非常有名的公式：<br>M=UΣV.T<br>U是有一组正交基构成的，V也是有一组正交基构成的，Σ是由δ1、δ2构成的，<strong>几何意义上来说，M的作用就是把一个向量由V的正交空间变换到U的正交空间上，而通过Σ的大小来控制缩放的力度。</strong></p>
<p>我们还需要知道一些简单的推论，</p>
<ul>
<li>通过MM.T，我们知道，δ的平方是MM.T的特征值</li>
<li>奇异值δ的数量决定了M=UΣV.T的复杂度，而奇异值的大小变化差异程度很大，通常前几个奇异值的平方就能占到全部奇异值的平方的90%，所以，我们可以通过控制奇异值的数量来优化原始矩阵乘积，去除掉一下噪声数据</li>
</ul>
<h1 id="svd重写"><a href="#svd重写" class="headerlink" title="svd重写"></a>svd重写</h1><h2 id="基础的svd"><a href="#基础的svd" class="headerlink" title="基础的svd"></a>基础的svd</h2><p>首先，我们在刚开始就知道，评分矩阵R可以用两个矩阵P和Q的乘积来表示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-00e14ca0f0f01fa2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，U表示用户数，I表示商品数，K=就是潜在因子个数。<br>首先通过那些已知的数据比如下方红色区域内的数据去训练这两个乘积矩阵：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d70136713c7e1da4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么未知的评分也就可以用P的某一行乘上Q的某一列得到了：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-388118169f4024e2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这是预测用户u对商品i的评分，它等于P矩阵的第u行乘上Q矩阵的第i列。这个是最基本的SVD算法，下面我们们来看如何确定Pu、Qi：</p>
<p>假设已知的评分为：rui则真实值与预测值的误差为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-687c46bac4086a5b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>继而可以计算出总的误差平方和：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-2b2a61f97585ca11?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>只要通过训练把SSE降到最小那么P、Q就能最好地拟合R了。常规的来讲，梯度下降是非常好的求解方式，常见的包括随机梯度下降，批量梯度下降。<br>随机梯度下降一定程度会避免局部最小但是计算量大，批量梯度计算量小但是会存在鞍点计算误区的问题。</p>
<p>先求得SSE在Puk变量（也就是P矩阵的第u行第k列的值）处的梯度：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-b8d9b03c876da715?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>现在得到了目标函数在Puk处的梯度了，那么按照梯度下降法，将Puk往负梯度方向变化。令更新的步长（也就是学习速率）为</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-391924916a6508ea?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>则Puk的更新式为</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-52caee085caf0eac?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>同样的方式可得到Qik的更新式为</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-cfb3747f3c3c319b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="Rsvd"><a href="#Rsvd" class="headerlink" title="Rsvd"></a>Rsvd</h2><p>很明显，上述这样去求的矩阵QP必然会存在过度拟合的问题，导致对实际数据预测的时候，效果远差于训练数据，仿造elastic net的思维：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-544bb9fa4913fb6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>对所有的变量就加入正则惩罚项，重新计算上面的梯度如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-e9400ba6e4db6e2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这就是正则svd，也叫做Rsvd，也是我们用的比较多的svd的方法。</p>
<h2 id="偏移Rsvd"><a href="#偏移Rsvd" class="headerlink" title="偏移Rsvd"></a>偏移Rsvd</h2><p>在最开始讲了，Koren在NetFlix大赛里面除了考虑了对原始数据的拟合情况，也考虑了用户的评分、商品的平均得分相对于整体数据的偏移情况，有了新的得分公式：Rui＝μ+bi+bu，影响的只有eui，后面的Pu、qi的正则不受影响，但是新增了bi、bu的正则项，重新计算每一项的偏导数：<br>bu、bi的更新式子：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-7017502a17f3f3b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>其余的都不发生改变，这就叫做有偏移下的Rsvd</p>
<p>无论是Rsvd还是有偏移的Rsvd，当原始的用户对商品的评分矩阵过大，比如有3亿用户，3亿商品，形成9亿商品集合的时候，这就是一个比较不可能完成的存储任务，而且里面绝大多数都是0的稀疏矩阵。</p>
<h2 id="Asvd及Svd"><a href="#Asvd及Svd" class="headerlink" title="Asvd及Svd++"></a>Asvd及Svd++</h2><p>这边，我们引入两个集合：R(u)表示用户u评过分的商品集合，N(u)表示用户u浏览过但没有评过分的商品集合，Xj和Yj是商品的属性。<br>Asvd的rui的评分方式：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-b4e7cb6fe8c2b926.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Svd++的rui的评分方式：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-8a93f3ab854ccd43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>无论是Asvd还是Svd++，都干掉了原来庞大的P矩阵，取而代之的是两个用户浏览评分矩阵大大缩小了存储的空间，但是随着而来的是一大把更多的未知参数及迭代的复杂程度，所有在训练时间上而言，会大大的增加。</p>
<p>我这边只重写了一下Rsvd的python版本，网上挺多版本的迭代条件有一定问题，稍作处理了一下，并写成了函数，大家可以参考一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svd</span><span class="params">(mat, feature, steps=<span class="number">2000</span>, gama=<span class="number">0.02</span>, lamda=<span class="number">0.3</span>)</span>:</span></span><br><span class="line"><span class="comment">#feature是潜在因子的数量，mat为评分矩阵</span></span><br><span class="line">    slowRate = <span class="number">0.99</span></span><br><span class="line">    preRmse = <span class="number">0.0000000000001</span></span><br><span class="line">    nowRmse = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    user_feature = matrix(numpy.random.rand(mat.shape[<span class="number">0</span>], feature))</span><br><span class="line">    item_feature = matrix(numpy.random.rand(mat.shape[<span class="number">1</span>], feature))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(steps):</span><br><span class="line">        rmse = <span class="number">0.0</span></span><br><span class="line">        n = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> range(mat.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(mat.shape[<span class="number">1</span>]):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> numpy.isnan(mat[u,i]):</span><br><span class="line"><span class="comment">#这边是判断是否为空，也可以改为是否为0:if mat[u,i]&gt;0:</span></span><br><span class="line">                    pui = float(numpy.dot(user_feature[u,:], item_feature[i,:].T))</span><br><span class="line">                    eui = mat[u,i] - pui</span><br><span class="line">                    rmse += pow(eui, <span class="number">2</span>)</span><br><span class="line">                    n += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">for</span> k <span class="keyword">in</span> range(feature):</span><br><span class="line"><span class="comment">#Rsvd的更新迭代公式</span></span><br><span class="line">                        user_feature[u,k] += gama*(eui*item_feature[i,k] - lamda*user_feature[u,k])</span><br><span class="line">                        item_feature[i,k] += gama*(eui*user_feature[u,k] - lamda*item_feature[i,k])</span><br><span class="line"><span class="comment">#n次迭代平均误差程度</span></span><br><span class="line">        nowRmse = sqrt(rmse * <span class="number">1.0</span> / n)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'step: %d      Rmse: %s'</span> % ((step+<span class="number">1</span>), nowRmse)</span><br><span class="line">        <span class="keyword">if</span> (nowRmse &gt; preRmse):</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="comment">#降低迭代的步长</span></span><br><span class="line">        gama *= slowRate</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> user_feature, item_feature</span><br></pre></td></tr></table></figure></p>
<p>直接调用的结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">step: 1956      Rmse: 0.782449675844</span><br><span class="line">step: 1957      Rmse: 0.782449675843</span><br><span class="line">step: 1958      Rmse: 0.782449675843</span><br><span class="line">step: 1959      Rmse: 0.782449675843</span><br><span class="line">step: 1960      Rmse: 0.782449675842</span><br><span class="line">step: 1961      Rmse: 0.782449675842</span><br><span class="line">step: 1962      Rmse: 0.782449675842</span><br><span class="line">step: 1963      Rmse: 0.782449675841</span><br><span class="line">step: 1964      Rmse: 0.782449675841</span><br><span class="line">step: 1965      Rmse: 0.78244967584</span><br><span class="line">step: 1966      Rmse: 0.78244967584</span><br><span class="line">step: 1967      Rmse: 0.78244967584</span><br><span class="line">step: 1968      Rmse: 0.782449675839</span><br><span class="line">step: 1969      Rmse: 0.782449675839</span><br><span class="line">step: 1970      Rmse: 0.782449675839</span><br><span class="line">step: 1971      Rmse: 0.782449675838</span><br><span class="line">step: 1972      Rmse: 0.782449675838</span><br><span class="line">step: 1973      Rmse: 0.782449675838</span><br><span class="line">step: 1974      Rmse: 0.782449675837</span><br><span class="line">step: 1975      Rmse: 0.782449675837</span><br><span class="line">step: 1976      Rmse: 0.782449675837</span><br><span class="line">step: 1977      Rmse: 0.782449675836</span><br><span class="line">step: 1978      Rmse: 0.782449675836</span><br><span class="line">step: 1979      Rmse: 0.782449675836</span><br><span class="line">step: 1980      Rmse: 0.782449675835</span><br><span class="line">step: 1981      Rmse: 0.782449675835</span><br><span class="line">step: 1982      Rmse: 0.782449675835</span><br><span class="line">step: 1983      Rmse: 0.782449675835</span><br><span class="line">step: 1984      Rmse: 0.782449675834</span><br><span class="line">step: 1985      Rmse: 0.782449675834</span><br><span class="line">step: 1986      Rmse: 0.782449675834</span><br><span class="line">step: 1987      Rmse: 0.782449675833</span><br><span class="line">step: 1988      Rmse: 0.782449675833</span><br><span class="line">step: 1989      Rmse: 0.782449675833</span><br><span class="line">step: 1990      Rmse: 0.782449675833</span><br><span class="line">step: 1991      Rmse: 0.782449675832</span><br><span class="line">step: 1992      Rmse: 0.782449675832</span><br><span class="line">step: 1993      Rmse: 0.782449675832</span><br><span class="line">step: 1994      Rmse: 0.782449675831</span><br><span class="line">step: 1995      Rmse: 0.782449675831</span><br><span class="line">step: 1996      Rmse: 0.782449675831</span><br><span class="line">step: 1997      Rmse: 0.782449675831</span><br><span class="line">step: 1998      Rmse: 0.78244967583</span><br><span class="line">step: 1999      Rmse: 0.78244967583</span><br><span class="line">step: 2000      Rmse: 0.78244967583</span><br><span class="line">Out[15]:</span><br><span class="line">(matrix([[-0.72426432,  0.40007415,  1.16887518],</span><br><span class="line">         [-0.73130968,  0.40240702,  1.14708432],</span><br><span class="line">         [ 0.34759923,  1.35065656, -0.29573789],</span><br><span class="line">         [ 1.17462156, -0.04964694,  0.73881335],</span><br><span class="line">         [ 2.04035441, -0.06798676,  1.28078727],</span><br><span class="line">         [ 0.30446306,  1.71648612, -0.4109819 ],</span><br><span class="line">         [ 1.71963828, -0.00833196,  1.25983483],</span><br><span class="line">         [-0.86341514,  0.47750529,  1.36135332],</span><br><span class="line">         [-0.48881234,  0.57942923,  0.77110915],</span><br><span class="line">         [ 0.29282908,  1.5164249 , -0.39811768],</span><br><span class="line">         [ 0.35369432, -0.00964055,  0.22328158]]),</span><br><span class="line"> matrix([[ 1.3381854 , -0.05425608,  0.87036818],</span><br><span class="line">         [ 1.07547853, -0.04515239,  0.69607952],</span><br><span class="line">         [ 1.39038494, -0.05799558,  0.90186332],</span><br><span class="line">         [-0.60218508,  0.36233309,  0.92136536],</span><br><span class="line">         [ 0.39117217,  1.9272062 , -0.50710187],</span><br><span class="line">         [-0.93202395,  0.52982297,  1.43309143],</span><br><span class="line">         [-0.19393338,  0.40429152,  0.29994372],</span><br><span class="line">         [ 1.40108031,  0.06915628,  0.87647945],</span><br><span class="line">         [ 1.39446638, -0.05197355,  0.920216  ],</span><br><span class="line">         [ 0.31981929,  1.84114505, -0.40108819],</span><br><span class="line">         [-0.82317832,  0.52798144,  1.51619052]]))</span><br></pre></td></tr></table></figure>
<h1 id="svd在推荐算法中的使用"><a href="#svd在推荐算法中的使用" class="headerlink" title="svd在推荐算法中的使用"></a>svd在推荐算法中的使用</h1><p>数据集中行代表用户user，列代表物品item，其中的值代表用户对物品的打分。<br>基于SVD的优势在于：用户的评分数据是稀疏矩阵，可以用SVD将原始数据映射到低维空间中，然后计算物品item之间的相似度，更加高效快速。</p>
<p>整体思路：<br>用户未知得分的商品评分计算方式：<br>1.用户的评分矩阵==》用户已经评分过得商品的得分<br>2.商品的用户评分==》用户已经评分过得商品和其他每个商品的相关性<br>3.“用户已经评分过得商品的得分”*“用户已经评分过得商品和某个未知评分商品的相关系数”=某个未知商品该用户的评分</p>
<p>计算上面三个步骤，我们需要：<br>1.考虑采取上面相关性的计算方式<br>2.考虑潜在因子的个数</p>
<p>依旧python代码，我会在代码中注释讲解：<br>1.首先相关性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 欧拉距离相似度，评分可用，程度不建议</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">oulasim</span><span class="params">(A, B)</span>:</span></span><br><span class="line">    distince = la.norm(A - B)  <span class="comment"># 第二范式：平方的和后求根号</span></span><br><span class="line">    similarity = <span class="number">1</span> / (<span class="number">1</span> + distince)</span><br><span class="line">    <span class="keyword">return</span> similarity</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 余弦相似度，评分、1/0、程度都可以用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cossim</span><span class="params">(A, B)</span>:</span></span><br><span class="line">    ABDOT = float(dot(A, B))</span><br><span class="line">    ABlen = la.norm(A) * la.norm(B)</span><br><span class="line">    <span class="keyword">if</span> ABlen == <span class="number">0</span>:</span><br><span class="line">        similarity = <span class="string">'异常'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        similarity = ABDOT / float(ABlen)</span><br><span class="line">    <span class="keyword">return</span> similarity</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 皮尔逊相关系数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearsonsim</span><span class="params">(A, B)</span>:</span></span><br><span class="line">    A = A - mean(A)</span><br><span class="line">    B = B - mean(B)</span><br><span class="line">    ABDOT = float(dot(A, B))</span><br><span class="line">    ABlen = la.norm(A) * la.norm(B)</span><br><span class="line">    <span class="keyword">if</span> ABlen == <span class="number">0</span>:</span><br><span class="line">        similarity = <span class="string">'异常'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        similarity = ABDOT / float(ABlen)</span><br><span class="line">    <span class="keyword">return</span> similarity</span><br></pre></td></tr></table></figure></p>
<p>2.指定用户及对应商品的相似度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># svd</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommender</span><span class="params">(datamat,user,index,function)</span>:</span></span><br><span class="line">    n = shape(datamat)[<span class="number">1</span>]  <span class="comment"># 商品数目</span></span><br><span class="line">    U, sigma, VT = la.svd(datamat)</span><br><span class="line">    <span class="comment"># 规约最小维数</span></span><br><span class="line">    sigma2 = sigma ** <span class="number">2</span></span><br><span class="line">    k = len(sigma2)</span><br><span class="line">    n_sum2 = sum(sigma2)</span><br><span class="line">    nsum = <span class="number">0</span></span><br><span class="line">    max_sigma_index = <span class="number">0</span></span><br><span class="line"><span class="comment">#奇异值的平方占比总数的90%，确定潜在因子数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> sigma:</span><br><span class="line">        nsum = nsum + i ** <span class="number">2</span></span><br><span class="line">        max_sigma_index = max_sigma_index + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> nsum &gt;= n_sum2 * <span class="number">0.9</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># item new matrix</span></span><br><span class="line">    item = datamat.T * U[:, <span class="number">0</span>:max_sigma_index] * matrix(diag(sigma[<span class="number">0</span>:max_sigma_index])).I</span><br><span class="line">    key=item[index,:]</span><br><span class="line">    total_similarity=<span class="number">0</span></span><br><span class="line">    rank_similarity=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line"><span class="comment">#如果用户没有评分或者与用户选择想知道的商品一致则跳过，不跳过计算出来前者是得分0，后者直接是用户已评分的结果，没有意义</span></span><br><span class="line">            <span class="keyword">if</span> datamat[user,i]==<span class="number">0</span> <span class="keyword">or</span> i==index:<span class="keyword">continue</span></span><br><span class="line">            similarity=function(key,item[i,:].T)</span><br><span class="line">            total_similarity=total_similarity+similarity</span><br><span class="line"><span class="comment">#用户的评分*相关系数</span></span><br><span class="line">            rank_similarity=rank_similarity+similarity*datamat[user,i]*similarity</span><br><span class="line">    score = rank_similarity/total_similarity</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure></p>
<p>比如：用户1在商品1的得分为3.99分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [18]: recommender(data,1,1,cossim)</span><br><span class="line">Out[18]: 3.98921610058786</span><br></pre></td></tr></table></figure></p>
<p>3.指定商品，与所有其它商品的相似度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommender</span><span class="params">(datamat, item_set, method)</span>:</span></span><br><span class="line">    col = shape(datamat)[<span class="number">1</span>]  <span class="comment"># 物品数量</span></span><br><span class="line">    item = datamat[:, item_set]</span><br><span class="line">    similarity_matrix = zeros([col, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(col):</span><br><span class="line">        index = nonzero(logical_and(item &gt; <span class="number">0</span>, datamat[:, i] &gt; <span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> sum(index) &gt; <span class="number">0</span>:</span><br><span class="line">            similarity = method(datamat[index, item_set].T, datamat[index, i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            similarity = <span class="string">'-1'</span></span><br><span class="line">        similarity_matrix[i] = similarity</span><br><span class="line">    <span class="keyword">return</span> similarity_matrix</span><br></pre></td></tr></table></figure></p>
<p>比如：商品0，对其它所有商品的相似度：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [24]: recommender(data,0,cossim)</span><br><span class="line">Out[24]:</span><br><span class="line">array([[ 1.        ],</span><br><span class="line">       [ 0.99439606],</span><br><span class="line">       [ 0.99278096],</span><br><span class="line">       [-1.        ],</span><br><span class="line">       [-1.        ],</span><br><span class="line">       [-1.        ],</span><br><span class="line">       [-1.        ],</span><br><span class="line">       [ 0.98183139],</span><br><span class="line">       [ 0.97448865],</span><br><span class="line">       [-1.        ],</span><br><span class="line">       [ 1.        ]])</span><br></pre></td></tr></table></figure></p>
<p>4.直接算出所有商品间的相似度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similarity</span><span class="params">(datamat, method)</span>:</span></span><br><span class="line">    item_sum = shape(datamat)[<span class="number">1</span>]</span><br><span class="line">    similarity = pd.DataFrame([])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(item_sum):</span><br><span class="line">        res = recommender(datamat, i, method)</span><br><span class="line">        similarity = pd.concat([similarity, pd.DataFrame(res)], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> similarity</span><br></pre></td></tr></table></figure></p>
<p>比如商品的相似矩阵：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">In [25]: similarity(data,cossim)</span><br><span class="line">Out[25]:</span><br><span class="line">           0         0         0         0         0         0         0  \</span><br><span class="line">0   1.000000  0.994396  0.992781 -1.000000 -1.000000 -1.000000 -1.000000</span><br><span class="line">1   0.994396  1.000000  0.999484 -1.000000 -1.000000 -1.000000 -1.000000</span><br><span class="line">2   0.992781  0.999484  1.000000 -1.000000 -1.000000 -1.000000 -1.000000</span><br><span class="line">3  -1.000000 -1.000000 -1.000000  1.000000 -1.000000  0.990375  1.000000</span><br><span class="line">4  -1.000000 -1.000000 -1.000000 -1.000000  1.000000 -1.000000  1.000000</span><br><span class="line">5  -1.000000 -1.000000 -1.000000  0.990375 -1.000000  1.000000  1.000000</span><br><span class="line">6  -1.000000 -1.000000 -1.000000  1.000000  1.000000  1.000000  1.000000</span><br><span class="line">7   0.981831  0.956858  0.955304 -1.000000  1.000000 -1.000000 -1.000000</span><br><span class="line">8   0.974489  0.956858  0.955304 -1.000000 -1.000000 -1.000000 -1.000000</span><br><span class="line">9  -1.000000 -1.000000 -1.000000  1.000000  0.994536  1.000000  0.384615</span><br><span class="line">10  1.000000  1.000000  1.000000  1.000000 -1.000000  0.981307  1.000000</span><br><span class="line"></span><br><span class="line">           0         0         0         0</span><br><span class="line">0   0.981831  0.974489 -1.000000  1.000000</span><br><span class="line">1   0.956858  0.956858 -1.000000  1.000000</span><br><span class="line">2   0.955304  0.955304 -1.000000  1.000000</span><br><span class="line">3  -1.000000 -1.000000  1.000000  1.000000</span><br><span class="line">4   1.000000 -1.000000  0.994536 -1.000000</span><br><span class="line">5  -1.000000 -1.000000  1.000000  0.981307</span><br><span class="line">6  -1.000000 -1.000000  0.384615  1.000000</span><br><span class="line">7   1.000000  0.991500  1.000000  1.000000</span><br><span class="line">8   0.991500  1.000000 -1.000000  1.000000</span><br><span class="line">9   1.000000 -1.000000  1.000000  1.000000</span><br><span class="line">10  1.000000  1.000000  1.000000  1.000000</span><br></pre></td></tr></table></figure></p>
<p>5.指定用户下的top5商品推荐<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def fianl_recommender(datamat,user,function):</span><br><span class="line">    unratedItems=nonzero(datamat[user,:].A==0)[1]</span><br><span class="line">    if len(unratedItems)==0: print &apos;ok&apos;</span><br><span class="line">    score=[]</span><br><span class="line">    for i in unratedItems:</span><br><span class="line">        i_score=recommender(datamat,user,i,function)</span><br><span class="line">        score.append((i,i_score))</span><br><span class="line">    score=sorted(score,key=lambda x:x[1],reverse=True)</span><br><span class="line">    return score[:5]</span><br></pre></td></tr></table></figure></p>
<p>比如指定用户1，最适合推荐的商品如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">31</span>]: fianl_recommender(data,<span class="number">1</span>,pearsonsim)</span><br><span class="line">Out[<span class="number">31</span>]:</span><br><span class="line">[(<span class="number">7</span>, <span class="number">3.3356853252871588</span>),</span><br><span class="line"> (<span class="number">8</span>, <span class="number">3.3349455396520296</span>),</span><br><span class="line"> (<span class="number">0</span>, <span class="number">3.33492840157654</span>),</span><br><span class="line"> (<span class="number">2</span>, <span class="number">3.334920725716121</span>),</span><br><span class="line"> (<span class="number">1</span>, <span class="number">3.334919898261294</span>)]</span><br></pre></td></tr></table></figure></p>
<p>最后，谢谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习下的电商商品推荐]]></title>
      <url>/2017/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E7%9A%84%E7%94%B5%E5%95%86%E5%95%86%E5%93%81%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<h1 id="常见算法套路"><a href="#常见算法套路" class="headerlink" title="常见算法套路"></a>常见算法套路</h1><p>电商行业中，对于用户的商品推荐一直是一个非常热门而且重要的话题，有很多比较成熟的方法，但是也各有利弊，大致如下：</p>
<ul>
<li><p>基于商品内容：比如食物A和食物B，对于它们价格、味道、保质期、品牌等维度，可以计算它们的相似程度，可以想象，我买了包子，很有可能顺路带一盒水饺回家。<br>优点：冷启动，其实只要你有商品的数据，在业务初期用户数据不多的情况下，也可以做推荐<br>缺点：预处理复杂，任何一件商品，维度可以说至少可以上百，如何选取合适的维度进行计算，设计到工程经验，这些也是花钱买不到的<br>典型：亚马逊早期的推荐系统</p>
</li>
<li><p>基于关联规则：最常见的就是通过用户购买的习惯，经典的就是“啤酒尿布”的案例，但是实际运营中这种方法运用的也是最少的，首先要做关联规则，数据量一定要充足，否则置信度太低，当数据量上升了，我们有更多优秀的方法，可以说没有什么亮点，业内的算法有apriori、ftgrowth之类的<br>优点：简单易操作，上手速度快，部署起来也非常方便<br>缺点：需要有较多的数据，精度效果一般<br>典型：早期运营商的套餐推荐</p>
</li>
<li><p>基于物品的协同推荐：假设物品A被小张、小明、小董买过，物品B被小红、小丽、小晨买过，物品C被小张、小明、小李买过；直观的看来，物品A和物品C的购买人群相似度更高（相对于物品B），现在我们可以对小董推荐物品C，小李推荐物品A，这个推荐算法比较成熟，运用的公司也比较多<br>优点：相对精准，结果可解释性强，副产物可以得出商品热门排序<br>缺点：计算复杂，数据存储瓶颈，冷门物品推荐效果差<br>典型：早期一号店商品推荐</p>
</li>
<li><p>基于用户的协同推荐：假设用户A买过可乐、雪碧、火锅底料，用户B买过卫生纸、衣服、鞋，用户C买过火锅、果汁、七喜；直观上来看，用户A和用户C相似度更高（相对于用户B），现在我们可以对用户A推荐用户C买过的其他东西，对用户C推荐用户A买过买过的其他东西，优缺点与<strong>基于物品的协同推荐</strong>类似，不重复了。</p>
</li>
<li><p>基于模型的推荐：svd++、特征值分解、概率图、聚分类等等。比如潜在因子分解模型，将用户的购买行为的矩阵拆分成两组权重矩阵的乘积，一组矩阵代表用户的行为特征，一组矩阵代表商品的重要性，在用户推荐过程中，计算该用户在历史训练矩阵下的各商品的可能性进行推荐。<br>优点：精准，对于冷门的商品也有很不错的推荐效果<br>缺点：计算量非常大，矩阵拆分的效能及能力瓶颈一直是受约束的<br>典型：惠普的电脑推荐</p>
</li>
<li><p>基于时序的推荐：这个比较特别，在电商运用的少，在Twitter，Facebook，豆瓣运用的比较多，就是只有赞同和反对的情况下，怎么进行评论排序，详细的可以参见我之前写的一篇文章：<a href="http://www.jianshu.com/p/b3e9b300a100" target="_blank" rel="noopener">应用：推荐系统-威尔逊区间法</a></p>
</li>
</ul>
<ul>
<li>基于深度学习的推荐：现在比较火的CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)都有运用在推荐上面的例子，但是都还是试验阶段，但是有个基于word2vec的方法已经相对比较成熟，也是我们今天介绍的重点。<br>优点：推荐效果非常精准，所需要的基础存储资源较少<br>缺点：工程运用不成熟，模型训练调参技巧难<br>典型：当前电商的会员商品推荐</li>
</ul>
<hr>
<h1 id="item2vec的工程引入"><a href="#item2vec的工程引入" class="headerlink" title="item2vec的工程引入"></a>item2vec的工程引入</h1><p>现在某电商的商品有约3亿个，商品的类目有10000多组，大的品类也有近40个，如果通过传统的协同推荐，实时计算的话，服务器成本，计算能力都是非常大的局限，之前已经有过几篇应用介绍：<a href="http://www.jianshu.com/p/fd245999ebfe" target="_blank" rel="noopener">基于推荐的交叉销售</a>、<a href="http://www.jianshu.com/p/e932b9744da6" target="_blank" rel="noopener">基于用户行为的推荐预估</a>。会员研发部门因为不是主要推荐的应用部门，所以在选择上，我们期望的是更加<strong>高效高速且相对准确的简约版</strong>模型方式，所以我们这边基于了word2vec的原始算法，仿造了itemNvec的方式。</p>
<p>首先，让我们对itemNvec进行理论拆分：</p>
<h2 id="part-one：n-gram"><a href="#part-one：n-gram" class="headerlink" title="part one：n-gram"></a>part one：n-gram</h2><p><strong>目标商品的前后商品对目标商品的影响程度</strong><br><img src="http://upload-images.jianshu.io/upload_images/1129359-5449db58928ebdd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这是两个用户userA，userB在易购上面的消费time line，灰色方框内为我们观察对象，试问一下，如果换一下灰色方框内的userA、userB的购买物品，直观的可能性有多大？</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-44806387baa860d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>直观的体验告诉我们，这是不可能出现，或者绝对不是常出现的，所以，我们就有一个初始的假设，<strong>对于某些用户在特定的类目下，用户的消费行为是连续影响的</strong>，换句话说，就是我买了什么东西是依赖我之前买过什么东西。如何通过算法语言解释上面说的这件事呢？<br>大家回想一下，naive bayes做垃圾邮件分类的时候是怎么做的？<br>假设“我公司可以提供发票、军火出售、航母维修”这句话是不是垃圾邮件？</p>
<p>P1(“垃圾邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“垃圾邮件”)p(“我公司可以提供发票、军火出售、航母维修”/“垃圾邮件”)/p(“我公司可以提供发票、军火出售、航母维修”)<br>=p(“垃圾邮件”)p(“发票”，“军火”，“航母”/“垃圾邮件”)/p(“发票”，“军火”，“航母”)</p>
<p>同理<br>P2(“正常邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“正常邮件”)p(“发票”，“军火”，“航母”/“正常邮件”)/p(“发票”，“军火”，“航母”)</p>
<p>我们只需要比较p1和p2的大小即可，在<strong>条件独立的情况下</strong>可以直接写成：<br>P1(“垃圾邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“垃圾邮件”)p(“发票”/“垃圾邮件”)p(“军火”/“垃圾邮件”)p(“航母”/“垃圾邮件”)<br>P2(“正常邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“正常邮件”)p(“发票”/“正常邮件”)p(“军火”/“正常邮件”)p(“航母”/“正常邮件”)</p>
<p>但是，我们看到，无论“我公司可以提供发票、军火出售、航母维修”词语的顺序怎么变化，不影响它最后的结果判定，但是我们这边的需求里面前面买的东西对后项的影响会更大。<br>冰箱=&gt;洗衣机=&gt;衣柜=&gt;电视=&gt;汽水，这样的下单流程合理<br>冰箱=&gt;洗衣机=&gt;汽水=&gt;电视=&gt;衣柜，这样的下单流程相对来讲可能性会更低<br>但是对于naive bayes，它们是一致的。<br>所以，我们这边考虑顺序，还是上面那个垃圾邮件的问题。<br>P1(“垃圾邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“垃圾邮件”)p(“发票”)p(“军火”/“发票”)p(“军火”/“航母”)<br>P1(“正常邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“正常邮件”)p(“发票”)p(“军火”/“发票”)p(“军火”/“航母”)<br>这边我们每个词只依赖前一个词，理论上讲依赖1-3个词通常都是可接受的。以上的考虑顺序的bayes就是基于著名的马尔科夫假设（Markov Assumption）：下一个词的出现仅依赖于它前面的一个或几个词下的联合概率问题，相关详细的理论数学公式就不给出了，这边这涉及一个思想。</p>
<h2 id="part-two：Huffman-Coding"><a href="#part-two：Huffman-Coding" class="headerlink" title="part two：Huffman Coding"></a>part two：Huffman Coding</h2><p><strong>更大的数据存储形式</strong><br>我们常用的user到item的映射是通过one hot encoding的形式去实现的，这有一个非常大的弊端就是数据存储系数且维度灾难可能性极大。<br>回到最初的那组数据：<strong>现在商品有约4亿个，商品的类目有10000多组，大的品类也有近40个</strong>，同时现在会员数目达到5亿，要是需要建造一个用户商品对应的购买关系矩阵做<strong>基于用户的协同推荐</strong>的话，我们需要做一个4亿X6亿的1/0矩阵，这个是几乎不可能的，Huffman采取了一个近似二叉树的形式进行存储：<br>我们以商品购买量为例，讲解一下如何以二叉树的形式替换one hot encoding存储方式：<br>假设，促销期间，经过统计，有冰箱=&gt;洗衣机=&gt;烘干机=&gt;电视=&gt;衣柜=&gt;钻石的用户下单链条（及购买物品顺序如上），其中冰箱总售出15万台，洗衣机总售出8万台，烘干机总售出6万台，电视总售出5万台，衣柜总售出3万台，钻石总售出1万颗</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-50a5488068b9b882.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Huffman树构造过程"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.给定&#123;15,8,6,5,3,1&#125;为二叉树的节点，每个树仅有一个节点，那就存在6颗单独的树</span><br><span class="line">2.选择节点权重值最小的两颗树进行合并也就是&#123;3&#125;、&#123;1&#125;，合并后计算新权重3+1=4</span><br><span class="line">3.将&#123;3&#125;，&#123;1&#125;树从节点列表删除，将3+1=4的新组合树放回原节点列表</span><br><span class="line">4.重新进行2-3，直到只剩一棵树为止</span><br></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-62059d8ae81872a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>针对每层每次分支过程，我们可以将所有权重大的节点看做是1，权重小的节点看做是0，相反亦可。现在，我们比如需要知道钻石的code，就是1000，也就是灰色方框的位置，洗衣机的code就是111；这样的存储利用了0/1的存储方式，也同时考虑了组合位置的排列长度，节省了数据的存储空间。</p>
<h2 id="part-three：node-probility"><a href="#part-three：node-probility" class="headerlink" title="part three：node probility"></a>part three：node probility</h2><p><strong>最大化当前数据出现可能的概率密度函数</strong><br>对于钻石的位置而言，它的Huffman code是1000，那就意味着在每一次二叉选择的时候，它需要一次被分到1，三次被分到0，而且每次分的过程中，只有1/0可以选择，这是不是和logistic regression里面的0/1分类相似，所以这边我们也直接使用了lr里面的交叉熵来作为loss function。</p>
<p><strong>其实对于很多机器学习的算法而言，都是按照先假定一个模型，再构造一个损失函数，通过数据来训练损失函数求argmin(损失函数)的参数，放回到原模型。</strong></p>
<p>让我们详细的看这个钻石这个例子：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-99ecf230215dbef1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="第一步"><br>p(1|No.1层未知参数)=sigmoid(No.1层未知参数)</p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-fe820cfac29888f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="第二步"><br>p(0|No.2层未知参数)=sigmoid(No.2层未知参数)<br>同理，第三第四层：<br>p(0|No.3层未知参数)=sigmoid(No.3层未知参数)<br>p(0|No.4层未知参数)=sigmoid(No.4层未知参数)<br>然后求p(1|No.1层未知参数)xp(0|No.2层未知参数)xp(0|No.3层未知参数)xp(0|No.4层未知参数)最大下对应的每层的未知参数即可，求解方式与logistic求解方式近似，未知参数分布偏导，后续采用梯度下降的方式（极大、批量、牛顿按需使用）</p>
<h2 id="part-four：approximate-nerual-network"><a href="#part-four：approximate-nerual-network" class="headerlink" title="part four：approximate nerual network"></a>part four：approximate nerual network</h2><p><strong>商品的相似度</strong><br>刚才在part three里面有个p(1|No.1层未知参数)这个逻辑，这个NO.1层未知参数里面有一个就是商品向量。<br>举个例子：<br>存在1000万个用户有过：“啤酒=&gt;西瓜=&gt;剃须刀=&gt;百事可乐”的商品购买顺序<br>10万个用户有过：“啤酒=&gt;苹果=&gt;剃须刀=&gt;百事可乐”的商品购买顺序，如果按照传统的概率模型比如navie bayes 或者n-gram来看，P（啤酒=&gt;西瓜=&gt;剃须刀=&gt;百事可乐）&gt;&gt;p（啤酒=&gt;苹果=&gt;剃须刀=&gt;百事可乐），但是实际上这两者的人群应该是同一波人，他们的属性特征一定会是一样的才对。</p>
<p>我们这边通过了随机初始化每个商品的特征向量，然后通过part three的概率模型去训练，最后确定了词向量的大小。除此之外，还可以通过神经网络算法去做这样的事情。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-24a3b7c78716ac0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br> Bengio 等人在 2001 年发表在 NIPS 上的文章《A Neural Probabilistic Language Model》介绍了详细的方法。<br>我们这边需要知道的就是，对于最小维度商品，我们以商品向量（0.8213，0.8232，0.6613，0.1234，…）的形式替代了0-1点（0，0，0，0，0，1，0，0，0，0…），单个的商品向量无意义，但是成对的商品向量我们就可以比较他们间的余弦相似度，就可以比较类目的相似度，甚至品类的相似度。</p>
<h1 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h1><p>1.数据读取<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mt</span><br><span class="line">from gensim.models import word2vec</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">order_data = pd.read_table(&apos;C:/Users/17031877/Desktop/SuNing/cross_sell_data_tmp1.txt&apos;)</span><br><span class="line">dealed_data = order_data.drop(&apos;member_id&apos;, axis=1)</span><br><span class="line">dealed_data = pd.DataFrame(dealed_data).fillna(value=&apos;&apos;)</span><br></pre></td></tr></table></figure></p>
<p>2.简单的数据合并整理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 数据合并</span><br><span class="line">dealed_data = dealed_data[&apos;top10&apos;] + [&quot; &quot;] + dealed_data[&apos;top9&apos;] + [&quot; &quot;] + dealed_data[&apos;top8&apos;] + [&quot; &quot;] + \</span><br><span class="line">              dealed_data[&apos;top7&apos;] + [&quot; &quot;] + dealed_data[&apos;top6&apos;] + [&quot; &quot;] + dealed_data[&apos;top5&apos;] + [&quot; &quot;] + dealed_data[</span><br><span class="line">                  &apos;top4&apos;] + [&quot; &quot;] + dealed_data[&apos;top3&apos;] + [&quot; &quot;] + dealed_data[&apos;top2&apos;] + [&quot; &quot;] + dealed_data[&apos;top1&apos;]</span><br><span class="line"></span><br><span class="line"># 数据分列</span><br><span class="line">dealed_data = [s.encode(&apos;utf-8&apos;).split() for s in dealed_data]</span><br><span class="line"></span><br><span class="line"># 数据拆分</span><br><span class="line">train_data, test_data = train_test_split(dealed_data, test_size=0.3, random_state=42)</span><br></pre></td></tr></table></figure></p>
<p>3.模型训练<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据训练</span><br><span class="line"># sg=1,skipgram;sg=0,SBOW</span><br><span class="line"># hs=1:hierarchical softmax,huffmantree</span><br><span class="line"># nagative = 0 非负采样</span><br><span class="line">model = word2vec.Word2Vec(train_data, sg=1, min_count=10, window=2, hs=1, negative=0)</span><br></pre></td></tr></table></figure></p>
<p>接下来就是用model来训练得到我们的推荐商品，这边有三个思路，可以根据具体的业务需求和实际数据量来选择：<br>3.1 相似商品映射表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 最后一次浏览商品最相似的商品组top3</span><br><span class="line">x = 1000</span><br><span class="line">result = []</span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line">for i in range(x):</span><br><span class="line">    test_data_split = [s.encode(&apos;utf-8&apos;).split() for s in test_data[i]]</span><br><span class="line">    k = len(test_data_split)</span><br><span class="line">    last_one = test_data_split[k - 1]</span><br><span class="line">    last_one_recommended = model.most_similar(last_one, topn=3)</span><br><span class="line">    tmp = last_one_recommended[0] + last_one_recommended[1] + last_one_recommended[2]</span><br><span class="line">    last_one_recommended = pd.concat([pd.DataFrame(last_one), pd.DataFrame(np.array(tmp))], axis=0)</span><br><span class="line">    last_one_recommended = last_one_recommended.T</span><br><span class="line">    result = pd.concat([pd.DataFrame(last_one_recommended), result], axis=0)</span><br></pre></td></tr></table></figure></p>
<p>考虑用户最后一次操作的关注物品x，干掉那些已经被用户购买的商品，剩下的商品表示用户依旧有兴趣但是因为没找到合适的或者便宜的商品，通过商品向量之间的相似度，可以直接计算出，与其高度相似的商品推荐给用户。</p>
<p>3.2 最大可能购买商品<br>根据历史上用户依旧购买的商品顺序，判断根据当前这个目标用户近期买的商品，接下来他最有可能买什么？<br>比如历史数据告诉我们，购买了手机+电脑的用户，后一周内最大可能会购买背包，那我们就针对那些近期购买了电脑+手机的用户去推送电脑包的商品给他，刺激他的潜在规律需求。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 向量库</span><br><span class="line">rbind_data = pd.concat(</span><br><span class="line">    [order_data[&apos;top1&apos;], order_data[&apos;top2&apos;], order_data[&apos;top3&apos;], order_data[&apos;top4&apos;], order_data[&apos;top5&apos;],</span><br><span class="line">     order_data[&apos;top6&apos;], order_data[&apos;top7&apos;], order_data[&apos;top8&apos;], order_data[&apos;top9&apos;], order_data[&apos;top10&apos;]], axis=0)</span><br><span class="line">x = 50</span><br><span class="line">start = []</span><br><span class="line">output = []</span><br><span class="line">score_final = []</span><br><span class="line">for i in range(x):</span><br><span class="line">    score = np.array(-100000000000000)</span><br><span class="line">    name = np.array(-100000000000000)</span><br><span class="line">    newscore = np.array(-100000000000000)</span><br><span class="line">    tmp = test_data[i]</span><br><span class="line">    k = len(tmp)</span><br><span class="line">    last_one = tmp[k - 2]</span><br><span class="line">    tmp = tmp[0:(k - 1)]</span><br><span class="line">    for j in range(number):</span><br><span class="line">        tmp1 = tmp[:]</span><br><span class="line">        target = rbind_data_level[j]</span><br><span class="line">        tmp1.append(target)</span><br><span class="line">        test_data_split = [tmp1]</span><br><span class="line">        newscore = model.score(test_data_split)</span><br><span class="line">        if newscore &gt; score:</span><br><span class="line">            score = newscore</span><br><span class="line">            name = tmp1[len(tmp1) - 1]</span><br><span class="line">        else:</span><br><span class="line">            pass</span><br><span class="line">    start.append(last_one)</span><br><span class="line">    output.append(name)</span><br><span class="line">    score_final.append(score)</span><br></pre></td></tr></table></figure></p>
<p>3.3 联想记忆推荐<br>在3.2中，我们根据了这个用户近期购买行为，从历史已购用户的购买行为数据发现规律，提供推荐的商品。还有一个近似的逻辑，就是通过目标用户最近一次的购买商品进行推测，参考的是历史用户的单次购买附近的数据，详细如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-0a50817d8b364009.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这个实现也非常的简单，这边代码我自己也没有写，就不贴了，采用的还是word2vec里面的<code>predict_output_word(context_words_list, topn=10)</code>，Report the probability distribution of the center word given the context words as input to the trained model</p>
<p>其实，这边详细做起来还是比较复杂的，我这边也是简单的贴了一些思路，如果有不明白的可以私信我，就这样，最后，谢谢阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于自然语言识别下的流失用户预警]]></title>
      <url>/2017/08/15/%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%AF%86%E5%88%AB%E4%B8%8B%E7%9A%84%E6%B5%81%E5%A4%B1%E7%94%A8%E6%88%B7%E9%A2%84%E8%AD%A6/</url>
      <content type="html"><![CDATA[<blockquote>
<p>在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，<strong>通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券安慰”，”特权享受”等行为，有效的降低了用户的流失。</strong>根据实际的业务营销效果，在模型上线后，<strong>abtest检验下模型识别用户人群进行营销后的流失率比随意营销下降9.2%，效果显著。</strong></p>
</blockquote>
<p>当前文本文义识别存在一些问题：<br>（1）准确率而言，很多线上数据对特征分解的过程比较粗糙，<strong>很多直接基于df或者idf结果进行排序</strong>，在算法设计过程中，<strong>也是直接套用模型</strong>，只是工程上的实现，缺乏统计意义上的分析；</p>
<p>（2）<strong>文本越多，特征矩阵越稀疏，计算过程越复杂</strong>。常规的文本处理过程中只会对文本对应的特征值进行排序，其实在文本选择中，可以先剔除相似度较高的文本，这个课题比较大，后续会单独开一章进行研究；</p>
<p>（3）<strong>扩展性较差</strong>。比如我们这次做的流失用户预警是基于电商数据，你拿去做通信商的用户流失衡量的话，其质量会大大下降，所以重复开发的成本较高，这个属于非增强学习的硬伤，目前也在攻克这方面的问题。</p>
<p>首先，我们来看下，整个算法设计的思路：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.通过hive将近期的用户评价hadoop文件下载为若干个text文件</span><br><span class="line">2.通过R语言将若干个text整合读取为一个R内的dataframe</span><br><span class="line">3.利用R里面的正则函数将文本中的异常符号‘#！@￥%%’，英文，标点等去除</span><br><span class="line">（这边可以在hive里面提前处理好，也可以在后续的分词过程中利用停顿词去除）</span><br><span class="line">4.文本分词，这边可以利用R中的Rwordseg，jiebaR等，我写这篇文章之前看到很多现有的语义分析的文章中，Rwordseg用的挺多，所以这边我采用了jiebaR</span><br><span class="line">5.文本分词特征值提取,常见的包括互信息熵，信息增益，tf-idf，本文采取了tf-idf，剩余方法会在后续文章中更新</span><br><span class="line">6.模型训练</span><br><span class="line">      这边我采取的方式是利用概率模型naive bayes+非线性模型random forest先做标签训练，最后用nerual network对结果进行重估</span><br><span class="line">（原本我以为这样去做会导致很严重的过拟合，但是在实际操作之后发现，过拟合并不是很严重，至于原因我也不算很清楚，后续抽空可以研究一下）</span><br></pre></td></tr></table></figure></p>
<p>下面，我们来剖析文本分类识别的每一步</p>
<h1 id="定义用户属性"><a href="#定义用户属性" class="headerlink" title="定义用户属性"></a>定义用户属性</h1><p>首先，我们定义了已经存在的流失用户及非流失用户，易购的用户某品类下的购买周期为27天，针对前60天-前30天下单购物的用户，观察近30天是否有下单行为，如果有则为非流失用户，如果没有则为流失用户。提取每一个用户最近一次商品评价作为msg。</p>
<h1 id="文本合成"><a href="#文本合成" class="headerlink" title="文本合成"></a>文本合成</h1><p>通过hive -e的方式下载到本地，会形成text01，text02…等若干个文本，通过R进行<strong>文本整合</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#先设置文本路径</span><br><span class="line">path &lt;- &quot;C:/Users/17031877/Desktop/Nlp/answer/Cmsg&quot;</span><br><span class="line">completepath &lt;- list.files(path, pattern = &quot;*.txt$&quot;, full.names = TRUE)</span><br><span class="line"></span><br><span class="line">#批量读入文本</span><br><span class="line">readtxt &lt;- function(x) &#123;</span><br><span class="line">  ret &lt;- readLines(x)                   #每行读取</span><br><span class="line">  return(paste(ret, collapse = &quot;&quot;))     #通过paste将每一行连接起来</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#lappy批量操作，形成list，个人感觉对非关系数据，list处理更加便捷</span><br><span class="line">msg &lt;- lapply(completepath, readtxt)</span><br><span class="line"></span><br><span class="line">#用户属性</span><br><span class="line">user_status &lt;- list.files(path, pattern = &quot;*.txt$&quot;)</span><br><span class="line"></span><br><span class="line">#stringsAsFactors=F，避免很多文本被读成因子类型</span><br><span class="line">comment &lt;- as.data.frame(cbind(user_status, unlist(msg)),stringsAsFactors = F)</span><br><span class="line">colnames(comment) &lt;- c(&quot;user_status&quot;, &quot;msg&quot;)</span><br></pre></td></tr></table></figure></p>
<p>基础的数据整合就完成了。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-44f62ac8f31504a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="数据整理"><a href="#数据整理" class="headerlink" title="数据整理"></a>数据整理</h1><p>也可以看到，基础数据读取完成后，还是很多评论会有一些<strong>不规则的数据</strong>，包括‘#￥%……&amp;’，英文，数字，下面通过正则、停顿词的方式进行处理：</p>
<h2 id="正则化处理"><a href="#正则化处理" class="headerlink" title="正则化处理"></a>正则化处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#直接处理</span><br><span class="line">comment$msg &lt;- gsub(pattern = &quot; &quot;, replacement =&quot;&quot;, comment$msg)  #gsub是字符替换函数，去空格</span><br><span class="line">comment$msg&lt;- gsub(&quot;[[:digit:]]*&quot;, &quot;&quot;, comment$msg) #清除数字[a-zA-Z]</span><br><span class="line">comment$msg&lt;- gsub(&quot;[a-zA-Z]&quot;, &quot;&quot;, comment$msg)   #清除英文字符</span><br><span class="line">comment$msg&lt;- gsub(&quot;\\.&quot;, &quot;&quot;, comment$msg)      #清除全英文的dot符号</span><br><span class="line">--------------------------------------------------------------------------------------------------</span><br><span class="line">#如果是常做nlp处理，可以写成函数打包，后期直接library就可以了</span><br><span class="line">#数值删除</span><br><span class="line">removeNumbers =</span><br><span class="line">    function(x)&#123;</span><br><span class="line">    ret = gsub(&quot;[0-9]&quot;,&quot;&quot;,x)</span><br><span class="line">    return(ret)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">#字符删除</span><br><span class="line">removeLiters =</span><br><span class="line">    function(x)&#123;</span><br><span class="line">    ret = gsub(&quot;[a-z|A-Z]&quot;,&quot;&quot;,x)</span><br><span class="line">    return(ret)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">#各种操作符处理,\s表示空格,\r表示回车,\n表示换行</span><br><span class="line">removeActions =</span><br><span class="line">    function(x)&#123;</span><br><span class="line">    ret = gsub(&quot;\\s|\\r|\\n&quot;, &quot;&quot;, x)</span><br><span class="line">    return(ret)</span><br><span class="line">    &#125;</span><br><span class="line">comment$msg=removeNumbers(comment$msg)</span><br><span class="line">comment$msg=removeLiters(comment$msg)</span><br><span class="line">comment$msg=removeActions (comment$msg)</span><br></pre></td></tr></table></figure>
<p>这边需要对正则化里面的一些表示有所了解，详细可以百度，一般我都是具体需求具体去看，因为太多，自己又懒，所以没记</p>
<h2 id="停顿词"><a href="#停顿词" class="headerlink" title="停顿词"></a>停顿词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#加载jiebaR包</span><br><span class="line">library(jiebaR)</span><br><span class="line"></span><br><span class="line">#找jiebaR存停顿词的地方，自行将需要处理掉的符号存进去，我这边是C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8</span><br><span class="line">tagger&lt;-worker(stop_word=&quot;C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8&quot;)</span><br></pre></td></tr></table></figure>
<p>至于位置可以通过直接输入<code>worker()</code>查看，</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-685ed65594d6c27b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>当前的是没有stop_word的，所有词存储的位置在：C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/下</p>
<h1 id="文本分词"><a href="#文本分词" class="headerlink" title="文本分词"></a>文本分词</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#jieba 分词,去除停顿词</span><br><span class="line">library(jiebaR)</span><br><span class="line">tagger&lt;-worker(stop_word=&quot;C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8&quot;)</span><br><span class="line">words=list()</span><br><span class="line">for (i in 1:nrow(comment))&#123;</span><br><span class="line">    tmp=tagger[comment[i,2]]</span><br><span class="line">    words=c(words,list(tmp))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>直接先分词，但是分词结果会存在很多只有一个字比如‘的’、‘你’、‘我’等或者很多无意义的长句‘中华人民共和国’、‘长使英雄泪满襟’等，需要把这些<strong>词长异常</strong>明显无意义的词句去掉。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#词长统计</span><br><span class="line">whole_words_set=unlist(words)</span><br><span class="line">whole_words_set_rank=data.frame(table(whole_words_set))</span><br><span class="line"></span><br><span class="line">whole_words_set_dealed=c()</span><br><span class="line">for (i in 1:nrow(whole_words_set_rank))&#123;</span><br><span class="line">    tmp=nchar(as.character(whole_words_set_rank[i,1]))</span><br><span class="line">    whole_words_set_dealed=c(whole_words_set_dealed,tmp)</span><br><span class="line">&#125;</span><br><span class="line">whole_words_set_dealed=cbind(whole_words_set_rank,whole_words_set_dealed)</span><br><span class="line">whole_words_set_dealed=whole_words_set_dealed[whole_words_set_dealed$whole_words_set_dealed&gt;1&amp;whole_words_set_dealed$whole_words_set_dealed&lt;5,]</span><br><span class="line">whole_words_set_dealed=whole_words_set_dealed[order(whole_words_set_dealed$Freq,decreasing=T),]</span><br><span class="line"></span><br><span class="line">#words的删除异常值,排序</span><br><span class="line">whole_words_set_sequence=words</span><br><span class="line">key_word=nrow(words)</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">    for (j in 1:length(words[[i]]))&#123;</span><br><span class="line">    tmp=ifelse(nchar(words[[i]][j])&gt;1 &amp; nchar(words[[i]][j])&lt;5,words[[i]][j],&apos;&apos;)</span><br><span class="line">    whole_words_set_sequence[[i]][j]=tmp</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">    whole_words_set_sequence[[i]]=whole_words_set_sequence[[i]][whole_words_set_sequence[[i]]!=&apos;&apos;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="tf-idf词特征值重要性排序"><a href="#tf-idf词特征值重要性排序" class="headerlink" title="tf-idf词特征值重要性排序"></a>tf-idf词特征值重要性排序</h1><p>首先，我们大致看一下排序的数据依旧：</p>
<p>TF = 某词在文章中出现的次数/文章包含的总词数（或者文章有价值词次数）<br>DF = （包含某词的文档数）/（语料库的文档总数）<br>IDF = log（（语料库的文档总数）/（包含某词的文档数+1））<br>这边的+1是为了避免（语料库的文档总数）/（包含某词的文档数）=1，log(1)=0，使得最后的重要性中出现0的情况，与有意义的前提相互驳斥。<br>TF-IDF = TF*IDF</p>
<p>分别看下，里面的每一项的意义：<br>TF，我们可以看出，<strong>在同一个评论中，词数出现的越多，代表这个词越能成为这篇文章的代表</strong>，当然前提是非无意义的助词等。</p>
<p>IDF，我们可以看出，<strong>所以评论中，包含目标词的评论的占比，占比数越高，目标词的意义越大</strong>，假设1000条评论中，“丧心病狂”在一条评论里面重复了10次，但是其他999条里面一次也没有出现，那就算“丧心病狂”非常能代表这条评论，但是在做文本集特征考虑的情况下，它的价值也是不大的。</p>
<p>下面，我们来看代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#tfidf_partone 为对应的tf</span><br><span class="line">tdidf_partone=whole_words_set_sequence</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp1=as.data.frame(prop.table(table(whole_words_set_sequence[[i]])))</span><br><span class="line">tdidf_partone[[i]]=tmp1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#tdidf_partfour 为对应的idf</span><br><span class="line">tdidf_parttwo=unique(unlist(whole_words_set_sequence))</span><br><span class="line">tdidf_max=length(tdidf_parttwo)</span><br><span class="line">tdidf_partthree=tdidf_parttwo</span><br><span class="line">for (i in 1:tdidf_max)&#123;</span><br><span class="line">tmp=0</span><br><span class="line">aimed_word=tdidf_parttwo[i]</span><br><span class="line">    for (j in 1:key_word)&#123;</span><br><span class="line">    tmp=tmp+sum(tdidf_parttwo[i] %in% whole_words_set_sequence[[j]])</span><br><span class="line">    &#125;</span><br><span class="line">tdidf_partthree[i]=log(as.numeric(key_word)/(tmp+1))</span><br><span class="line">&#125;</span><br><span class="line">tdidf_partfour=cbind(tdidf_parttwo,tdidf_partthree)</span><br><span class="line">tdidf_partfive=tdidf_partone</span><br><span class="line">colnames(tdidf_partfour)&lt;-c(&apos;Var1&apos;,&apos;Freq1&apos;)</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tdidf_partfive[[i]]=merge(tdidf_partone[[i]],tdidf_partfour,by=c(&quot;Var1&quot;))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#计算tf-idf结果，并排序key_word</span><br><span class="line">tdidf_partsix=tdidf_partfive</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp=tdidf_partfive[[i]][,2:3]</span><br><span class="line">tdidf_partsix[[i]][,2]=as.numeric(tmp[,1])*as.numeric(tmp[,2])</span><br><span class="line">tdidf_partsix[[i]]=tdidf_partsix[[i]][order(tdidf_partsix[[i]][,2],decreasing=T),][]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">key_word=c()</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp=tdidf_partsix[[i]][1:5,1]</span><br><span class="line">key_word=rbind(key_word,as.character(tmp))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>理论上讲，如果这边数据存储方式用的是data.frame的话，可以利用spply、apply等批量处理函数，这边用得是list的方式，对lpply不是很熟悉的我，选择了for的循环，后续这边会优化一下，这样太消耗资源了。</p>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><p>这边，我最后采取的是概率模型naive bayes+非线性模型random forest先做标签训练，最后用nerual network对结果进行重估方式，但是在训练过程中，我还有几种模型的尝试，这边也一并贴出来给大家做参考。</p>
<h2 id="数据因子化的预处理"><a href="#数据因子化的预处理" class="headerlink" title="数据因子化的预处理"></a>数据因子化的预处理</h2><p>这边得到了近400维度的有效词，现在将每一维度的词遍做一维的feature，同时，此处的feature的意义为要么评论存在该词，要么评论中不存在该词的0-1问题，<strong>需要因子化一下</strong>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#整合数据</span><br><span class="line">well_dealed_data=cbind(as.character(comment[,1]),key_word)</span><br><span class="line">names=as.data.frame(table(key_word))[,1]</span><br><span class="line">names_count=length(names)</span><br><span class="line">names=as.matrix(names,names_count,1)</span><br><span class="line">feature_matrix=matrix(rep(0,names_count*key_word),key_word,names_count)</span><br><span class="line">for (i in 1:names_count)&#123;</span><br><span class="line">    for(j in 1:key_word)&#123;</span><br><span class="line">    feature_matrix[j,i]=ifelse(names[i] %in% key_word[j,],1,0)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#art=1,literature=-1,标签0-1化</span><br><span class="line">feature_matrix=cbind(well_dealed_data[,1],feature_matrix)</span><br><span class="line">feature_matrix[feature_matrix[,1]==&apos;aimed&apos;,1]=&apos;1&apos;</span><br><span class="line">feature_matrix[feature_matrix[,1]==&apos;unaimed&apos;,1]=&apos;-1&apos;</span><br><span class="line"></span><br><span class="line">feature_matrix=as.data.frame(feature_matrix)</span><br><span class="line"></span><br><span class="line">num=1:(ncol(feature_matrix)-1)</span><br><span class="line">value_name=paste(&quot;feature&quot;,num)</span><br><span class="line">value_name=c(&apos;label&apos;,value_name)</span><br><span class="line">colnames(feature_matrix)=value_name</span><br><span class="line"></span><br><span class="line">#feature0-1化</span><br><span class="line">for (i in 1:ncol(feature_matrix))&#123;</span><br><span class="line">feature_matrix[,i]=as.factor(as.numeric(as.character(feature_matrix[,i])))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="数据切分训练测试"><a href="#数据切分训练测试" class="headerlink" title="数据切分训练测试"></a>数据切分训练测试</h2><p>这边就不适用切分函数了，自己写了一个更加快速。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n_index=sample(1:nrow(feature_matrix),round(0.7*nrow(feature_matrix)))</span><br><span class="line">train_feature_matrix=feature_matrix[n_index,]</span><br><span class="line">test_feature_matrix=feature_matrix[-n_index,]</span><br></pre></td></tr></table></figure></p>
<h2 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="backpropagation-neural-network"><a href="#backpropagation-neural-network" class="headerlink" title="backpropagation neural network"></a>backpropagation neural network</h3><p><strong>这边需要用网格算法对size和decay进行交叉检验，这边不贴细节，可以百度搜索详细过程。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(nnet)</span><br><span class="line">nn &lt;- nnet(label~., data=train_feature_matrix, size=2, decay=0.01, maxit=1000, linout=F, trace=F)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">nn.predict_train = predict(nn,train_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),nn.predict_train)</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">nn.predict_test = predict(nn,test_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),nn.predict_test)</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="Linear-Support-Vector-Machine"><a href="#Linear-Support-Vector-Machine" class="headerlink" title="Linear Support Vector Machine"></a>Linear Support Vector Machine</h3><p><strong>这边需要用网格算法对cost进行交叉检验，这边不贴细节，可以百度搜索详细过程。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">svmfit &lt;- svm(label~., data=train_feature_matrix, kernel = &quot;linear&quot;, cost = 10, scale = FALSE) # linear svm, scaling turned OFF</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">svmfit.predict_train=predict(svmfit, train_feature_matrix, type = &quot;probabilities&quot;)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(svmfit.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">svmfit.predict_test = predict(svmfit,test_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(svmfit.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h3><p><strong>这边我没调参，我觉得这边做的好坏在于数据预处理中剩下来的特征词</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">sms_classifier &lt;- naiveBayes(train_feature_matrix[,-1], train_feature_matrix$label)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">sms.predict_train=predict(sms_classifier, train_feature_matrix)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(sms.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">sms.predict_test = predict(sms_classifier,test_feature_matrix)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(sms.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p><strong>这边因为是最后的整合模型，需要调参的地方比较多，首先根据oob确定在mtry=log（feature）下的最优trees数量，在根据确定的trees的数量，反过来去确定mtry的确定值。除此之外，还需要对树的最大深度，子节点的停止条件做交叉模拟，是整体模型训练过程中最耗时的地方</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(randomForest)</span><br><span class="line">randomForest=randomForest(train_feature_matrix[,-1], train_feature_matrix$label)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">rf.predict_train=predict(randomForest, train_feature_matrix)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(rf.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">rf.predict_test = predict(randomForest,test_feature_matrix)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(rf.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<p>就单模型下的test集合的准确率如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-c2eca9e5da73375b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>整体上看，nnet是过拟合的，所以在测试集上的效果折扣程度最大；naive bayes模型的拟合效果应该是最弱的，但是好在它的开发成本低，逻辑简单，有统计意义；svm和randomforest这边的效果不相上下。本次训练的数据量在20w条左右，理论上讲再扩大数据集的话，randomforest的效果应该会稳定，svm会下降，nnet会上升。</p>
<h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p><img src="http://upload-images.jianshu.io/upload_images/1129359-e20c4b10905b9090.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这边的train_data的准确率在92.1%，test_data的准确率在84.3%，与理想的test_data90%以上的准确率还是有差距，所以后续准备：<br>1.细化流失用户的定义方式，当前定义过于笼统粗糙<br>2.以RNN的模型去替代BpNN去做整合训练，探索特征到特征本身的激活会对结果的影响<br>3.重新定义词重要性，考虑互信息熵及isolation forest的判别方式</p>
<p>最后谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 自然语言 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据预处理-异常值识别]]></title>
      <url>/2017/08/09/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%BC%82%E5%B8%B8%E5%80%BC%E8%AF%86%E5%88%AB/</url>
      <content type="html"><![CDATA[<p>系统总结了常用的异常值识别思路，整理如下：</p>
<h1 id="空间识别"><a href="#空间识别" class="headerlink" title="空间识别"></a>空间识别</h1><h2 id="分位数识别"><a href="#分位数识别" class="headerlink" title="分位数识别"></a>分位数识别</h2><p>代表的执行方法为<strong>箱式图</strong>：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-51aed3325eb4ffc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="箱式图（来自百度百科）"></p>
<p>上四分位数Q3，又叫做升序数列的75%位点<br>下四分位数Q1，又叫做升序数列的25%位点<br>箱式图检验就是摘除大于<code>Q3+3/2*（Q3-Q1）</code>，小于<code>Q1-3/2*（Q3-Q1）</code>外的数据，并认定其为异常值；针对全量样本已知的问题比较好，缺点在于数据量庞大的时候的排序消耗<br>R语言中的<code>quantile</code>函数，python中的<code>percentile</code>函数可以直接实现。</p>
<h2 id="距离识别"><a href="#距离识别" class="headerlink" title="距离识别"></a>距离识别</h2><p>最常用的就是<strong>欧式距离</strong>，<br>比如：两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：<br> <a href="http://photo.blog.sina.com.cn/showpic.html#blogid=52510b1d01015nrg&amp;url=http://s16.sinaimg.cn/orignal/52510b1dt7bf041c28faf" target="_blank" rel="noopener"><img src="http://upload-images.jianshu.io/upload_images/1129359-0c59a539345df0a5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="欧式距离"></a></p>
<p>可以直观感受的到，图中，距离蓝色B点距离为基准衡量的话，红色A1，红色A2，红色A3为距离较近点，A4为距离较远的异常点。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-85a082d25a621303.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>但是这样看问题会有一个隐患，我们犯了“就点论点”的错误没有考虑到全局的问题，让我在看下面这张图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d98be861833fd35e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>还是刚才那张图，橙色背景为原始数据集分布，这样看来A4的位置反而比A1、A3相对更靠近基准点B，所以在存在纲量不一致且数据分布异常的情况下，可以使用<strong>马氏距离</strong>代替<strong>欧式距离</strong>判断数据是否离群。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-00335f3bc0d0c60c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="马氏距离"><br>其中，<code>μ</code>为feature的均值，<code>X</code>为观察值，<code>Σ</code>为feature的协方差矩阵<br>马氏距离除了用来判断点是否异常，也可以用来判断两个数据集相识度，在图像识别，反欺诈识别中应用的也是非常普遍；问题在于太过于依赖<code>Σ</code>，不同的base case对应的<code>Σ</code>都是不一致的，不是很稳定</p>
<h2 id="密度识别"><a href="#密度识别" class="headerlink" title="密度识别"></a>密度识别</h2><p>密度识别的方式方法比较多，这边就提供其中比较经典的，首先我们可以通过密度聚类中大名鼎鼎的<code>dbscan</code>入手，这边只讲思路，详细的算法过程另行介绍。<br>简单的来讲，下面这张图可以协助理解</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-baa6be7f93b82ab9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们可以通过随机选择联通点，人为设置联通点附近最小半径a，半径内最小容忍点个数b，再考虑密度可达，形成蓝色方框内的正常数据区域，剩下的黄色区域内的点即为异常点。</p>
<p>除此之外，密度识别里面还有一种方式，是参考单点附近的<strong>点密度</strong>判断，伪代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.从特征集合中任选历史上没有被选择过的两维</span><br><span class="line">2.将原始点集映射到该两维平面上，刻画点集中心a</span><br><span class="line">3.以点集中心a，x为半径画圆，不断扩大x的值，直到被覆盖的点集数/原始点集数的最低阈值</span><br><span class="line">4.没有被覆盖到的点集打上outlier_label</span><br><span class="line">5.repeat 1-4</span><br><span class="line">6.统计点对应的outlier_label个数</span><br><span class="line">7.排序高则优先为异常点</span><br></pre></td></tr></table></figure></p>
<p>这边参考了<code>Clique</code>的映射+<code>Denclue</code>的密度分布函数的思路，注意问题就是计算量大，所以对小样本的适用程度更高一些，针对大样本多特征的数据可以考虑对样本进行子集抽样，再根据子集进行1-5，汇总后整体进行6-7步骤，实际检验效果仍然可以达到不抽样的85%以上</p>
<h2 id="拉依达准则"><a href="#拉依达准则" class="headerlink" title="拉依达准则"></a>拉依达准则</h2><p>这个方法更加偏统计一些，设计到一些距离的计算，勉强放在空间识别里面</p>
<p>这种判别处理原理及方法仅局限于对<strong>正态或近似正态分布的样本</strong>数据处理，它是以<strong>数据充分大</strong>为前提的，当数据较少的情况下，最好不要选用该准则。</p>
<p>正态分布（高斯分布）是最常用的一种概率分布，通常正态分布有两个参数μ和σ为标准差。N(0,1)即为标准正态分布。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-ec554b00cb9bdd18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>从上面这张图可以看出，当数据对象值偏离均值3倍标准差的时候，该数据合理的出现可能性小于3‰，所以可以直接认定，数据偏离均值±3倍标准差时，为异常点。但是最后再次强调一遍：<strong>它是以数据充分大为前提的，当数据较少的情况下，最好不要选用该准则。</strong></p>
<h1 id="计量识别"><a href="#计量识别" class="headerlink" title="计量识别"></a>计量识别</h1><h2 id="G-test或者说是likelihood-Ratio方法"><a href="#G-test或者说是likelihood-Ratio方法" class="headerlink" title="G-test或者说是likelihood Ratio方法"></a>G-test或者说是likelihood Ratio方法</h2><p>G-test这类方法运用在医学方面较多，常用于检验观测变量值是否符合理论期望的比值。现在也用在电商、出行、搜索领域检验一些无监督模型的质量、数据质量。</p>
<p>当我们新上一个模型，部分用户的反馈特别异常，我们不知道是不是异常数据，在接下来的分析中需不需要剔除，我们可以用统计学方法予以取舍。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-46850d0fc19e37d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>其中O为观测值，E为期望值，假如我们的网站每天24个小时的订单量分布稳定，分时段计算出一个均值，E1，E2，..E24，新模型产出后，我们问题用户群对应的24个小时的订单分布值O1,O2,…..O24,套用上面的公式，我们就可以计算出一个G值出来。</p>
<p>然后在根据G-test的base值，观察目标用户可信的最大置信度，判断置信度是否符合我们的最低要求；likelihood Ratio方法类似，相关论文可以直接搜索。</p>
<h2 id="模型拟合"><a href="#模型拟合" class="headerlink" title="模型拟合"></a>模型拟合</h2><p>这类方法属于简单有监督识别，常见的包括贝叶斯识别，决策树识别，线性回归识别等等。</p>
<p>需要提前知道两组数据：正常数据及非正常类数据，再根据它们所对应的特征，去拟合一条尽可能符合的曲线，后续直接用该条曲线去判断新增的数据是否正常。<br>举个例子：<br>金融借贷中，我们事先活动一批正常借贷用户，和逾期不还用户，我们通过<strong>打分卡模型</strong>去识别已知用户的特征，假设得到芝麻分、手机使用时长、是否为男性为关键特征。接下来判断未知标签的新增数据是否为正常用户的话，直接根据之前判断出来的拟合打分卡曲线去做0-1概率预估就行了。</p>
<p>但是模型拟合的方式使用情况较为局限，绝大多数异常识别问题是无法拿到前置的历史区分数据，或者已分好的数据不能够覆盖全量可能，导致时间判断误差较大，顾一般只做emsemble model的其中一种组合模块，不建议做主要依赖标注。</p>
<p>明尼苏达州大学有过一篇异常论文识别的总结，里面关于有监督模型、半监督模型、无监督模型等模型拟合讲的非常细致，如果感兴趣可以研究一下，附上论文Survey：<a href="http://cucis.ece.northwestern.edu/projects/DMS/publications/AnomalyDetection.pdf" target="_blank" rel="noopener">http://cucis.ece.northwestern.edu/projects/DMS/publications/AnomalyDetection.pdf</a></p>
<h2 id="变维识别"><a href="#变维识别" class="headerlink" title="变维识别"></a>变维识别</h2><p>首先，我们来看一下PCA的伪代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.去除平均值,方便后续协方差，方差矩阵的计算</span><br><span class="line">2.计算协方差矩阵及其特征值和特征向量</span><br><span class="line">3.将特征值从大到小排序，特征值可以反映方差贡献度，特征值越大，方差贡献度越大</span><br><span class="line">4.保留最大的N个特征值以及它们的所对应的特征向量</span><br><span class="line">5.将数据映射到上述 N 个特征向量构造的新空间中</span><br></pre></td></tr></table></figure></p>
<p>pca的核心思路在于<strong>尽可能通过feature的组合代替原始feature，使得原始数据的方差最大化</strong>。<br>通过pca可以得到第一主成分、第二主成分…。<br>对于正常数据集来说，正常数据量远远大于异常数据，所以正常数据所贡献的方差远远大于异常数据；通过pca得到的排名靠前的主成分解释了原始数据较大的方差占比，所以理论上讲，<strong>第一主成分反映了正常值的方差，最后一个主成分反映了异常点的方差。</strong>通过第一主成分对原始数据进行映射后，原始数据中的正常样本和异常样本的属性不会随之改变。</p>
<p>存在一个p个维度的数据集orgin_data，X为其协方差矩阵，通过<strong>奇异值分解</strong>可以得到：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-1291fad0ed73f044.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>其中，D为对角阵，其每一个值为X所对应的特征值；P的每一列为X的所对应的特征向量，并将D中的特征值从大到小排列，相应的改变P所对应的列向量。<br>我们选取top(j)个D中的特征值，及其P所对应的特征向量构成(p,j) 维的矩阵 pj ，再将目标数据集orgin_data进行映射：<code>new_data = orgin_data*pj</code>；new_data是一个 (N,j) 维的矩阵。如果考虑拉回映射的话（也就是从主成分空间映射到原始空间），重构之后的数据集合是:<code>back_data=transpose(pj*transpose(new_data))=new_data*transpose(pj)</code>,是使用 top-j 的主成分进行重构之后形成的数据集，是一个 (N,p) 维的矩阵。</p>
<p>所以，我们有如下的outlier socres的定义：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-2f9ebfb4dfec0b7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>ev(j) subject to.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-8245f5bfc8abe382.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>解释一下上面两个公式，先计算score中orgindata的列减去前j个主成分映射回原空间的newdata下的欧式范数值；再考虑不同主成分所需乘以的权重，这边，我们认为，第一主成分所代表的数据中正常数据更多，所以权重越小；当j取到最后一维的主成分下，我们认为权重最高，达到1。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-b3a8ef0320eb1e67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>对于outlier socres过高的点，即为异常点。</p>
<h2 id="神经网络识别"><a href="#神经网络识别" class="headerlink" title="神经网络识别"></a>神经网络识别</h2><p>之前比较火的神经网络分析，同样可以用来做有监督的异常点识别，这边介绍一下Replicator Neural Networks (RNNs)。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-3f1bcac360434516.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这边我们通过图像可以看出：<br>1.输入层中，输入变量个数与输出层中，输出变量一致<br>2.中间层的节点数小于输入输出层节点<br>3.整个训练过程是一个先压缩后解压的过程</p>
<p>常规的，我们通过mse来看模型的误差<br><img src="http://upload-images.jianshu.io/upload_images/1129359-ec67e22ba3fc0ef5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>我们来大致了解一下RNN的运行逻辑，首先，最左边的为输入层即为原始数据，最右层的为输出层即为输出数据。中间各层的激活函数不同，入参经过激活函数所得到的出参的值也不一致，但是在同一层激活函数都是一致的。<br>对于我们异常识别而言，第二层和第四层 (k=2,4)，激活函数选择为<br><img src="http://upload-images.jianshu.io/upload_images/1129359-81cb8a53026c11ab?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>tanh图像如下，可以将原始数据压缩在-1到1之间，使得原始数据有界。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-6da2e1a7721d8241.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<p>对于中间层 (k=3) 而言，激活函数是一个类阶梯 (step-like) 函数。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-2cfe6252e0b63f75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>其中，N为阶梯分层数，a3为提升的效率。N的个数越多，层次分的更多。</p>
<p>比如N=5的形式下：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-6289fcc6b49de116.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="N=5"></p>
<p>比如N=3的形式下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-bae95319a6b3a967.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="N=3"><br>这样做的好处就是，随着N的增加可以将异常点或者异常点群集中在某一个离散阶梯范围内。<br>通过对RNN的有监督训练，构造异常样本分类器，进行异常值识别。</p>
<h2 id="isolation-forest"><a href="#isolation-forest" class="headerlink" title="isolation forest"></a>isolation forest</h2><p>2010年南大的周志华教授提出了一个基于二叉树的异常值识别算法，在工业界来说，效果是非常不错的，最近我也做了一个流失用户模型，实测效果优秀。</p>
<p>和random forest一样，isolation forest是由isolation tree构成，先看一下isolation tree的逻辑：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">method:</span><br><span class="line">1.从原始数据中随机选择一个属性feature；</span><br><span class="line">2.从原始数据中随机选择该属性的下的一个样本值value；</span><br><span class="line">3.根据feature下的value对每条记录进行分类，把小于value的记录放在左子集，把大于等于value的记录放在右子集；</span><br><span class="line">4.repeat 1-3 until：</span><br><span class="line">　　　　4.1.传入的数据集只有一条记录或者多条一样的记录；</span><br><span class="line">　　　　4.2.树的高度达到了限定高度；</span><br></pre></td></tr></table></figure></p>
<p>大致的思路如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-6fb1a1e8d80d6f5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>理论上，异常数据一般都是离群数据，非常容易在早期就被划分到最终子节点。所以，通过计算每个子节点的深度h(x)，来判断数据为异常数据的可能性。论文中，以s(x,n)为判断数据是否异常的衡量指标。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-049beb7fea9ff710.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-2d72b38a7b3b0d12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，h(x)为x对应的节点深度，c(n)为样本可信度，s(x,n)~[0,1]，正常数据来讲s(x,n)小于0.8，s(x,n)越靠近1，数据异常的可能性越大。</p>
<p>单棵树的可信性不足，所以我们通过用emsemble model的思路，去构造一个forest的树群来提高准确性。<br>但是作为isolation forest的时候，需要对原s(x,n)的公式有所更改，通过E(h(x))来替代h(x),其中E(h(x))为数据x在各棵树上的h(x)的平均。<br>同时，<br>1.因为树的个数大大增加，所以需要控制计算的开销，所以每个棵树我们可以采取数据抽样的方式，使得抽样数据集远远小于原始数据集，且根据周志华老师的论文，采样大小超过256效果就提升不大了。<br>2.我们可以控制深度，使得没棵树的最大深度limit length=ceiling(log2(样本大小))，当树深度大于最大深度时，其产生的子节点绝大多数均为正常数据节点，失去异常检验的意义。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据处理 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何校验用户画像的准确性？]]></title>
      <url>/2017/07/20/%E5%A6%82%E4%BD%95%E6%A0%A1%E9%AA%8C%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E7%9A%84%E5%87%86%E7%A1%AE%E6%80%A7%EF%BC%9F/</url>
      <content type="html"><![CDATA[<blockquote>
<p>在用户研究的课题中，用户画像是几乎每个公司都会去做的，浅层的包括统计类的：上月购买量，上周活跃天数等；深层的包括洞察类的：潜在需求偏好，生命周期阶段等；前者的校验简单，后者的校验需要通过一些特别的方式。本文就洞察类画像校验做一系列的梳理。</p>
</blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-e48cbe975dca601c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>省略掉预处理设计的过程，画像校验的步骤主要集中在画像开发，画像上线，画像更新中，并且三个阶段中，每个阶段的校验方式完全不同</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-8ae0b22c2c485037.gif?imageMogr2/auto-orient/strip" alt=""></p>
<h1 id="用户画像开发中"><a href="#用户画像开发中" class="headerlink" title="用户画像开发中"></a>用户画像开发中</h1><p>当我们所开发的用户画像是类似于用户的下单需求、用户的购车意愿、用户是否有注册意愿这一类存在<strong>历史的正负样本</strong>的有监督的问题，我们可以利用历史确定的数据来校验我们的画像准确性。比如，银行在设计用户征信的画像前，会有一批外部购买的坏样本和好样本，其实画像问题就转化为分类问题去解决评估了。<br><strong>1.1 Recall、Pecision、K-S、F1曲线、Roc曲线、Confusion Matrix、AUC</strong><br>针对这类问题，已经有较为成熟的理论基础，直接利用测试样本判断的准确程度判断画像是否准确</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-92eb834f8a12454d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这张图是一张非常常见也是有效的来总结Recall、Pecision、Lift曲线、Roc曲线、Confusion Matrix的图。<br>FPR = FP/(FP + TN)<br>Recall=TPR=TP/(TP+FN)<br>Precision=TP/(TP+FP)<br>F1曲线:2<code>*</code>Precision<code>*</code>Recall/(Precision+Recall)<br>Roc曲线：TPR vs FPR，也就是Precision vs Recall<br>Auc：area under the roc curve ，也就是roc曲线下面的面积，积分或者投点法均可求解。<br>这边不详细讲细节，需要的可以参考<a href="https://www.zhihu.com/question/30643044" target="_blank" rel="noopener">精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？</a></p>
<p><strong>1.2 交叉验证</strong><br>并不是所有画像都是有监督训练的画像，举个例子，用户的性别画像，是一个无监督的刻画，当你无法通过app端资料填写直接获取到的时候，你只能够通过其他数据特征的对用户进行分群。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-84b306d7fb8e875a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">input :</span><br><span class="line">Data Set：测试数据集</span><br><span class="line"></span><br><span class="line">output :</span><br><span class="line">model：画像模型</span><br><span class="line">label：0(无效)，1（有效）</span><br><span class="line"></span><br><span class="line">methods：</span><br><span class="line">1.从原始数据集中确定画像模型关键features</span><br><span class="line">2.关键features分层，分为train features、test features</span><br><span class="line">3.train featrues训练画像</span><br><span class="line">4.test freatrues校验画像</span><br><span class="line">5.输出值对（model，label）</span><br><span class="line">6.重复2~5</span><br></pre></td></tr></table></figure>
<p>首先，我们在总的数据集中筛选出所有关键影响特征，每次将筛选出的特征分为两块，测试特征训练特征，利用训练特征建立模型，再利用测试特征去判断模型是否合理（比如女鞋用户群的女鞋购买次数小于男性用户群，则次模型异常，删除），最后集成所有合理模型。<br>这样的逻辑中，我们将所有异常不合理的模型全部剔除，训练过程中就校验了用户画像的准确性<br><img src="http://upload-images.jianshu.io/upload_images/1129359-8ae0b22c2c485037.gif?imageMogr2/auto-orient/strip" alt=""></p>
<h1 id="用户画像上线后"><a href="#用户画像上线后" class="headerlink" title="用户画像上线后"></a>用户画像上线后</h1><h2 id="ABTest"><a href="#ABTest" class="headerlink" title="ABTest"></a>ABTest</h2><p><strong>不得不说，abtest是用户画像校验最为直观有效的校验方式。</strong></p>
<p>用户分流模块：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-825c0d26686ef2dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">methods:</span><br><span class="line">1. 全量用户流量为Users，切分流量为三块Users：A1、Users：A2、Users：B，且满足Users：A1+Users：A2=Users：B</span><br><span class="line"></span><br><span class="line">2. 对流量Users：A1、Users：A2不做任何动作</span><br><span class="line">3. 对流量Users：B做相应的模型预测，保存结果</span><br><span class="line">4. 以用户活跃度为例子，选取观察日期周下平均登陆次数y为代价函数，</span><br><span class="line">if y（Users：A1）=y（Users：A2）then</span><br><span class="line">        if  y（Users：B）&gt; y（Users：A1+Users：A2）</span><br><span class="line">                 then 模型有效（差值越大代表准确越高）</span><br><span class="line">        else 模型无效（差值越小代表准确越差）</span><br><span class="line">else    模型无效</span><br><span class="line">5.准确程度量化：K=(p*exp(-(dist(y(Users:B),y(Users:A1+Users:A2)))^2/(2*最小容忍度^2)))^(-1)</span><br></pre></td></tr></table></figure>
<p>一句话解释，就是A1=A2保证分配随机，A3好于A1+A2的效果检验画像是否准确？多准确？<br><img src="http://upload-images.jianshu.io/upload_images/1129359-8ae0b22c2c485037.gif?imageMogr2/auto-orient/strip" alt=""></p>
<h1 id="用户画像更新"><a href="#用户画像更新" class="headerlink" title="用户画像更新"></a>用户画像更新</h1><h2 id="用户回访"><a href="#用户回访" class="headerlink" title="用户回访"></a>用户回访</h2><p>在画像刻画完成后，必然会存在画像优化迭代的过程，客服回访是非常常见且有效的方式。<br>比如，我们定义了一波潜在流失用户10万人，随机抽取1000人，进行回访，根据回访结果做文本挖掘，提取关键词，看消极词用户的占比；<br><img src="http://upload-images.jianshu.io/upload_images/1129359-e2d386e07faf5b90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="这个图随机找的，别在意"></p>
<h2 id="机制检测"><a href="#机制检测" class="headerlink" title="机制检测"></a>机制检测</h2><p>再比如，我们定义了一波忠诚用户10万人，随机抽取100人，后台随机获取用户安装app的列表，看用户同类app的下载量数目的分布；</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-b3facbf505ae7149.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br><em>横轴为用户手机中同类竞品安装量的个数，纵轴为对应的随机抽样的100人中的个数</em><br>人群1分布为忠诚用户画像最准确的，同类app下载量集中在1附近，定义的用户极为准确<br>人群2分布杂乱，人群3分布在下降量异常高的数值附近，定义人群不准确<br><img src="http://upload-images.jianshu.io/upload_images/1129359-8ae0b22c2c485037.gif?imageMogr2/auto-orient/strip" alt=""><br><strong>用户画像是数据运营运营的基础，也是做深度挖掘的一个不可或缺的模块，只有先打好画像基础，确保画像质量，后续的深挖行为才有突破的可能</strong>，最后，谢谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 特征刻画 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 用户画像 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[大数据量下的划分聚类方法]]></title>
      <url>/2017/07/19/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%8F%E4%B8%8B%E7%9A%84%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>在常规聚类案例中，数据一般都是以iris集或者不足GB级的数据作为测试案例，实际商业运用中，数据量级要远远大于这些。比如滴滴出行15年日均单量就达到1000万单，出行轨迹的数据存储达到上百TB，常规的k均值聚类，二分聚类等无法完成如此量级的数据聚类，这边就提供一个以CLARANS为基础的算法思路。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-4cab9283dd9595f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>什么是聚类?<br>定义是这样的，把一个数据对象，划分成子集的过程，使得子集内相似度大，子集外相似度小。这样的一个过程叫做聚类。</p>
<p>大学课程老师以一个公式概括过这样的过程：<code>max(子集内相似度/子集间相似度)</code>，我觉得也很形象便于理解。</p>
<p>什么是划分聚类？<br>聚类方法有很多种，包括基于划分、基于密度、基于网格、基于层次、基于模型等等，这边主要介绍基于划分的聚类方法，剩余的方法会在后续的文章中持续更新[如果不鸽的话]。划分聚类一般是：<strong>采取互斥族（子集）划分</strong>，说的更直白一点就是<strong>每个点属于且仅属于一个族（子集）</strong>。</p>
<p>常见的划分聚类有哪些？<br>k均值划分：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">input：</span><br><span class="line">- k：族的个数</span><br><span class="line">- D：输入数据集合</span><br><span class="line">output：</span><br><span class="line">k个族（子集）的数据集合</span><br><span class="line">methods：</span><br><span class="line">1.在D中任选（常用的包库中都是这样做，但是建议自己写的同学以密度先分块，在密度块中任选）k个对象作为初始中心</span><br><span class="line">2.计算剩余对象到k对象的聚类，聚类远近分配到对应的族</span><br><span class="line">3.更新族均值作为新的族中心</span><br><span class="line">4.重复2-4直到中心不变化</span><br></pre></td></tr></table></figure></p>
<p>如图过程：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-c9e398faa4dcb9fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="kmeans"></p>
<p>以上为最简单的k均值，很容易看出，它存在几个问题，首先计算量非常的大，假设有m条数据，k个中心点，那距离计算的次数就是<code>o(mkt)=k*(m-k)*迭代次数t</code>，重复t次直到收敛的过程是非常大的计算过程；再而，如果数据均为‘男、女’，‘高、中、低’等，那距离定义就是非常不合理的，此外，初始k难确定，非凸数据，离群点等等都存在问题</p>
<p>围绕中心划分（PAM）：<br>刚才说到了异常点会影响k均值，那么我们看看为什么？<br>假设与点1、2、3、8、9、10、25，一眼就知道(1、2、3)，(8，9，10)为族，但如果由k均值的话，以k=2为例，((1，2，3)，(8，9，10，25))，mse=196；<br>((1，2，3，8)，(9，10，25))，mse=189.7；它重复以mse为损失函数，而不去考虑数据是否合理，所以针对的，我们有<strong>绝对误差标准</strong>：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-50252c5be5a42881.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="绝对误差标准"><br>这样做，通过全局距离最小化，可以一定程度上避免异常点的问题，但是，思考一下计算量是什么？是o(n**2)，这意味着对数据量大的问题，这就是一个典型的NP问题（一定有解，但是不一定在有限时间资源内可以被解出来）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">input:</span><br><span class="line">- k：族的个数</span><br><span class="line">- D：输入数据集合</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">k个族（子集）的数据集合</span><br><span class="line"></span><br><span class="line">methods:</span><br><span class="line">1.D中任选k个对象最为初始种子</span><br><span class="line">2.仿照k均值分配剩余对象</span><br><span class="line">3.随机选取非种子对象O</span><br><span class="line">4.计算若是以O为中心下的总损失函数代价S=原始种子下的绝对误差E-新的对象O下的绝对误差E</span><br><span class="line">5.如果S&gt;0,则以新对象O替换旧的种子对象，否则不变化</span><br><span class="line">6.重复2-5，直到收敛</span><br></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-eff32da981807420.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>我们看这个图好理解一点，就是存在族（集合）中任一点p，当前的初始种子为Q1，随机选取剩余其他对象为族中心Qrandom1，计算PQ1的距离与PQrandom1的距离，图中dist(PQ1)<dist(pqrandom1)的距离，所以p仍属于q1为中心的族，若存在族中心qrandom2，使得dist(pq1)>dist(PQrandom2)，则更新族中心为Qrandom2,此时绝对误差E会变化，计算是否降低了绝对误差E以确定是否更好族中心。<br>如何解决大数据量下的聚类问题？<br>其实看了以上两个算法，大同小异，但是都不可避免有一个弱点，就是计算量上都是随着初始数据量的增大而几何增长的，所以这边需要对数据量进行控制。</dist(pqrandom1)的距离，所以p仍属于q1为中心的族，若存在族中心qrandom2，使得dist(pq1)></p>
<p>大家回想一下，同样的对数据量进行控制的算法有哪些给我们有启发？<br><a href="http://www.jianshu.com/writer#/notebooks/14156301/notes/14303651" target="_blank" rel="noopener">数据平衡算法</a><br>这种方法好像可以减少数据量，哪有没有历史成功案例支持呢？<br><a href="http://www.jianshu.com/writer#/notebooks/13640327/notes/13715869" target="_blank" rel="noopener">基于决策树引申出的集成算法</a><br>貌似存在一个叫做adaboost、randomforest这类的算法，好像就用了<strong>数据平衡的算法</strong>。</p>
<p>那么，我们是否可以用在聚类里面呢？答案是可以的，我们现在看一个由上述思路得到的CLARANS算法，实际开发中，我们team对其进行了优化，内部称之为’CLARANS+’<br>在理解CLARANS+之前，我们先理解CLARA:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-952a50fff2be6900.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>从这张图上，我们可以很清晰的看出，CLARA首先通过类似randomforest里面的随机抽样的方法，将原始数据集随机抽样成若干个子数据集sample data，理论上采样的子集分布应该与原分布近似，所以样本中心点必然与原分布中心近似。</p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-7057574da2788939.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="确定中心"><br>在数据量较少的子集上，我们可以重复确定每个子集的中心Medoid，<strong>这边计算中心的方法有很多，包括上述讲到的K均值，PAM，也可以参考相似度比如常见的余弦相似，likelihood rate，高斯核相似等等</strong></p>
<p>最后采取随机抽取，或者投票加权等方法确定原始样本的中心即可。</p>
<p>CLARA的有效性依赖于样本的大小，分布及质量，所以该算法一定程度上会依赖于初始抽样的质量。除此之外，每一个随机样本的计算负责度为O（ks*s+k（n-k）），s为样本的大小，k为族数，n为总对象数，若抽取样本子集过少，其简化计算的程度也越低。</p>
<p>说到这里，CLARA的算法是确定了中心后不在改变，这就有一定的运气成分，假设确定的k个钟均离最佳中心很远的情况下，CLARA最后无论如何去选已知中心，都得不到最优秀的聚类中心。</p>
<p>所以，<strong>我们来看看可以提高CLARA的聚类质量及可伸缩性的CLARANS算法</strong></p>
<p>上述思路不变，但在CLARA确定中心之后，我们新增了一步，就是按照PAM中的方法一样，我们在子集上选取一个与当前中心x(Medoid)不一样的对象y(New Medoid)，计算用y(New Medoid)替换x(Medoid)后绝对误差是否下降，下降则替换否则不变，重复l次之后，我们可以认为此时的中心点为局部中心最优解；整体数据集所有子集均重复m次后，得出的中心点为全局局部最优解。如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-b6b9f0087384a02e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="新增New Medoid"></p>
<p>实际上，我们可以做的还很多<br>理论上讲，以上的算法结果已经尽可能的保证了数据的合理压缩，压缩后的数据集内的中心点足够鲁棒，但是实际运用过程中，我们没有尽可能的考虑到开头说的那句：<br><strong>什么是聚类?</strong><br>定义是这样的，把一个数据对象，划分成子集的过程，使得子集内相似度大，子集外相似度小。这样的一个过程叫做聚类。</p>
<p>所以，我们尝试性的做了CLARANS+，我们把CLARANS里面确定出来的每个sample data子集里面最优秀的top k个New Medoids映射回同一个空间：<br>以绿色和天蓝色数据集为例子：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-9284466e2db74bc6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="CLARANS"><br>橘色方框内为CLARANS最后确定中心后做的随机或者加权投票后采纳的被橘黄色框框住的天蓝色数据与绿色数据的中心点，很明显可以看出，这样导致的结果违背了“子集外相似度最小的原则”。</p>
<p>我们，仿照Lasso对应lambda.1se的方式，考虑除了最优点外，在其可接受的范围附近，认为他们同样属于最优点，也就是top k个New Medoids重新选择距离最远的点作为最优中心，也就是如下图中的紫色方框中的点：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d3e7601c5a9bdfa0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="最远距离点"><br>通过实际的业务测试，我们建议top k个点中的个默认为2-3比较好（数据分布差异大选择2，否则选择3）,如果不能确定，就默认为3。</p>
<p>以上理论方法就解释了如何在大量数据量下，简单快速的寻找到最优中心点的过程，谢谢大家。</p>
<p>参考文献：<br>[1] Jiawei Han.[数据挖掘概念与技术]2001，8<br>[2] 毛国君等.数据挖掘原理与算法[M].北京:清华大学出版社,2005.<br>[3] <a href="http://www.jianshu.com/writer#/notebooks/14156301/notes/14303651" target="_blank" rel="noopener">数据平衡算法</a><br>[4] <a href="http://www.jianshu.com/writer#/notebooks/13640327/notes/13715869" target="_blank" rel="noopener">基于决策树引申出的集成算法</a><br>[5] <a href="http://scikit-learn.org/stable/modules/clustering.html#clustering" target="_blank" rel="noopener">http://scikit-learn.org/stable/modules/clustering.html#clustering</a><br>[6] <a href="https://en.wikipedia.org/wiki/Clustering" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Clustering</a></p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据抽样平衡方法重写]]></title>
      <url>/2017/07/13/%E6%95%B0%E6%8D%AE%E6%8A%BD%E6%A0%B7%E5%B9%B3%E8%A1%A1%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99/</url>
      <content type="html"><![CDATA[<p>之前在R里面可以通过调用Rose这个package调用数据平衡函数，这边用python改写了一下，也算是自我学习了。</p>
<p>R：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定工作目录</span></span><br><span class="line">setwd(path)</span><br><span class="line"><span class="comment"># 安装包</span></span><br><span class="line">install.packages(<span class="string">"ROSE"</span>)</span><br><span class="line"><span class="keyword">library</span>(ROSE)</span><br><span class="line"><span class="comment"># 检查数据</span></span><br><span class="line">data(hacide)</span><br><span class="line">table(hacide.train$cls)</span><br><span class="line">  <span class="number">0</span>     <span class="number">1</span></span><br><span class="line"><span class="number">980</span>    <span class="number">20</span></span><br></pre></td></tr></table></figure></p>
<hr>
<p>过抽样实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_balanced_over &lt;- ovun.sample(cls ~ ., data = hacide.train, method = &quot;over&quot;,N = 1960)$data</span><br><span class="line">table(data_balanced_over$cls)</span><br><span class="line">0    1</span><br><span class="line">980 980</span><br></pre></td></tr></table></figure></p>
<p><strong>这边需要注意是<code>ovun</code>不是<code>over</code></strong></p>
<hr>
<p>欠采样实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_balanced_under &lt;- ovun.sample(cls ~ ., data = hacide.train, method = &quot;under&quot;, N = 40, seed = 1)$data</span><br><span class="line">table(data_balanced_under$cls)</span><br><span class="line">0    1</span><br><span class="line">20  20</span><br></pre></td></tr></table></figure></p>
<p>这边需要注意的是欠采样是不放回采样，同时对数据信息的损失也是极大的</p>
<hr>
<p>组合采样实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_balanced_both &lt;- ovun.sample(cls ~ ., data = hacide.train, method = &quot;both&quot;, p=0.5, N=1000, seed = 1)$data</span><br><span class="line">table(data_balanced_both$cls)</span><br><span class="line">0    1</span><br><span class="line">520 480</span><br></pre></td></tr></table></figure></p>
<p><code>method</code>的不同值代表着不同的采样方法，p这边是控制正类的占比，seed保证抽取样本的固定，也就是种子值。</p>
<hr>
<hr>
<p>在python上，我也没有发现有现成的package可以import，所以就参考了R的实现逻辑重写了一遍，新增了一个分层抽样<code>group_sample</code>,删除了过采样，重写了组合抽样<code>combine_sample</code>,欠抽样<code>under_sample</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math <span class="keyword">as</span> ma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sample_s</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">''''this is my pleasure'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">group_sample</span><span class="params">(self, data_set, label, percent=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 分层抽样</span></span><br><span class="line">        <span class="comment"># data_set:数据集</span></span><br><span class="line">        <span class="comment"># label:分层变量</span></span><br><span class="line">        <span class="comment"># percent:抽样占比</span></span><br><span class="line">        <span class="comment"># q:每次抽取是否随机,null为随机</span></span><br><span class="line">        <span class="comment"># 抽样根据目标列分层，自动将样本数较多的样本分层按percent抽样，得到目标列样本较多的特征欠抽样数据</span></span><br><span class="line">        x = data_set</span><br><span class="line">        y = label</span><br><span class="line">        z = percent</span><br><span class="line">        diff_case = pd.DataFrame(x[y]).drop_duplicates([y])</span><br><span class="line">        result = []</span><br><span class="line">        result = pd.DataFrame(result)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(diff_case)):</span><br><span class="line">            k = np.array(diff_case)[i]</span><br><span class="line">            data_set = x[x[y] == k[<span class="number">0</span>]]</span><br><span class="line">            nrow_nb = data_set.iloc[:, <span class="number">0</span>].count()</span><br><span class="line">            data_set.index = range(nrow_nb)</span><br><span class="line">            index_id = rd.sample(range(nrow_nb), int(nrow_nb * z))</span><br><span class="line">            result = pd.concat([result, data_set.iloc[index_id, :]], axis=<span class="number">0</span>)</span><br><span class="line">        new_data = pd.Series(result[<span class="string">'label'</span>]).value_counts()</span><br><span class="line">        new_data = pd.DataFrame(new_data)</span><br><span class="line">        new_data.columns = [<span class="string">'cnt'</span>]</span><br><span class="line">        k1 = pd.DataFrame(new_data.index)</span><br><span class="line">        k2 = new_data[<span class="string">'cnt'</span>]</span><br><span class="line">        new_data = pd.concat([k1, k2], axis=<span class="number">1</span>)</span><br><span class="line">        new_data.columns = [<span class="string">'id'</span>, <span class="string">'cnt'</span>]</span><br><span class="line">        max_cnt = max(new_data[<span class="string">'cnt'</span>])</span><br><span class="line">        k3 = new_data[new_data[<span class="string">'cnt'</span>] == max_cnt][<span class="string">'id'</span>]</span><br><span class="line">        result = result[result[y] == k3[<span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">under_sample</span><span class="params">(self, data_set, label, percent=<span class="number">0.1</span>, q=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 欠抽样</span></span><br><span class="line">        <span class="comment"># data_set:数据集</span></span><br><span class="line">        <span class="comment"># label:抽样标签</span></span><br><span class="line">        <span class="comment"># percent:抽样占比</span></span><br><span class="line">        <span class="comment"># q:每次抽取是否随机</span></span><br><span class="line">        <span class="comment"># 抽样根据目标列分层，自动将样本数较多的样本按percent抽样，得到目标列样本较多特征的欠抽样数据</span></span><br><span class="line">        x = data_set</span><br><span class="line">        y = label</span><br><span class="line">        z = percent</span><br><span class="line">        diff_case = pd.DataFrame(pd.Series(x[y]).value_counts())</span><br><span class="line">        diff_case.columns = [<span class="string">'cnt'</span>]</span><br><span class="line">        k1 = pd.DataFrame(diff_case.index)</span><br><span class="line">        k2 = diff_case[<span class="string">'cnt'</span>]</span><br><span class="line">        diff_case = pd.concat([k1, k2], axis=<span class="number">1</span>)</span><br><span class="line">        diff_case.columns = [<span class="string">'id'</span>, <span class="string">'cnt'</span>]</span><br><span class="line">        max_cnt = max(diff_case[<span class="string">'cnt'</span>])</span><br><span class="line">        k3 = diff_case[diff_case[<span class="string">'cnt'</span>] == max_cnt][<span class="string">'id'</span>]</span><br><span class="line">        new_data = x[x[y] == k3[<span class="number">0</span>]].sample(frac=z, random_state=q, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> new_data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combine_sample</span><span class="params">(self, data_set, label, number, percent=<span class="number">0.35</span>, q=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 组合抽样</span></span><br><span class="line">        <span class="comment"># data_set:数据集</span></span><br><span class="line">        <span class="comment"># label:目标列</span></span><br><span class="line">        <span class="comment"># number:计划抽取多类及少类样本和</span></span><br><span class="line">        <span class="comment"># percent：少类样本占比</span></span><br><span class="line">        <span class="comment"># q:每次抽取是否随机</span></span><br><span class="line">        <span class="comment"># 设定总的期待样本数量，及少类样本占比，采取多类样本欠抽样，少类样本过抽样的组合形式</span></span><br><span class="line">        x = data_set</span><br><span class="line">        y = label</span><br><span class="line">        n = number</span><br><span class="line">        p = percent</span><br><span class="line">        diff_case = pd.DataFrame(pd.Series(x[y]).value_counts())</span><br><span class="line">        diff_case.columns = [<span class="string">'cnt'</span>]</span><br><span class="line">        k1 = pd.DataFrame(diff_case.index)</span><br><span class="line">        k2 = diff_case[<span class="string">'cnt'</span>]</span><br><span class="line">        diff_case = pd.concat([k1, k2], axis=<span class="number">1</span>)</span><br><span class="line">        diff_case.columns = [<span class="string">'id'</span>, <span class="string">'cnt'</span>]</span><br><span class="line">        max_cnt = max(diff_case[<span class="string">'cnt'</span>])</span><br><span class="line">        k3 = diff_case[diff_case[<span class="string">'cnt'</span>] == max_cnt][<span class="string">'id'</span>]</span><br><span class="line">        k4 = diff_case[diff_case[<span class="string">'cnt'</span>] != max_cnt][<span class="string">'id'</span>]</span><br><span class="line">        n1 = p * n</span><br><span class="line">        n2 = n - n1</span><br><span class="line">        fre1 = n2 / float(x[x[y] == k3[<span class="number">0</span>]][<span class="string">'label'</span>].count())</span><br><span class="line">        fre2 = n1 / float(x[x[y] == k4[<span class="number">1</span>]][<span class="string">'label'</span>].count())</span><br><span class="line">        fre3 = ma.modf(fre2)</span><br><span class="line">        new_data1 = x[x[y] == k3[<span class="number">0</span>]].sample(frac=fre1, random_state=q, axis=<span class="number">0</span>)</span><br><span class="line">        new_data2 = x[x[y] == k4[<span class="number">1</span>]].sample(frac=fre3[<span class="number">0</span>], random_state=q, axis=<span class="number">0</span>)</span><br><span class="line">        test_data = pd.DataFrame([])</span><br><span class="line">        <span class="keyword">if</span> int(fre3[<span class="number">1</span>]) &gt; <span class="number">0</span>:</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; (int(fre3[<span class="number">1</span>])):</span><br><span class="line">                data = x[x[y] == k4[<span class="number">1</span>]]</span><br><span class="line">                test_data = pd.concat([test_data, data], axis=<span class="number">0</span>)</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        result = pd.concat([new_data1, new_data2, test_data], axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p>
<p>后续使用，只需要复制上述code，存成<code>.py</code>的文件，后续使用的时候：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载函数</span></span><br><span class="line"><span class="keyword">import</span> sample_s <span class="keyword">as</span> sa</span><br><span class="line"><span class="comment">#这边可以选择你需要的分层抽样、欠抽样、组合抽样的函数</span></span><br><span class="line">sample = sa.group_sample()</span><br><span class="line"><span class="comment">#直接调用函数即可</span></span><br><span class="line">new_data3 = sample.combine_sample(data_train, <span class="string">'label'</span>, <span class="number">60000</span>, <span class="number">0.4</span>)</span><br><span class="line"><span class="comment">#将data_train里面的label保持正样本（少类样本）达到0.4的占比下，总数抽取到60000个样本</span></span><br></pre></td></tr></table></figure></p>
<p>其实不是很难的一个过程，只是强化自己对python及R语言的书写方式的记忆，谢谢。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据平衡 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[订单需求预估]]></title>
      <url>/2017/07/12/%E8%AE%A2%E5%8D%95%E9%9C%80%E6%B1%82%E9%A2%84%E4%BC%B0/</url>
      <content type="html"><![CDATA[<p>之前写了一篇以基于elastic的需求预估的文章，只不过用的是R语言开发的，最近在学python，就仿照逻辑写了一篇python的，主要修改点如下：</p>
<ul>
<li>用决策树替换了elastic算法</li>
<li>用分层抽样替换了组合抽样</li>
</ul>
<p><strong>需要看详细理论及思考过程参考链接：<a href="http://www.jianshu.com/p/e932b9744da6" target="_blank" rel="noopener">商品需求预估</a></strong></p>
<p>python code如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import random as rd</span><br><span class="line">from sklearn import tree</span><br><span class="line"></span><br><span class="line"># 读取数据</span><br><span class="line">data_orgin = pd.read_table(&quot;C:/Users/17031877/Desktop/supermarket_second_hair_washing_train.txt&quot;)</span><br><span class="line">data_deal_1 = data_orgin.drop([&apos;aimed_date&apos;, &apos;member_id&apos;, &apos;age&apos;, &apos;gender&apos;, &apos;diff_rgst&apos;], axis=1)</span><br></pre></td></tr></table></figure></p>
<p>这边是常规的数据读取，删除了不必要的列</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#因变量单列</span><br><span class="line">label = data_deal_1[&apos;label&apos;]</span><br><span class="line"></span><br><span class="line"># 用户分量级</span><br><span class="line">value00 = [&apos;max_date_diff&apos;, &apos;aimed_max_date_diff&apos;]</span><br><span class="line">data00 = data_deal_1[value00]</span><br><span class="line"></span><br><span class="line">value01 = [&apos;max_pay&apos;, &apos;per_pay&apos;, &apos;six_month_max_pay&apos;, &apos;six_month_per_pay&apos;, &apos;three_month_max_pay&apos;, &apos;three_month_per_pay&apos;,</span><br><span class="line">           &apos;one_month_max_pay&apos;, &apos;one_month_per_pay&apos;, &apos;fifteen_day_max_pay&apos;, &apos;fifteen_day_per_pay&apos;, &apos;aimed_max_pay&apos;,</span><br><span class="line">           &apos;aimed_per_pay&apos;, &apos;aimed_six_month_max_pay&apos;, &apos;aimed_six_month_per_pay&apos;, &apos;aimed_three_month_max_pay&apos;,</span><br><span class="line">           &apos;aimed_three_month_per_pay&apos;, &apos;aimed_one_month_max_pay&apos;, &apos;aimed_one_month_per_pay&apos;,</span><br><span class="line">           &apos;aimed_fifteen_day_max_pay&apos;, &apos;aimed_fifteen_day_per_pay&apos;, &apos;qty_drtn_seven&apos;, &apos;qty_drtn_fourteen&apos;]</span><br><span class="line">data01 = data_deal_1[value01]</span><br><span class="line"></span><br><span class="line">value02 = [&apos;cnt_time&apos;, &apos;six_month_cnt_time&apos;, &apos;three_month_cnt_time&apos;, &apos;one_month_cnt_time&apos;, &apos;fifteen_day_cnt_time&apos;,</span><br><span class="line">           &apos;aimed_cnt_time&apos;, &apos;aimed_six_month_cnt_time&apos;, &apos;aimed_three_month_cnt_time&apos;, &apos;aimed_one_month_cnt_time&apos;,</span><br><span class="line">           &apos;aimed_fifteen_day_cnt_time&apos;, &apos;pv_times_seven&apos;, &apos;pv_times_fourteen&apos;, &apos;search_times_seven&apos;,</span><br><span class="line">           &apos;search_times_fourteen&apos;, &apos;clc_times_seven&apos;, &apos;clc_times_fourteen&apos;, &apos;cart2_times_seven&apos;,</span><br><span class="line">           &apos;cart2_times_fourteen&apos;, &apos;cart1_times_seven&apos;, &apos;cart1_times_fourteen&apos;, &apos;unpay_times_seven&apos;,</span><br><span class="line">           &apos;unpay_times_fourteen&apos;]</span><br><span class="line">data02 = data_deal_1[value02]</span><br><span class="line"></span><br><span class="line">value03 = [&apos;pv_visit_last_period&apos;, &apos;search_last_period&apos;, &apos;clc_last_period&apos;, &apos;cart2_last_period&apos;, &apos;cart1_last_period&apos;,</span><br><span class="line">           &apos;unpay_last_period&apos;]</span><br><span class="line">data03 = data_deal_1[value03]</span><br></pre></td></tr></table></figure>
<p>因为不同量级的数据之后做异常点处理的时候截断位置不同，所有需要分割数据处理</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def test_function_one(x, l):</span><br><span class="line">    k = x.dropna(how=&apos;any&apos;)</span><br><span class="line">    y = k.quantile(l)</span><br><span class="line">    z = k.max()</span><br><span class="line">    x[x &gt; y] = y</span><br><span class="line">    x = x.fillna(value=z)</span><br><span class="line">    return x</span><br><span class="line">for i in range(len(data00.columns)):</span><br><span class="line">    data00.iloc[:, i] = test_function_one(data00.iloc[:, i], 0.98)</span><br><span class="line"></span><br><span class="line">def test_function_two(x, l):</span><br><span class="line">    k = x.dropna(how=&apos;any&apos;)</span><br><span class="line">    y = k.quantile(l)</span><br><span class="line">    z = 0</span><br><span class="line">    x[x &gt; y] = y</span><br><span class="line">    x = x.fillna(value=z)</span><br><span class="line">    return x</span><br><span class="line">for i in range(len(data01.columns)):</span><br><span class="line">    data01.iloc[:, i] = test_function_two(data01.iloc[:, i], 0.95)</span><br><span class="line">for i in range(len(data02.columns)):</span><br><span class="line">    data02.iloc[:, i] = test_function_two(data02.iloc[:, i], 0.99)</span><br><span class="line"></span><br><span class="line">def test_function_three(x):</span><br><span class="line">    z = 14</span><br><span class="line">    x[x &gt; z] = z</span><br><span class="line">    x = x.fillna(value=z)</span><br><span class="line">    return x</span><br><span class="line">for i in range(len(data03.columns)):</span><br><span class="line">    data03.iloc[:, i] = test_function_three(data03.iloc[:, i])</span><br><span class="line"># 数据合并</span><br><span class="line">data_train = pd.concat([label, data00, data01, data02, data03], axis=1)</span><br></pre></td></tr></table></figure>
<p>根据数据量的不同做数据分割，跑上面写完的code函数就可以</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#数量级对比</span><br><span class="line">zero_case = data_train[data_train[&apos;label&apos;] == 0][&apos;label&apos;].count()</span><br><span class="line">print &apos;负样本数：%d&apos; % zero_case</span><br><span class="line">one_case = data_train[data_train[&apos;label&apos;] == 1][&apos;label&apos;].count()</span><br><span class="line">print &apos;正样本数: %d&apos; % (one_case)</span><br><span class="line"></span><br><span class="line">负样本数：292936</span><br><span class="line">正样本数: 3973</span><br><span class="line">Backend TkAgg is interactive backend. Turning interactive mode on.</span><br></pre></td></tr></table></figure>
<p>实际看下来，正负样本的差异的确还是很大，这个其实做多了就有经验，常规的来看，潜在的浏览、搜索到最后的成单，普遍自然转化不到1%，也正是这么低的转化，才需要一些算法来做信息抓去。</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def case_sample(x, y, z):</span><br><span class="line">    diff_case = pd.DataFrame(x[y]).drop_duplicates([y])</span><br><span class="line">    result = []</span><br><span class="line">    result = pd.DataFrame(result)</span><br><span class="line">    for i in range(len(diff_case)):</span><br><span class="line">        k = np.array(diff_case)[i]</span><br><span class="line">        data_set = x[x[y] == k[0]]</span><br><span class="line">        nrow_nb = data_set.iloc[:, 0].count()</span><br><span class="line">        data_set.index = range(nrow_nb)</span><br><span class="line">        index_id = rd.sample(range(nrow_nb), int(nrow_nb * z))</span><br><span class="line">        result = pd.concat([result, data_set.iloc[index_id, :]], axis=0)</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">zero_case = data_train[data_train[&apos;label&apos;] == 0]</span><br><span class="line">one_case = data_train[data_train[&apos;label&apos;] == 1]</span><br><span class="line"># 开始分层抽样</span><br><span class="line">new_zero_case = case_sample(zero_case, &apos;unpay_last_period&apos;, 0.1)</span><br><span class="line"># 新数量级对比</span><br><span class="line">new_zero_case_count = new_zero_case[new_zero_case[&apos;label&apos;] == 0][&apos;label&apos;].count()</span><br><span class="line"># 数据集合并</span><br><span class="line">new_data_train = pd.concat([new_zero_case, one_case], axis=0)</span><br></pre></td></tr></table></figure>
<p><code>case_sample</code>是一个简单的分层抽样的小函数，<code>x</code>是数据集，<code>y</code>是分层变量，<code>z</code>是抽样占比；新的样本<code>new_data_train</code>中正负样本比例在1:10左右，这边的样本比是我自己设置的，不一定是最合理的；且此处也不一定要求一定用分层抽样，只是我用来练练手的；推荐还是遵从奥卡姆原理，在未知的情况下，尽可能简单的解决问题，比如组合抽样就是很不错的方法。</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#函数设置</span><br><span class="line">clf = tree.DecisionTreeRegressor(criterion=&apos;mse&apos;, max_features=&apos;log2&apos;, random_state=1)</span><br><span class="line"></span><br><span class="line">#函数拟合</span><br><span class="line">y = new_data_train[&apos;label&apos;]</span><br><span class="line">x = new_data_train.drop(&apos;label&apos;, 1)</span><br><span class="line">clf.fit(x, y)</span><br><span class="line"></span><br><span class="line">#数据预测</span><br><span class="line">y_predict = clf.predict(x)</span><br><span class="line"></span><br><span class="line"># 结果对比</span><br><span class="line">y.index = range(len(y))</span><br><span class="line">combined_date = pd.concat([y, pd.DataFrame(y_predict)], axis=1)</span><br><span class="line">combined_date.columns = [&apos;actual&apos;, &apos;predict&apos;]</span><br></pre></td></tr></table></figure>
<p>这边稍微讲解一下，我认为的<code>sklearn</code>中<code>DecisionTreeRegressor</code>中比较终于的参数设置，<code>criterion</code>这边为模型优化的标准，常规的有<code>mse</code>和<code>mae</code>，建议在数据量差异不大的时候多考虑<code>mse</code>；<code>max_features</code>是每次训练用的特征个数，综合特征量级考虑，一般有<code>log2</code>，<code>sqrt</code>，尽可能是抽取比例在70%；<code>max_depth</code>刚开始可以默认，第一类模型出来后，可在结果附近迭代，寻找<code>out of bag</code>最小的error下的值；<strong>另外，我没有发现有weight设置，可能是我不熟悉，但是如果sklearn这边不提供weight的化，我们在做数据预处理的时候一定要平衡数据，不然当数据集过偏的时候最后的结果会以“牺牲”少类的判断正确率去完善整体正确率。</strong></p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># case 1</span><br><span class="line">x = []</span><br><span class="line">y = []</span><br><span class="line">for i in range(1, 10):</span><br><span class="line">    test_data = combined_date</span><br><span class="line">    i = i / float(10)</span><br><span class="line">    for j in range(combined_date[&apos;actual&apos;].count()):</span><br><span class="line">        if test_data.iloc[j, 1] &gt; i:</span><br><span class="line">            test_data.iloc[j, 1] = 1</span><br><span class="line">        else:</span><br><span class="line">            continue</span><br><span class="line">    z = test_data[test_data[&apos;actual&apos;] == test_data[&apos;predict&apos;]][&apos;actual&apos;].count() / float(test_data[&apos;actual&apos;].count())</span><br><span class="line">    x.append(i)</span><br><span class="line">    y.append(z)</span><br></pre></td></tr></table></figure>
<p>这边写了检查函数，检查了分别0.1~1，以0.1为间隔的情况下的分割点，每个分割点下<strong>预测正确的数量/所有统计的样本数</strong>，也就是下面的<strong>accuracy</strong>.<br><img src="http://upload-images.jianshu.io/upload_images/1129359-2f786208ac1617c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># case 2</span><br><span class="line">test_data = combined_date</span><br><span class="line">aimed_data = test_data[test_data[&apos;predict&apos;]&gt;0]</span><br><span class="line">k1=aimed_data[aimed_data[&apos;actual&apos;]==1][&apos;predict&apos;].count()</span><br><span class="line">k2=float(aimed_data[&apos;predict&apos;].count())</span><br><span class="line"></span><br><span class="line">print &apos;所有预测可能下单用户中真实下单用户数：%d&apos; %(k1)</span><br><span class="line">print &apos;所有预测可能下单用户数：%d&apos; %(k2)</span><br></pre></td></tr></table></figure>
<p>因为这边需要对用户营销，所以更关系topN的转化率，需要看一下实际正样本被覆盖了多数，以上即为code，这边的效果值为98.7%，还是比较高的，但是应该是过拟合了，所有一般不建议单纯使用决策树模型</p>
<hr>
<p>所有的python code到这里就结束了，后续我做项目的同时会同时更新R及python两种code的思考，和大家讨论分享学习，谢谢。</p>
<p><em>参考文献：</em></p>
<ul>
<li><em><a href="http://scikitlearn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" target="_blank" rel="noopener">sklearn.tree.DecisionTreeRegressor</a></em></li>
<li><em><a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">维基百科中对ROC的介绍</a></em></li>
<li><em><a href="https://www.zhihu.com/question/27205203/answer/148900663" target="_blank" rel="noopener">决策树常见问题及面试关键点介绍</a></em></li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 预测 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[推荐系统-威尔逊区间法]]></title>
      <url>/2017/06/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%A8%81%E5%B0%94%E9%80%8A%E5%8C%BA%E9%97%B4%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>我推荐一种之前在惠普做过一种排序方法：威尔逊区间法</p>
<p>我们先做如下设定：</p>
<p>（1）每个用户的打分都是独立事件。</p>
<p>（2）用户只有两个选择，要么投喜欢’1’，要么投不喜欢’0’。</p>
<p>（3）如果总人数为n，其中喜欢的为k，那么喜欢的比例p就等于k/n。</p>
<p>这是一种统计分布，叫做”二项分布”（binomial distribution）</p>
<p>理论上讲，p越大应该越好，但是n的不同，导致p的可信性有差异。100个人投票，50个人投喜欢；10个人投票，6个人喜欢，我们不能说后者比前者要好。</p>
<p>所以这边同时要考虑（p，n）</p>
<p>刚才说满足二项分布，这里p可以看作”二项分布”中某个事件的发生概率，因此我们可以计算出p的置信区间。</p>
<p>所谓”置信区间”，就是说，以某个概率而言，p会落在的那个区间。</p>
<p>置信区间展现的是这个参数的真实值有一定概率落在测量结果的周围的程度。置信区间给出的是被测量参数的测量值的可信程度，即前面所要求的“一个概率”，也就是结论的可信程度。</p>
<p>二项分布的置信区间有多种计算公式，最常见的是”正态区间”（Normal approximation interval）。但是，它只适用于样本较多的情况（np &gt; 5 且 n(1 − p) &gt; 5），对于小样本，它的准确性很差。</p>
<p>这边，我推荐用t检验来衡量小样本的数据，可以解决数据过少准确率不高的问题。</p>
<p>这样一来，排名算法就比较清晰了：</p>
<p>第一步，计算每个case的p（好评率）。</p>
<p>第二步，计算每个”好评率”的置信区间（参考z Test或者t Test，以95%的概率来处理）。</p>
<p>第三步，根据置信区间的下限值，进行排名。这个值越大，排名就越高。</p>
<p>解释一下，n为评价数，p为好评率，z为对应检验对应概率区间下的统计量</p>
<p>比如t-分布：</p>
<p>可以看到，当n的值足够大时，这个下限值会趋向p，如果n非常小，这个下限值会大大小于p，更加符合实际。</p>
<p>Reddit的评论排名，目前就使用这个算法。国内的化，滴滴也有部分业务涉及，效果也不错。</p>
<hr>
<p>更新一下，没想到这个话题还是有高达9个人关注，所以这边我再说一些更细化的过程吧</p>
<p>在计算排名的时候，我们通常会考虑三个事情</p>
<p>1.上文讲到的，次数+好评率的分布，次数越多好评率越可靠，好评率越高该项越值得推荐</p>
<p>2.时间因素，如果一个项目是10天前推送的，一个项目是昨天推送的，很明显前者的次数远大于后者</p>
<p>3.影响权重，你这边只考虑了喜欢和不喜欢，其实所有的排序不可能只以1个维度考虑，通常会考虑多个维度，比如浏览次数，搜索次数等，你需要考虑每个的重要性或者说权重大小</p>
<p>1这里就不讲了，其他方法也有很多，比如贝叶斯平均的优化版本、再比如经典的Hacker公式：</p>
<p>2.时间因素：</p>
<p>时间越久，代表之前的投票结果对当前的影响越小，这边有很多不同的影响方式，举几个例子：</p>
<p>比如艾宾浩斯遗忘规律：</p>
<p>这里的c、k决定下降速度，业务运用过程中，c值一般在[1,2],k值一般在[1.5,2.5]</p>
<p>比如时效衰减：</p>
<p>这里就是比较常见的移动窗口式的，永远只看近期某一段时间，而且时间内呈线性下降，不过可以改变变化方式</p>
<p>3.不同种的属性对于结果的影响自然不同</p>
<p>举个例子，用户主动搜索和用户浏览相比，用户主动搜索的情况下，用户的需求更为强烈</p>
<p>通常需要判断这些强烈程度都是通过：</p>
<p>相关性：看因变量与自变量之间的相关系数，如：cor函数</p>
<p>importance：看删除或者修改自变量，对应变量的判断影响大小，如：randomForest的重要性</p>
<p>离散程度：看自变量的数据分布是否足够分散，是否具有判断依据，如：变异系数或者pca</p>
<p>等等</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[协同过滤推荐]]></title>
      <url>/2017/06/21/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<p>set.seed ( 1234 )</p>
<h1 id="加载数据包"><a href="#加载数据包" class="headerlink" title="加载数据包"></a>加载数据包</h1><p>library ( “recommenderlab” )</p>
<h1 id="构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as-函数转为raringMatrix类型。"><a href="#构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as-函数转为raringMatrix类型。" class="headerlink" title="构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as()函数转为raringMatrix类型。"></a>构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as()函数转为raringMatrix类型。</h1><p>val1&lt;- matrix ( sample ( c ( as.numeric ( 0 : 5 ) ,NA ) ,50 ,replace = TRUE ,prob = c ( rep ( .4 / 6 , 6 ) , .6 ) ) ,ncol = 10 , dimnames = list ( user = paste ( “u” ,1 : 5 ,sep = ‘’ ) ,item = paste ( “i” ,1 : 10 ,sep = ‘’ ) ) )</p>
<p>val2 &lt;- as ( val1, “realRatingMatrix” )</p>
<p>数据转换</p>
<p>val3&lt;- normalize ( val2 )</p>
<h1 id="二元分类转换，normalize-函数进行标准化处理，标准化的目的是为了去除用户评分的偏差"><a href="#二元分类转换，normalize-函数进行标准化处理，标准化的目的是为了去除用户评分的偏差" class="headerlink" title="二元分类转换，normalize()函数进行标准化处理，标准化的目的是为了去除用户评分的偏差"></a>二元分类转换，normalize()函数进行标准化处理，标准化的目的是为了去除用户评分的偏差</h1><p>val4 &lt;- binarize ( val3 , minRating = 4 )</p>
<p>val5 &lt;- as ( val4 , “matrix” )</p>
<p>数据可视化</p>
<p>接下来，我们采用MovieLense数据集，</p>
<p>data ( MovieLense )</p>
<p>key1 &lt;- sample ( MovieLense , 943 , replace = F )<br>image ( MovieLense )</p>
<p>hist ( getRatings ( normalize ( MovieLense ) ) , breaks = 100 )</p>
<p>hist ( rowCounts ( key1 ) , breaks = 50 )</p>
<p>建立模型</p>
<p>对于realRatingMatrix有六种方法：IBCF(基于物品的推荐)、UBCF（基于用户的推荐）、PCA（主成分分析）、RANDOM（随机推荐）、SVD（矩阵因子化）、POPULAR（基于流行度的推荐）</p>
<p>建立协同过滤推荐算法模型，主要运用recommender(data=ratingMatrix,method,parameter=NULL)函数，getModel()可查看模型参数</p>
<p>key1_recom &lt;- Recommender (key1 , method = “IBCF” )</p>
<p>key1_popul &lt;- Recommender ( key1, method = “POPULAR” )</p>
<h1 id="查看模型方法"><a href="#查看模型方法" class="headerlink" title="查看模型方法"></a>查看模型方法</h1><p>names ( getModel ( key1_recom ) )</p>
<p>模型预测</p>
<p>TOP-N预测</p>
<p>对模型预测可运用predict()函数，在此分别以TOP-N预测及评分预测为例，预测第940-943位观影者的评分情况。n表示最终为TOP-N的列表推荐，参数type = “ratings”表示运用评分预测观影者对电影评分，模型结果均需转为list或矩阵表示</p>
<p>pred &lt;- predict ( key1_popul ,key1 [ 940 : 943,] , n = 5 )</p>
<p>as ( pred , “list” )</p>
<h1 id="top-N为有序列表，抽取最优推荐子集"><a href="#top-N为有序列表，抽取最优推荐子集" class="headerlink" title="top-N为有序列表，抽取最优推荐子集"></a>top-N为有序列表，抽取最优推荐子集</h1><p>pred3 &lt;- bestN ( pred , n = 3 )</p>
<p>as ( pred3 , “list” )</p>
<h1 id="评分预测"><a href="#评分预测" class="headerlink" title="评分预测"></a>评分预测</h1><p>rate &lt;- predict ( key1_popul , key1 [ 940 : 943 ] , type = “ratings” )</p>
<p>as ( rate , “matrix” ) [ , 1 : 5 ]</p>
<p>预测模型评价</p>
<p>评分预测模型评价</p>
<p>eva &lt;- evaluationScheme (key1 [ 1 : 800 ] , method = “split” , train = 0.9,given = 15)<br>method=”split”&amp;train=0.9为按90%划分训练测试集合,given为评价的类目数</p>
<p>r_eva1&lt;- Recommender ( getData ( eva , “train” ) , “UBCF” )</p>
<p>p_eva1&lt;- predict ( r_eva1 , getData ( eva, “known” ) , type = “ratings” )</p>
<p>r_eva2 &lt;- Recommender ( getData ( eva, “train” ) , “IBCF” )</p>
<p>p_eva2 &lt;- predict ( r_eva2 , getData ( eva, “known” ) , type = “ratings” )<br>c_eva1 &lt;- calcPredictionAccuracy ( p_eva1 , getData ( eva , “unknown” ) )</p>
<p>c_eva2 &lt;- calcPredictionAccuracy ( p_eva2 , getData ( eva , “unknown” ) )</p>
<p>error &lt;- rbind ( c_eva1 , c_eva2 )</p>
<p>rownames ( error ) &lt;- c ( “UBCF” , “IBCF” )<br>计算预测模型的准确度</p>
<p>TOP-N预测模型评价</p>
<p>通过4-fold交叉验证方法分割数据集，运用evaluate()进行TOP-N预测模型评价,评价结果可通过ROC曲线及准确率-召回率曲线展示:</p>
<h1 id="4-fold交叉验证"><a href="#4-fold交叉验证" class="headerlink" title="4-fold交叉验证"></a>4-fold交叉验证</h1><p>tops &lt;- evaluationScheme ( key1 [ 1 : 800 ] , method = “cross” , k = 4 , given = 3 ,goodRating = 5 )</p>
<p>results &lt;- evaluate ( tops , method = “POPULAR” , type = “topNList” ,n = c ( 1 , 3 , 5 , 10 ) )</p>
<h1 id="获得混淆矩阵"><a href="#获得混淆矩阵" class="headerlink" title="获得混淆矩阵"></a>获得混淆矩阵</h1><p>getConfusionMatrix ( results ) [ [ 1 ] ]</p>
<p>avg ( results )</p>
<p>推荐算法的比较</p>
<p>除了对预测模型进行评价，还可以对不同推荐算法进行比较。可首先构建一个推荐算法列表，通过ROC曲线、、准确率-召回率曲线或RMSE直方图进行比较</p>
<p>TOP-N算法比较</p>
<p>set.seed ( 2016 )</p>
<p>scheme &lt;- evaluationScheme ( key1 , method = “split” , train = 0.9 , k = 1 , given = 10 , goodRating = 5 )</p>
<h1 id="构建不同算法模型"><a href="#构建不同算法模型" class="headerlink" title="构建不同算法模型"></a>构建不同算法模型</h1><p>results &lt;- evaluate ( scheme ,test_data ,n = c ( 1 ,3 ,5 ,10 ,15 ,20 ) )</p>
<h1 id="模型比较-ROC曲线"><a href="#模型比较-ROC曲线" class="headerlink" title="模型比较#ROC曲线"></a>模型比较#ROC曲线</h1><p>plot ( results , annotate = c ( 1 , 3 ) , legend = “bottomright” )</p>
<h1 id="准确率-召回率曲线"><a href="#准确率-召回率曲线" class="headerlink" title="准确率-召回率曲线"></a>准确率-召回率曲线</h1><p>plot ( results , “prec/rec” , annotate = c ( 2 , 3 , 4 ) , legend = “topleft” )</p>
<p>预测评分算法比较</p>
<p>results2 &lt;- evaluate ( scheme , algorithms , type = “ratings” )</p>
<p>plot ( results2 , ylim = c ( 0 , 20 ) )</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[聚类算法思路总结]]></title>
      <url>/2017/06/20/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>1.cost function</p>
<p>1.1 距离</p>
<p>常见的为欧式距离（L1 norm）&amp;&amp;p=2，拓展的可以有闵可夫斯基距离（L2 norm）&amp;&amp;p=1：</p>
<p>当p趋向于无穷的时候，切比雪夫距离（Chebyshev distance）：</p>
<p>红色的时候为切比雪夫距离，蓝色为闵可夫斯基距离，绿色为欧式距离。</p>
<p>1.2相似系数</p>
<p>夹角余弦及相关系数，相关系数不受线性变换的影响，但是计算速度远慢于距离计算。</p>
<p>1.3dynamic time warping动态时间规整</p>
<p>举例子：</p>
<p>序列A：1,1,1,10,2,3，序列B：1,1,1,2,10,3</p>
<p>欧式距离：distance[i][j]=(b[j]-a[i])*(b[j]-a[i])来计算的话，总的距离和应该是128</p>
<p>应该说这个距离是非常大的，而实际上这个序列的图像是十分相似的。因为序列A中的10对应得是B中的2，A中的2对应的B中的10，导致计算膨胀，现在将A中的10对应B中的10，A中的1对应B中的2再计算，膨胀因素会小很多（时间前推一步）。</p>
<p>2.聚类算法</p>
<p>2.1分层聚类：</p>
<p>自上而下：所有点先聚为一类，然后分层次的一步一步筛出与当前类别差异最大的点</p>
<p>自下而上：所有点先各自为一类，组合成n个类的集合，然后寻找出最靠近的两者聚为新的一类，循环往复</p>
<p>数值类分类：（适用于计算量巨大或者数据量巨大的时候）</p>
<p>BIRCH算法，层次平衡迭代规约和聚类，</p>
<p>主要参数包含：聚类特征和聚类特征树：</p>
<p>聚类特征：</p>
<p>给定N个d维的数据点{x1,x2,….,xn}，CF定义如下：CF=（N，LS，SS）,其中，N为子类中的节点的个数，LS是子类中的N个节点的线性和，SS是N个节点的平方和</p>
<p>存在计算定义：CF1+CF2=（n1+n2, LS1+LS2, SS1+SS2）</p>
<p>假设簇C1中有三个数据点：（2,3），（4,5），（5,6），则CF1={3，（2+4+5,3+5+6），（2^2+4^2+5^2,3^2+5^2+6^2）}={3，（11,14），（45,70）}</p>
<p>假设一个簇中，存在质心C和半径R，若有xi，i=1…n个点属于该簇，质心为：C=(X1+X2+…+Xn)/n，R=(|X1-C|^2+|X2-C|^2+…+|Xn-C|^2)/n</p>
<p>其中，簇半径表示簇中所有点到簇质心的平均距离。当有一个新点加入的时候，属性会变成CF=（N，LS，SS）的统计值，会压缩数据。</p>
<p>聚类特征树：</p>
<p>内节点的平衡因子B，子节点的平衡因子L，簇半径T。</p>
<p>B=6，深度为3，T为每个子节点中簇的范围最大不能超过的值，T越大簇越少，T越小簇越多。</p>
<p>名义分类：</p>
<p>ROCK算法：凝聚型的层次聚类算法</p>
<p>1.如果两个样本点的相似度达到了阈值（θ），这两个样本点就是邻居。阈值（θ）有用户指定，相似度也是通过用户指定的相似度函数计算。常用的分类属性的相似度计算方法有：Jaccard系数，余弦相似度</p>
<p>Jaccard系数：J=|A∩B|/|A∪B|，一般用于分类变量之间的相似度</p>
<p>余弦相似度：【-1，1】之间，越趋近于0的时候，方向越一致，越趋向同一。</p>
<p>2.目标函数（criterion function）：最终簇之间的链接总数最小，而簇内的链接总数最大</p>
<p>3.相似度合并：遵循最终簇之间的链接总数最小，而簇内的链接总数最大的规则计算所有对象的两两相似度，将相似性最高的两个对象合并。通过该相似性度量不断的凝聚对象至k个簇，最终计算上面目标函数值必然是最大的。</p>
<p>load(‘country.RData’)</p>
<p>d&lt;-dist(countries[,-1])</p>
<p>x&lt;-as.matrix(d)</p>
<p>library(cba)</p>
<p>rc &lt;-rockCluster(x, n=4, theta=0.2, debug=TRUE)</p>
<p>KNN算法：</p>
<p>先确定K的大小，计算出每个点之外的所有点到这个目标点的距离，选出K个最近的作为一类。一般类别之间的归类的话，投票和加权为常用的，投票及少数服从多数，投票的及越靠近的点赋予越大的权重值。</p>
<p>2.2分隔聚类：</p>
<p>需要先确定分成的类数，在根据类内的点都足够近，类间的点都足够远的目标去做迭代。</p>
<p>常用的有K-means，K-medoids，K-modes等，只能针对数值类的分类，且只能对中等量级数据划分，只能对凸函数进行聚类，凹函数效果很差。</p>
<p>2.3密度聚类：</p>
<p>有效的避免了对分隔聚类下对凹函数聚类效果不好的情况，有效的判别入参主要有1:单点外的半径2：单点外半径内包含的点的个数</p>
<p>DBSCAN为主要常见的算法，可优化的角度是现在密度较高的地方进行聚类，再往密度较低的地方衍生，优化算法：OPTICS。</p>
<p>2.4网格聚类：</p>
<p>将n个点映射到n维上，在不同的网格中，计算点的密度，将点更加密集的网格归为一类。</p>
<p>优点是：超快，超级快，不论多少数据，计算速度只和维度相关。</p>
<p>缺点：n维的n难取，受分布影响较大（部分行业数据分布及其不规则）</p>
<p>2.5模型聚类：</p>
<p>基于概率和神经网络聚类，常见的为GMM，高斯混合模型。缺点为，计算量较大，效率较低。</p>
<p>GMM：每个点出现的概率：将k个高斯模型混合在一起，每个点出现的概率是几个高斯混合的结果</p>
<p>假设有K个高斯分布，每个高斯对data points的影响因子为πk，数据点为x，高斯参数为theta，则：</p>
<p>利用极大似然的方法去求解均值Uk，协方差矩阵（Σk），影响因子πk，但是普通的梯度下降的方法在这里求解会很麻烦，这边就以EM算法代替估计求解。</p>
<p>3.优化数据结构：</p>
<p>1.数据变换：</p>
<p>logit处理，对所有数据进行log变换</p>
<p>傅里叶变换</p>
<p>小波变换</p>
<p>2.降维：</p>
<p>PCA：</p>
<p>利用降维（线性变换)的思想，整体方差最大的情况下（在损失很少信息的前提下），把多个指标转化为几个不相关的综合指标（主成分),将变量线性组合代替原变量，保持代替后的数据信息量最大（方差最大）。</p>
<p>LLE：</p>
<p>(1) 寻找每个样本点的k个近邻点；</p>
<p>(2)由每个样本点的近邻点计算出该样本点的局部重建权值矩阵；</p>
<p>(3)由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值。</p>
<p>(换句话说，就是由周围N个点构成改点的一个向量矩阵表示）</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 聚类 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[常用R语言包介绍]]></title>
      <url>/2017/06/19/%E5%B8%B8%E7%94%A8R%E8%AF%AD%E8%A8%80%E5%8C%85%E4%BB%8B%E7%BB%8D/</url>
      <content type="html"><![CDATA[<p>r与python差异比较大的一个地方就是，python的机器学习算法集中程度比较高，比如sklearn，就集成了很多的算法，而R语言更多时候需要一个包一个包去了解，比较费时费力，对于python转过来的朋友非常不友好，抽空整理了工作中常用的R包如下：</p>
<p>常用检验函数：</p>
<p>基本上分布中常见的都罗列了：</p>
<p>常用作图函数包：</p>
<p>ggplot2：万能，基本上excel能画的图它都能画</p>
<p>rattle：fancyRpartPlot函数，决策树画图函数</p>
<p>基础包函数：barplot、pie、dotchart、hist、densityplot、boxplot、contour等等</p>
<p>正态检验：qqplot、qqline、qqnorm</p>
<p>连续分类回归模型：</p>
<p>stats包 lm函数，实现多元线性回归；glm函数，实现广义线性回归；nls函数，实现非线性最小二乘回归；knn函数，k最近邻算法</p>
<p>rpart包 rpart函数，基于CART算法的分类回归树模型</p>
<p>randomForest包 randomForest函数，基于rpart算法的集成算法</p>
<p>e1071包 svm函数，支持向量机算法</p>
<p>kernlab包 ksvm函数，基于核函数的支持向量机</p>
<p>nnet包 nnet函数，单隐藏层的神经网络算法</p>
<p>neuralnet包 neuralnet函数，多隐藏层多节点的神经网络算法</p>
<p>RSNNS包 mlp函数，多层感知器神经网络；rbf函数，基于径向基函数的神经网络</p>
<p>离散分类回归模型：</p>
<p>stats包 glm函数，实现Logistic回归，选择logit连接函数</p>
<p>kknn包 kknn函数，加权的k最近邻算法</p>
<p>rpart包 rpart函数，基于CART算法的分类回归树模型</p>
<p>adabag包bagging函数，基于rpart算法的集成算法；boosting函数，基于rpart算法的集成算法</p>
<p>party包ctree函数，条件分类树算法</p>
<p>RWeka包OneR函数，一维的学习规则算法；JPip函数，多维的学习规则算法；J48函数，基于C4.5算法的决策树</p>
<p>C50包C5.0函数，基于C5.0算法的决策树</p>
<p>e1071包naiveBayes函数，贝叶斯分类器算法</p>
<p>klaR包NaiveBayes函数，贝叶斯分类器算分</p>
<p>MASS包lda函数，线性判别分析；qda函数，二次判别分析</p>
<p>聚类：Nbclust包Nbclust函数可以确定应该聚为几类</p>
<p>stats包kmeans函数，k均值聚类算法；hclust函数，层次聚类算法</p>
<p>cluster包pam函数，k中心点聚类算法</p>
<p>fpc包dbscan函数，密度聚类算法；kmeansruns函数，相比于kmeans函数更加稳定，而且还可以估计聚为几类；pamk函数，相比于pam函数，可以给出参考的聚类个数</p>
<p>mclust包Mclust函数，期望最大（EM）算法</p>
<p>关联规则：arules包apriori函数</p>
<p>Apriori关联规则算法</p>
<p>recommenderlab协调过滤</p>
<p>DRM：重复关联</p>
<p>ECLAT算法： 采用等价类，RST深度搜索和集合的交集： eclat</p>
<p>降维算法：</p>
<p>psych包prcomp函数、factanal函数</p>
<p>时序分析：</p>
<p>ts时序构建函数</p>
<p>timsac包时序分析</p>
<p>holtwinter包时序分析</p>
<p>decomp、tsr、stl成分分解</p>
<p>zoo 时间序列数据的预处理</p>
<p>统计及预处理：</p>
<p>常用的包 Base R, nlme</p>
<p>aov, anova 方差分析</p>
<p>density 密度分析</p>
<p>t.test, prop.test, anova, aov:假设检验</p>
<p>rootSolve非线性求根</p>
<p>reshape2数据预处理</p>
<p>plyr及dplyr数据预处理大杀器</p>
<p>最后剩下常用的就是读入和写出了：</p>
<p>RODBC 连接ODBC数据库接口</p>
<p>jsonlite 读写json文件</p>
<p>yaml 读写yaml文件</p>
<p>rmakdown写文档</p>
<p>knitr自动文档生成</p>
<p>一般业务中使用比较多的就是上面这些了，当然R里面有很多冷门的包，也很好用滴~</p>
]]></content>
      
        <categories>
            
            <category> 工具 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> R语言工具 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[决策树及衍射指标]]></title>
      <url>/2017/06/02/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%8A%E8%A1%8D%E5%B0%84%E6%8C%87%E6%A0%87/</url>
      <content type="html"><![CDATA[<p>一、常用的决策树节点枝剪的衡量指标：</p>
<p>熵：</p>
<p>如果一件事有k种可的结果，每种结果的概率为 pi（i＝1…k）</p>
<p>该事情的信息量：</p>
<p>熵越大，随机变量的不确定性越大。</p>
<p>信息增益：</p>
<p>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下的经验条件熵H(D|A)之差</p>
<p>换句话说，就是原信息集下的信息量－在A特征条件下的信息集的信息量</p>
<p>信息增益越大，信息增多，不确定性减小</p>
<p>信息增益率：</p>
<p>信息增益率定义:特征A对训练数据集D的信息增益比定义为其信息增益与训练数据D关于特征A的值的熵HA(D)之比</p>
<p>注：p：每个唯独上，每个变量的个数／总变量个数</p>
<p>二、常用的决策树介绍：</p>
<p>ID3算法：</p>
<p>ID3算法的核心是在决策树各个子节点上应用信息增益准则选择特征，递归的构建决策树，具体方法是:从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。</p>
<p>解释：在做每次选择差分枝的时候，以不确定性最小点作为loss fuction，直到无法细分</p>
<p>缺点：</p>
<p>1.ID3算法只有树的生成，所以该算法生成的树容易产生过拟合，分得太细，考虑条件太多。</p>
<p>2.不能处理连续属性</p>
<p>3.选择具有较多分枝的属性，而分枝多的属性不一定是最优的选择。</p>
<p>4.局部最优化，整体熵值最小，贪心算法算子节点的分支</p>
<p>C4.5算法：</p>
<p>基于ID3算法，用信息增益比来选择属性，对非离散数据也能处理，能够对不完整数据进行处理。</p>
<p>采用增益率（GainRate）来选择分裂属性。计算方式如下：</p>
<p>CART算法：</p>
<p>CART算法选择分裂属性的方式是比较有意思的，首先计算不纯度，然后利用不纯度计算Gini指标。</p>
<p>计算每个子集最小的Gini指标作为分裂指标。</p>
<p>不纯度的计算方式为：</p>
<p>pi表示按某个变量划分中，目标变量不同类别的概率。</p>
<p>某个自变量的Gini指标的计算方式如下：</p>
<p>计算出每个每个子集的Gini指标，选取其中最小的Gini指标作为树的分支（Gini（D）越小，则数据集D的纯度越高）。连续型变量的离散方式与信息增益中的离散方式相同。</p>
<p>三、基于决策树的一些集成算法：</p>
<p>随机森林：</p>
<p>随机生成n颗树，树之间不存在关联，取结果的时候，以众数衡量分类结果；除了分类，变量分析，无监督学习，离群点分析也可以。</p>
<p>生成过程：</p>
<p>1.n个样本，随机选择n个样本（有放回），训练一颗树</p>
<p>从原始训练数据集中,应用bootstrap方法有放回地随机抽取 K个新的自助样本集,并由此构建 K棵分类回归树,每次未被抽到的样本组成了 K个袋外数据(Out-of-bag,OOB)</p>
<p>2.每个样本有M个属性，随机选m个，采取校验函数（比如信息增益、熵啊之类的），选择最佳分类点</p>
<p>3.注意，每个树不存在枝剪</p>
<p>4.将生成的多棵树组成随机森林,用随机森林对新的数据进行分类,分类结果按树分类器的投票多少而定</p>
<p>树的个数随机选取，一般500，看三个误差函数是否收敛；变量的个数一般取均方作为mtry</p>
<p>GBDT：</p>
<p>DT步骤：</p>
<p>GBDT里面的树是回归树！</p>
<p>GBDT做每个节点上的分支的时候，都会以最小均方误差作为衡量（真实值－预测值）的平方和／N，换句话说，就是存在真实线l1，预测线l2，两条线之间的间距越小越好。</p>
<p>BT步骤：</p>
<p>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。</p>
<p>换句话说，就是第一次预测的差值记为下一次预测的初始值，一直到某一次计算出的差值为0，把前n次的结果相加，就是一个真实预测。</p>
<p>Adaboost：</p>
<p>步骤：</p>
<p>1.初始化所有训练样例的权重为1 / N,其中N是样本数</p>
<p>2.对其中第1~m个样本:</p>
<p>a.训练m个弱分类器，使其最小化bias：</p>
<p>b.接下来计算该弱分类器的权重α，降低错判的分类器的权重，：</p>
<p>c.更新权重：</p>
<p>3.最后得到组合分类器：</p>
<p>核心的思想如下图：</p>
<p>全量数据集在若干次训练后，降低训练正确的样本的权重，提高训练错误样本的权重，得到若干个Y对应的分类器，在组合投票得到最终的分类器</p>
<p>四、惠普实验室-集成并行化的随机森林：</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 树划分问题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[因子分析原理剖析]]></title>
      <url>/2017/06/02/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>因子分析概述：</p>
<p>因子分析分为Q型和R型，我们对R型进行如下研究：</p>
<p>一.因子分析步骤：</p>
<p>1.确认是是否适合做因子分析</p>
<p>2.构造因子变量</p>
<p>3.旋转方法解释</p>
<p>4.计算因子变量得分</p>
<p>二.因子分析的计算过程：</p>
<p>1.将原始数据标准化</p>
<p>目的：消除数量级量纲不同</p>
<p>2.求标准化数据的相关矩阵</p>
<p>3.求相关矩阵的特征值和特征向量</p>
<p>4.计算方差贡献率和累计方差贡献率</p>
<p>5.确定因子</p>
<p>F1,F2,F3…为前m个因子包含数据总量（累计贡献率）不低于80%。可取前m各因子来反映原评价</p>
<p>6.因子旋转</p>
<p>当所得因子不足以明显确定或不易理解时选择此方法</p>
<p>7.原指标的线性组合求各因子的得分</p>
<p>两种方法：回归估计和barlett估计法</p>
<p>8.综合得分：以各因子的方差贡献率为权，各因子的线性组合得到各综合评价指标函数</p>
<p>F=（λ1F1+…λmFm）/(λ1+…λm)</p>
<p>=W1F1+…WmFm</p>
<p>9.得分排序</p>
<p>因子分析详解：</p>
<p>因子分析模型，又名正交因子模型</p>
<p>X=AF+ɛ</p>
<p>其中：</p>
<p>X=[X1,X2,X3…XP]‘</p>
<p>A=</p>
<p>F=[F1,F2…Fm]’</p>
<p>ɛ=[ɛ1,ɛ2…ɛp]’</p>
<p>以上满足：</p>
<p>（1）m小于等于p</p>
<p>（2）cov(F,ɛ)=0</p>
<p>(3)Var(F)=Im</p>
<p>D(ɛ)=Var(ɛ)=</p>
<p>ɛ1,ɛ2…ɛp不相关，且方差不同</p>
<p>我们把F成为X公共因子，A为荷载矩阵，ɛ为X特殊因子</p>
<p>A=(aij)</p>
<p>数学上证明：aij就是i个变量与第j个因子的相关系数，参见层次分析法aij定义。</p>
<p><1>荷载矩阵</1></p>
<p>就荷载矩阵的估计和解释方法有主因子和极大似然估计，我们就主因子分析而言：（是主因子不是主成份）</p>
<p>设随机向量X的协方差阵为Ʃ</p>
<p>λ1,λ2,λ3..&gt;0为Ʃ的特征根</p>
<p>μ1，μ2，μ3…为对应的标准正交向量</p>
<p>我们大一学过线代或者高代，里面有个东西叫谱分析：</p>
<p>Ʃ=λ1μ1μ1’+……+λpμpμp’</p>
<p>=</p>
<p>当因子个数和变量个数一样多，特殊因子方差为0.</p>
<p>此时，模型为X=AF,其中Var(F)=Ip</p>
<p>于是，Var(X)=Var(AF)=AVar(F)A’=AA’</p>
<p>对照Ʃ分解式,A第j列应该是</p>
<p>也就是说，除了uj前面部分，第j列因子签好为第j个主成份的系数，所以为主成份法。</p>
<p>如果非要作死考虑ɛ</p>
<p>原来的协方差阵可以分解为：</p>
<p>Ʃ=AA’+D=</p>
<p>以上分析的目的；</p>
<p>1.因子分析模型是描述原变量X的协方差阵Ʃ的一种模型</p>
<p>2.主成份分析中每个主成份相应系数是唯一确定的，然而因子分析中的每个因子的相应系数不是唯一的，因而我们的因子荷载矩阵不是唯一的</p>
<p>(主成分分析是因子分析的特例，非常类似，有兴趣的可以去看看，这两者非常容易混淆)</p>
<p><2>共同度和方差贡献</2></p>
<p>无论是在spss或者R的因子分析中都围绕着贡献度，我们来看下，它到底是什么意思。</p>
<p>由因子分析模型，当仅有一个公因子F时，</p>
<p>Var(Xi)=Var(aiF)+Var(ɛi)</p>
<p>由于数据标准化，左端为1，右端分别为共性方差和个性方差</p>
<p>共性方差越大，说明共性因子作用越大。</p>
<p>因子载荷矩阵A中的第i行元素之平方和记为hi2</p>
<p>成为变量(Xi)共同度</p>
<p>它是公共因子对(Xi)的方差锁做出的贡献，反映了全部公共因子对变量(Xi)的影响。</p>
<p>hi2大表明第i个分量对F的每一个分量F1,F2,…Fm的共同依赖程度大</p>
<p>将因子载荷矩阵A的第j列的各元素的平方和记为gj2</p>
<p>成为公共因子Fj对x的方差贡献。</p>
<p>gj2表示第j个公共因子Fj对x的每一个分量Xi所提供的方差的总和，他就是衡量公共因子的相对重要行的指标。gj2越大，表明公共因子Fj对x的贡献越大，或者说对x的影响和作用就越大。</p>
<p>如果将载荷矩阵A的所有gj2都计算出来，按大小排列，就可以提炼最有影响力的公共因子。</p>
<p><3>因子旋转</3></p>
<p>这方面涉及较为简单，我就简单提一下</p>
<p>目的：建立因子分析模型不是只要找主因子，更加重要的是意义，以便对实际进行分析，因子旋转就是使所得结论更加清晰的表示。</p>
<p>方法：正交旋转，斜交旋转两大类，常用正交。</p>
<p>便于理解，我解释下旋转的意义，以平面直角坐标系为例，我们想得到的数据正好为：y=x和y=-x上的点，我们能解释的却在x=0和y=0上，这时候我们就可以旋转坐标系，却不影响结果。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 因子分析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[正则化-Lasso规约]]></title>
      <url>/2017/06/02/%E6%AD%A3%E5%88%99%E5%8C%96-Lasso%E8%A7%84%E7%BA%A6/</url>
      <content type="html"><![CDATA[<blockquote>
<p>摘要：lasso的目的主要是避免数据拟合过渡，导致训练数据效果优秀，测试数据效果较差</p>
</blockquote>
<hr>
<p>先看一波过拟合：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-ef56928a462ec258?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="拟合曲线"></p>
<p>图中，红色的线存在明显的过拟合，绿色的线才是合理的拟合曲线，为了避免过拟合，我们可以引入正则化。</p>
<p>下面可以利用正则化来解决曲线拟合过程中的过拟合发生，存在均方根误差也叫标准误差，即为√[∑di^2/n]=Re，n为测量次数；di为一组测量值与真值的偏差。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-d4a10e15eb588a58?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>实际考虑回归的过程中，我们需要考虑到误差项，<br><img src="http://upload-images.jianshu.io/upload_images/1129359-e06d7ab80cc112e9?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-4488780e621c8d45?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这个和简单的线性回归的公式相似，而在正则化下来优化过拟合这件事情的时候，会加入一个约束条件，也就是惩罚函数：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-052572ba619894cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这边这个惩罚函数有多种形式，比较常用的有l1,l2，大概有如下几种：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-acde75b94b40d61a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>讲一下比较常用的两种情况，q＝1和q＝2的情况：<br>q＝1，也就是今天想讲的lasso回归，为什么lasso可以控制过拟合呢，因为在数据训练的过程中，可能有几百个，或者几千个变量，再过多的变量衡量目标函数的因变量的时候，可能造成结果的过度解释，而通过q＝1下的惩罚函数来限制变量个数的情况，可以优先筛选掉一些不是特别重要的变量，见下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-2b285d27a06188fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>作图只要不是特殊情况下与正方形的边相切，一定是与某个顶点优先相交，那必然存在横纵坐标轴中的一个系数为0，起到对变量的筛选的作用。<br>q＝2的时候，其实就可以看作是上面这个蓝色的圆，在这个圆的限制下，点可以是圆上的任意一点，所以q＝2的时候也叫做岭回归，岭回归是起不到压缩变量的作用的，在这个图里也是可以看出来的。</p>
<hr>
<p>lasso回归：<br>lasso回归的特色就是在建立广义线型模型的时候，这里广义线型模型包含一维连续因变量、多维连续因变量、非负次数因变量、二元离散因变量、多元离散因变，除此之外，无论因变量是连续的还是离散的，lasso都能处理，总的来说，lasso对于数据的要求是极其低的，所以应用程度较广；除此之外，lasso还能够对变量进行筛选和对模型的复杂程度进行降低。这里的变量筛选是指不把所有的变量都放入模型中进行拟合，而是有选择的把变量放入模型从而得到更好的性能参数。 复杂度调整是指通过一系列参数控制模型的复杂度，从而避免过度拟合(Overfitting)。 对于线性模型来说，复杂度与模型的变量数有直接关系，变量数越多，模型复杂度就越高。 更多的变量在拟合时往往可以给出一个看似更好的模型，但是同时也面临过度拟合的危险。<br>lasso的复杂程度由<code>λ</code>来控制，<code>λ</code>越大对变量较多的线性模型的惩罚力度就越大，从而最终获得一个变量较少的模型。除此之外，另一个参数α来控制应对高相关性(highly correlated)数据时模型的性状。 lasso回归<code>α=1</code>，Ridge回归<code>α=0</code>，这就对应了惩罚函数的形式和目的。我们可以通过尝试若干次不同值下的λ，来选取最优λ下的参数，还可以结合CV选择最优秀的模型。</p>
<p>######读取数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">setwd(&quot;~/Desktop&quot;)</span><br><span class="line">library(glmnet)</span><br><span class="line">train_origin&lt;-read.table(&apos;trian.txt&apos;,header = T,fill = T)</span><br><span class="line">test_origin&lt;-read.table(&apos;test.txt&apos;,header = T,fill = T)</span><br><span class="line">train_test1&lt;-train_origin</span><br><span class="line">train_test1&lt;-train_test1[,-9]</span><br><span class="line">train_test1$tag&lt;-as.factor(train_test1$tag)</span><br><span class="line">train_test1$risk_level&lt;-as.factor(train_test1$risk_level)</span><br><span class="line">x&lt;-train_test1[,3:11]</span><br><span class="line">y&lt;-train_test1[,2]</span><br><span class="line">## one hot encoding</span><br><span class="line">x1&lt;-model.matrix(~., x)</span><br></pre></td></tr></table></figure></p>
<p>通常数据中会存在离散点，而lasso在R里面是通过数值矩阵来做输入的，所以需要对原数据做一步预处理，不然这边会抛错误；除此之外，如果数据之间差别的数量级较大，还需要进行标准化，R里面也是可以进行处理的，这边就不赘述了，<code>glmnet()</code>函数中添加参数<code>standardize = TRUE</code>来实现，<code>scale()</code>函数也可以实现，自行选择即可。</p>
<p>######模型训练<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = glmnet(x1, y, family=&quot;binomial&quot;, nlambda=50, alpha=1)</span><br></pre></td></tr></table></figure></p>
<p><code>family</code>里面是指选择函数的类型：<br><code>family explation</code>,<code>gaussian univariate</code>,<code>mgaussian multivariate</code>,<code>poisson count</code>,<code>binomial binary</code>,<code>multinomial category</code><br>lambda是指随机选择λ，做lambda个模型；alpha是上述讲到的α，选择惩罚函数，正常情况下，1是lasso，0是岭回归<br>这边模型拓展可以交叉检验一下，有内置的函数：<br>cvmodel = cv.glmnet(x1, y, family = “binomial”, type.measure = “class”,nfolds=10)<br>这边会多出来一个<code>type.measure</code>，这个<code>type.measure</code>是指期望最小化的目标参量是什么，换句话说，就是衡量这个模型的指标函数是啥：<br><code>type.measure</code>  details<br><code>deviance</code>:  <em>-2倍的Log-likelihood</em><br><code>mse</code>:<em>mean squred error</em><br><code>mae</code>:<em>mean absolute error</em><br><code>class</code>:<em>missclassification error</em><br><code>auc</code>:<em>area under the ROC curve</em><br>比较常用的是<code>auc</code>，这个就是现在比较主流的衡量一个模型好坏的roc所衍生出来的一个值；我们这边用的是<code>class</code>，也就是模型错误分配的概率，结合我这次业务开发的实际业务场景，这个更合适一点；<code>nfolds</code>是指folds数目，也可以通过foldid数来控制每个fold里面的数据数量。<br>对于glmnet，可以通过<code>plot(model)</code>来观察每个自变量的变化轨迹，<code>cv.glmnet</code>可以通过<code>plot(cvmodel)</code><br>举个<code>plot(cvmodel)</code>的例子：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-6905b27c01038db6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以通过<code>c(cvfit$lambda.min, cvfit$lambda.1se)</code>来看在所有的<code>λ</code>值中，得到最小目标函数<code>type.measure</code>均值的<code>cvfit$lambda.min</code>，以及其所对应的<code>λ</code>值可接受的一个标准误差之内对应的<code>cvfit$lambda.1se</code>。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-9fb083e9aaa14975.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>我们可以<code>print(model)</code>，在实际的选择模型中<code>λ</code>值的过程里，存在三个指标：<code>df</code>：自由度， <code>%Dev</code>：残差被解释的占比，也就是模型的好坏程度，类似于线性模型中的R平方，Lambda也就是<code>λ</code>值所对应的值，然后我们可以通过<code>coef(fit, s=c(fit$lambda[35],0.002))</code>得出当时模型所对应的系数。</p>
<hr>
<p>最后，讲一下elastic net<br>elastic net融合了l1范数和l2范数两种正则化的方法，上面的岭回归和lasso回归都可以看做它的特例：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-9d1352f5686c9a37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>elastic net对于p远大于n,或者严重的多重共线性情况有明显的效果，很好理解，当alpha接近1时，elastic net表现很接近lasso，一般来说，elastic net是岭回归和lasso的很好的折中，当alpha从0变化到1，目标函数的稀疏解（部分变量的系数为0）也从0单调增加到lasso的稀疏解。</p>
<p><strong>特征规约初步总结如下：</strong><br>1）子集选择 这是传统的方法，包括逐步回归和最优子集法等，对可能的部分子集拟合线性模型，利用判别准则 （如AIC,BIC,Cp,调整R2 等）决定最优的模型<br>2）收缩方法（shrinkage method） 收缩方法又称为正则化（regularization）。主要是岭回归（ridge regression）和lasso回归。通过对最小二乘估计加入罚约束，使某些系数的估计为0。（岭回归：消除共线性；模的平方处理；Lasso回归：压缩变量，起降维作用；模处理）<br>(3)维数缩减 主成分回归（PCR）和偏最小二乘回归（PLS）的方法。把p个预测变量投影到m维空间</p>
<p><em>部分图片转载于：<a href="http://bbs.pinggu.org/thread-3848519-1-1.html" target="_blank" rel="noopener">http://bbs.pinggu.org/thread-3848519-1-1.html</a></em></p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 正则化问题 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
