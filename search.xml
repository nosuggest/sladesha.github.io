<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[博客导读公告(置顶)]]></title>
    <url>%2F2029%2F01%2F20%2F%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%AF%BB%E5%85%AC%E5%91%8A%2F</url>
    <content type="text"><![CDATA[首先，我很荣幸您出现在我的个人博客，下面请允许我花费您3分钟左右的时间，简单的为您介绍一下博客中的相关功能，这将极大的提高您在后续阅读中的体验： 打个广告，欢迎各位老爷关注我的微信公众号：ml_trip，期待与大家交流！ 我的个人介绍大家可以在首页的标题下面找到me这个图标，点击即可，里面有我的个人介绍： 我的个人简历下载地址在me跳转页面中的位置如下： 快速阅读您可在任何一篇文章的右侧看到红色方框： 如果您对其中部分内容感兴趣，可直接点击绿色方框内的文章，会自动跳转到您关心的模块； 如果您想要看到我的更多联系方式，可以点击蓝色模块中的站点概览； 如果您不想红色方框影响您的阅读，在黑色条块的最下方的小叉点击即可。 赞助激励如果您觉得我写的东西对您有一些帮助，在您宽裕的情况下可以在文章下面的打赏中给我发一个小红包，或者直接扫描下面的二维码，感谢您对我的认可： 如果您还是一个学生或者您正处于人生的低谷，感谢您对我的认可，打赏就不需要了，我会一如既往的给大家整理工作中的一些想法和心得 自定义搜索为了方便大家找到自己关心的内容，建议直接点击搜索图标： 比如搜索svm，有模糊匹配结果如下： 标签检索大家可以在首页的标题下面找到标签这个图标，点击即可： 该类别下生成了标签云，为较为仔细的文章内容概括： 其他 因为本博客部署在GitHub，如果您有时候遇到打不开网页的问题，建议您重复刷新或者收藏slade_sal简书地址，两者内容是一致的 如果您有任何疑惑或者疑问都可以通过站点概览的邮箱联系我，非常愿意解答您的问题 如果您遇到算法学习过程的困难，需要内推或者就业方向建议，也可以通过站点概览的邮箱联系我，非常愿意和您进行交流 绝大多数代码都可以在我的Github上找到，所有的数据都经过脱敏处理，您可以放心使用，希望对您有所帮助 最后，感谢大家一路以来对我的认可。]]></content>
      <categories>
        <category>基础</category>
      </categories>
      <tags>
        <tag>公告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle&TianChi分类问题相关纯算法理论剖析]]></title>
    <url>%2F2019%2F05%2F10%2FKaggle%26TianChi%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9B%B8%E5%85%B3%E7%BA%AF%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[17/12/30-update ：很多朋友私密我想要代码，甚至利用金钱诱惑我，好吧，我沦陷了。因为原始代码涉及到公司的特征工程及一些利益trick，所以我构造了一个数据集后复现了部分算法流程，需要看详细代码实现朋友可以移步Ensemble_Github 更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过stw386@sina.com联系我，知无不答。 导读在上一次的文章中，我们讲了，如何快速的利用bagging、boosting、stacking、ensemble的形式实现一个分类算法，当时我们直接看了代码以及核心的理论注意点。如果需要有更加优异的结果表现，对整套算法的设计及相关的理论了解是必不可少的。本文将从数学、工程、领域经验的角度去剖析如何用好bagging、boosting、stacking、ensemble去训练一个相对完善的模型。 再次提醒，本文中的数据公式较多，抽象概念较多，需要一定的高等代数、泛函分析、机器学习基础作为前置条件。如果你只需要知道如何运行或者完成分类识别，请参考Kaggle-TianChi分类问题相关算法快速实现。如果需要更详尽的理论解析或者有哪些地方不明白的同学，建议私下联系我stw386@sina.com。如果你想skip read本文，请直接阅读最后一个小节：调参流程梳理。 那么接下来让我们开始正文，虽然本文写的很冗长，我依旧建议阅读完此文，即便是处于懵懂的状态，对后续模型调整的理解也是有一定益处的，而且我会尽可能的用通俗易懂的语言来讲，很多地方会存在解释不严谨的地方但是更易于理解。 Bias-Variance-Tradeof在上次的文章中，我们就提到了一个好的模型应该有着非常好的拟合能力，就是说我的偏差要尽可能的小；同时，也要保证方差尽可能的小，这样我们才能在泛化能力上有很不错的表现。 设样本容量为n的训练集为随机变量的集合(X1, X2, …, Xn)，那么模型是以这些随机变量为输入的随机变量函数（这边的F虽然上函数，但是也是随机变化的）：F(X1, X2, …, Xn)。抽样的随机性带来了模型的随机性。那如何定义一个模型的Bias和Variance呢？这边我们采取的是基模型的加权均值E(F)=E(∑(γ*fi))移动来替代bias、基模型的加权方差Var(F)=Var(∑(γ*fi))替代Variance，更详细的数学定义如下：这边在Variance推导过程中用到了这个性质：Cov(X*Y) = E(X*Y) - E(X)*E(Y)，同时将方差拆分成协方差的形式。我们可以看到，组合后的模型的Bias和基模型的Bias的权重γ相关，组合后的模型的Var和基模型的权重γ、基模型个数m、基模型相关性ρ相关。请务必深刻的记得上述bias和var的数学式子，在后续无论是调参数还是模型设计，我们都是围绕着，降低bias和var的角度去做的。 Bagging Bias and Variance很多同学在没看这篇文章之前就知道，bagging算法和stacking算法是需要基模型保持强基模型（偏差低方差高）的，但是不知道大家有没有想过为什么？阿里15年校招的时候，我有幸回答过一个题目就是这个问题，下面让我们看看为什么是这样的？ 对于bagging算法而言，每次的抽样都是以尽可能使得基模型相互独立为前提的，为了维持这样的假设，我们做了三件事： 样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样 子抽样：整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量 弱抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）下的feature的抽样 同时，由此我们由此也可以得到bias和var公式中的γ=1/m，基模型的权重一定程度是可以看作是相等的，所以原来的E和Var公式就变成：组合模型F的bias和基模型fi的bias一致，这就是我们为什么要求基模型fi的bias要低的原因，因为组合模型F的拟合能力E(F)不随着基模型个数的增加而上升。 组合模型F的var与基模型fi的var、基模型fi的个数m、基模型的相关性ρ相关，很明显可以看出，随着基模型的个数上升Var(F)第二项是在下降的，所以基模型的个数上升会降低组合模型的方差，这就是为什么基模型的方差可以高一些。 除此之外，我们还可以看出，如果基模型相关性ρ越低，整体的方差是越小的，所以我们才去做样本抽样，子抽样，弱抽样等等行为。还有，基模型的个数上升一定程度上会降低组合模型F的方差，但是不是无限递减的，公式中它只能降低第二项的方差值，第一项的方差值不随基模型的个数而增减。 从这个角度看，是不是对Bagging算法的理解又深刻了一些？接下来让我们看看Boosting。 Boosting Bias and Variance依旧的是很多同学在没看这篇文章之前就知道，boosting算法是需要基模型保持弱基模型（偏差高方差低）的，让我们一探究竟。熟悉boosting算法的同学都知道，boosting算法的基模型的相关性几乎≈1的，后续模型强依赖于前模型，所以我们可以认为ρ=1，得到如上的简化式子。组合模型F的bias是基模型的bias的累加，基模型的准确度都不是很高（因为其在训练集上的准确度不高），随着基模型数的增多，整体模型的期望值增加，更接近真实值。而站在方差的角度，组合模型F的方差是随着基模型的fi个数上升而平方上升的，这就要求我们的基模型的方差不能太高，否则组合模型的F就会增长爆炸。这就是为什么我们在boosting模型设计的时候，需要基模型保持弱模型（偏差高方差低）的原因。 多说一句，大家也看到了，boosting的Var(F)是依赖于基模型的权重γ的，所以在后续的gbdt、xgboost，各位数据科学家选择了类似bagging的采样模式，降低模型的γ，控制方差，所以说，了解原理再去重新或者优化还是很重要的。 Bagging、Boosting、Stacking Bias&amp;Variance总结纵观刚才说了的这么多，我们在Bagging、Boosting、Stacking的模型设计中所围绕的就是降低bias的同时，降低Variance。而我们所做的sklearn里面的参数调整就可以说用来： a.构建基模型，变化基模型的Bias和Variance b.组合模型构建，控制基模型后，如何把这个基模型很好的组合成一个优秀的ensemble模型。 GBDT 理论剖析模型过程推导其实random forest和gbdt、xgboost都是非常好的Bagging、Boosting、Stacking算法的优化升级版本，我个人用的gbdt稍多，所以就以gbdt为例子给大家梳理一遍，从理论，到调参数，到trick分享。 我最讨厌很多博主贴个上面的伪代码就跑了，我念书的时候，老师讲题目也是，我靠，我要是看得懂还需要你贴？所以，我们选择一步一步的来看这个伪代码。 让我们先概览一下整个流程，gbdt有递归设计如下： 如果y(i)代表着第i个基模型，第i+1个基模型其实是基于第i个基模型的结果而追加了一个New function去修正前i个基模型的误差。如果以F代替y，h代替f的话，我们可以得到下面这个递归函数： 第i个基模型是由前i-1个基模型中的h(x)累计得到的，我们最后想要得到的分类器即为： 每一轮迭代中，只要集中精力使得Fi(x)每次loss下降的最快即可：每次构建一个残差负梯度作为更新方向的New function(即hi(x))，就可以解决这个复杂的递归问题，从而得到F(x)的解析式。 接下来，我们再看看更加详细的做法： 初始化部分，在这次梳理之前，我也一直认为是随机构造的，这边看完伪代码我才知道，在初始值设置的时候，考虑了直接使得损失函数极小化的常数值，它是只有一个根节点的树，即是一个c常数值。 构建回归树，这边就稍许复杂，让我们拆开一步一步来看： 首先，先求整体损失函数的反向梯度。先举个mse的例子，如果现在我们考虑的是mse(着重注意，只有mse的情况下以下的梯度才是这样的)的形式，我们要做的就是在每一步的时候让我们的预测值Fi(x)与真实值y的损失函数：1/2*(y-Fi(x))^2最小(前面的1/2是为了方便求导计算加上去的)，如果对梯度下降有了解的朋友就知道，此刻要求它的最小就是去求偏导： 按照这个方向去更新Fi(x)，可以保证，组合模型F(x)的每次都是按照最优的方向去优化。 MSE的损失函数确实是残差形式，不代表所有的损失函数下更新的方向都满足这样的残差形式。kaggle master 在blog里面提到Although we can minimize this function directly, gradient descent will let us minimize more complicated loss functions that we can’t minimize directly，我们就需要设计出一种更快速能提升泛化能力且不失一般性的解决方案，所以有大神提出了以梯度下降的值直接代替因变量y，也就是我每次预测不去比预测值与真实值y的差异，我们比较的是预测的梯度方向与真实的梯度方向。 再基于已经预估出来的负梯度去计算最优的更新步长，使得预测值与真实值更加靠近，因为每层的负梯度的方向不固定，所以每层i的步长都是变化的。 最后通过缩减率v（这边就是类似logistics里面的∂）控制速率。 综上，假设test集合第i轮预测中，根据训练集训练出来的负梯度拟合模型不妨记为fi(x)、最优步长γi、缩减率v，可得到最终的递归公式为： 损失函数介绍刚才上面我举了一个mse作为损失函数的例子，其实还有很多其他的，参考如下： 这边说个有意思的东西，就是如果有兴趣的朋友可以把exponential：指数损失函数计算一下，反向梯度为： 则有第i轮损失函数： 这货就是adaboost的第i轮损失函数的非归一化的结果，是不是很有趣，虽然知道了没啥用，但是起码得到了我们在用gbdt的时候，loss=’exponential’即为adaboost的结论啊，哈哈～所以说，我觉得去推推公式，还是很有意思的。 到此为止，gbdt是怎么构造得来的就讲完了，其实这个和bias&amp;variance关系不大，但是为了铺垫后续的GBDT实战剖析，我觉得还是非常有必要梳理一遍的，但就比起Bias-Variance-Tradeof这节的内容，我觉得各位还是着重理解Bias-Variance-Tradeof，这节可以看作是’甜品’缓解下气氛。 GBDT 实战剖析我们以python下的sklearn.ensemble中的GradientBoosting及RandomForest为例子，实战分析一下，如何能够理性的调好参数而并非玄学的gridsearch。 在Bias-Variance-Tradeof我们提到了参数设置分为两块：a.构建基模型,b.构建组合模型，我们分GradientBoosting参数如下： 构建组合模型：1) n_estimators:基模型的个数，对于gbdt来说，因为我们需要通过基模型的个数来提升准确率所以n_estimators一般都会大于random forest的n_estimators的个数，实际上RandomForestClassifier默认为10，GradientBoostingClassifier默认为100也证明了这点。 2）learning_rate：步长，对于gbdt来说，步长依赖于n_estimators，100=2*50=5*20，就是这个道理。而random forest里面不存在步长这个概念。在gbdt里面，关于步长的优化一般是伴随着基模型的变化而变化的。 3）subsample：子采样率，还记得我们上面对子采样的描述么？一般来说，降低“子采样率”（subsample），也会造成子模型间的关联度降低，整体模型的方差减小，但是当子采样率低到一定程度时，子模型的偏差增大，将引起整体模型的准确度降低。 4) init：初始化，更多见GBDT 理论剖析中，我们对初始化的描述。 5）loss：对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。分类模型不说了，刚才在GBDT 理论剖析中讲了，一般用”deviance”比较多；回归模型中，”ls”我们也在GBDT 理论剖析中讲了，异常点多的情况下”huber”,训练集分段预测的话用”quantile”，但是我个人建议异常点或者分段预测还是在数据已处理中完成。 6) alpha：这个参数只有Huber损失”huber”和分位数损失”quantile”下的GradientBoostingRegressor，alpha越小对噪声处理的力度越强，alpha越小分位数的值越小。 构建基模型：1）max_features：每次划分最大特征数，有log2，sqrt，None等等，默认的是sqrt，该值越小，我们每次能获得信息越少，造成偏差时变大的，同时方差是变小的，所以当我们模型拟合能力不足的时候，可以考虑提升该值。 2）max_depth:基模型最大深度，深度越大，模型的拟合能力越强，bias越小。根据Bias-Variance-Tradeof我们对bagging和boosting里面的Var和Bias的描述可知，如果在boost（gbdt）采用了过深的基模型，组合模型的var会很大，在泛化能力会降低，造成训练集效果优秀，测试集差；如果在bagging（random forest）采取了过浅的基模型，组合模型的拟合能力会不足，我们可以考虑增加深度，甚至不控制生长。 3）min_samples_split：内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。随着分裂所需的最小样本数的增加，子模型的结构变得越来越简单，极端情况下，方差减小导致整体模型的拟合能力不足。 4）min_weight_fraction_leaf：叶节点最小权重总值，这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和其他子叶节点一起被剪枝，会使得模型变得简单，降低了方差，提高了偏差，如果正负样本不一致，需要考虑调整这个值。 5）max_leaf_nodes：最大叶子节点数，通过限制最大叶子节点数，可以防止过拟合，会使得模型变得简单，降低了方差，提高了偏差。这边需要注意如果设置了max_leaf_nodes，会忽略max_depth参数值。 梳理完以上每个参数的对模型拟合的能力及对Vaild集合泛化能力的影响，我们可以根据项目训练中，实际模型的训练集拟合效果，检验集的泛化效果进行优化参数。 调参流程梳理ok，问题来了，很多同学看了之后说，你说了这么多参数作用，你还是没告诉我改如何调参数？我就喜欢这种很关注结果的同学，以下干货来自个人及个人朋友及我从知乎等网站”剽窃”来的观点，不负任何理论责任。 我第一任老大，现在在阿里做算法专家，他根据24个数据集合上以不同的调参流程去训练相同的测试集得出的效果对比，总结出以下一个流程： 先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值 再确定合适的subsample 再调优最大树深度（max_depth） 再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）,这边是不一定使用的 再调优最大叶节点数（max_leaf_nodes） 再组合优化分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf） 最后，优化分裂时考虑的最大特征数（max_features） 组合调整n_estimators和learning_rate 但是，我今年在逛知乎的时候偶然看到一个帖子，里面讲的就是调参数的困扰，提到了一个点，就是先确定了max_depth=3后，无论怎么优化min_samples_split和min_samples_leaf对结果都没有任何影响了。当时我想了很久，最后是一位知友解答了这疑惑，其实这样的： 假设原始数据中正负样本比是1:1000，在做max_depth=3的时候，因为样本不均衡，已经可以通过非常简单的少量feature对正负样本进行区分，所以，在之后怎么调节分裂所需要最小样本树和子节点最小样本数都不能够影响到回归树的构造，然而该区分的回归树是没有泛化能力的。 要解决这个问题要么平衡数据，要么就是先确定回归决策树每个叶子结点最小的样本数(min_samples_leaf),再确定分裂所需最小样本数（min_samples_split），才能确定最大深度,这样就能保证不会出现某棵树通过一个feature将数量较少的的正类以较过拟合的简单浅层树拟合出来，而是优先保证了每一次我构造树都尽可能的平衡满足了数据量合理，数据具有样本具有代表性，不会过拟合这样的假设。所以，可以优化为： 先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值 再确定合适的subsample 再组合调优最大树深度（max_depth）和叶节点最小样本数（min_samples_leaf） 再调优最大叶节点数（max_leaf_nodes） 再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）,这边是不一定使用的 再组合优化分裂所需最小样本数（min_samples_split） 最后，优化分裂时考虑的最大特征数（max_features） 组合调整n_estimators和learning_rate 去年Aarshay Jain大神总结的调参数整理也给出了一种调优思路： 优先，调整最大叶节点数和最大树深度 其次，分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf）及叶节点最小权重总值（min_weight_fraction_leaf） 然后，分裂时考虑的最大特征数（max_features） 容我多嘴一句，我们思考了这么多，其实如果能在最开始做一个正负样本平衡就会避免很多问题，所以，再次强调数据预处理的重要性。 除此在外，很多人会选择在以上模型调优结束后再以10*learning_rate进行”鞍点逃逸”，以0.1*learning_rate进行”极限探索”。至于random forest及xgboost的更多调参数的细节与gbdt类似，我就不赘述了，有问题可以问我。 终于结束了，这篇文章真的是又繁琐又冗长，希望能够给一些同学对gbdt更深刻的理解。 没啥广告要打，就这样吧。 另求一个比较好的公式编辑器，鬼知道我现在在excel里面写完公式截图过来有多扯淡，而且图片质量超差，谢谢了。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>模型设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle&TianChi分类问题相关纯算法理论剖析]]></title>
    <url>%2F2019%2F05%2F10%2FKaggle-TianChi%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9B%B8%E5%85%B3%E7%BA%AF%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[17/12/30-update ：很多朋友私密我想要代码，甚至利用金钱诱惑我，好吧，我沦陷了。因为原始代码涉及到公司的特征工程及一些利益trick，所以我构造了一个数据集后复现了部分算法流程，需要看详细代码实现朋友可以移步Ensemble_Github 更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过stw386@sina.com联系我，知无不答。 导读在上一次的文章中，我们讲了，如何快速的利用bagging、boosting、stacking、ensemble的形式实现一个分类算法，当时我们直接看了代码以及核心的理论注意点。如果需要有更加优异的结果表现，对整套算法的设计及相关的理论了解是必不可少的。本文将从数学、工程、领域经验的角度去剖析如何用好bagging、boosting、stacking、ensemble去训练一个相对完善的模型。 再次提醒，本文中的数据公式较多，抽象概念较多，需要一定的高等代数、泛函分析、机器学习基础作为前置条件。如果你只需要知道如何运行或者完成分类识别，请参考Kaggle-TianChi分类问题相关算法快速实现。如果需要更详尽的理论解析或者有哪些地方不明白的同学，建议私下联系我stw386@sina.com。如果你想skip read本文，请直接阅读最后一个小节：调参流程梳理。 那么接下来让我们开始正文，虽然本文写的很冗长，我依旧建议阅读完此文，即便是处于懵懂的状态，对后续模型调整的理解也是有一定益处的，而且我会尽可能的用通俗易懂的语言来讲，很多地方会存在解释不严谨的地方但是更易于理解。 Bias-Variance-Tradeof在上次的文章中，我们就提到了一个好的模型应该有着非常好的拟合能力，就是说我的偏差要尽可能的小；同时，也要保证方差尽可能的小，这样我们才能在泛化能力上有很不错的表现。 设样本容量为n的训练集为随机变量的集合(X1, X2, …, Xn)，那么模型是以这些随机变量为输入的随机变量函数（这边的F虽然上函数，但是也是随机变化的）：F(X1, X2, …, Xn)。抽样的随机性带来了模型的随机性。那如何定义一个模型的Bias和Variance呢？这边我们采取的是基模型的加权均值E(F)=E(∑(γ*fi))移动来替代bias、基模型的加权方差Var(F)=Var(∑(γ*fi))替代Variance，更详细的数学定义如下：这边在Variance推导过程中用到了这个性质：Cov(X*Y) = E(X*Y) - E(X)*E(Y)，同时将方差拆分成协方差的形式。我们可以看到，组合后的模型的Bias和基模型的Bias的权重γ相关，组合后的模型的Var和基模型的权重γ、基模型个数m、基模型相关性ρ相关。请务必深刻的记得上述bias和var的数学式子，在后续无论是调参数还是模型设计，我们都是围绕着，降低bias和var的角度去做的。 Bagging Bias and Variance很多同学在没看这篇文章之前就知道，bagging算法和stacking算法是需要基模型保持强基模型（偏差低方差高）的，但是不知道大家有没有想过为什么？阿里15年校招的时候，我有幸回答过一个题目就是这个问题，下面让我们看看为什么是这样的？ 对于bagging算法而言，每次的抽样都是以尽可能使得基模型相互独立为前提的，为了维持这样的假设，我们做了三件事： 样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样 子抽样：整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量 弱抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）下的feature的抽样 同时，由此我们由此也可以得到bias和var公式中的γ=1/m，基模型的权重一定程度是可以看作是相等的，所以原来的E和Var公式就变成：组合模型F的bias和基模型fi的bias一致，这就是我们为什么要求基模型fi的bias要低的原因，因为组合模型F的拟合能力E(F)不随着基模型个数的增加而上升。 组合模型F的var与基模型fi的var、基模型fi的个数m、基模型的相关性ρ相关，很明显可以看出，随着基模型的个数上升Var(F)第二项是在下降的，所以基模型的个数上升会降低组合模型的方差，这就是为什么基模型的方差可以高一些。 除此之外，我们还可以看出，如果基模型相关性ρ越低，整体的方差是越小的，所以我们才去做样本抽样，子抽样，弱抽样等等行为。还有，基模型的个数上升一定程度上会降低组合模型F的方差，但是不是无限递减的，公式中它只能降低第二项的方差值，第一项的方差值不随基模型的个数而增减。 从这个角度看，是不是对Bagging算法的理解又深刻了一些？接下来让我们看看Boosting。 Boosting Bias and Variance依旧的是很多同学在没看这篇文章之前就知道，boosting算法是需要基模型保持弱基模型（偏差高方差低）的，让我们一探究竟。熟悉boosting算法的同学都知道，boosting算法的基模型的相关性几乎≈1的，后续模型强依赖于前模型，所以我们可以认为ρ=1，得到如上的简化式子。组合模型F的bias是基模型的bias的累加，基模型的准确度都不是很高（因为其在训练集上的准确度不高），随着基模型数的增多，整体模型的期望值增加，更接近真实值。而站在方差的角度，组合模型F的方差是随着基模型的fi个数上升而平方上升的，这就要求我们的基模型的方差不能太高，否则组合模型的F就会增长爆炸。这就是为什么我们在boosting模型设计的时候，需要基模型保持弱模型（偏差高方差低）的原因。 多说一句，大家也看到了，boosting的Var(F)是依赖于基模型的权重γ的，所以在后续的gbdt、xgboost，各位数据科学家选择了类似bagging的采样模式，降低模型的γ，控制方差，所以说，了解原理再去重新或者优化还是很重要的。 Bagging、Boosting、Stacking Bias&amp;Variance总结纵观刚才说了的这么多，我们在Bagging、Boosting、Stacking的模型设计中所围绕的就是降低bias的同时，降低Variance。而我们所做的sklearn里面的参数调整就可以说用来： a.构建基模型，变化基模型的Bias和Variance b.组合模型构建，控制基模型后，如何把这个基模型很好的组合成一个优秀的ensemble模型。 GBDT 理论剖析模型过程推导其实random forest和gbdt、xgboost都是非常好的Bagging、Boosting、Stacking算法的优化升级版本，我个人用的gbdt稍多，所以就以gbdt为例子给大家梳理一遍，从理论，到调参数，到trick分享。 我最讨厌很多博主贴个上面的伪代码就跑了，我念书的时候，老师讲题目也是，我靠，我要是看得懂还需要你贴？所以，我们选择一步一步的来看这个伪代码。 让我们先概览一下整个流程，gbdt有递归设计如下： 如果y(i)代表着第i个基模型，第i+1个基模型其实是基于第i个基模型的结果而追加了一个New function去修正前i个基模型的误差。如果以F代替y，h代替f的话，我们可以得到下面这个递归函数： 第i个基模型是由前i-1个基模型中的h(x)累计得到的，我们最后想要得到的分类器即为： 每一轮迭代中，只要集中精力使得Fi(x)每次loss下降的最快即可：每次构建一个残差负梯度作为更新方向的New function(即hi(x))，就可以解决这个复杂的递归问题，从而得到F(x)的解析式。 接下来，我们再看看更加详细的做法： 初始化部分，在这次梳理之前，我也一直认为是随机构造的，这边看完伪代码我才知道，在初始值设置的时候，考虑了直接使得损失函数极小化的常数值，它是只有一个根节点的树，即是一个c常数值。 构建回归树，这边就稍许复杂，让我们拆开一步一步来看： 首先，先求整体损失函数的反向梯度。先举个mse的例子，如果现在我们考虑的是mse(着重注意，只有mse的情况下以下的梯度才是这样的)的形式，我们要做的就是在每一步的时候让我们的预测值Fi(x)与真实值y的损失函数：1/2*(y-Fi(x))^2最小(前面的1/2是为了方便求导计算加上去的)，如果对梯度下降有了解的朋友就知道，此刻要求它的最小就是去求偏导： 按照这个方向去更新Fi(x)，可以保证，组合模型F(x)的每次都是按照最优的方向去优化。 MSE的损失函数确实是残差形式，不代表所有的损失函数下更新的方向都满足这样的残差形式。kaggle master 在blog里面提到Although we can minimize this function directly, gradient descent will let us minimize more complicated loss functions that we can’t minimize directly，我们就需要设计出一种更快速能提升泛化能力且不失一般性的解决方案，所以有大神提出了以梯度下降的值直接代替因变量y，也就是我每次预测不去比预测值与真实值y的差异，我们比较的是预测的梯度方向与真实的梯度方向。 再基于已经预估出来的负梯度去计算最优的更新步长，使得预测值与真实值更加靠近，因为每层的负梯度的方向不固定，所以每层i的步长都是变化的。 最后通过缩减率v（这边就是类似logistics里面的∂）控制速率。 综上，假设test集合第i轮预测中，根据训练集训练出来的负梯度拟合模型不妨记为fi(x)、最优步长γi、缩减率v，可得到最终的递归公式为： 损失函数介绍刚才上面我举了一个mse作为损失函数的例子，其实还有很多其他的，参考如下： 这边说个有意思的东西，就是如果有兴趣的朋友可以把exponential：指数损失函数计算一下，反向梯度为： 则有第i轮损失函数： 这货就是adaboost的第i轮损失函数的非归一化的结果，是不是很有趣，虽然知道了没啥用，但是起码得到了我们在用gbdt的时候，loss=’exponential’即为adaboost的结论啊，哈哈～所以说，我觉得去推推公式，还是很有意思的。 到此为止，gbdt是怎么构造得来的就讲完了，其实这个和bias&amp;variance关系不大，但是为了铺垫后续的GBDT实战剖析，我觉得还是非常有必要梳理一遍的，但就比起Bias-Variance-Tradeof这节的内容，我觉得各位还是着重理解Bias-Variance-Tradeof，这节可以看作是’甜品’缓解下气氛。 GBDT 实战剖析我们以python下的sklearn.ensemble中的GradientBoosting及RandomForest为例子，实战分析一下，如何能够理性的调好参数而并非玄学的gridsearch。 在Bias-Variance-Tradeof我们提到了参数设置分为两块：a.构建基模型,b.构建组合模型，我们分GradientBoosting参数如下： 构建组合模型：1) n_estimators:基模型的个数，对于gbdt来说，因为我们需要通过基模型的个数来提升准确率所以n_estimators一般都会大于random forest的n_estimators的个数，实际上RandomForestClassifier默认为10，GradientBoostingClassifier默认为100也证明了这点。 2）learning_rate：步长，对于gbdt来说，步长依赖于n_estimators，100=2*50=5*20，就是这个道理。而random forest里面不存在步长这个概念。在gbdt里面，关于步长的优化一般是伴随着基模型的变化而变化的。 3）subsample：子采样率，还记得我们上面对子采样的描述么？一般来说，降低“子采样率”（subsample），也会造成子模型间的关联度降低，整体模型的方差减小，但是当子采样率低到一定程度时，子模型的偏差增大，将引起整体模型的准确度降低。 4) init：初始化，更多见GBDT 理论剖析中，我们对初始化的描述。 5）loss：对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。分类模型不说了，刚才在GBDT 理论剖析中讲了，一般用”deviance”比较多；回归模型中，”ls”我们也在GBDT 理论剖析中讲了，异常点多的情况下”huber”,训练集分段预测的话用”quantile”，但是我个人建议异常点或者分段预测还是在数据已处理中完成。 6) alpha：这个参数只有Huber损失”huber”和分位数损失”quantile”下的GradientBoostingRegressor，alpha越小对噪声处理的力度越强，alpha越小分位数的值越小。 构建基模型：1）max_features：每次划分最大特征数，有log2，sqrt，None等等，默认的是sqrt，该值越小，我们每次能获得信息越少，造成偏差时变大的，同时方差是变小的，所以当我们模型拟合能力不足的时候，可以考虑提升该值。 2）max_depth:基模型最大深度，深度越大，模型的拟合能力越强，bias越小。根据Bias-Variance-Tradeof我们对bagging和boosting里面的Var和Bias的描述可知，如果在boost（gbdt）采用了过深的基模型，组合模型的var会很大，在泛化能力会降低，造成训练集效果优秀，测试集差；如果在bagging（random forest）采取了过浅的基模型，组合模型的拟合能力会不足，我们可以考虑增加深度，甚至不控制生长。 3）min_samples_split：内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。随着分裂所需的最小样本数的增加，子模型的结构变得越来越简单，极端情况下，方差减小导致整体模型的拟合能力不足。 4）min_weight_fraction_leaf：叶节点最小权重总值，这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和其他子叶节点一起被剪枝，会使得模型变得简单，降低了方差，提高了偏差，如果正负样本不一致，需要考虑调整这个值。 5）max_leaf_nodes：最大叶子节点数，通过限制最大叶子节点数，可以防止过拟合，会使得模型变得简单，降低了方差，提高了偏差。这边需要注意如果设置了max_leaf_nodes，会忽略max_depth参数值。 梳理完以上每个参数的对模型拟合的能力及对Vaild集合泛化能力的影响，我们可以根据项目训练中，实际模型的训练集拟合效果，检验集的泛化效果进行优化参数。 调参流程梳理ok，问题来了，很多同学看了之后说，你说了这么多参数作用，你还是没告诉我改如何调参数？我就喜欢这种很关注结果的同学，以下干货来自个人及个人朋友及我从知乎等网站”剽窃”来的观点，不负任何理论责任。 我第一任老大，现在在阿里做算法专家，他根据24个数据集合上以不同的调参流程去训练相同的测试集得出的效果对比，总结出以下一个流程： 先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值 再确定合适的subsample 再调优最大树深度（max_depth） 再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）,这边是不一定使用的 再调优最大叶节点数（max_leaf_nodes） 再组合优化分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf） 最后，优化分裂时考虑的最大特征数（max_features） 组合调整n_estimators和learning_rate 但是，我今年在逛知乎的时候偶然看到一个帖子，里面讲的就是调参数的困扰，提到了一个点，就是先确定了max_depth=3后，无论怎么优化min_samples_split和min_samples_leaf对结果都没有任何影响了。当时我想了很久，最后是一位知友解答了这疑惑，其实这样的： 假设原始数据中正负样本比是1:1000，在做max_depth=3的时候，因为样本不均衡，已经可以通过非常简单的少量feature对正负样本进行区分，所以，在之后怎么调节分裂所需要最小样本树和子节点最小样本数都不能够影响到回归树的构造，然而该区分的回归树是没有泛化能力的。 要解决这个问题要么平衡数据，要么就是先确定回归决策树每个叶子结点最小的样本数(min_samples_leaf),再确定分裂所需最小样本数（min_samples_split），才能确定最大深度,这样就能保证不会出现某棵树通过一个feature将数量较少的的正类以较过拟合的简单浅层树拟合出来，而是优先保证了每一次我构造树都尽可能的平衡满足了数据量合理，数据具有样本具有代表性，不会过拟合这样的假设。所以，可以优化为： 先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值 再确定合适的subsample 再组合调优最大树深度（max_depth）和叶节点最小样本数（min_samples_leaf） 再调优最大叶节点数（max_leaf_nodes） 再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）,这边是不一定使用的 再组合优化分裂所需最小样本数（min_samples_split） 最后，优化分裂时考虑的最大特征数（max_features） 组合调整n_estimators和learning_rate 去年Aarshay Jain大神总结的调参数整理也给出了一种调优思路： 优先，调整最大叶节点数和最大树深度 其次，分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf）及叶节点最小权重总值（min_weight_fraction_leaf） 然后，分裂时考虑的最大特征数（max_features） 容我多嘴一句，我们思考了这么多，其实如果能在最开始做一个正负样本平衡就会避免很多问题，所以，再次强调数据预处理的重要性。 除此在外，很多人会选择在以上模型调优结束后再以10*learning_rate进行”鞍点逃逸”，以0.1*learning_rate进行”极限探索”。至于random forest及xgboost的更多调参数的细节与gbdt类似，我就不赘述了，有问题可以问我。 终于结束了，这篇文章真的是又繁琐又冗长，希望能够给一些同学对gbdt更深刻的理解。 没啥广告要打，就这样吧。 另求一个比较好的公式编辑器，鬼知道我现在在excel里面写完公式截图过来有多扯淡，而且图片质量超差，谢谢了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[PKUseg在货运领域的评测]]></title>
    <url>%2F2019%2F01%2F14%2FPKUseg%E5%9C%A8%E8%B4%A7%E8%BF%90%E9%A2%86%E5%9F%9F%E7%9A%84%E8%AF%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[先说结论，再和大家闲聊，对比jieba与PKUseg在公路货运切词能力上： 默认模型下，jieba效果优于PKUseg PKUseg提供场景精细化的预训练（还没有提供入口），长远来讲适合专业领域使用 PKUseg在特定的场景下有令人惊喜的效果（地址切分） 给大家的建议就是，如果大家赶时间求稳定适应范围需要非常广的时候，目前来说jieba是非常好的选择，如果说在面临一些精细化领域的特殊需求的时候，可以用PKUseg进行一波尝试，有意外惊喜。 那是一个风和日丽的早上，突然群里老大发出一条消息：我感觉我的心脏有一丝隐隐作痛的感觉，人在办公室坐，活从天上来，虽然身后站着一堆催上线的产品，我还是屈服于老大的正义（淫威），简单测评了新出来的PKUseg与Jieba在公路货运/运输行业上的效果对比。 在我们的热词数据库中已经有人工切词完成的2万多条货运的词条：1234567891011121314151617181920description standard高博集团装货卸宝华 高博 集团 装货 卸 宝华北安到吉林农安饲料90吨每吨105 北安 到 吉林 农安 饲料 90吨 每吨 105需要4个车 需要 4个 车叶张公路装香闵路曲吴路两卸 叶张公路 装 香闵路 曲吴路 两卸从福通物流到吴滩镇 从 福通 物流 到 吴滩镇霞浦宏霞路到中通物流 霞浦宏霞路 到 中通物流石大路3场到德兴西门山 石大路 3场 到 德兴 西门山公园西路装 公园 西路 装不押车每吨150 不 押车 每吨 150速订价钱好商量 速订 价钱 好商量慈溪胜山装 慈溪 胜山装好装好卸高价急走 好装好卸 高价急走九顶山路与东方大道位置装货可以配货 九顶 山路 与 东方 大道 位置 装货 可以 配货要二部 要 二部青浦工业园区久远路提货到奉贤新杨公路进仓 青浦 工业园区 久远路 提货 到 奉贤 新杨公路 进仓园光路装博学南路卸 园光路 装 博学南路 卸公兴装卸荣昌广顺 公兴 装卸 荣昌 广顺打备注电话18458331112 打 备注 电话 18458331112... 首先看，不加任何词库，预训练下的，最后的效果对比：可以看到，在默认的分词模型下，jieBa分词还是拥有绝对优势的，但是在pkuSeg的git里面 所以我想看看能不能进行一下预训练下后再对比一下，可惜的是我在git（git地址传送门）上找了半天也没有找到预训练的入口，只有已经被官方预训练好的词库等有时间了，可以邮件沟通一下再补充这个部分的效果对比，我觉得，应该还是有提升的。 但是，在我们实际去测的过程中，我们发现了一些差异话的东西比较有意思。我们其实现在在做一个语音发货的产品，涉及到把一串地址切分开的需求： 其中涉及到地址切分的时候，jieba的能力会比如PKUseg要弱不少，比如“山西大同”，“上海浦东”，我们需要把一级二级地址切开的时候，PKUseg可以做到，而jieba并不能按照需求切块。所以，我们已经打算在地址模块切换PKUseg的模型来适应了。 最后吐槽一下，虽然我知道PKUseg需要加载模型，但是一加载就是一二十秒也是有点夸张了。酒浆，各位下回见。 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>开源项目</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas中的问题记录]]></title>
    <url>%2F2018%2F10%2F23%2Fpandas%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[最近发现pandas的一个问题，记录一下：有一组数据（test.txt）如下：12345620181016 1483068029890327320181016 1483960347395306920181016 1483960347395307920181016 1483960347395308920181016 1483960347395309920181016 14839603473953019 剖析出来看，数据是按照\t进行分隔的：&#39;20181016\t14830680298903273\n&#39;123with open('test.txt','r') as f:line = f.readline()print(line) 我平时一直在用pandas去读数据，所以我很熟练的写下来如下的代码：pd.read_table(&#39;test.txt&#39;,header=None)然后发现，第一列变成了科学记数法的方式进行存储了： 很明显，科学记数法是可以转换的：12345678def as_number(value):try:return &apos;&#123;:.0f&#125;&apos;.format(value)except:return value# 应用到目标列去即可data.uid.apply(as_number) 诡异的事情发生了，对于14830680298903273在as_number函数转换下变成了14830680298903272，理论上讲14830680298903273没有小数部分不存在四舍五入的原因，网上搜了也没有很明确的解释，初步讨论后猜测应该是pandas在用float64去存这种长度过长的数字的时候有精度丢失的问题。 要解决也是很简单的： 用open的形式打开，在切割逐步去用list进行append，在合并 用read_table的函数的时候，默认是用float64去存在的，改成object去存(dtype=object) 在生产数据的时候，对于这种过长的数据采取str的形式去存 也是给自己提个醒，要规范一下自己的数据存储操作，并养成数据核对的习惯。 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>代码集合</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YoutubeNet的数据答疑]]></title>
    <url>%2F2018%2F10%2F16%2FYoutubeNet%E7%9A%84%E6%95%B0%E6%8D%AE%E7%AD%94%E7%96%91%2F</url>
    <content type="text"><![CDATA[实在是太忙了，抽空给大家解析一下之前写的YoutubeNet的数据是怎么构造的，协助大家可以自行构造一下。 这边和大家说一下，我没有上传数据的原因有两个： 涉及公司的数据财产，不方便上传 懒得做脱敏处理 数据一共有1300多万条，传输实在不方便 主要数据处理的部分在map_id_idx.py脚本下，其中包含all_item_20180624.txt和click_thirty_day_data_20180609.txt两个数据集合。 其中，all_item_20180624.txt是当日所有的商品集合：包含’Prd_Id’, ‘ItemId’, ‘BrandId’, ‘MsortId’和‘GenderId’五列，分别代表着商品id，skuid，低级品牌id，中级品牌id，产品性别，最后形如： 1234567891011125675 50000055 175 1500 32577 50000056 187 66 32002 50000057 63 11 22007 50000058 137 58 32075 50000060 80 50 32348 50000061 138 16 2423 50000062 162 237 3469 50000063 10 1500 31102 50000064 176 11 11896 50000066 37 27 12489 50000067 27 44 1... click_thirty_day_data_20180609.txt为近三十天的用户点击流，包含’UId’, ‘ItemId’, ‘clickTime’三列，分别代表着uid、点击的skuid，点击时间，最后形如： 12345678910111234 51668064 152860240634 51890512 152878838934 51884724 152878839334 51884720 152878839934 51884718 152878841434 51580974 152878844234 51854970 152878848734 51514910 152878849934 51855000 152878853534 51854990 152878856934 51854998 1528788572... 通过map_id_idx.py对所有的商品进行标序号，然后带入用户的点击流中，方便后期做embedding操作，就酱。 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GloVe向量化做文本分类]]></title>
    <url>%2F2018%2F09%2F25%2FGloVe%E5%90%91%E9%87%8F%E5%8C%96%E5%81%9A%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[向量化在之前，我对向量化的方法一直局限在两个点， 第一种是常规方法的one-hot-encoding的方法，常见的比如tf-idf生成的0-1的稀疏矩阵来代表原文本： 这种方法简单暴力，直接根据文本中的单词进行one-hot-encoding，但是数据量一但大了，这个单句话的one-hot-encoding结果会异常的长，而且没办法得到词与词之间的关系。 第二种是基于神经网络的方法，常见的比如word2vec，YouTubeNet： 这种方法（这边以CBOW为例子）都是初始一个固定长度的随机向量作为每个单词的向量，制定一个目标词的向量，以上下文词向量的sum结果作为input进行前向传递，使得传递的结果和目标词向量尽可能一致，以修正初始的随机向量。换句话说，就是刚开始，我随意定义生成一个vector代表一个词，然后通过上下文的联系去修正这个随机的vector。好处就是我们可以得到词与词之间的联系，而且单个词的表示不复杂，坏处就是需要大量的训练样本，毕竟涉及到了神经网络。 最近，我们突然发现了第三种方法，GloVe向量化。它也是开始的时候随机一个vector作为单词的表示，但是它不利用神经网络去修正，而是利用了一个自己构造的损失函数： 通过我们已有的文章内容，去是的这个损失函数最小，这就变成了一个机器学习的方法了，相比较暴力的前馈传递，这也高快速和高效的多。同时，它还兼具了word2vec最后结果里面vector方法的优点，得到词与词之间的联系，而且单个词的表示不复杂。 这边就不展开GloVe算法的细节了，后面有空和大家补充，这个算法的构造非常巧妙，值得大家借鉴一下。 文本分类刚才开门见山的聊了蛮久向量化，看起来和文本分类没什么关系，确实在通常意义上来讲，我们的最简单最常用的方法并不是向量化的方法，比如通过朴素贝叶斯，N-Grams这些方法来做分类识别。 tfidf+N-grams1.其实很简单，首先对语料库进行切词，维护自己的词典，做高频词的人工复审，将无意词进行stop_words归总 可以看到，高频词其实是非常非常少的，而且如果你真的去做了，你就会发现，”了”、“的”、“啊”这种语气词，和一些你公司相关的领域词汇会非常靠前，这些词作为stop_words会有效的降低训练成本、提高模型效果。 2.进行tf-idf，将词进行重赋权，字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降，有效的将向量化中的one hot encoding结果进行了修正。但是依然存在问题：在TFIDF算法中并没有体现出单词的位置信息。 123456# sublinear_tf：replace tf with 1 + log(tf)# max_df：用来剔出高于词频0.5的词# token_pattern：(?u)\b\w+\b是为了匹配出长度为1及以上的词，默认的至少需要词长度为2# ngram_range：这边我做了3-grams处理，如果只想朴素计算的话(1,1)即可# max_features：随着我做了各种宽松的条件，最后生成的词维度会异常大，这边限制了前3万vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5, token_pattern=r"(?u)\b\w+\b",ngram_range=(1, 3), max_features=30000) 不得不说，python处理机器学习，深度学习的便捷程度是异常的高。 3.在经过TfidfVectorizer处理之后的结果是以稀疏矩阵的形式来存的，如果想看内容的话，可以用todense()转化为matrix来看。接下来，用贝叶斯来训练刚才得到的矩阵结果就可以了。 12mnb_tri = MultinomialNB(alpha=0.001)mnb_tri.fit(tri_train_set.tdm, tri_train_set.label) tf-idf + n-grams + naive-bayes + lr这种方法是上面方法的升级版本，我们先看下架构： 其实主要差异在于右侧的算法模型详细部分，我们做了一个由3-grams到3-grams+naive-bayes+lr的扩充，提升精度。 在模型的过程中，上面的第一步，都是一样的，在第二、三步有所差异：2.在第二步中，我们除了要构造出一个3-grams的sparse matrix也需要构造出一个朴素的sparse matrix 12# 朴素结果vectorizerby = TfidfVectorizer(stop_words=stpwrdlst, token_pattern=r"(?u)\b\w+\b", max_df=0.5, sublinear_tf=True,ngram_range=(1, 1), max_features=100000) 3.不仅仅用bayes进行一次分类，而是根据3-grams和朴素情况下的sparse matrix进行预测，再用logistics regression来合并两个的结果做个stack进行0-1压缩。 12345# 构造出一个3-grams的sparse matrix也需要构造出一个朴素的sparse matrixmnb_tri = MultinomialNB(alpha=0.001)mnb_tri.fit(tri_train_set.tdm, tri_train_set.label)mnb_by = MultinomialNB(alpha=0.001)mnb_by.fit(by_train_set.tdm, by_train_set.label) 123# 加bias，cv选择最优正则结果，lbfgs配合l2正则lr = LogisticRegressionCV(multi_class="ovr", fit_intercept=True, Cs=np.logspace(-2, 2, 20), cv=2, penalty="l2",solver="lbfgs", tol=0.01)re = lr.fit(adv_data[['f1', 'f2']], adv_data['rep_label']) 总结一下上面两种方法，我觉得是入门快，效果也不错的小练手，也是完全可以作为我们开始一个项目的时候，用来做baseline的方法，主要是快啊～/斜眼笑 GloVe+lr因为我目前的带标签数据比较少，所以之前一直没有敢用word2vec去向量化作死，但是GloVe不存在这个问题啊，我就美滋滋的进行了一波。首先，先讲下GloVe的使用： https://github.com/stanfordnlp/GloVe 在最大的代码抄袭网站下载(git clone)坦福大佬的代码，友情提醒，不要作死自己看了理论就觉得自己会写，自己搞个GloVe。(别问我是怎么知道的) cd到对应目录下，vim demo.sh这个文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#!/bin/bashset -e# Makes programs, downloads sample data, trains a GloVe model, and then evaluates it.# One optional argument can specify the language used for eval script: matlab, octave or [default] python# 请把make这边注释掉，这个是让你去下个demo，我们直接改成自己的数据# make# if [ ! -e text8 ]; then# if hash wget 2&gt;/dev/null; then# wget http://mattmahoney.net/dc/text8.zip# else# curl -O http://mattmahoney.net/dc/text8.zip# fi# unzip text8.zip# rm text8.zip# fi# CORPUS需要对应自己的欲训练的文档CORPUS=content.txtVOCAB_FILE=vocab.txtCOOCCURRENCE_FILE=cooccurrence.binCOOCCURRENCE_SHUF_FILE=cooccurrence.shuf.binBUILDDIR=buildSAVE_FILE=vectorsVERBOSE=2MEMORY=4.0# 单词至少出现几次VOCAB_MIN_COUNT=3# 向量长度VECTOR_SIZE=128# 迭代次数MAX_ITER=30# 窗口长度WINDOW_SIZE=15BINARY=2NUM_THREADS=8X_MAX=10echoecho "$ $BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE &lt; $CORPUS &gt; $VOCAB_FILE"$BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE &lt; $CORPUS &gt; $VOCAB_FILEecho "$ $BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE &lt; $CORPUS &gt; $COOCCURRENCE_FILE"$BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE &lt; $CORPUS &gt; $COOCCURRENCE_FILEecho "$ $BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE &lt; $COOCCURRENCE_FILE &gt; $COOCCURRENCE_SHUF_FILE"$BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE &lt; $COOCCURRENCE_FILE &gt; $COOCCURRENCE_SHUF_FILEecho "$ $BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE"$BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSEif [ "$CORPUS" = 'text8' ]; thenif [ "$1" = 'matlab' ]; thenmatlab -nodisplay -nodesktop -nojvm -nosplash &lt; ./eval/matlab/read_and_evaluate.m 1&gt;&amp;2 elif [ "$1" = 'octave' ]; thenoctave &lt; ./eval/octave/read_and_evaluate_octave.m 1&gt;&amp;2elseecho "$ python eval/python/evaluate.py"python eval/python/evaluate.pyfifi 这边多说一下，CORPUS=content.txt这边content.txt里面的格式需要按照空格为分隔符进行存储，我之前一直以为是\t。 直接sh demo.sh，你会得到vectors.txt，这个里面就对应每个词的向量表示 1234天气 -0.754142 0.386905 -1.200074 -0.587121 0.758316 0.373824 0.342211 -1.275982 -0.300846 0.374902 -0.548544 0.595310 0.906426 0.029255 0.549932 -0.650563 -0.425185 1.689703 -1.063556 -0.790254 -1.191287 0.841529 1.080641 -0.082830 1.062107 -0.667727 0.573955 -0.604460 -0.601102 0.615299 -0.470923 0.039398 1.110345 1.071094 0.195431 -0.155259 -0.781432 0.457884 1.093532 -0.188207 -0.161646 0.246220 -0.346529 0.525458 0.617904 -0.328059 1.374414 1.020984 -0.959817 0.670894 1.091743 0.941185 0.902730 0.609815 0.752452 1.037880 -1.522382 0.085098 0.152759 -0.562690 -0.405502 0.299390 -1.143145 -0.183861 0.383053 -0.013507 0.421024 0.025664 -0.290757 -1.258696 0.913482 -0.967165 -0.131502 -0.324543 -0.385994 0.711393 1.870067 1.349140 -0.541325 -1.060084 0.078870 0.773146 0.358453 0.610744 0.407547 -0.552853 1.663435 0.120006 0.534927 0.219279 0.682160 -0.631311 1.071941 -0.340337 -0.503272 0.150010 1.347857 -1.024009 -0.181186 0.610240 -0.218312 -1.120266 -0.486539 0.264507 0.266192 0.347005 0.172728 0.613503 -0.131925 -0.727304 -0.504488 1.773406 -0.700505 -0.159963 -0.888025 -1.358476 0.540589 -0.243272 -0.236959 0.391855 -0.133703 -0.071120 1.050547 -1.087613 -0.467604 1.779341 -0.449409 0.949411好了 1.413075 -0.226177 -2.024229 -0.192003 0.628270 -1.227394 -1.054946 -0.900683 -1.958882 -0.133343 -1.014088 -0.434961 0.026207 -0.066139 0.608682 -0.362021 0.314323 0.261955 -0.571414 1.738899 -1.013223 0.503853 -0.536511 -0.212048 0.611990 -0.627851 0.297657 -0.187690 -0.565871 -0.234922 -0.845875 -0.767733 0.032470 1.508012 -0.204894 -0.495031 -0.159262 0.181380 0.050582 -0.333469 0.454832 -2.091174 0.448453 0.940212 0.882077 -0.617093 0.616782 -0.993445 -0.385087 0.251711 0.259918 -0.222614 -0.595131 0.661472 0.194740 0.619222 -1.253610 -0.838179 0.781428 -0.396697 -0.530109 0.022801 -0.558296 -0.656034 0.842634 -0.105293 0.586823 -0.603681 -0.605727 -0.556468 0.924275 -0.299228 -1.121538 0.237787 0.498935 -0.045423 0.171536 -1.026385 -0.262225 0.390662 1.263240 0.352172 0.261121 0.915840 1.522183 -0.498536 2.046169 0.012683 -0.073264 -0.361662 0.759529 -0.713268 0.281747 -0.811104 -0.002061 -0.802508 0.520559 0.092275 -0.623098 0.199694 -0.134896 -1.390617 0.911266 -0.114067 1.274048 1.108440 -0.266002 1.066987 0.514556 0.144796 -0.606461 0.197114 0.340205 -0.400785 -0.957690 -0.327456 1.529557 -1.182615 0.431229 -0.084865 0.513266 -0.022768 -0.092925 -0.553804 -2.269741 -0.078390 1.376199 -1.163337随意 0.410436 0.776917 -0.381131 0.969900 -0.804778 -0.785379 -0.887346 -1.463543 -1.574851 0.313285 0.685253 -0.918359 0.199073 -0.305374 -0.642721 0.098114 -0.723331 0.353159 0.042807 0.369208 -1.534930 -0.084871 0.020417 -0.384782 0.276833 -0.160028 1.107051 0.884343 -0.204381 -0.459738 -0.387128 0.125867 0.093569 1.192471 -0.473752 -0.314541 -1.029249 0.481447 1.358753 -1.688778 -0.113080 -0.401443 -0.958206 0.605638 1.083126 0.131617 0.092507 0.476506 0.801755 1.096883 -0.102036 0.461804 0.820297 -0.104053 -0.126638 0.957708 -0.722038 0.223686 0.583582 0.201246 -1.254708 0.770717 -1.271523 -0.584094 -1.142426 1.066567 0.071951 -0.182649 0.014365 -0.577141 0.037340 -0.166832 -0.247827 0.165994 1.143665 -0.258421 -0.335195 0.170218 -0.212838 0.013709 0.088847 0.663238 -0.597439 0.632847 0.370871 0.652707 0.306935 0.195127 -0.252443 0.588479 0.191633 -1.587564 0.564600 -0.306158 -0.648177 -0.488595 1.532795 -0.462473 -0.643878 1.292369 -0.051494 -1.032738 0.453587 0.411327 -0.469373 0.428398 -0.020839 0.307422 0.518331 -0.860913 -2.170098 -0.277532 -0.966210 0.615336 -0.924783 0.042679 1.289640 1.272992 1.367773 0.426600 -0.187254 -0.781009 1.331301 -0.088357 -1.113550 -0.262879 0.300137 0.437905.. 有了每个词的向量，我们这边采取了借鉴YoutubeNet网络的想法： 举个例子：存在一句话”我爱中国”，“我”的向量是[0.3,0.2,0.3]，”爱”的向量是[0.1,0.2,0.3]，“中国”的向量是[0.6,0.6,0.4]，那么average后就是[0.33,0.33,0.33]，然后这就类似一个特征为三的input。 这种方法的好处就是快捷，预处理的工作代价要小，随着数据量的增多，模型的效果要更加的好。 效果对比最后这边粗略的给出一下业务数据对比： experiment date intercepted_recall 3-grams 20180915 79.3% 3-grams 20180917 78.7% 3-grams+bayes+lr 20180915 83.4% 3-grams+bayes+lr 20180917 88.6% gloVe+lr 20180915 93.1% gloVe+lr 20180917 93.9% 欢迎大家关注我的个人bolg，知乎，相关代码已经上传到我的Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google团队在DNN的实际应用方式的整理]]></title>
    <url>%2F2018%2F08%2F29%2FGoogle%E5%9B%A2%E9%98%9F%E5%9C%A8DNN%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E6%96%B9%E5%BC%8F%E7%9A%84%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[很荣幸有机会和论文作者Emre Sargin关于之前发的Deep Neural Networks for YouTube Recommendations进行交流，梳理如下： 提问对话汇总： 如何进行负采样的？ 构造了千万量级热门视频集合，每个用户的负采样结果来源于这个集合，会有一些筛选的tricks，比如剔除浏览过的商品，负采样的数量Google在200万条。（也就是说，在计算loss的时候，google的label是一个200万长度的向量，瑟瑟发抖.jpg。） 推荐算法应用上，有什么评估方式和评估指标？ 主要基于线上进行小批量的abtest进行对比，在考虑ctr指标的同时也会综合全站的信息加以分析，同时对新颖程度和用户兴趣变换也是我们考察的对象。 冷启动的解决方式？从来没有被点击过的video如何处理？新上的video如何处理？ google的推荐基于多种推荐算法的组合，YouTubeNet主要解决的是热门商品的一个推荐问题，冷启动或者没有被点击的video会有其他算法进行计算。换句话说，解决不了。 example age如何定义？ user+vedio的组合形式，train过程中，是用户点击该vedio的时间距离当前时间的间隔；predict过程中，为0。该部分对模型的鲁棒性非常重要。 是否遇到神经元死亡的问题？ 有，解决方案很常规，都是大家了解的，降低learning_rate，使用batchnormalization。 是否预到过拟合？ 没，youtube的用户上亿，可以构造出上千亿的数据，过拟合的情况不明显。但是会存在未登录用户，我们会通过一些其他CRM类的算法补充构造出他们的基本信息，比如gender、age… vedio vector在哪边进行构造和修正？ history click部分进行vedio embedding，并进行修正。另外，50是我们尝试的历史点击长度，20-30也有不错的效果。 会有工程计算压力么？ 不存在，建议在GPU上计算，后面由于VPN网络信号抖动没听清，大概就说Google在训练模型的时候会有大量GPU支持，每天大概更新2-3模型，没有遇到什么计算瓶颈。 (以上为我个人针对提问结果的理解及总结) 个人感想如下：有钱任性 最后，我觉得算法还是要适应实际情况，大公司的方法可以借鉴但是可能很多时候抄不来，也没条件抄。 原问题如下（实际有删改）：How to do video embedding?Is there any pre-training?How to use example age in the model?How to deal dead ReLU neurons？How to sample negative classes？How does the video embedding generated？How to recommend the video never been clicked and new uploaded videos？How to do ab testing? What’s the metrics?Have you facing overfitting？How to solve it?There is any difficulty in calculating the embedding for millions of videos and users.During input embedding generation, are they simply averaged? 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stanford Word Segmenter问题整理]]></title>
    <url>%2F2018%2F08%2F27%2FSegmenter%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[最近在做一些nlp相关的项目，在涉及到Stanford CoreNLP工具包处理中文分词的时候，发现耗时问题很严重： Item time(s) jieba 0.4 snownlp 7.4 pynlpir 0.8 StanfordCoreNLP 21.5 pyltp 5.3 因为Stanford CoreNLP调用的是这个pipeline，而我们实际用的是切词功能，所以尝试只用它的切词部分功能，但是在做的过程中发现一些问题，整理如下： 官网给出的方法nltk.tokenize.stanford_segmenter module是这么写的： 123from nltk.tokenize.stanford_segmenter import StanfordSegmenterseg = StanfordSegmenter()seg.default_config('zh') 但是这个缺少各种数据路径的，是完全不通的。 然后度娘的top1的答案给出的解决方案是：`1segmenter = StanfordSegmenter(path_to_jar="stanford-segmenter-3.4.1.jar", path_to_sihan_corpora_dict="./data", path_to_model="./data/pku.gz", path_to_dict="./data/dict-chris6.ser.gz") 如果你的nltk的版本比较新，恭喜你，你会遇到下面这个问题：TypeError: expected str, bytes or os.PathLike object, not NoneType 我在stackoverflow上找了半天，发现有如下的解决方案：1234from nltk.parse.corenlp import CoreNLPParser corenlp_parser = CoreNLPParser('http://localhost:9001', encoding='utf8')result = corenlp_parser.api_call(text, &#123;'annotators': 'tokenize,ssplit'&#125;)tokens = [token['originalText'] or token['word'] for sentence in result['sentences'] 可以完美解决，原因之前作者也说了，据称升级版本后不兼容，各位看看就好“TypeError: expected str, bytes or os.PathLike object, not NoneType” about Stanford NLP 。 这个坑花了我两个多小时（主要在下载各种gz包），希望大家能够避免。 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>代码集合</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logistic regression一点理解]]></title>
    <url>%2F2018%2F08%2F14%2Fregression%E4%B8%80%E7%82%B9%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Hexo没有办法用Latex，所以采取了截图的方式，更舒适的阅读体验可以参见logistic regression一点理解。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>理论解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Tensorflow实现FFM]]></title>
    <url>%2F2018%2F08%2F06%2F%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0FFM%2F</url>
    <content type="text"><![CDATA[没错，这次登场的是FFM。各大比赛中的“种子”算法，中国台湾大学Yu-Chin Juan荣誉出品，美团技术团队背书，Michael Jahrer的论文的field概念灵魂升华，土豪公司鉴别神器。通过引入field的概念，FFM把相同性质的特征归于同一个field，相当于把FM中已经细分的feature再次拆分，可不可怕，厉不厉害？好，让我们来看看怎么一个厉害法。 FFM理论特征交互网上已经说烂了的美团技术团队给出的那张图：针对Country这个变量，FM的做法是one-hot-encoding，生成country_USA，country_China两个稀疏的变量，再进行embedding向量化。FFM的做法是cross-one-hot-encoding，生成country_USA_Day_26/11/15，country_USA_Ad_type_Movie…M个变量，再进行embedding向量化。就和上图一样，fm做出来的latent factor是二维的，就是给每个特征一个embedding结果；而暴力的ffm做出的latent factor是三维的，出来给特征embedding还考虑不同维度特征给不同的embedding结果，也是FFM中“field-aware”的由来。同时从公式中看，对于xi这个特征为什么embedding的latent factor向量的V是Vifj，其实就是因为xi乘以的是xj，所以latent factor向量的信息提取才是field j，也就是fj。多说一句，网上很多给出的现成的代码，这边都是写错了的，都写着写着变成了vifi，可能是写的顺手。 都说到这里了，我再多说一句，为什么说ffm是土豪公司鉴别神器呢？我们看下仅仅是二次项，ffm需要计算的参数有 nfk 个，远多于FM模型的 nk个，而且由于每次计算都依赖于乘以的xj的field，所以，无法用fm的那个计算技巧(ab = 1/2(a+b)^2-a^2-b^2)，所以计算复杂度是 O(kn^2)。这种情况下，没有GPU就不要想了，有GPU的特征多于50个，而且又很离散的，没有个三位数的GPU也算了。之前我看美团说他们在用，我想再去看看他们的实际用的过程的时候，发现文章被删了，真的是可惜，我其实一直也想知道如何减轻这个变态的计算量的方法。 给个实例给大家看下以上的这些的应用：依旧来自美团技术研发团队中给出的案例，有用户数据如下：这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。红色部分对应的是field，来自于原始特征的个数；蓝色部分对应的是feature，来自于原始特征onehot之后的个数。对于特征Feature:User=YuChin而言，有Movie=3Idiots、Genre=Comedy、Genre=Drama、Price四项要交互：User=YuChin与Movie=3Idiots交互是·1·1，也就是第一项，为什么是V1,2呢？因为User=YuChin是Featureindex=1，而交互的Movie=3Idiots是Fieldindex=2，同理V2,1也是这样的，以此类推，那么，FFM的组合特征有10项，如下图所示：这就是一个案例的实际操作过程。 特征处理为什么要把这个单拎出来说呢？我看了网上不少的对于特征的处理过程，版本实在是太多了，而且差异化也蛮大，这边就和大家一起梳理一下：1.feature index * feature value这个就是上面我这个实际案例的方式，对于分类变量采取onehot，对于连续变量之间进行值的点积，不做处理。优点是快速简单，不需要预处理，但是缺点也很明显，离群点影响，值的波动大等。 2.连续值离散化这个方法借鉴了Cart里面对连续值的处理方式，就是把所有的连续值都当成一个分类变量处理。举例，现在有一个年龄age的连续变量[10,19,20,22,22,34]，这种方法就生成了age_10,age_19,age_20,age_22,age_34这些变量，如果连续值一多，这个方法带来的计算量就直线上升。 3.分箱下的连续值离散化这种方法优化了第二种方法，举例解释，现在有一个年龄age的连续变量[10,19,20,22,22,34]，我们先建立一个map，[0,10):0,[10,20):1,[20,30):2,[30,100):3。原始的数据就变成了[1,1,2,2,2,3]，再进行2的连续值离散化方法，生成了age_1,age_2,age_3这几个变量，优化了计算量，而且使得结果更具有解释性。 损失函数logisitc loss这个是官方指定的方法，是-1/1做二分类的时候常用的loss计算方法：这边需要注意的是，在做的时候，需要把label拆分成-1/1而不是0/1，当我们预测正确的时候，predlabel&gt;0且越大正确的程度越高，相应的log项是越小的，整体loss越小；相反，如果我们预测的越离谱，predlabel&lt;0且越小离谱的程度越高，相应的log项是越大的，整体loss越大。 交互熵我看到很多人的实现依旧用了tf.nn.softmax_cross_entropy_with_logits，其实就是多分类中的损失函数，和大家平时的图像分类、商品推荐召回一模一样：这边需要注意的是，在做的时候，需要把label拆分成[1,0]和[0,1]进行计算。不得不说，大家真的是为了省事很机智(丧心病狂)啊！ 代码实现我这边只给一些关键地方的代码，更多的去GitHub里面看吧。 embedding part1self.v = tf.get_variable('v', shape=[self.p, self.f, self.k], dtype='float32',initializer=tf.truncated_normal_initializer(mean=0, stddev=0.01)) 看到了，这边生成的v就是上面Vffm的形式。 inference part12345678910111213for i in range(self.p):# 寻找没有match过的特征，也就是论文中的j = i+1开始for j in range(i + 1, self.p):print('i:%s,j:%s' % (i, j))# vifjvifj = self.v[i, self.feature2field[j]]# vjfivjfi = self.v[j, self.feature2field[I]]# vi · vjvivj = tf.reduce_sum(tf.multiply(vifj, vjfi))# xi · xjxixj = tf.multiply(self.X[:, i], self.X[:, j])self.field_cross_interaction += tf.multiply(vivj, xixj) 我这边强行拆开了写，这样看起来更清晰一点，注意这边的vifj和vjfi的生成，这边也可以看到找对于的field的方法是用了filed这个字典，这就是为什么不能用fm的点击技巧。 loss part12# -1/1情况下的logistic lossself.loss = tf.reduce_mean(tf.log(1 + tf.exp(-self.y * self.y_out))) 这边记得论文中的负号，如果有batch的情况下记得求个平均再进行bp过程。 论文结论原始的ffm论文中给出了一些结论，我们在实际使用中值得参考： k值不用太大，没啥提升 正则项lambda和学习率alpha需要着重调参 epoch别太大，既会拖慢速度，而且造成过拟合；在原论文中甚至要考虑用early-stopping避免过拟合，所以epoch=1，常规的来讲就可以了，论文中提到的early-stopping操作：12345671. Split the data set into a training set and a validation set.2. At the end of each epoch, use the validation set to calcu-late the loss.3. If the loss goes up, record the number of epochs. Stop orgo to step 4.4. If needed, use the full data set to re-train a model withthe number of epochs obtained in step 3. 总结FFM是一个细化隐向量非常好的方法，虽然很简单，但还是有很多细节之处值得考虑，比如如何线上应用，如何可解释，如何求稀疏解等等。在部署实现FFM之前，我还是建议大家先上线FM，当效果真的走投无路的时候再考虑FFM，FFM在工业界的影响着实不如学术界那么强大，偷偷说一句，太慢了，真的是太慢了，慢死了，我宁可去用deepfm。 最后，给出代码实现的Github地址FFM，这边是我自己写的，理解理解算法可以，但是实际用的时候建议参考FFM的实现比较好的项目比如libffm，最近比较火的xlearn。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征交叉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Tensorflow实现DeepFM]]></title>
    <url>%2F2018%2F07%2F30%2F%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0DeepFM%2F</url>
    <content type="text"><![CDATA[前言DeepFM，Ctr预估中的大杀器，哈工大与华为诺亚方舟实验室荣耀出品，算法工程师面试高频考题，有效的结合了神经网络与因子分解机在特征学习中的优点：同时提取到低阶组合特征与高阶组合特征，这样的称号我可以写几十条出来，这也说明了DeepFM确实是一个非常值得手动撸一边的算法。 当然，早就有一票人写了一车封装好的deepFM的模型，大家随便搜搜肯定也能搜到，既然这样，我就不再搞这些东西了，今天主要和大家过一遍，deepFM的代码是咋写的，手把手入门一下，说一些我觉得比较重要的地方，方便大家按需修改。（只列举了一部分，更多的解释参见GitHub代码中的注释） 本文的数据和部分代码构造参考了nzc大神的deepfm的Pytorch版本的写法，改成tensorflow的形式，需要看原版的自取。 网络结构 DeepFM包含两部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取，两部分权重共享。DeepFM的预测结果可以写为：y = sigmoid(y(fm)+y(DNN)) FM部分 FM公式为： 我很久之前一篇文章细节讲过，这边就不多扯了，更多详见FM理论解析及应用。 DNN部分 这边其实和我上篇文章说的MLPS差距不大，也就是简单的全链接，差就差在input的构造，这边采取了embedding的思想，将每个feature转化成了embedded vector作为input，同时此处的input也是上面计算FM中的V，更多的大家看代码就完全了解了。 代码部分我一共写了两个script，build_data.py和deepfm.py，也很好理解。build_data.py用来预处理数据，deepfm.py用来跑模型。 build_data.py1234567891011121314151617181920212223242526for i in range(1, data.shape[1]):target = data.iloc[:, I]col = target.namel = len(set(target))if l &gt; 10:target = (target - target.mean()) / target.std()co_feature = pd.concat([co_feature, target], axis=1)feat_dict[col] = cntcnt += 1co_col.append(col)else:us = target.unique()print(us)feat_dict[col] = dict(zip(us, range(cnt, len(us) + cnt)))ca_feature = pd.concat([ca_feature, target], axis=1)cnt += len(us)ca_col.append(col)feat_dim = cntfeature_value = pd.concat([co_feature, ca_feature], axis=1)feature_index = feature_value.copy()for i in feature_index.columns:if i in co_col:feature_index[i] = feat_dict[I]else:feature_index[i] = feature_index[i].map(feat_dict[I])feature_value[i] = 1. 核心部分如上，重要的是做了两件事情，生成了feature_index和feature_value。 feature_index是把所有特征进行了标序，feature1，feature2……featurem，分别对应0，1，2，3，…m，但是，请注意分类变量需要拆分！就是说如果有性别：男|女|未知，三个选项。需要构造feature男，feature女，feature未知三个变量，而连续变量就不需要这样。 feature_value就是特征的值，连续变量按真实值填写，分类变量全部填写1。 更加形象的如下： deepfm.py特征向量化123456789# 特征向量化，类似原论文中的vself.weight['feature_weight'] = tf.Variable(tf.random_normal([self.feature_sizes, self.embedding_size], 0.0, 0.01),name='feature_weight')# 一次项中的w系数，类似原论文中的wself.weight['feature_first'] = tf.Variable(tf.random_normal([self.feature_sizes, 1], 0.0, 1.0),name='feature_first') 可以对照下面的公式看，更有感觉。 deep网络部分的weight123456789101112131415161718192021222324252627# deep网络初始input：把向量化后的特征进行拼接后带入模型，n个特征*embedding的长度input_size = self.field_size * self.embedding_sizeinit_method = np.sqrt(2.0 / (input_size + self.deep_layers[0]))self.weight['layer_0'] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(input_size, self.deep_layers[0])), dtype=np.float32)self.weight['bias_0'] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(1, self.deep_layers[0])), dtype=np.float32)# 生成deep network里面每层的weight 和 biasif num_layer != 1:for i in range(1, num_layer):init_method = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[I]))self.weight['layer_' + str(i)] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(self.deep_layers[i - 1], self.deep_layers[i])),dtype=np.float32)self.weight['bias_' + str(i)] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(1, self.deep_layers[i])),dtype=np.float32)# deep部分output_size + 一次项output_size + 二次项output_sizelast_layer_size = self.deep_layers[-1] + self.field_size + self.embedding_sizeinit_method = np.sqrt(np.sqrt(2.0 / (last_layer_size + 1)))# 生成最后一层的结果self.weight['last_layer'] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(last_layer_size, 1)), dtype=np.float32)self.weight['last_bias'] = tf.Variable(tf.constant(0.01), dtype=np.float32) input的地方需要注意一下，这边用了个技巧，直接把把向量化后的特征进行拉伸拼接后带入模型，原来的v是batch*n个特征*embedding的长度，直接改成了batch*（n个特征*embedding的长度），这样的好处就是全值共享，又快又有效。 网络传递部分都是一些正常的操作，稍微要注意一下的是交互项的计算：12345678910111213141516# second_orderself.sum_second_order = tf.reduce_sum(self.embedding_part, 1)self.sum_second_order_square = tf.square(self.sum_second_order)print('sum_square_second_order:', self.sum_second_order_square)# sum_square_second_order: Tensor("Square:0", shape=(?, 256), dtype=float32)self.square_second_order = tf.square(self.embedding_part)self.square_second_order_sum = tf.reduce_sum(self.square_second_order, 1)print('square_sum_second_order:', self.square_second_order_sum)# square_sum_second_order: Tensor("Sum_2:0", shape=(?, 256), dtype=float32)# 1/2*((a+b)^2 - a^2 - b^2)=abself.second_order = 0.5 * tf.subtract(self.sum_second_order_square, self.square_second_order_sum)self.fm_part = tf.concat([self.first_order, self.second_order], axis=1)print('fm_part:', self.fm_part) 直接实现了下面的计算逻辑： loss部分我个人重写了一下我认为需要正则的地方，和一些loss的计算方式：123456789101112# lossself.out = tf.nn.sigmoid(self.out)# loss = tf.losses.log_loss(label,out) 也行，看大家想不想自己了解一下loss的计算过程self.loss = -tf.reduce_mean(self.label * tf.log(self.out + 1e-24) + (1 - self.label) * tf.log(1 - self.out + 1e-24))# 正则：sum(w^2)/2*l2_reg_rate# 这边只加了weight，有需要的可以加上bias部分self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg_rate)(self.weight["last_layer"])for i in range(len(self.deep_layers)):self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg_rate)(self.weight["layer_%d" % I]) 大家也可以直接按照我注释掉的部分简单操作，看个人的理解了。 梯度正则12345678self.global_step = tf.Variable(0, trainable=False)opt = tf.train.GradientDescentOptimizer(self.learning_rate)trainable_params = tf.trainable_variables()print(trainable_params)gradients = tf.gradients(self.loss, trainable_params)clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)self.train_op = opt.apply_gradients(zip(clip_gradients, trainable_params), global_step=self.global_step) 很多网上的代码跑着跑着就NAN了，建议加一下梯度的正则，反正也没多复杂。 执行结果1234567891011121314151617181920212223242526272829/Users/slade/anaconda3/bin/python /Users/slade/Documents/Personalcode/machine-learning/Python/deepfm/deepfm.py[2 1 0 3 4 6 5 7][0 1 2][6 0 8 2 4 1 7 3 5 9][2 3 1 0]W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.embedding_part: Tensor("Mul:0", shape=(?, 39, 256), dtype=float32)first_order: Tensor("Sum:0", shape=(?, 39), dtype=float32)sum_square_second_order: Tensor("Square:0", shape=(?, 256), dtype=float32)square_sum_second_order: Tensor("Sum_2:0", shape=(?, 256), dtype=float32)fm_part: Tensor("concat:0", shape=(?, 295), dtype=float32)deep_embedding: Tensor("Reshape_2:0", shape=(?, 9984), dtype=float32)output: Tensor("Add_3:0", shape=(?, 1), dtype=float32)[&lt;tensorflow.python.ops.variables.Variable object at 0x10e2a9ba8&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112885ef0&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3c18&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3da0&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3f28&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3c50&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112a03dd8&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112a03b38&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x16eae5c88&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112b937b8&gt;]time all:7156epoch 0:the times of training is 0, and the loss is 8.54514the times of training is 100, and the loss is 1.60875the times of training is 200, and the loss is 0.681524the times of training is 300, and the loss is 0.617403the times of training is 400, and the loss is 0.431383the times of training is 500, and the loss is 0.531491the times of training is 600, and the loss is 0.558392the times of training is 800, and the loss is 0.51909... 看了下没啥大问题。 还有一些要说的 build_data.py中我为了省事，只做了标准化，没有进行其他数据预处理的步骤，这个是错误的，大家在实际使用中请按照我在公众号里面给大家进行的数据预处理步骤进行，这个非常重要！ learing_rate是我随便设置的，在实际大家跑模型的时候，请务必按照1.0，1e-3，1e-6，三个节点进行二分调优。 如果你直接搬上面代码，妥妥过拟合，请在真实使用过程中，务必根据数据量调整batch的大小，epoch的大小，建议在每次传递完成后加上tf.nn.dropout进行dropout。 如果数据量连10万量级都不到，我还是建议用机器学习的方法，xgboost+lr，mixed logistics regression等等都是不错的方法。 好了，最后附上全量代码的地址Github，希望对大家有所帮助。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>特征交叉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Tensorflow实现多层感知机网络MLPs]]></title>
    <url>%2F2018%2F07%2F25%2F%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%BD%91%E7%BB%9CMLPs%2F</url>
    <content type="text"><![CDATA[之前在基于Tensorflow的神经网络解决用户流失概率问题写了一个MLPs的网络，很多人在问，其实这个网络看起来很清晰，但是却写的比较冗长，这边优化了一个版本更方便大家修改后直接使用。 直接和大家过一遍核心部分： 上次我们计算过程中，通过的是先定义好多层网络中每层的weight，在通过tf.matual进行层与层之间的计算，最后再通过tf.contrib.layers.l2_regularizer进行正则；而这次我们直接通过图像识别中经常使用的全连接（FC）的接口，只需要确定每层的节点数，通过layers_nodes进行声明，自动可以计算出不同层下的weight，更加清晰明了。另外，还增加了dropout的部分，降低过拟合的问题。 123456din_all = tf.layers.batch_normalization(inputs=din_all, name='b1')layer_1 = tf.layers.dense(din_all, self.layers_nodes[0], activation=tf.nn.sigmoid,use_bias=True,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name='f1')layer_1 = tf.nn.dropout(layer_1, keep_prob=self.drop_rate[0])layer_2 = tf.layers.dense(layer_1, self.layers_nodes[1], activation=tf.nn.sigmoid,use_bias=True,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name='f2')layer_2 = tf.nn.dropout(layer_2, keep_prob=self.drop_rate[1])layer_3 = tf.layers.dense(layer_2, self.layers_nodes[2], activation=tf.nn.sigmoid,use_bias=True,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name='f3') tf.layers.dense接口信息如下：12345678910111213141516tf.layers.dense(inputs,units,activation=None,use_bias=True,kernel_initializer=None,bias_initializer=tf.zeros_initializer(),kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,trainable=True,name=None,reuse=None) inputs：必需，即需要进行操作的输入数据。 units：必须，即神经元的数量。 activation：可选，默认为 None，如果为 None 则是线性激活。 use_bias：可选，默认为 True，是否使用偏置。 kernel_initializer：可选，默认为 None，即权重的初始化方法，如果为 None，则使用默认的 Xavier 初始化方法。 bias_initializer：可选，默认为零值初始化，即偏置的初始化方法。 kernel_regularizer：可选，默认为 None，施加在权重上的正则项。 bias_regularizer：可选，默认为 None，施加在偏置上的正则项。 activity_regularizer：可选，默认为 None，施加在输出上的正则项。 kernel_constraint，可选，默认为 None，施加在权重上的约束项。 bias_constraint，可选，默认为 None，施加在偏置上的约束项。 trainable：可选，默认为 True，布尔类型，如果为 True，则将变量添加到 GraphKeys.TRAINABLE_VARIABLES 中。 name：可选，默认为 None，卷积层的名称。 reuse：可选，默认为 None，布尔类型，如果为 True，那么如果 name 相同时，会重复利用。 除此之外，之前我们定义y和y_的时候把1转化为[1,0]，转化为了[0,1]，增加了工程量，这次我们通过： 12cross_entropy_mean = -tf.reduce_mean(self.y_ * tf.log(self.output + 1e-24))self.loss = cross_entropy_mean 直接进行计算，避免了一些无用功。 最后，之前对于梯度的值没有进行限制，会导致整体模型的波动过大，这次优化中也做了修改，如果大家需要也可以参考一下：123456# 我们用learning_rate_base作为速率η，来训练梯度下降的loss函数解，对梯度进行限制后计算lossopt = tf.train.GradientDescentOptimizer(self.learning_rate_base)trainable_params = tf.trainable_variables()gradients = tf.gradients(self.loss, trainable_params)clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)self.train_op = opt.apply_gradients(zip(clip_gradients, trainable_params), global_step=self.global_step) MLPs是入门级别的神经网络算法，实际的工业开发中使用的频率也不高，后面我准备和大家过一下常见的FM、FFM、DeepFM、NFM、DIN、MLR等在工业开发中更为常见的网络，欢迎大家持续关注。 完整代码已经上传到Github中。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伪标签半监督学习]]></title>
    <url>%2F2018%2F07%2F24%2F%E4%BC%AA%E6%A0%87%E7%AD%BE%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[之前在训练YoutubeNet和DCN的时候，我都发现平台用户中基础用户的信息数据缺失率特别高，比如性别一栏准确填写的不足60%，所以我一直想调研一下有没有什么更好的填充方法，要保证既不能太复杂太耗时，也要有足够好的效果。 其实这个问题就是一个缺失值填充，之前的文章中也写过很多办法，常规的也总结过： 均值、众数填充最简单的填充，效果也惨不忍睹 根据没有缺失的数据线性回归填充填充的好会造成共线性错误，填充的不好就没价值，很矛盾 剔除丢失信息量 设置哑变量会造成数据分布有偏 smote连续值有效，离散值就无法实施了 我在Google上看imbalance问题的时候，偶然看到了这个视频教程，上面讲了图像的缺失处理，提到了伪标签处理的半监督学习方式。我就在国内的论坛上找了下，阿里云技术论坛也同样注意到了这个问题，但是只给出了如下的粗糙的构思图： 有一份整理了的流程图，具体执行步骤总结，和大家一起看一下： 将有标签部分数据分为两份：train_set&amp;validation_set，并训练出最优的model1 用model1对未知标签数据(test_set)进行预测，给出伪标签结果pseudo-labeled 将train_set中抽取一部分做新的validation_set，把剩余部分与pseudo-labeled部分融合作为新的train_set，训练出最优的model2 再用model2对未知标签数据(test_set)进行预测，得到最终的final result label 我利用了已知标签的数据对这个方法进行测试，用了最简单的mixed logistic regression模型作为Basic Model，得到结果如下：利用伪标签半监督的方式，同样的mixed logistic regression模型AUC值会提高0.1pp左右，效果还不错，而且实施并不复杂，大家可以在缺失值处理或者分类问题中应用尝试一下。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>特征刻画</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[量化评估推荐系统效果]]></title>
    <url>%2F2018%2F07%2F19%2F%E9%87%8F%E5%8C%96%E8%AF%84%E4%BC%B0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%95%88%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[推荐系统最有效的方法就是A/B test进行模型之间的对比，但是由于现实原因的局限，存在现实实时的困难性，所以，梳理了一些可以补充替代的指标如下，但是离线评估也存在相应的问题：1) 数据集的稀疏性限制了适用范围，用户之间的交集稀疏。2) 评价结果的客观性，由于用户的主观性，不管离线评测的结果如何，都不能得出用户是否喜欢某推荐系统的结论，只是一个近似的评估。3) 深度评估指标的缺失。(如点击深度、购买客单价、购买商品类别、购买偏好)之间的关联关系。4）冷启动5）Exploration 和 Exploitation问题 离线模型之间的评估召回集测试 recall命中skn个数/用户真实点击skn个数 precision命中skn个数/所有预测出来的skn总数 F1-Measure2/(1/recall+1/precison) 交互熵 MAE RMSE 相关性常见的比如：Pearson、Spearman和Kendall’s Tau相关，其中Pearson是更具数值之间的相似度，Spearman是根据数值排序之间的相似度，Kendall’s Tau是加权下的数值排序之间的相似度。 基尼系数 信息熵 排序部分测试 NDCG（Normalize DCG） RBP（rank-biased precision） RBP和NDCG指标的唯一不同点在于RBP把推荐列表中商品的浏览概率p按等比数列递减，而ND CG则是按照log调和级数形式。 离线模型与在线模型之间的评估很多时候，我们需要确定离线模型的效果足够的健壮才能允许上线进行线上测试，那如何进行离线模型与线上模型的评估对比就是一个比较复杂的问题。 难点 缺乏公平的测试数据实际处理过程中，我们发现，所有的已知点击都是来自线上模型推荐的结果，所以极端情况下，线上的recall是100% 缺乏公认的衡量指标在线下对比中，我们发现比如recall、precision、F1-Measure等指标都是大家约定俗成的，不存在很大的争议，而离线在线模型对比却没有一个准确公认的衡量指标 指标设计 online_offline_cover_rate&amp;first_click_hit_rate 这一组指标是结合在一起看的，其中online_offline_cover_rate是指针对每一个用户计算理线模型推荐的商品与在线模型推荐的商品的重合个数/在线模型的推荐商品个数，online_offline_cover_rate越低代表离线模型相对在线模型越独立；first_click_hit_rate是指offline模型对用户每天第一次点击的命中率，也就是命中次数/总统计用户数。结合这两个指标，我们可以得到在online_offline_cover_rate越低的情况下，却能覆盖线上用户真实点击的次数越多，代表offline模型的效果优于线上模型。 online_precision_rate/offline_precision_rate 离线模型的准确率和在线模型的准确率。这边在实际计算的时候采取了一个技巧，针对某个推荐位计算在线模型准确率的时候，用的是从来没有浏览过这个推荐位的用户的浏览历史匹配这个用户这个推荐位的推荐结果。这样可以避免用户的点击结果受到推荐位推荐结果影响的问题。 举个例子：用户在推荐位A上没有浏览过，他的点击是不受推荐位A推荐的商品影响的，拿这个用户推荐位A我们给他线上推荐的结果作为线上模型的推荐结果去计算，这样才更加合理。 online_recall_rate/offline_recall_rate 离线模型的召回率和在线模型的召回率。同上解释。 roi_reall/roi_precision 同上解释，只是把未来的点击作为match源更换成了加购物车、购买、收藏这些数据。 其他评估方向覆盖率推荐覆盖率越高， 系统给用户推荐的商品种类就越多 ，推荐多样新颖的可能性就越大。如果一个推荐算法总是推荐给用户流行的商品，那么它的覆盖率往往很低，通常也是多样性和新颖性都很低的推荐。 多样性采用推荐列表间的相似度（hamming distance、Cosine Method），也就是用户的推荐列表间的重叠度来定义整体多样性。 新颖性计算推荐列表中物品的平均流行度。 其他用户满意度、用户问卷、信任度、鲁棒性、实时性、 评测维度最后说一下评测维度分为如下3种，多角度评测： 用户维度主要包括用户的人口统计学信息、活跃度以及是不是新用户等。 物品维度包括物品的属性信息、流行度、平均分以及是不是新加入的物品等。 时间维度包括季节，是工作日还是周末，是白天还是晚上等。 附常规评价指标的整理结果(来自论文Evaluation Metrics for Recommender Systems)： 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐评估方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[热传导-物质扩散算法应用于推荐]]></title>
    <url>%2F2018%2F07%2F19%2F%E7%83%AD%E4%BC%A0%E5%AF%BC-%E7%89%A9%E8%B4%A8%E6%89%A9%E6%95%A3%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E4%BA%8E%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[没有大量的数据，没有大量的人力就不能做好推荐么？当然不是，热传导/物质扩散推荐算法就是作为冷启动及小规模团队非常实用的推荐召回部分的算法。 目标是为a图中标有星号（不妨记为用户1）的用户推荐商品，该用户已经购买过的两件商品是我们可以利用的信息，用来给目标用户进行推荐。 物质扩散算法：初始，我们认为每件被目标用户购买过的商品的信息量为1。商品把自己的信息平均分给所有购买过它的用户，用户的信息值则是从所有商品所得到的信息值得总和，比如上图(b)中的第一个节点的信息就等于第一个商品平均分给三个用户的的平均信息1/3，再加上第四个商品平均分给两个用户的平均信息1/2，即为1/3+1/2=5/6；接下来，每一个用户再把自己的信息平均分给所有购买过的商品，商品的信息则是从所有用户收到的信息值得总和，如对于图(c)中的第一个商品，它的信息值就等于第一个用户信息值的一半，为5/12，加上第二个用户信息值的1/4，为5/24，再加上第三个用户信息值得一半，为1/6，总的能量值即为:5/12+5/24+1/6=19/24。 以上两个步骤加起来为从商品到商品信息扩散一步。针对大规模系统的推荐，为了保持实时性和效率，往往只需扩散三步以内。如果以一步为界，基于图(c)中的结果，则在目标用户没有购买过的所有商品中，第三个商品的信息值最大，因此基于物质扩散算法的推荐系统则会将此商品推荐给目标用户，同时可以得到对于用户1的商品得分排序，自然可以得到用户召回集。值得注意的是物质扩散这种算法得到的所有商品最后的信息值之和就等于初始时所有商品的信息值，即能量是守恒的，图(c)中所有商品的信息之和仍为2。 热传导算法： 初始，我们认为目标用户购买过的每件商品的信息量为1。目标用户的信息等于所有他购买过的商品信息的平均值，如图(d)所示，目标用户购买了商品1和商品4，则该用户的信息值即为(1 + 1) / 2 = 1。再根据目标用户浏览过的商品给所以商品计算信息，第一个商品、第四个商品信息量为1/2，其他商品的信息量为0（因为目标用户没有买过），接下来根据每一个商品的信息计算其他的用户的信息，如图(d)中的第二个用户的信息就为商品1,2,3,4的信息的平均值（1/2 + 1/2）/4 = 1/2；再根据每个用户的信息量平均分配信息到每个商品，如图(e)中的第一个商品来自第一个、第二个、第三个用户的信息的和，即为1/21/2+1/21/3+1/2*/12=2/3。 以上两个步骤加起来为从商品到商品热传导一步。因此基于热传导算法的推荐系统则会将此信息量大的商品推荐给目标用户，同时可以得到对于用户1的商品得分排序，自然可以得到用户召回集。与物质扩散不同的是这种算法得到的所有商品最后的信息值之和就不一定等于初始时所有商品的信息值，即不满足守恒定律，这是因为在信息传到的第二步过程中，有的用户的信息可能会被多次计算，从而导致不守恒。 基于物质扩散和基于热传导的推荐算法的区别在于： 基于物质扩散的方法在进行个性化推荐时，系统的总信息是守恒的；而热传导在推荐过程中，目标用户（即被推荐用户）的收藏品将被视作信息初始点，负责提供能量，所以系统的总信息量随着传递步骤的增加是在不断增加的。 如果对物理比较熟悉的朋友很容易联想到凸透镜和凹透镜，是的，我个人在理解的时候也是这样迁移理解，原理上确实一致。 基于物质扩散的方法相当于凸透镜一样把用户历史点击的信息聚焦到了少量优势的skn上了； 基于热传导的方法相当于是凹透镜一样把用户的历史点击信息发散到了那些较不流行的物品上，从而提高了推荐的新颖多样性。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow在GPU下的Poolallocator Message]]></title>
    <url>%2F2018%2F06%2F28%2FTensorflow%E5%9C%A8GPU%E4%B8%8B%E7%9A%84Poolallocator%20Message%2F</url>
    <content type="text"><![CDATA[我在在用GPU跑我一个深度模型的时候，发生了以下的问题：1234567891011121314151617...2018-06-27 18:09:11.701458: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 63521 get requests, put_count=63521 evicted_count=1000 eviction_rate=0.0157428 and unsatisfied allocation rate=0.01731712018-06-27 18:09:11.701503: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110Global_step 2000 Train_loss: 0.0758Global_step 3000 Train_loss: 0.0618Global_step 4000 Train_loss: 0.0564Global_step 5000 Train_loss: 0.0521Global_step 6000 Train_loss: 0.0492Global_step 7000 Train_loss: 0.0468Global_step 8000 Train_loss: 0.0443Global_step 9000 Train_loss: 0.0422Global_step 10000 Train_loss: 0.0410Global_step 11000 Train_loss: 0.0397Global_step 12000 Train_loss: 0.03832018-06-27 18:13:59.743133: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 71532 get requests, put_count=71532 evicted_count=1000 eviction_rate=0.0139798 and unsatisfied allocation rate=0.01430132018-06-27 18:13:59.743167: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281... 除了常规的loss数据之外，我看到穿插在之间的warming informations ，虽然最后的结果没有任何问题，但是我抱着好奇的心态在stackoverflow找到了原因： TensorFlow has multiple memory allocators, for memory that will be used in different ways. Their behavior has some adaptive aspects.In your particular case, since you’re using a GPU, there is a PoolAllocator for CPU memory that is pre-registered with the GPU for fast DMA. A tensor that is expected to be transferred from CPU to GPU, e.g., will be allocated from this pool.The PoolAllocators attempt to amortize the cost of calling a more expensive underlying allocator by keeping around a pool of allocated then freed chunks that are eligible for immediate reuse. Their default behavior is to grow slowly until the eviction rate drops below some constant. (The eviction rate is the proportion of free calls where we return an unused chunk from the pool to the underlying pool in order not to exceed the size limit.) In the log messages above, you see “Raising pool_sizelimit“ lines that show the pool size growing. Assuming that your program actually has a steady state behavior with a maximum size collection of chunks it needs, the pool will grow to accommodate it, and then grow no more. It behaves this way rather than simply retaining all chunks ever allocated so that sizes needed only rarely, or only during program startup, are less likely to be retained in the pool.These messages should only be a cause for concern if you run out of memory. In such a case the log messages may help diagnose the problem. Note also that peak execution speed may only be attained after the memory pools have grown to the proper size. 加粗部分解释机制、处理方式和原因。总结起来就是，PoolAllocator会有一个内存分配机制，GPU和CPU之间不是独立的可以相互传输，如果你使用的空间太多，他就会提高原有的预设的空间大小，如果够用了，就没有什么影响了，但是，需要注意的是，兄弟你的数据加载量太大了，看看是不是改改batch size，一次性少加载点数据，或者干掉隔壁同事的任务。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow代码解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于'Deep Neural Networks for YouTube Recommendations'的一些思考和实现]]></title>
    <url>%2F2018%2F06%2F26%2F%E5%85%B3%E4%BA%8EDeep-Neural-Networks-for-YouTube-Recommendations%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83%E5%92%8C%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[论文 Deep Neural Networks for YouTube Recommendations 来自google的YouTube团队，发表在16年9月的RecSys会议。我想应该很多人都读过，之前参与了公司的推荐系统优化的项目，本来想从各大搜索引擎中寻找到现成的分析，但是出人意料的一无所获。Github上的代码实现也出奇的少以及不清晰，所以就借着这个机会和大家分享一下自己做的过程中的一些理论心得、工程坑、代码实现等等。 本文基于大家对Deep Neural Networks for YouTube Recommendations已经完成通读的基础上，不会做细致的论文解析，只会涉及到自己实现过程中的一些总结，如果没有论文了解，会非常不易理解。 系统概览上面这张图可以说是比较详细的涵盖了基础框架部分，整体的模型的优点我就不详述了，包括规模容纳的程度大啊、鲁棒性好啊、实时性优秀啊、延展性好啊等等，网上很多水字数的文章很多，我们主要总结几个愿论文上的亮点和实际去做的时候需要注意的地方： DNN网络可以怎么改 负采样的“避坑” example age有没有必要构造 user feature的选择方向 attention 机制的引入 video vectors的深坑 实时化的选择 整体上来说，G厂的这套算法基于的是两个部分：matching+ranking，这个的也给我们带来了更大的工作量，在做的时候，分成两个部分，我们在实际处理的时候，通过recall rate来判断matching部分的好坏，通过NDCG来判断排序部分的好坏。总体如下： candidate generation就是我们matching的模块，目的是把百万级的商品、视频筛选出百级、千级的可排序的量级；再通过ranking模块，选出十位数的展示商品、视频作为最后的推送内容。之所以把推荐系统划分成Matching和Ranking两个阶段，主要是从性能方面考虑的。Matching阶段面临的是百万级，而Ranking阶段的算法则非常消耗资源，不可能对所有目标都算一遍，而且就算算了，其中大部分在Ranking阶段排名也很低，也是浪费计算资源。 Matching &amp; Ranking Problems首先，我们都知道，G厂给出的这个解决方案用的就是基于DNN的超大规模多分类思想，即在时刻t，为用户U（上下文信息C）在视频库V中精准的预测出视频i的类别（每个具体的视频视为一个类别，i即为一个类别），用数学公式表达如下： 很显然上式为一个softmax多分类器的形式。向量u是信息的高纬“embedding”，而向量v则是视频 j 的embedding向量，通过u与v的点积大小来判断用户与对应视频的匹配程度。所以DNN的目标就是在用户信息和上下文信息为输入条件下学习视频的embedding向量v，从而得到用户的向量u。 说完基本思想，让我们看看实际的效果对比： DNN网络可以怎么改：softmax及revise的考虑 如我图中两处红色标记，论文中虽然给出了模型的整体流程，但是却没有指明，1处的video vectors需要单独的embedding还是沿用最下方的embedded video watches里面的已经embedding好的结果，我们称之为softmax问题；2处论文没有提及一个问题，就是在固定好历史watch的长度，比如过去20次浏览的video，可能存在部分用户所有的历史浏览video数量都不足20次，在average的时候，是应该除以固定长度（比如上述例子中的20）还是选择除以用户真实的浏览video数量，我们称之为revise问题。 根据我们的数据实测，效果对比如下： nosoft:沿用最下方的embedded video watches里面的已经embedding好的结果revise:除以用户真实的浏览video数量 我们尝试的去探求原因发现，nosoft比softmax好的原因在于user vector是由最下方的embedded video watches里面的已经embedding好的结果进行多次FC传递得来的，如果新增一个video embedded vector 的话，和FC传递得到的u vector的点积的意义就难以解释；revise比norevise好的原因是，实际在yoho!buy的购物场景下，用户的点击历史比较我们实际选取的window size要短不少，如果所有的用户都除以固定长度的话，大量的用户history click average的vector大小接近于0。 DNN网络可以怎么改：神经元死亡及网络的内部构造这是一个异常恶心还有没什么好方法的问题，在刚开始做的时候，我们遇到了一个常见的问题，神经元批量死亡的问题。在增加了batch normalization、clip_by_global_norm和exponential_decay learning rate 后有所缓解。 网络结构的变化比较常规，对比场景的激活函数，参考了论文中推荐的深度、节点数，效果对比如下： 虽然我们看到增加网络的深度（3–&gt;4）一定程度上会提高模型的命中率，增加leakyrelu的一层网络也可以有些许的提升，但是总的来说，对模型没有啥大的影响，所以在之后的实际模型中，我们选择了原论文中relu+relu+relu，1024+512+256的框架。 负采样的“避坑”我们都知道，算法写起来小半天就可以搞定，但是前期的数据处理要搞个小半个月都不一定能出来。作为爱省事的我，为了快速实现算法，没有重视负采样的部分，采取了列表页点击为label=1，未点击为label=0的方式，详情如下： 看上去没什么问题，省略了从全量样本中抽样作为负样本的复杂过程，实际上，我把代码狂改了n边效果也一直维持在1.57%，可以说是没有任何提升，在此过程期间，我还是了拿用户的尾次点击（last_record）进行训练，拿了有较多行为的用户的尾次点击（change_last_record）进行训练，效果很感人： 在我孤注一掷一致，选择按照原论文中说的，每次label=0的我不拿展现给用户但是用户没有点击的商品，而是随机从全量商品中抽取用户没有点击过的商品作为label=0的商品后，奇迹发生了： 事后我仔细分析了原因：a.在当次展现的情况下，虽然用户只点击了click商品，其他商品没有点击，但是很多用户在后续浏览的时候未click的商品也在其他非列表页的地方进行click，我实际上将用户感兴趣的商品误标记为了负样本b.后来我咨询看了论文，也发现了原论文中也提及到，展现商品极有可能为热门商品，虽然该商品该用户未点击，但是我们不能降低热门商品的权重（通过label=0的方式），实际上最后的数据也证明了这一点c.“偷窥未来的行为”，如下图，原论文中指出input构造时候不能拿还未发生的点击，只能拿label=1产生时之前的所有历史点击作为input；同理，在构造label=0的时候，只能拿在label=0的时候已经上架的商品，由于训练时间的拉长，不能偷窥label=1发生时还未上架的商品作为label=0的负样本 example age有没有必要构造首先，先稍微解释一下我对example age的概念的理解。所有的训练数据其实都是历史数据，距离当前的时刻都过去了一段时间，站在当前来看，距离当前原因的数据，对当前的影响应该是越小的。就比如1年前我买了白色的铅笔，对我现在需要不需要再买一支黑色的钢笔的影响是微乎其微的。而example age其实就是给了每一条数据一个权重，引用一下原论文的描述In (5b), the example age is expressed as tmax − tN where tmax is the maximum observed time in the training data，我这边采取了(tmax − tN)/tmax的赋权方式： 很悲催的是，直观的离线训练数据并没有给出很直观的效果提升，但是由于评估机制的问题（我们后面会说到），我会在实际上线 做abtest的时候重新验证我的这个example age的点，但是可以肯定的是，理论和逻辑上，给样本数据进行权重的更改，是一个可以深挖的点，对线上的鲁棒性的增强是正向的。 user feature的选择方向很不幸的是，在这一块的提升，确实没有论文中说的那么好，对于整个网络的贡献，以我做的实际项目的结果来说，history click embedded item &gt; history click embedded brand &gt; history click embedded sort &gt; user info &gt; example age &gt; others。不过，因为时间、数据质量、数据的真实性的原因，可能作为原始input的数据构造的就没有那么好。这边主要和大家说两个点： 1.topic数据原论文中在第四节的RANKING中指出:We observe that the most important signals are those that describe a user’s previous interaction with the item itself and other similar items, matching others’ experience in ranking ads论文中还举出了比如用户前一天的每个频道（topic）的浏览视频个数，最后一次浏览距今时间，其实说白了就是强调了过去的行为汇总对未来的预测的作用，认为过去的行为贯穿了整体的用户点击轨迹。除此之外，G厂大佬还认为一些用户排序性质的描述特征对后面的ranking部分的提高也是蛮重要的，这边还举出了用户视频评分的例子，更多的内容大家可以自己去看一下原论文的部分，应该都会有自己的体会。 回到我们的项目，因为yoho!buy是电商，我类比着做了用户每个类目（裤子、衣服、鞋子…）的历史浏览点击购买次数、最后一次点击距今时长等等的topic信息，提升不是很明显。但是在大家做G厂这边论文，准确率陷入困境的时候，可以尝试一下这边的思路。 2.query infomation相比于论文中的user information的添加，在实际模型测试中，我们发现，query的information的部分有更多的”遐想”。 原论文中点名指出user language and video language 做为basic info的重要性，这边给出的提升也是相对于user info有明显的增长的： 有提升也自然有该部分的缺点：1.语言模型的处理复杂，耗时久在该部分的处理中，我强行拖着隔壁组的nlp博士和我一起搞了一周，每天都加班的搞去做数据清理，句法分析，语句树解析。如果需要让一个常规做推荐的人去弄，会有各种各样的坑，而且耗时还久2.语言新增问题商品的标题这类的文本处理还好，毕竟每日更新的数据存在一个可控的范围，但是用户搜索内容的变化是巨大的，粗略估测一下，一周时间间隔后，原提纯文本数据和新提纯文本数据的交集覆盖率不到78%，这意味着要重复的做nlp工作 attention 机制的引入attention 机制的引入是我老大的硬性需求，我这边也就做了下，如果不了解attention 机制的朋友，可以阅读以下这边文章：Attention model。 我通俗的解释一下，不准确但是方便理解，Attention model就是让你每一个input与你的output计算一个similarity，再通过这些similarities给出每个input的权重。但是，很明显，我们离线训练还好，既有input也有output，但是线上预测的时候，就没有output了，所以，我们采取了lastclick替代的方式： 不得不说，老祖宗传下来的东西确实有独到之处，但是在提升了近1pp的rate代价之下，会有一个让人头疼的问题耗时。因为每一个input的weight需要和output进行一次相似度计算，而且后续还要对计算出的相似度进行处理，原本只需要6-7小时训练完的模型，在我加了3层Multihead Attention后被拖到了一天。数据量还只采样了一半，确实需要斟酌带来的提升与投入的成本之间的平衡问题。 video vectors的深坑G厂一句话，我们测断腿。这句话不是瞎说的，大家应该还记得一开始我给出的那张图，在最上面有一行不是很明显的小字：video vectors。G厂的大佬们既没有说这些video vectors该怎么构造，也没有说video vectors需不需要变动，留下了一个乐趣点让大家体验。 刚开始我很傻的用了我们最开始的embedded item作为video vectors，与模型FC出来的user vectors进行点击，计算top items。我来来回回测了一个月，老命都快改没了，最后提升rate到4pp。然而RNN随便跑跑就能到达3pp，我说很不服气的，所以拉着同事一起脑洞了一下，我们之前做图片相似度匹配的时候，喜欢把图片的向量拆成颜色+款式+性别，所以我们就借用了一下，改成了embedded item + embedded brand + embedded sort作为video vectors，历史总是给我们惊喜，效果上一下子就能大到5.2pp左右，这个点的提升应该是得来的最意外的，建议大家在用的时候考虑一下。 实时化的选择实时部署上，我们用了tensor flow serving，没什么好说的，给一下关键代码，大家看下自己仿一下就行，一般自己做做demo不需要，企业级上线才需要，企业级上线的那些大佬可能也比我有更多想法，所以就不展开了。 123456789101112部署及用python作为Client进行调用的测试：#1.编译服务bazel build //tensorflow_serving/model_servers:tensorflow_model_server#2.启动服务bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9005 --model_name=test --model_base_path=/Data/sladesha/tmp/test/ #3.编译文件 bazel build //tensorflow_serving/test:test_client#4.注销报错的包注销：/Data/muc/serving/bazel-bin/tensorflow_serving/test/test_client.runfiles/org_tensorflow/tensorflow/contrib/image/__init__.pyc中的from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms参考：https://github.com/tensorflow/serving/issues/421#5.运行bazel-bin/tensorflow_serving/test/test_client --server=localhost:9005 相关的问题，有大佬已经梳理好了，自取其他可选的一些参数设置：tensorflow serving 参数设置。 还有一些评估技巧，模型之间的对比技巧，这边就不细讲了，可借鉴的意义也不大。 总结虽然早就读过这篇文章，但是实现之后，发现新收获仍然不少。我特别赞成清凇的一句话:’对于普通的学术论文，重要的是提供一些新的点子，而对于类似google这种工业界发布的paper，特别是带有practical lessons的paper，很值得精读。’G厂的这个推荐代码和attention model的代码之前是准备放GitHub的，想想还是算了。一是之前也放过很多此代码，也没什么反馈，二是这两个代码自己写也不是很难，可以作为练手项目。 鸣谢以上我个人在Yoho!Buy团队在实践中的一点总结，不代表公司的任何言论，仅仅是我个人的观点。最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！溜了溜了。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.nn.embedding_lookup]]></title>
    <url>%2F2018%2F06%2F11%2FTensorflow%E4%B8%93%E9%A2%98tf-nn-embedding-lookup%2F</url>
    <content type="text"><![CDATA[我觉得这张图就够了，实际上tf.nn.embedding_lookup的作用就是找到要寻找的embedding data中的对应的行下的vector。 1tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None) 官方文档位置，其中，params是我们给出的，可以通过：1.tf.get_variable(&quot;item_emb_w&quot;, [self.item_count, self.embedding_size])等方式生产服从[0,1]的均匀分布或者标准分布2.tf.convert_to_tensor转化我们现有的array然后，ids是我们要找的params中对应位置。 举个例子：1234567import numpy as npimport tensorflow as tfdata = np.array([[[2],[1]],[[3],[4]],[[6],[7]]])data = tf.convert_to_tensor(data)lk = [[0,1],[1,0],[0,0]]lookup_data = tf.nn.embedding_lookup(data,lk)init = tf.global_variables_initializer() 先让我们看下不同数据对应的维度：123456In [76]: data.shapeOut[76]: (3, 2, 1)In [77]: np.array(lk).shapeOut[77]: (3, 2)In [78]: lookup_dataOut[78]: &lt;tf.Tensor 'embedding_lookup_8:0' shape=(3, 2, 2, 1) dtype=int64&gt; 这个是怎么做到的呢？关键的部分来了，看下图：lk中的值，在要寻找的embedding数据中下找对应的index下的vector进行拼接。永远是look(lk)部分的维度+embedding(data)部分的除了第一维后的维度拼接。很明显，我们也可以得到，lk里面值是必须要小于等于embedding(data)的最大维度减一的。 以上的结果就是：12345678910111213141516171819202122232425262728293031323334353637In [79]: dataOut[79]:array([[[2],[1]],[[3],[4]],[[6],[7]]])In [80]: lkOut[80]: [[0, 1], [1, 0], [0, 0]]# lk[0]也就是[0,1]对应着下面sess.run(lookup_data)的结果恰好是把data中的[[2],[1]],[[3],[4]]In [81]: sess.run(lookup_data)Out[81]:array([[[[2],[1]],[[3],[4]]],[[[3],[4]],[[2],[1]]],[[[2],[1]],[[2],[1]]]]) 最后，partition_strategy是用于当len(params) &gt; 1，params的元素分割不能整分的话，则前(max_id + 1) % len(params)多分一个id.当partition_strategy = ‘mod’的时候，13个ids划分为5个分区：[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]，也就是是按照数据列进行映射，然后再进行look_up操作。当partition_strategy = ‘div’的时候，13个ids划分为5个分区：[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]，也就是是按照数据先后进行排序标序，然后再进行look_up操作。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow代码解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.scan]]></title>
    <url>%2F2018%2F06%2F04%2FTensorflow%E4%B8%93%E9%A2%98tf-scan%2F</url>
    <content type="text"><![CDATA[tf.scan(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=None) fn：计算函数elems：以elems的第一维度的变量list作函数计算直到遍历完整个elemsinitializer：fn计算的初始值，替代elems做第一次计算 举个好理解的例子：123456789101112131415x = [1,2,3]z = 10x = tf.convert_to_tensor(x)z = tf.convert_to_tensor(z)def f(x,y):return x+yg = tf.scan(fn=f,elems = x,initializer=z)sess = tf.Session()sess.run(tf.global_variables_initializer)sess.run(g) 会得到：12In [97]: sess.run(g)Out[97]: array([11, 13, 16], dtype=int32) 详细的计算逻辑如下：11 = 10(初始值initializer)+ 1(x[0])13 = 11(上次的计算结果)+2(x[1])16 = 13(上次的计算结果)+3(x[2])]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow代码解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写给想转行机器学习深度学习的同学]]></title>
    <url>%2F2018%2F03%2F18%2F%E5%86%99%E7%BB%99%E6%83%B3%E8%BD%AC%E8%A1%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%8C%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[update 1:很多同学还是私信我，让我推荐或者提供一些电子书给他们，我这边也打包了一些我认为比较重要的，如果有需要的同学可以「邮箱」联系我。申明，我所发送的书个人均已购买正版实体书，建议大家也支持正版，谢谢。 自从我毕业以来，先是火机器学习，然后火大数据，之后火深度学习，现在火人工智能这些算法领域。越来越多的朋友想从工业，金融等等行业转行到算法相关的行业，我一年前在知乎上写了一个答案本科生怎样通过努力拿到较好的机器学习/数据挖掘相关的offer？，当时拿了不少的赞，所以也一直有同学找我咨询相关的问题，确确实实也有相当一批人拿到了不错的offer。 我个人不是很喜欢更新非技术的文章，但是我还是觉得如果能帮助到一些人，其实也是另一种技术输出的展现，所以我就写下了下面这篇短文，希望对迷茫的人有所帮助。 评估转行难度今天一大早，我在刷知乎的时候，刷到这个题目非计算机专业学生如何转行AI，并找到算法offer?，我看到这个叫做BrianRWang的答主的一个“10问检验你的基础水平”，我觉得是至少我看来非常全面考验数学基础的，所以这边就和大家分享一下（答案我会在最后给出，有兴趣的最好自己做一下，括号里面的我个人觉得没有意义所以没有给出解释，有兴趣的却又解不出来的同学可以私信我）： 1.什么是贝叶斯定理？请简述其公式？现分别有 A，B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器 A 的概率是多少? 这题考了概率论的基础，虽然考了贝叶斯，但是后面的容器问题完全可以不用贝叶斯也可以算出来，算是一题数学敏感度的测试题，看看自己适不适合去努力切入这个方向。 2.请简述卡方分布和卡方检验的定义？(给你一个2*2的列表让你算卡方分布，你会怎么做？) 这题考了梳理统计的基础，括号里面的我个人觉得没有意义，有兴趣的可以查表算一下。 3.在概率统计学里，自由度是如何被定义的，又该怎样去应用？ 原作者BrianRWang认为这题比较偏，属于冷门题目。个人看法：其实我觉得如果是任何一个理工科的同学，这题都应该能答出来，大学的课程里，自由度的理解直接决定了统计科目大家的学习质量。 以上的三题考了概率论与数理统计的基础，在机器学习理论中，概率论和数理统计的基础是否扎实直接决定了能否很好的理解各个理论的前置条件，适用场景，提升方向等，着实重要。 4.请简述什么是线性代数里的矩阵特征值和特征向量？(求矩阵:A=np.array([[1,2],[3,4]])的特征值，特征向量，写出其运算公式) 线性代数题目，很简单给出对应的公式即可，我在SVD介绍的时候就完全讲过。如果换成，如何理解特征值及特征向量在空间中的实际意义，这题就会变得非常卡人。 5.如何使用级数分解的方法求解e^x?(并给出在数值计算中可能遇到的问题。) 数学分析的题目，一个公式。 以上的题目都是线性代数，数学分析的题目，都是比较考验大学的基本功，如果不记得也很正常，只要能说出大概的思想就行，比如空间选择啊，点导数展开。 6.数据结构的定义是什么？运用数据结构的意义是什么？ 计算机题，这题应该是几个问答中最简单的了。 7.请说明至少两种用于数据可视化（data visualization）的package。并且说明，在数据分析报告里用数据可视化的意义是什么？ 前一问如果主动接触过计算科学的人这题比较好答，如果是纯新手，这题就是无从下手的。后面一小问也是属于考察你的数据敏感度的，如果能够match到一些点，很加分。 8.假如让你用编程方法，比如python，处理一个你没见过的数学问题，比如求解一个pde或者整快速傅里叶变换，你应该查什么东西，找哪一个package的参考资料？ 同上一条前一部分。 9.请简述面向对象编程和函数式编程分别的定义，并举出其案例。 计算机题，考了基础的编程的一些风格的了解程度，说实话，这题我第一次看到也很懵，还去Google了一下。 原作者还有一个第10题，不涉及技术，我就没放。以上四题更偏向coding的能力，虽然说算法工程师、数据挖掘工程师、NLP工程师，等等，都是挂着科研的title，但是过硬的coding能力是完全不能缺少的，要其他人把很复杂的数学理论用代码帮你实现出来的交流成本巨大，我觉得精通或者熟悉至少一门语言还是非常重要的。 原作者认为： 以上提问如果能闭卷对7个及以上，证明一个学生的基础还是比较好的。只要聪明肯学，一定是有所裨益的。在7个，到3个之间，不妨提高一下自己的数学水平；努努力还是可以学会机器学习的。如果写对不了两个（“这都啥啊？”），郴州勃学院复读班欢迎你过去。 其实我还是比较认同的，答对3个或者2.5个以上的同学，完全可以试一试转一转，我觉得不存在说入不了门的情况。能答对7个或者7.5以上的同学，我觉得可以投简历了，如果我收到你的简历，即便是你没有历史的工作经验，我很愿意让你试一试的。 一些资料很多转行的朋友会问我，到底看什么书会比较好，我刚开始会推荐一堆，后来自己想了想发现，还是太天真，大家工作忙的要死，看一本就很难了，别说一堆。 我最后就浓缩了三本:：周志华老师的西瓜书（《机器学习》周志华 清华大学出版社），李航的带你玩转基础理论（《统计学习方法》李航 清华大学出版社），经典厕所读物（《数学之美》吴军 人民邮电出版社）。 确实是很经典很经典的书，我现在基本上每次必回答以上三本。 除此之外，在coursera上找吴恩达（Andrew Ng）教授的机器学习课程，他把要用到的数学知识也做了简单的讲解，机器学习方面的理论和算法讲的也很详细，而且很基础，肯定可以看懂。Machine Learning | Coursera，应该是最适合看的视频类的资料没有之一。 我不反对也不支持大家去参加几千几万的速成班，几十几百的live课程，但是我觉得你不妨先看完以上的书和视频再做决定，一定不会让你失望。之前我一直在给team做吴恩达（Andrew Ng）在线课程的分享，一直到最近我发现不如整理出来给team以外的大家一起看算了，所以在Gradient Checking(9-5)这节课之后的所有课程，如果有价值的地方，我都做了笔记后面会分享在我的GitHub中，希望给大家一些帮助。 最后，希望我们都不负自己的青春。 附录：1.BrianRWang的十条问题的答案链接2.吴恩达（Andrew Ng）Gradient Checking(9-5)这节课之后的课程整理（持续更新中）]]></content>
      <categories>
        <category>基础</category>
      </categories>
      <tags>
        <tag>公告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yoho!buy注册概率预估]]></title>
    <url>%2F2018%2F03%2F04%2Fyoho!buy%E6%B3%A8%E5%86%8C%E6%A6%82%E7%8E%87%E9%A2%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[GRU 部分的demo代码：model.py，注意，其中上传的lr_classification.py并没有做wide&amp;deep设计 前言本文主要介绍yoho!buy大数据团队在深度学习传统应用方向上的一些实践和思考。传统用户行为预估方向上，如何根据用户的行为数据，对用户行为建模，进而预测用户的购买行为，点击行为，注册行为等等一直以来都受到工业界及学术界的关注。相对而言，就用户注册概率的预测受限数据获取的局限性、传统的计算模型的时效性等原因并没有很多可参考的研究案例。我们想和大家分享的「yoho!buy基于GRU+LR算法下的用户注册概率预估」，基于循环神经网络的框架，充分的利用了用户在app上的行为信息，保证了高效的结果反馈速度，兼备算法框架良好的延拓性能。 注册概率预估定义注册概率预估，即预估用户下载app后，浏览app过程中主动注册的可能性。通过识别出有注册倾向的人群，辅助以人为介入的方式（优惠、折扣，关怀等），可以提高用户实际注册的概率。 基本的注册概率预估算法设计的流程如下： 数据整理节点：在于收集用户行为信息，包括地理位置，当时的时间，用户来源的渠道，用户点击行为等等；模型计算节点：在于根据数据整理节点的结果判断用户的注册概率高低；计算结果推送节点：在于根据不同注册概率用户采取不同的营销策略，个性化引导用户注册。 目前注册概率预估主要有两大难点： 传统模型难以实时预测：因为缺乏平台忠诚度，用户第一次也有可能是最后一次登陆app之前这段时间是相对暂短，如何压缩用户每一次步操作下的模型计算时间，提高反馈频率是需要考虑的重要问题，而传统模型在这方面的表现比较平庸。 传统模型特征加工复杂：因为用户可能是第一次接触app，在没有注册信息，历史行为信息，完成订单信息等等数据下，利用有限的数据进行的特征处理，如果想要有不错的效果相对而言特征加工过程的复杂程度和难度要比普通的项目更加具有挑战性。 我们通过以Recurrent Neural Network 及 Logistic Regression为基模型，通过Stacking方式，针对性的尝试去解决以上两个注册预估中的难点。其中Recurrent Neural Network最本质的能够work的地方在于，实际上在没有过多特征的时候，对于新用户来说，他的浏览路径实际上就是反映了他对这个app的喜好。 算法设计Recurrent Neural Network基模型数据整理部分无论在Kaggle还是天池大赛上，数据特征工程是非常重要而且繁琐的一个过程：关于预处理，通常我们会采取： 数据检查，提出异常字符、乱码等数据 缺失值处理，剔除、填充、拟合构造等 方差衡量，剔除方差低的低贡献特征 共线性检查，提高泛化能力 异常检验，剔除错误异常数据 … 在数据预处理结束之后，我们还会在更新完的数据集上进行特征筛选： 基于自变量与因变量之间的交互熵 基于模型中的特征贡献程度（Xgboost里面的importance/Lasso、Ridge中的参数绝对大小） 基于预训练模型中的特征参数的显著性 基于自变量之间的相关性 … 在数据特征筛选结束之后，我们还需要进行特征组合寻找最大方差下的新的特征，还会通过PCA/LDA/t-SNE/FM等寻找是否可以进行降维或者升维，交叉特征构造等等。通常无论是在离线训练还是线上预测中，对特征的加工处理过程都是非常耗时的，极有可能在用户已经离开app后，用户的注册概率还没有算出来。 我们利用Recurrent Neural Network来解决注册预估的时候，我们需要做的数据整理就非常的轻松，只需根据用户浏览的顺序，将用户浏览的页面编号Item_Page,同时记录用户浏览的先后顺序Time_Rank： 构造如下的数据形式： User_Imme Item_Page Time_Rank 012939003331092 92374573284354 1518422132 012939003331092 82374573771273 1518422142 012939003331092 92374573284354 1518422147 078939002221093 66774573284354 1518422247 078939002221093 66774573284442 1518422249 … … … 数据处理过程，只需要按照用户的浏览先后顺序进行排序即可，大大的降低了耗时，对整体算法的实效性上不会产生任何影响。因此，我们甚至可以在用户每一次产生动作之后就对其的注册概率进行重新判定，得到用户的浏览流对应的注册概率波动情况。 我们的用户可以大致可分为铁粉用户和普通用户两种。从用户的注册概率分布就可以很清晰的看出：铁粉用户浏览目的明确，寻找自己关注的商品，一旦找到立马注册下单，所以铁粉用户的时间流较短，注册概率呈现上升趋势，注册概率均值较高，需要我们运营手段干涉的情况较少；而普通用户属于无明确目的的浏览，所以注册概率的波动较大，注册概率均值较低，但是一旦察觉到用户有高概率注册的行为却未注册且注册概率持续下降时立刻进行营销引导，多次营销尝试后用户成功注册。 模型计算部分整体的模型框架概览如下： 我们试图通过循环网络部分提取出用户的浏览行为的汇总信息，再通过Logistic Regression部分融合用户基础信息，以行为特点+基础特征猜测用户每次浏览是属于随便点点还是认真挑选。 Recurrent Neural Network部分1.循环单元结构 在循环神经网络模块中，我们采取了Gated Recurrent Unit (GRU)代替普通RNN作为最小循环单元进行计算，以此来避免梯度消失等问题： 隐藏状态计算如下： 对比LSTM，GRU只用了两个gates，将LSTM中的输入门和遗忘门合并成了更新门。并且并不把线性自更新建立在额外的memory cell上，而是直接线性累积建立在隐藏状态上，并靠gates来调控，这样就可以大大的加快离线训练的速度，同时在RNN的官方论文中，我们看到了实测的效果如下： 很明显的可以看到:虽然GRU减少了一个门的存在，但是效果与LSTM相当，但是几乎每次测试的test效果都要优秀于传统方法，同时GRU是真的肉眼可见的比LSTM快。综合考虑了计算速度及最后计算结果的准确程度，我们选择了多层GRU模型而并非是以输入门、输出门、遗忘门为信息传递的LSTM模型。 2.循环网络结构 整体上看，网络结构也是非常简单的。如上图，先将用户浏览的所有商品进行embedding操作，然后根据用户每个商品的浏览顺序构建时序序列，进行多层的GRU模型训练，最后再以前馈网络传递，softmax后得到用户下次点击每个商品的概率，再根据预测结果Output Second Items 和 Real Second Items修正多层GRU layer中的参数。 通过已经训练好的循环网络，我们根据新用户浏览商品的顺序，得到用户每次浏览的后一次浏览每个商品的概率（output scores）及用户前N次浏览信息的trend，seasonality的汇总（隐藏状态GRU States）。与此同时，我们可以通过控制GRU layer的层数及优化多次隐藏状态GRU States的拼接方式控制整体模型框架的复杂程度。 3.用户数据构造 用户数据训练过程常常采用如上图这样的最小批处理。每一个Session就可以看作是一个用户，每一个i可以看作是一个商品，商品i的下标代表着商品的浏览顺序。 input中，每一行表示的为多个用户正常的浏览顺序中的起始商品，比如Session1用户浏览顺序：i1.1–&gt;i1.2–&gt;i1.3–&gt;1.4中的i1.1，i1.2，i1.3；对应的，在output中，每一行代表多个用户在input位置的下一次浏览内容，比如Session1用户浏览顺序：i1.1–&gt;i1.2，i1.2–i1.3，i1.3–&gt;i1.4中的i1.2，i1.3，i1.4；当input中的一个的用户或者说是一个的Session的点击信息被全部使用后，追加一个新的用户或者说是Session的点击信息，同时通过控制同时计算的用户或者说Session的个数，直到所有的Session信息都被使用完一遍，这样就构造完成了一个由用户的上次点击结果预测用户的下次点击结果的循环神经网络。 这样设计避免了通常的神经网络构造在用户行为上应用中的两个问题： 固定滑动窗口导致大量用户信息不能获取完整 拆分用户的浏览行为计算导致循环网络在信息理解上的歧义 4.损失函数选取 关于损失函数的选取，我们这边主要推荐两种方案：基于贝叶斯后验优化(BayesianPersonalizedRanking,BPR)和第一准则(TOP1)： 贝叶斯后验优化(BayesianPersonalizedRanking,BPR)是一个近似矩阵分解的方法，我们分别选取当前用户当前商品的下一个真实浏览的商品作为Positive Item，随机抽样的商品作为Negative Item，具体表达形如：，其中Ns即为随机采样个数，中k为i时对应Positive Item的真实计算得分，K为j时对应Negative Item，保持i=Positive Item不变，计算所有抽样出来的Negative Item作为j进行计算即可。 第一准则(TOP1)是我们自己设计的一个近似排序的方法，我们想要真实的Positive Item所计算出来的结果尽可能的接近1，Negative Item计算出来的结果尽可能的接近0，所以我们要保证采样出来的Negative Item比Positive Item在当前计算方式下的得分要低，所以，我们可以设计损失函数如下：，最末项增加了个正则项修正拟合程度。 除此之外，《SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS》中还提到了POP，S-POP，Item-KNN，BPR-MF等方法，可单独了解。 Logistics Regression部分通过Logistics Regression部分提高融合Recurrent Neural Network的潜藏层和传统的用户基础特征，进行一次重排序的操作，个性化的提供用户的注册激励也是非常重要的一环。整个Recurrent Neural Network部分在一定程度上帮助我们获取到了用户的浏览操作行为中的trend和seasonality（隐藏状态GRU States），但是缺乏考虑外部信息，比如热点爆品，用户区域，用户性别，用户需求等等。最典型的一个例子就是，我们不能向一个浏览了多个中性黑色太阳眼镜的正在试图走酷雅风格的东北女性推送潮牌男性短裤优惠券作为注册激励，很多时候会起到相反的作用。 如何兼顾Recurrent Neural Network部分的潜藏数据与为数不多的用户基础特征数据，并加以融合快速反馈出结果，是需要多方面考虑的：数据应用 数据类别 数据详解 基础用户画像 人口属性，地点，性别，消费力等 主动行为数据 品类偏好、品牌偏好、行为性别等 文本偏好数据 浏览商品文字描述特征 反馈数据 停留时长，复停留行为，当前时间段等 … … 基础用户画像&amp;主动行为数据：我们可以在用户原始日志中快速清洗出用户的地址，环境，设备等基础信息，结合用户浏览商品的性别+价格+品牌加权预估出用户的性别，消费力等价值属性，品类偏好、品牌偏好、行为性别等基础汇总属性。 文本偏好数据：根据用户的浏览商品，去匹配是否命中了我们预先提取出的注册用户浏览高频关键词，比如“鬼洗”，“典藏”等等及当前的一些热点词汇“小白鞋”，“华莱士”等。 反馈数据：在整个Recurrent Neural Network部分我们考虑的是用户的浏览顺序，但是忽略了用户的浏览质量，用户进入平台后的15s内，A商品重复浏览了3次，停留了9S，B商品重复浏览了1次，停留了1S，商品A的注册激励价值是远远高于商品B的。 通过以上的方式获取到的“用户画像”好处在于快速，再扩充了用户基本属性的同时还能够在规定的时间内完成所有的重排序计算，但是缺点在于一定程度上降低了用户特征刻画的精度。 数据融合 在实际的应用过程中，我们发现，在一定程度上交叉部分高价值的用户特征有助于提高最后的预测结果的准确性，构造的框架图如下： 这边借鉴但是没有完全采用wide&amp;deep的方法，借鉴了对原始用户特征需要通过embedding layer进行处理，比如通过简单的one hot encoding的形式，然后采取特征交叉的方式获取新的用户特征，最后再进行前向传播或者logistics regression；但是此处，在embedding layer的过程中会采取人为限制分箱逻辑去噪（剔除了比如地点归属000ex00f这样的错误数据），在交叉过程中只选取了部分对最后的用户注册影响较大的因素进行交叉，在提升了模型对用户拟合的能力的同时也保证了模型的实效性。 数据流设计 简单的数据执行流如下： 主要步骤如下： Kafka+Flume解析实时点击、搜索、浏览等用户操作日志流，在线进行用户操作数据的抽取 实时解析基础用户环境信息，获取环境特征：手机型号，网络，地址等，实时写入到线上Hadoop/Spark中的HDFS里 根据离线存储在HDFS中的用户操作数据、用户点击流数据和用户是否真实注册的结果离线更新循环网络GRU及LR模型参数 将新的模型参数应用于线上用户数据的预测 最后可得到个人及全站的注册概率变化可视化如下： 可优化方向 GRU卷积神经网络的层数优化，由多层隐藏层替代单层隐藏层提高对用户行为的汇总效果 Logistics Regression部分可以由多模型bagging替换，降低过拟合的可能 反馈数据清洗，对于有强烈意愿注册的用户进行识别，避免干扰正样本池 推荐内容干扰，那些热门爆款更多的用户看到，而且“被看到”这个行为也加深了它接下去被接连看到的可能性 GRU卷积神经网络的构造中，修改上一次操作预测下一次操作为上一次操作预测目标行为（常停留时长的点击、收藏点击等等高价值的行为节点） 总结传统的机器学习方案给用户行为预估的项目一个基准水平线，而深度学习的出现，一定程度上使得这个上限有所提高，但是以数据为基础，用算法去雕琢，只有将二者有机结合，才会带来更好的效果提升。 以上是yoho!buy团队在实践中的一点总结，当然我们还有还多事情要做，keep learning！最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>理论解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FM理论解析及应用]]></title>
    <url>%2F2017%2F12%2F04%2FFM%E7%90%86%E8%AE%BA%E8%A7%A3%E6%9E%90%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[FM的产生背景我其实没有做过很多CTR预估的事情，但是我在工作中常常遇到CRM流失预估、订单预估这些依赖于特征工程的事情，其中就涉及到特征的组合问题。 one-hot过程在feature选取过程中，不可避免的会出现，学历这种高中、大学、研究生等多分类的feature，在实际应用中，我们对单个feature需要进行一种one hot过程，就是将原来的学历拆解为 是否为高中，是否为大学，注意，可以不用加是否为研究生一列，因为是否为高中，是否为大学的两列已经可以推导这个用户是否为研究生，加上这一列有时候反而会共线性。但是这样做，看起来没什么问题，想想看要是100个这样的特征，每个特征有100个这样单独的feature value的话，整体数据将是一个非常庞大的稀疏矩阵，无论是计算还是分析都是会存在巨大的问题的，所以看看我们能不能组合一些特征降低维度。 什么叫做组合问题现在有一组数据，其中特征包含性别（男女），学历（高中，大学，研究生），想要判断这两个feature对是否对化妆品感谢兴趣。单独的观察性别这一栏，发现有一定相关性，但是比较弱，并不是所有的女性都对化妆品感兴趣；单独的观察学历这一栏也发现，学历与对化妆品感兴趣的程度并没有显著的相关性。其实，我们可以从自己的感知理解，首先，数据中女生可能比男生对化妆品更感兴趣，但是女生数据中存在大量的高中生，相对于高中生而言，大学生和研究生可能对化妆品更加感兴趣一点，所以原来的两个feature：性别，学历就组合成了是否为性别女+学历大于高中一个feature，这就是特征组合的过程。如果feature总个数少还可以，要是要有上千上万个，光两两组和就有n*(n-1)/2种可能，所以我们需要想一个其他办法。 组合特征后的表达形式首先，我们都知道一般的线性模型为： 为了考虑组合特征的作用，我们采用多项式来代表，形如特征xi与xj的组合用xixj表示，具体的表达式如下：其中，wij为组合特征xixj的权重，n表示样本的feature个数，xi为第i个feature。 方程定义完成了，下面就要开始数学定义对每一个特征xi引入辅助向量Vi=(vi1,vi2,…vik),这边的k就是矩阵拆解的规模值，利用ViVj.T对交叉项的系数wij进行估计,及则这边需要注意一点，k理论上讲，越大越能强化拟合的能力，但是实际在运算过程中，一来受限于计算能力，二来受限于数据量，过大的k只会带来过拟合的问题。我实测了40w左右的数据，观察到k值在6-8左右，valid集合数据拟合效果最优，仅供参考 很明显，上面这么多未知数：1+n是线性未知数个数，nxfeature是组合特征的未知数个数，常规求解的效率可想而知。但是看到xixj这样的形式，我们很容易联想到：2ab = (a+b)^2 -a^2 -b^2，所以在解决这个wij、xi、xj点积的问题上，我们采用了：1/2 * ( (a+b+c)^2 - a^2 - b^2 - c^2)的方式 下面让我们来解这个式子这边需要一点导数功底，我们先来看对w0也就是bias求导，这个毫无意外，梯度为1；再对wi求导，这个也很简单，xi即可，这个也很简单，少许繁琐的就是wij求导，让我来仔细看看：ok，我知道我的字很丑，别说话，看问题，所以我们可以总结为下面这个网上到处都有的式子：这个式子就是上面这么来的。把上面的那个点积形式代入求解及为： 引申一个FFM概念在FM模型中，每一个特征会对应一个隐变量，但在FFM模型中，认为应该将特征分为多个field，每个特征对应每个field分别有一个隐变量。 举个例子，我们的样本有3种类型的字段：qualifications, age, gender，分别可以代表学历，年龄段，性别。其中qualifications有3种数据，age有5种数据，gender有男女2种，经过one-hot编码以后，每个样本有7个特征，其中只有3个特征非空。如果使用FM模型，则7个特征，每个特征对应一个隐变量。如果使用FFM模型，则7个特征，每个特征对应3个隐变量，即每个类型对应一个隐变量，及对应qualifications, age, gender各占一个。 我看了Yu-Chin Juan实现了一个C++版的FFM模型的源码，倒过来想他的表达式应该是这样的：其他模块都与fm差不多，主要看Vj1f2Vj2f1这个东西。我们假设j1特征属于f1这个field，j2特征属于f2这个feild，则Vj1f2表示j1这个特征对应j2所属的field的隐变量。很恶心的解释，通俗的来讲就是，性别为女与学历这个field的组合有个隐变量，性别女与年龄这个field的组合又有一个不一样的隐变量，而却不考虑到底是什么学历是啥，年龄具体到什么细节。Yu-Chin Juan大神在实际写code的过程中，干掉来常数和一次项，可能是为了方便计算，保留的如下：整理的最优化损失函数如下：前面为l2正则，后面为交互熵形式，我们看到了y*Φ(V,x)这个及其类似hinge loss里面的1−t⋅y部分，所以注意这边的y属于{-1，1}这边的求导，我算了一个小时都没搞出来，等哪天有空了，再仔细的去算一下，去翻了原论文，最后的迭代形式如下：η是常规的速率，V是初始均匀分布即可 代码实现我这边完成了FM的代码实现，详细见我的github：fm代码为了方便不想看细节，只想撸代码的同学，我打包上传到了pypi，你只需要pip install Fsfm即可体验至于ffm，我下午实在没写出来，对不起彭老师，丢脸了，后续看什么时候有空再研究一下。 最后，着重提示，本文很多思路很解析都参考的Yu-Chin Juan的源代码，附上github地址，欢迎去关注原作者的内容，感谢大神带路，谢谢大家阅读。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征交叉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于SSD下的图像内容识别（二）]]></title>
    <url>%2F2017%2F12%2F01%2F%E5%9F%BA%E4%BA%8ESSD%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[上一节粗略的描述了如何关于图像识别，抠图，分类的理论相关，本节主要用代码，来和大家一起分析每一步骤。看完本节，希望你也能独立完成自己的图片、视频的内容实时定位。 首先，我们需要安装TensorFlow环境，建议利用conda进行安装，配置，90%尝试单独安装的人最后都挂了。 其次，我们需要安装从git上下载训练好的模型，git clone https://github.com/balancap/SSD-Tensorflow如果没有安装git的朋友，请自行百度安装。 最后找到你下载的位置进行解压，unzip ./SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt.zip这边务必注意，网上90%的教程这边就结束了，其实你这样是最后跑不通代码的，你需要把解压的文件进行移动到checkpoint的文件夹下面，这个问题git上这个同学解释了，详细的去看下https://github.com/balancap/SSD-Tensorflow/issues/150 最后的最后，下载你需要检测的网路图片，就ok了 预处理步骤完成了，下面让我们看代码。加载相关的包：123456789101112import osimport mathimport randomimport sysimport numpy as npimport tensorflow as tfimport cv2import matplotlib.pyplot as pltimport matplotlib.cm as mpcmsys.path.append('./SSD-Tensorflow/')from nets import ssd_vgg_300, ssd_common, np_methodsfrom preprocessing import ssd_vgg_preprocessing 配置相关TensorFlow环境123gpu_options = tf.GPUOptions(allow_growth=True)config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)isess = tf.InteractiveSession(config=config) 做图片的格式的处理，使他满足input的条件123456789101112131415161718192021222324#我们用的TensorFlow下的一个集成包slim，比tensor要更加轻便slim = tf.contrib.slim#训练数据中包含了一下已知的类别，也就是我们可以识别出以下的东西，不过后续我们将自己自己训练自己的模型，来识别自己想识别的东西l_VOC_CLASS = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle','bus', 'car', 'cat', 'chair', 'cow','diningTable', 'dog', 'horse', 'motorbike', 'person','pottedPlant', 'sheep', 'sofa', 'train', 'TV']# 定义数据格式net_shape = (300, 300)data_format = 'NHWC' # [Number, height, width, color]，Tensorflow backend 的格式# 预处理将输入图片大小改成 300x300，作为下一步输入img_input = tf.placeholder(tf.uint8, shape=(None, None, 3))image_pre, labels_pre, bboxes_pre, bbox_img = ssd_vgg_preprocessing.preprocess_for_eval(img_input,None,None,net_shape,data_format,resize=ssd_vgg_preprocessing.Resize.WARP_RESIZE)image_4d = tf.expand_dims(image_pre, 0) 下面我们来载入SSD作者已经搞定的模型123456789101112# 定义 SSD 模型结构reuse = True if 'ssd_net' in locals() else Nonessd_net = ssd_vgg_300.SSDNet()with slim.arg_scope(ssd_net.arg_scope(data_format=data_format)):predictions, localisations, _, _ = ssd_net.net(image_4d, is_training=False, reuse=reuse)# 导入官方给出的 SSD 模型参数#这边修改成你自己的路径ckpt_filename = '/Users/slade/SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt'isess.run(tf.global_variables_initializer())saver = tf.train.Saver()saver.restore(isess, ckpt_filename)ssd_anchors = ssd_net.anchors(net_shape) 下面让我们把SSD识别出来的结果在图片中表示出来1234567891011121314151617181920212223242526#不同类别，我们以不同的颜色表示def colors_subselect(colors, num_classes=21):dt = len(colors) // num_classessub_colors = []for i in range(num_classes):color = colors[i*dt]if isinstance(color[0], float):sub_colors.append([int(c * 255) for c in color])else:sub_colors.append([c for c in color])return sub_colors#画出在图中的位置def bboxes_draw_on_img(img, classes, scores, bboxes, colors, thickness=5):shape = img.shapefor i in range(bboxes.shape[0]):bbox = bboxes[i]color = colors[classes[i]]# Draw bounding box...p1 = (int(bbox[0] * shape[0]), int(bbox[1] * shape[1]))p2 = (int(bbox[2] * shape[0]), int(bbox[3] * shape[1]))cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)# Draw text...s = '%s:%.3f' % ( l_VOC_CLASS[int(classes[i])-1], scores[i])p1 = (p1[0]-5, p1[1])cv2.putText(img, s, p1[::-1], cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)colors_plasma = colors_subselect(mpcm.plasma.colors, num_classes=21) 让我们开始训练吧123456789101112131415def process_image(img, select_threshold=0.3, nms_threshold=.8, net_shape=(300, 300)):#先获取SSD网络的层相关的参数rimg, rpredictions, rlocalisations, rbbox_img = isess.run([image_4d, predictions, localisations, bbox_img],feed_dict=&#123;img_input: img&#125;)#获取分类结果，位置rclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(rpredictions, rlocalisations, ssd_anchors,select_threshold=select_threshold, img_shape=net_shape, num_classes=21, decode=True)rbboxes = np_methods.bboxes_clip(rbbox_img, rbboxes)rclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=400)rclasses, rscores, rbboxes = np_methods.bboxes_nms(rclasses, rscores, rbboxes, nms_threshold=nms_threshold)# 让我们在图中画出来就行了rbboxes = np_methods.bboxes_resize(rbbox_img, rbboxes)bboxes_draw_on_img(img, rclasses, rscores, rbboxes, colors_plasma, thickness=2)return img 预处理的函数都写完了，我们就可以执行了。12345#读取数据img = cv2.imread("/Users/slade/Documents/Yoho/picture_recognize/test7.jpg")img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)plt.imshow(process_image(img))plt.show() img的数据形式如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950In [8]: imgOut[8]:array([[[ 35, 59, 43],[ 37, 60, 44],[ 38, 61, 45],...,[ 73, 99, 62],[ 74, 99, 60],[ 72, 97, 57]],[[ 37, 60, 44],[ 37, 60, 44],[ 37, 60, 44],...,[ 66, 92, 57],[ 67, 93, 56],[ 67, 92, 53]],[[ 37, 60, 44],[ 36, 59, 43],[ 37, 58, 43],...,[ 56, 83, 48],[ 60, 86, 51],[ 61, 87, 50]],...,[[ 96, 101, 95],[107, 109, 104],[ 98, 97, 95],...,[ 84, 126, 76],[ 72, 118, 72],[ 78, 126, 86]],[[ 98, 103, 96],[114, 116, 111],[112, 113, 108],...,[ 94, 137, 84],[ 87, 133, 86],[105, 153, 111]],[[ 99, 105, 95],[110, 113, 106],[134, 135, 129],...,[127, 170, 116],[121, 167, 118],[131, 180, 135]]], dtype=uint8) 处理后的结果如下： 是不是非常无脑，上面的代码直接复制就可以完成。 下面在拓展一下视频的处理方式，其实相关的内容是一致的。利用moviepy.editor包里面的VideoFileClip的切片的功能，然后对每一次切片的结果进行process_image过程就可以了，这边就不贴代码了，需要的朋友私密我。 最后感谢大家阅读。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>图像识别</tag>
      </tags>
  </entry>
</search>
