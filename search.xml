<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[博客导读公告(置顶)]]></title>
      <url>/2029/01/20/%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%AF%BB%E5%85%AC%E5%91%8A/</url>
      <content type="html"><![CDATA[<p><img src="/2029/01/20/博客导读公告/wechat.png" alt=""><br><a id="more"></a></p>
<p><strong>首先，我很荣幸您出现在我的个人博客，下面请允许我花费您3分钟左右的时间，简单的为您介绍一下博客中的相关功能，这将极大的提高您在后续阅读中的体验：</strong></p>
<p>打个广告，欢迎各位老爷关注我的微信公众号：ml_trip，期待与大家交流！</p>
<h2 id="frac-1-1-我的个人介绍"><a href="#frac-1-1-我的个人介绍" class="headerlink" title="$\frac{1}{1}$我的个人介绍"></a>$\frac{1}{1}$我的个人介绍</h2><p>大家可以在首页的标题下面找到me这个图标，点击即可，里面有我的个人介绍：<br><img src="/2029/01/20/博客导读公告/个人介绍.png" alt=""></p>
<p>我的个人简历下载地址在me跳转页面中的位置如下：<br><img src="/2029/01/20/博客导读公告/cv.png" alt=""></p>
<hr>
<h2 id="frac-1-2-快速阅读"><a href="#frac-1-2-快速阅读" class="headerlink" title="$\frac{1}{2}$ 快速阅读"></a>$\frac{1}{2}$ 快速阅读</h2><p>您可在任何一篇文章的右侧看到红色方框：</p>
<ul>
<li>如果您对其中部分内容感兴趣，可直接点击绿色方框内的文章，会自动跳转到您关心的模块；</li>
<li>如果您想要看到我的<strong>更多联系方式</strong>，可以点击蓝色模块中的站点概览；</li>
<li>如果您不想红色方框影响您的阅读，在黑色条块的最下方的小叉点击即可。<br><img src="/2029/01/20/博客导读公告/read.png" alt=""></li>
</ul>
<hr>
<h2 id="frac-1-3-赞助激励"><a href="#frac-1-3-赞助激励" class="headerlink" title="$\frac{1}{3}$ 赞助激励"></a>$\frac{1}{3}$ 赞助激励</h2><p>如果您觉得我写的东西对您有一些帮助，在您宽裕的情况下可以在文章下面的打赏中给我发一个小红包，或者直接扫描下面的二维码，感谢您对我的认可：<br><img src="/2029/01/20/博客导读公告/pay.png" alt=""></p>
<p><strong>如果您还是一个学生或者您正处于人生的低谷，感谢您对我的认可，打赏就不需要了，我会一如既往的给大家整理工作中的一些想法和心得</strong></p>
<hr>
<h2 id="frac-1-4-自定义搜索"><a href="#frac-1-4-自定义搜索" class="headerlink" title="$\frac{1}{4}$ 自定义搜索"></a>$\frac{1}{4}$ 自定义搜索</h2><p>为了方便大家找到自己关心的内容，建议直接点击搜索图标：<br><img src="/2029/01/20/博客导读公告/search.png" alt=""></p>
<p>比如搜索svm，有模糊匹配结果如下：<br><img src="/2029/01/20/博客导读公告/content.png" alt=""></p>
<hr>
<h2 id="frac-1-5-标签检索"><a href="#frac-1-5-标签检索" class="headerlink" title="$\frac{1}{5}$ 标签检索"></a>$\frac{1}{5}$ 标签检索</h2><p>大家可以在首页的标题下面找到标签这个图标，点击即可：<br><img src="/2029/01/20/博客导读公告/tag.png" alt=""></p>
<p>该类别下生成了标签云，为较为仔细的文章内容概括：<br><img src="/2029/01/20/博客导读公告/cloud.png" alt=""></p>
<hr>
<h2 id="frac-1-6-其他"><a href="#frac-1-6-其他" class="headerlink" title="$\frac{1}{6}$ 其他"></a>$\frac{1}{6}$ 其他</h2><ul>
<li>因为本博客部署在GitHub，如果您有时候遇到打不开网页的问题，建议您重复刷新或者收藏<a href="https://www.jianshu.com/u/79b57248a6c3" target="_blank" rel="noopener">slade_sal简书地址</a>，两者内容是一致的</li>
<li>如果您有任何疑惑或者疑问都可以通过站点概览的邮箱联系我，非常愿意解答您的问题</li>
<li>如果您遇到算法学习过程的困难，需要内推或者就业方向建议，也可以通过站点概览的邮箱联系我，非常愿意和您进行交流</li>
<li>绝大多数代码都可以在我的Github上找到，所有的数据都经过脱敏处理，您可以放心使用，希望对您有所帮助</li>
</ul>
<p>最后，感谢大家一路以来对我的认可。</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 公告 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[19年阶段性总结]]></title>
      <url>/2019/09/15/19%E5%B9%B4%E9%98%B6%E6%AE%B5%E6%80%A7%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>这篇文章无关技术，写的目的有两个：</p>
<ul>
<li>19年已经过去了2/3，回顾一下19年到目前为止的得失，总结复盘，使得自己更好的完成19年初定的目标</li>
<li>反省一下最近几个月自己焦虑的心态，引以为戒，调整心态，避免以后再次发生这样的事情</li>
</ul>
<h1 id="半年度回顾"><a href="#半年度回顾" class="headerlink" title="半年度回顾"></a>半年度回顾</h1><h2 id="年初既定目标完成情况"><a href="#年初既定目标完成情况" class="headerlink" title="年初既定目标完成情况"></a>年初既定目标完成情况</h2><ul>
<li>leetcode：从0开始，进入10000名，持续整理的刷题笔记，产出10篇相关的心得体会<ul>
<li><em>目前13000名，距离目标还差3000名</em></li>
<li>持续进行笔记整理中<a href="https://github.com/sladesha/LeetCode" target="_blank" rel="noopener">LeetCode+校招真题分析</a></li>
<li><em>只有一篇心得体会</em>：<br>  <a href="http://www.shataowei.com/2019/09/05/DynamicProgramming动态规划整理/" target="_blank" rel="noopener">DynamicProgramming动态规划整理</a></li>
</ul>
</li>
<li>学习两门新语言<ul>
<li>java学习笔记:<a href="https://github.com/sladesha/sladeRode" target="_blank" rel="noopener">java笔记</a></li>
<li>c++学习笔记:<a href="https://github.com/sladesha/sladeRode3" target="_blank" rel="noopener">c++笔记</a></li>
</ul>
</li>
<li>整理python常用工具，真正做到高复用，高可用<ul>
<li><a href="https://github.com/sladesha/PyTls" target="_blank" rel="noopener">PyTls</a>，该包已经用作公司项目python包库，并整理出对应的<a href="http://www.shataowei.com/2019/07/29/Python自用工具包PyTls/" target="_blank" rel="noopener">使用手册</a></li>
<li>贡献了<a href="https://github.com/brennerm/PyTricks" target="_blank" rel="noopener">PyTricks</a>，理解了python的部分自建函数的实际价值，并真实运用在项目中</li>
<li>整理了若干python技巧：<ul>
<li><a href="http://www.shataowei.com/2019/07/15/Python踩坑指南（第三季）/" target="_blank" rel="noopener">Python踩坑指南（第三季）</a></li>
<li><a href="http://www.shataowei.com/2019/05/28/Python踩坑指南（第二季）/" target="_blank" rel="noopener">Python踩坑指南（第二季）</a></li>
<li><a href="http://www.shataowei.com/2019/05/14/Python踩坑指南（第一季）/" target="_blank" rel="noopener">Python踩坑指南（第一季）</a></li>
</ul>
</li>
</ul>
</li>
<li>3篇深度学习全新技术的理解：<ul>
<li><a href="http://www.shataowei.com/2019/07/01/transformer工程实现笔记/" target="_blank" rel="noopener">transform的理解</a></li>
<li><a href="https://github.com/sladesha/deep_learning/tree/master/RCNN_GRU" target="_blank" rel="noopener">RCNN</a></li>
<li><em>待完成</em></li>
</ul>
</li>
<li>10种熟悉的机器学习算法的深刻理解+源码分析：<ul>
<li>进展0</li>
</ul>
</li>
</ul>
<h2 id="得失"><a href="#得失" class="headerlink" title="得失"></a>得失</h2><p>leetcode部分是从5月份开始启动的，进展的一般，现在也逐步进入了每周3题-5题的水平，整体上来看，完成年初的目标没有多大问题。<br>得：收获还是比较大的：熟悉了基础的数据结构：比如栈堆、hash表、树、列表、队列，也学会了很多小技巧，比如dp、贪心、双指针、链表、回溯等，确实在工作中对代码质量有一定的提升；<br>失：能力还远远不够，还不能快速写出状态转移方程，遇到题目还是只会第一反应回溯，树还很生疏，各种排序还是不能拿过来1分钟之内写出来，还是需要多看多练，不够熟练。</p>
<p>语言方向上也是一直稳步提升。<br>得：Java真有意思，C++写小游戏真好玩<br>失：学习效率有点低，目前占据了大量的时间，希望后期能够优化</p>
<p>python部分<br>得：学的很nice，该做的都做了给自己点个赞<br>失：对协程还是理解不够，需要进一步加强，可以设置为2020年的既定目标</p>
<p>深度学习和机器学习：<br>得：几乎没有<br>失：思维过于狭隘，只是抱着能用就行，先上线后优化的思想，每次遇到新的任务都采取了非常保守的策略及方法，速度效果都是可以达到baseline，但是没有任何深度上的提升，这个很危险</p>
<h1 id="谈谈焦虑"><a href="#谈谈焦虑" class="headerlink" title="谈谈焦虑"></a>谈谈焦虑</h1><p>其实，这一年以来，我已经花费了绝大多数私人时间在自己的能力提升上，为此我甚至放弃了WOW怀旧服的工会邀请，但是在日常工作中还是被各种负能量影响着，每天都过的特别焦虑。</p>
<ul>
<li>各种公众号上Ai工程师三个月40万入门培训；</li>
<li>各路Hr，猎头的疯狂邀约，最高的达到了13万刀的Facebook的offer，最差的也有月薪5000的助理算法工程师的邀约；</li>
<li>身边的同事整天被各种大厂拒绝的死去活来，甚至连简历都过不了；</li>
<li>身边的各种朋友发着阿里的周年庆，拼多多的年度盛典；</li>
<li>认识的大佬一年换了三个公司最后入职阿里负责整个算法后台，也有另外的大佬因为部门裁撤连房贷都没有钱去还；</li>
<li>工作项目的各种变动，带着一个自己都不清楚明天会怎么样的团队步履蹒跚的前进着</li>
<li>生活上各种意想不到的事情的发生</li>
</ul>
<p>《摩拜创始人套现15亿：你的同龄人，正在抛弃你》、《北京，有2000万人假装在生活》、《第一批Ai工程师已经财富自由》这种垃圾文章在朋友圈，微信群，钉钉群，各种地方无孔不入，肆意蔓延，有时候我尽可能不去看不去点，但是真的是到处都是，到处都有。</p>
<p>我之前一直在想到底是什么导致我每天过的非常紧凑努力的生活中，依然认为自己很垃圾，认为自己不够尽力，认为自己很差劲；看到认识的朋友抱怨自己明明有很好的学校，很不错的背景，很强的代码能力，很不错的算法知识，但是还是一个offer都拿不到。我一直在想为什么？为什么会这样。</p>
<p>这个中秋节，我没有写什么代码，也没有看什么算法，不去关心谁跳槽去了哪拿了多高的工资，不在意哪个项目是否抛出了bug，我想我有点想通了一个道理：可能不是所有事情的增益都是线性的，因为一开始就给自己背上了沉重的包袱，当你真正开始着手的时候，你会倍感压力，你想快点去做，快点达到自己想要的，完成了就有收获。其实，不然。</p>
<p>警醒一下自己，我们都是普通人，把普通的事情做好，已经很不普通了，不可以再被周边的环境所影响。</p>
<p>你背单词时，阿拉斯加的鳕鱼正跃出水面;<br>你算数学时，太平洋彼岸的海鸥振翅掠过城市上空;<br>你晚自习时，极圈中的夜空散漫了五彩斑斓。<br>但是少年你别着急, 在你为自己未来踏踏实实地努力时，那些你感觉从来不会看到的景色,那些你觉得终生不会遇到的人， 正一步一步向你走来。</p>
<p>这次，没有链接。</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 公告 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[DynamicProgramming动态规划整理]]></title>
      <url>/2019/09/05/DynamicProgramming%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>整理一下刷题过程中的一些想法，方便以后高效复习，动态规划部分整理如下：</p>
<p>主要的思路有如下几种：</p>
<h1 id="基础类型"><a href="#基础类型" class="headerlink" title="基础类型"></a>基础类型</h1><ul>
<li><p>只需要写出i和i-1之间的状态转移方程即可，没有任何额外操作的行为，比如：</p>
<ul>
<li>一维度：<ul>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode53maximum-subarray.py" target="_blank" rel="noopener">53. 最大子序和</a>,状态方程是:<code>dp[i] = max(dp[i-1]+nums[i],nums[i])</code></li>
<li><a href="https://leetcode-cn.com/problems/climbing-stairs/s" target="_blank" rel="noopener">70. 爬楼梯</a>,状态方程是:<code>result[i]=result[i-1]+result[i-2]</code></li>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode198house-robber.py" target="_blank" rel="noopener">198. 打家劫舍</a>,状态方程是:<code>dp[i] = max(dp[i-3],dp[i-2])+nums[i]</code></li>
</ul>
</li>
<li><p>高维度：</p>
<ul>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode120triangle.py" target="_blank" rel="noopener">120. 三角形最小路径和</a>,状态方程是:<code>dp[k][z] = min((dp[k-1][z]+triangle[k][z]),(dp[k-1][z-1]+triangle[k][z]))</code></li>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/src/Solution63.java" target="_blank" rel="noopener">63. 不同路径 II</a>,状态方程是:<code>obstacleGrid[i][j] = obstacleGrid[i][j - 1] + obstacleGrid[i - 1][j]</code></li>
<li><p><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode64minimum-path-sum.py" target="_blank" rel="noopener">64. 最小路径和</a>,状态方程是:<code>mat[i][j] = min(mat[i - 1][j], mat[i][j - 1]) + grid[i][j]</code></p>
</li>
<li><p><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode221maximal-square.py" target="_blank" rel="noopener">221. 最大正方形</a>,状态方程是:<code>dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1</code></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="中间变量类型"><a href="#中间变量类型" class="headerlink" title="中间变量类型"></a>中间变量类型</h1><ul>
<li>需要额外借助<strong>第三方变量</strong>进行中间值判断的：<ul>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/src/Solution121.java" target="_blank" rel="noopener">121. 买卖股票的最佳时机</a>,状态方程是:<code>Math.max(profit, prices[i] - Math.min(minprofit, prices[i]));</code></li>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode139word-break.py" target="_blank" rel="noopener">139. 单词拆分</a>,状态方程是:<code>dp[j] = True if dp[i] and s[i:j] in wordDict</code></li>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/src/Solution264.java" target="_blank" rel="noopener">264. 丑数 II</a>,状态方程是:<code>int minval = Math.min(last_2,Math.min(last_3,last_5));</code></li>
</ul>
</li>
</ul>
<h1 id="定制判断类型"><a href="#定制判断类型" class="headerlink" title="定制判断类型"></a>定制判断类型</h1><ul>
<li>带有<strong>定制化的判断条件</strong>的：<ul>
<li><a href="https://leetcode-cn.com/problems/edit-distance/submissions/" target="_blank" rel="noopener">72. 编辑距离</a>,状态方程条件是：<code>word1[j-1]==word2[i-1]</code></li>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/src/Solution85.java" target="_blank" rel="noopener">85. 最大矩形</a>,状态方程条件是：<code>matrix[i][j] != &#39;0&#39;</code></li>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode91decode-ways.py" target="_blank" rel="noopener">91. 解码方法</a>,状态方程条件是：<code>s[i] != &quot;0&quot;和s[i - 1] != &quot;0&quot; and int(s[i - 1:i + 1]) &lt;= 26</code></li>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode152maximum-product-subarray.py" target="_blank" rel="noopener">152. 乘积最大子序列</a>,状态方程条件是：<code>MAX, MIN = MIN, MAX if nums[i] &lt; 0</code></li>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/src/Solution300.java" target="_blank" rel="noopener">300. 最长上升子序列</a>,状态方程条件是：<code>if (nums[i] &gt; nums[j]) {dp[i] = Math.max(dp[i], dp[j] + 1);}</code></li>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/src/Solution338.java" target="_blank" rel="noopener">338. 比特位计数</a>,状态方程条件是：<code>if (i%2==1){ans[i] = ans[i-1]+1;} else {ans[i] = ans[i/2];}</code></li>
</ul>
</li>
</ul>
<h1 id="倒叙逆向类型"><a href="#倒叙逆向类型" class="headerlink" title="倒叙逆向类型"></a>倒叙逆向类型</h1><ul>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode174dungeon-game.py" target="_blank" rel="noopener">174. 地下城游戏</a>,状态转移方程是:<code>dp[i][j] = max(1, min(dp[i + 1][j], dp[i][j + 1]) - dungeon[i][j])</code><br>反方向的的递归，比较打破常规思维</li>
</ul>
<h1 id="循环判断类型"><a href="#循环判断类型" class="headerlink" title="循环判断类型"></a>循环判断类型</h1><ul>
<li>在dp的基础上需要再遍历已经扫过的数组。此类方法的复杂度一般较高:<ul>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode279perfect-squares.py" target="_blank" rel="noopener">279. 完全平方数</a>,状态转移方程是：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while i - j ** 2 &gt;= 0:</span><br><span class="line">    dp[i] = min(dp[i], dp[i - j ** 2] + 1)</span><br><span class="line">    j += 1</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h1 id="预处理类型"><a href="#预处理类型" class="headerlink" title="预处理类型"></a>预处理类型</h1><ul>
<li><a href="https://github.com/sladesha/LeetCode/blob/master/LeetCode/LeetCode740delete-and-earn.py" target="_blank" rel="noopener">740. 删除与获得点数</a><br>这题麻烦在预处理每一个值，因为值有可能重复：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">target = [0] * (max(nums) + 1)</span><br><span class="line">for num in nums:</span><br><span class="line">    target[num] += num</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>之后的转移方程反而非常简单:<code>d[i] = max(d[i - 1], d[i - 2] + target[i])</code></p>
<h1 id="技巧类型"><a href="#技巧类型" class="headerlink" title="技巧类型"></a>技巧类型</h1><ul>
<li><a href="https://leetcode-cn.com/problems/distinct-subsequences-ii/" target="_blank" rel="noopener">940. 不同的子序列 II</a></li>
</ul>
<p>这题在于要想到把字符转化对应的ASCII码，转移方程:<code>array[ord(c)-ord(&quot;a&quot;)]=sum(array)+1</code>，除此之外还需要想到新增一个字母等于当前所有字母后面可加+单独自身，这个方程确实非常非常有技巧</p>
<hr>
<p>以上为LeetCode中比较有代表性的一些题目，把这些搞懂之后可以解决绝大多数dp的类似题型，以2020校招为例：</p>
<p><strong>携程 - Deep learning Engineer</strong></p>
<blockquote>
<p>给定一个时刻表，根据目的地进行分组，同一个目的地必须尽可能的多分，不能打乱顺序，比如aabbcddc，需要分成aa|bb|cddc,而不能分成aabb|cddc，因为这种情况下不是最多，也不能分成aa|bb|c|dd|c,因为相同的c没有被分在一起；求分组个数；</p>
</blockquote>
<p>这题就是典型的循环判断类型+中间变量类型，需要储存历史遍历结果并多次查询，<a href="https://github.com/sladesha/LeetCode/blob/master/random/%E6%95%B0%E7%BB%84%E5%88%86%E7%BB%84%E9%97%AE%E9%A2%98.py" target="_blank" rel="noopener">代码解答</a></p>
<p><strong>Tencent - Deep learning Engineer</strong><br><a href="http://codeforces.com/contest/474/problem/D" target="_blank" rel="noopener">花匠摆花问题</a>，两个坑：<br>1.用traceback的就直接gg，比如我<br>2.dp[i]为当前的长度为i的可摆放个数，dp[i] = dp[i-1]+dp[i-k]为状态转移矩阵，更多解释看代码注释</p>
<p>这题就是简单的基础类型，唯一难的地方是要想到i-k这个dp结果</p>
<p>Dynamic Programming 动态规划就给大家整理到这，如果大家想看更多的数据结构问题，可以关注一下<a href="https://github.com/sladesha/LeetCode" target="_blank" rel="noopener">我的Github</a>，希望有所帮助。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过邮箱发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python自用工具包PyTls]]></title>
      <url>/2019/07/29/Python%E8%87%AA%E7%94%A8%E5%B7%A5%E5%85%B7%E5%8C%85PyTls/</url>
      <content type="html"><![CDATA[<p>我们搞了个python的工具包<a href="https://github.com/sladesha/PyTls" target="_blank" rel="noopener">PyTls</a>。</p>
<p>做这件事的初衷是发生了一个星期要用python同时开发3个项目的情况，我发现了两个现象：1.有很多定制化的需求是极度高频反复重写的；2.有很多功能之前写过，可能因为稍许复杂又忘了，再用的时候又要去Google。所以，拉着同组的一个实习生，一起维护了PyTls的这个项目，为的就是那句”life is short, we need python”。</p>
<p>所有的详细的测试demo都可以在我的git找到，<a href="https://github.com/sladesha/PyTls/blob/master/demos.py" target="_blank" rel="noopener">PyTls的测试demos</a> 希望可以帮助到你一些，以下简单的介绍解释一下每个功能。</p>
<hr>
<h1 id="dictt"><a href="#dictt" class="headerlink" title="dictt"></a>dictt</h1><h2 id="get-map-value"><a href="#get-map-value" class="headerlink" title="get_map_value()"></a>get_map_value()</h2><pre><code>由于工程设计问题，我们的数据流中存在多层的字典数据格式：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [7]: msg = &#123;&apos;time&apos;:&#123;&apos;20190714&apos;:234,&apos;20190715&apos;:311&#125;&#125;</span><br><span class="line">In [8]: dictt.get_map_value(msg,None,True,&apos;time&apos;,&apos;20190714&apos;)</span><br><span class="line">Out[8]: 234</span><br></pre></td></tr></table></figure>

这边None是查找不到的时候的默认值，True可以不用管，是我这边设计的是否一定要底层查找，比较业务化通用性不好，True之后就是依次需要按逻辑查找的dict字段。
</code></pre><h2 id="update-map-value"><a href="#update-map-value" class="headerlink" title="update_map_value()"></a>update_map_value()</h2><pre><code>is_strict：True不允许新增kv，这边不会报错，只会增加不成功，需要注意一下。
</code></pre><h2 id="sort-map-key"><a href="#sort-map-key" class="headerlink" title="sort_map_key()"></a>sort_map_key()</h2><pre><code>按key排序字典，desc：是否降序
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [24]: msg = &#123;11: 2, 2: 3, 9: 2&#125;</span><br><span class="line">    ...: print(dictt.sort_map_key(msg, desc=False))</span><br><span class="line">[(2, 3), (9, 2), (11, 2)]</span><br></pre></td></tr></table></figure>
</code></pre><h2 id="sort-map-value"><a href="#sort-map-value" class="headerlink" title="sort_map_value()"></a>sort_map_value()</h2><pre><code>按value排序，其他同上
</code></pre><h2 id="get-tree"><a href="#get-tree" class="headerlink" title="get_tree()"></a>get_tree()</h2><pre><code>构建一个以字典为基础的树结构，字典套字典
</code></pre><h2 id="swap"><a href="#swap" class="headerlink" title="swap()"></a>swap()</h2><pre><code>key,value交换
</code></pre><h2 id="merge"><a href="#merge" class="headerlink" title="merge()"></a>merge()</h2><pre><code>合并两个dict,顺便说一句，reduce(merge,*dicts)，合并N个，这个比较少用就没整理
</code></pre><h2 id="func-dict"><a href="#func-dict" class="headerlink" title="func_dict()"></a>func_dict()</h2><pre><code>这个功能比较拗口，比如我们现在有个function，input 2，output 4，且想存储所有的kv结果，所以可以用这个功能进行记录。注意，这边的入参应该是一个**函数**。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">F = dictt.func_dict(lambda x: x * 2)</span><br><span class="line">print((F[2], F))</span><br><span class="line"># (4, defaultdict(&lt;function &lt;lambda&gt; at 0x103d812f0&gt;, &#123;2: 4&#125;))</span><br></pre></td></tr></table></figure>
</code></pre><h2 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount()"></a>WordCount()</h2><pre><code>字典树，快速查询和高效存储，支持string和list/tuple；支持计数、查找、位置校验三个核心功能
目前只output了是否存在查询词的功能，稍微改一下output就可以得到计数结果和位置结果，看有需要后期扩展一下这个功能，要说的是里面用了dfs，有点慢所以加了cache，这个点大家可以自行优化。
</code></pre><hr>
<h1 id="StrBuffer"><a href="#StrBuffer" class="headerlink" title="StrBuffer"></a>StrBuffer</h1><p>参考java中的StringButter而写的字符类，增加一些功能，重命名了我比较熟悉喜欢用的函数名，加了一些str内置没有的功能</p>
<h2 id="append"><a href="#append" class="headerlink" title="append()"></a>append()</h2><pre><code>增加一个字符
</code></pre><h2 id="index-at"><a href="#index-at" class="headerlink" title="index_at()"></a>index_at()</h2><pre><code>查找某个元素的index
</code></pre><h2 id="sort"><a href="#sort" class="headerlink" title="sort()"></a>sort()</h2><pre><code>排序元素，字典序
</code></pre><h2 id="reverse"><a href="#reverse" class="headerlink" title="reverse()"></a>reverse()</h2><pre><code>倒排所有字符
</code></pre><h2 id="char-at"><a href="#char-at" class="headerlink" title="char_at()"></a>char_at()</h2><pre><code>查找某个index的元素
</code></pre><h2 id="to-str-storge"><a href="#to-str-storge" class="headerlink" title="to_str()/storge()"></a>to_str()/storge()</h2><pre><code>to_str:以string的方式打印当前字符串
storge:以list的方式打印当前字符串
</code></pre><hr>
<h1 id="strt"><a href="#strt" class="headerlink" title="strt"></a>strt</h1><h2 id="str-reverse"><a href="#str-reverse" class="headerlink" title="str_reverse()"></a>str_reverse()</h2><pre><code>字符串反转
</code></pre><h2 id="str-repeat"><a href="#str-repeat" class="headerlink" title="str_repeat()"></a>str_repeat()</h2><pre><code>字符串快速复制，n为复制的次数
</code></pre><h2 id="str-splits"><a href="#str-splits" class="headerlink" title="str_splits()"></a>str_splits()</h2><pre><code>字符串批切割，split_chars为分隔符，支持多个，以|连接，warning_info可以设置为False进行关闭，取消提示信息
</code></pre><hr>
<h1 id="typet"><a href="#typet" class="headerlink" title="typet"></a>typet</h1><p>基础的格式判断类，复用程度不高，主要是用来支持这个项目中的其他功能</p>
<h2 id="is-none"><a href="#is-none" class="headerlink" title="is_none()"></a>is_none()</h2><pre><code>是否为None，支持None和np.nan判断
</code></pre><h2 id="is-type"><a href="#is-type" class="headerlink" title="is_type()"></a>is_type()</h2><pre><code>判断是否为对应类型，类似于isinstance
</code></pre><h2 id="is-empty"><a href="#is-empty" class="headerlink" title="is_empty()"></a>is_empty()</h2><pre><code>判断是否为空
</code></pre><h2 id="is-has-attr"><a href="#is-has-attr" class="headerlink" title="is_has_attr()"></a>is_has_attr()</h2><pre><code>判断是否含有某个属性，类中是否有某个属性变量
</code></pre><hr>
<h1 id="loaddatat"><a href="#loaddatat" class="headerlink" title="loaddatat"></a>loaddatat</h1><p>pickle 的数据持久化，这个我用的最多，特别好用，可以支持在保持模型，整理数据，保存日志，存储报错信息等各个方向，我基本上每次必加载</p>
<h2 id="readbunchobj"><a href="#readbunchobj" class="headerlink" title="readbunchobj()"></a>readbunchobj()</h2><pre><code>读数据
</code></pre><h2 id="writebunchobj"><a href="#writebunchobj" class="headerlink" title="writebunchobj()"></a>writebunchobj()</h2><pre><code>存数据
</code></pre><hr>
<h1 id="randomt"><a href="#randomt" class="headerlink" title="randomt"></a>randomt</h1><p>随机数</p>
<h2 id="get-random"><a href="#get-random" class="headerlink" title="get_random()"></a>get_random()</h2><pre><code>迭代器办，每次随机取一个;min_value, max_value限制范围，limit限制可取的迭代次数
</code></pre><hr>
<h1 id="Chinese2num-py"><a href="#Chinese2num-py" class="headerlink" title="Chinese2num.py"></a>Chinese2num.py</h1><p>数字相关，提取数字更加强大的功能建议参考YMMNlpUtils</p>
<h2 id="Chinese-2-num"><a href="#Chinese-2-num" class="headerlink" title="Chinese_2_num()"></a>Chinese_2_num()</h2><pre><code>中文转成数字，支持繁体中文简体中文的转换，如果是提取手机号的或者更复杂的数据建议参考YMMNlpUtils项目
</code></pre><h2 id="isdigit"><a href="#isdigit" class="headerlink" title="isdigit()"></a>isdigit()</h2><pre><code>判断是否为数字，同样也支持小数，也可以识别str后的数字，会自动解析掉str，&quot;0.234&quot;--&gt;True
</code></pre><hr>
<h1 id="matht"><a href="#matht" class="headerlink" title="matht"></a>matht</h1><p>数学相关的类，包括信息熵，ln，互信息，条件概率等;<strong>部分熵计算的函数中有个expiation的参数，设置为False就不提醒是否越大越稳定还是不稳定的提示信息了</strong>，主要是我老忘，自己提醒自己的</p>
<h2 id="ln"><a href="#ln" class="headerlink" title="ln()"></a>ln()</h2><pre><code>以e为底的对数
</code></pre><h2 id="entropy"><a href="#entropy" class="headerlink" title="entropy()"></a>entropy()</h2><pre><code>熵值，这边分两种情况，type=&quot;list&quot;的时候input为每种情况发生的结果，比如[1,1,2,3,4,2];type=&quot;prob&quot;的时候input为每种情况发生的概率，比如[1/3,1/3,1/6,1/6]
</code></pre><h2 id="condition-entropy"><a href="#condition-entropy" class="headerlink" title="condition_entropy()"></a>condition_entropy()</h2><pre><code>条件熵，求和 H（X|Y）= Σ p(Y=yi)*H（X|Y=yi)
</code></pre><h2 id="MI"><a href="#MI" class="headerlink" title="MI()"></a>MI()</h2><pre><code>来自于条件概率计算法：H(x)-H(x/y)，这边input默认的是每种情况发生的结果而非概率，比如：[1,0,1,0],[2,3,2,3]
</code></pre><h2 id="NMI"><a href="#NMI" class="headerlink" title="NMI()"></a>NMI()</h2><pre><code>来自于公式的互信息熵计算：2*∑pxylog(pxy/(px*py))/(H(x)+H(y))，这边input默认的是每种情况发生的结果而非概率，比如：[1,0,1,0],[2,3,2,3]
</code></pre><h2 id="word-edit-distince"><a href="#word-edit-distince" class="headerlink" title="word_edit_distince()"></a>word_edit_distince()</h2><pre><code>文本编辑距离，衡量两个字符串之间的相似度
</code></pre><h2 id="BM25"><a href="#BM25" class="headerlink" title="BM25"></a>BM25</h2><pre><code>在文档包含查询词的情况下，或者说查询词精确命中文档的前提下，计算相似度，对内容进行排序
</code></pre><h2 id="relative-entropy"><a href="#relative-entropy" class="headerlink" title="relative_entropy"></a>relative_entropy</h2><pre><code>相对熵，也叫KL散度，H(p||q) = ∑pxlog(px/py),如果px与py分布一致，则return 0，差异越大return的值越大;H(p||q) = H(p,q) - H(p)
</code></pre><h2 id="cross-entropy"><a href="#cross-entropy" class="headerlink" title="cross_entropy"></a>cross_entropy</h2><pre><code>交叉熵，H(p,q) = -∑pi*log(qi) , H(p||q) = H(p,q) - H(p)
</code></pre><h2 id="JSD"><a href="#JSD" class="headerlink" title="JSD"></a>JSD</h2><pre><code>衡量两个多项分布的距离，衡量两个多项分布的相似度
</code></pre><h2 id="Hellinger-Distince"><a href="#Hellinger-Distince" class="headerlink" title="Hellinger_Distince"></a>Hellinger_Distince</h2><pre><code>海林格距离，用来衡量概率分布之间的相似性
</code></pre><hr>
<h1 id="listt"><a href="#listt" class="headerlink" title="listt"></a>listt</h1><p>list相关的处理，list是我最常用的一个type，所以我写了比较多而且以后也会最常去更新的一个地方，确实很多东西固化下来很好用;该部分代码尽可能考虑了时间复杂度，可能会牺牲一些空间复杂度，各位自行取舍</p>
<h2 id="index-hash-map"><a href="#index-hash-map" class="headerlink" title="index_hash_map()"></a>index_hash_map()</h2><pre><code>list元素出现位置，等同于numpy array中的`np.where`
</code></pre><h2 id="Pi"><a href="#Pi" class="headerlink" title="Pi()"></a>Pi()</h2><pre><code>list元素出现的个数分布
</code></pre><h2 id="single-one"><a href="#single-one" class="headerlink" title="single_one()"></a>single_one()</h2><pre><code>当数据是**成对出现**时候，找出其中落单的element；[2,2,1]--&gt;[1]
</code></pre><h2 id="subset"><a href="#subset" class="headerlink" title="subset()"></a>subset()</h2><pre><code>找出对应list的子集;[1,2]--&gt;[[],[1],[2],[1,2]]
</code></pre><h2 id="permute"><a href="#permute" class="headerlink" title="permute()"></a>permute()</h2><pre><code>找出对应list的全排列;[1,2]--&gt;[1,2],[2,1]
</code></pre><h2 id="flatten"><a href="#flatten" class="headerlink" title="flatten()"></a>flatten()</h2><pre><code>高维列表展开;[[1,2,3],[3,[4]]]--&gt; [1, 2, 3, 3, 4]
</code></pre><h2 id="duplicates"><a href="#duplicates" class="headerlink" title="duplicates()"></a>duplicates()</h2><pre><code>原序去重；[3,3,1,2] -- &gt;[3,1,2]
</code></pre><h2 id="topn"><a href="#topn" class="headerlink" title="topn()"></a>topn()</h2><pre><code>高频统计，返回list中高频出现的topn的对象，n为需要返回的个数；list:[3,3,1,2],n=1--&gt;[(3,2)]
</code></pre><h2 id="getindex"><a href="#getindex" class="headerlink" title="getindex()"></a>getindex()</h2><pre><code>返回list中最大/最小元素的位置:[3,1,2],flag=&quot;max&quot;--&gt;0/[3,1,2],flag=&quot;min&quot;--&gt;1;这边参数flag记得要小写min/max，大写暂不识别
</code></pre><h2 id="split"><a href="#split" class="headerlink" title="split()"></a>split()</h2><pre><code>list按照指定个数切分，比如split([1,2,3,4],3)--&gt;[(1,2,3)]；如果list%需要切分的长度!=0，则末端部分会被舍弃，需要注意一下；如果确实需要的话可以进行尾部以0填充，保证不丢失数据；
</code></pre><h2 id="unzip"><a href="#unzip" class="headerlink" title="unzip()"></a>unzip()</h2><pre><code>把zip后的数据还原
</code></pre><hr>
<h1 id="trickt"><a href="#trickt" class="headerlink" title="trickt"></a>trickt</h1><p>日常使用的一些trick，可能不好归纳为当前的分类中的某一类，所以暂时先放在trickt包中，待认领</p>
<h2 id="choose-method"><a href="#choose-method" class="headerlink" title="choose_method()"></a>choose_method()</h2><pre><code>根据condition不同的情况下，判断使用哪个函数
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def add(a, b):</span><br><span class="line">   	return a + b</span><br><span class="line">def subtract(a, b):</span><br><span class="line">    return a - b</span><br><span class="line">param1 = 2</span><br><span class="line">param2 = 1</span><br><span class="line">choose_method(add, subtract, param1 &gt; param2, param1, param2)</span><br><span class="line">choose_method(add, subtract, param1 &lt; param2, param1, param2)</span><br></pre></td></tr></table></figure>
</code></pre><h2 id="Timer"><a href="#Timer" class="headerlink" title="Timer()"></a>Timer()</h2><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [42]: with Timer():</span><br><span class="line">   ...:     time.sleep(2)</span><br><span class="line">   ...:</span><br><span class="line">total time is 2.0000739097595215 second.</span><br></pre></td></tr></table></figure>
</code></pre><p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转&gt;行疑问都欢迎通过邮箱发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 开源项目 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python踩坑指南（第三季）]]></title>
      <url>/2019/07/15/Python%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%88%E7%AC%AC%E4%B8%89%E5%AD%A3%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>在代码维护的过程中，发现需要对某个变量进行保护，不允许调用修改，之前我一直使用的是_x或者__x的形式，这样做只是避免展示，但是如果想要强行修改，还是可以变更的，为了保证高安全性，这边可以参考@property的形式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class test(object):</span><br><span class="line">	&quot;&quot;&quot;docstring for test&quot;&quot;&quot;</span><br><span class="line">	def __init__(self, arg):</span><br><span class="line">		self._arg = arg</span><br><span class="line">	</span><br><span class="line">	@property</span><br><span class="line">	def arg(self):</span><br><span class="line">	    return self._arg</span><br><span class="line">	</span><br><span class="line">	@arg.setter</span><br><span class="line">	def set_arg(self,val):</span><br><span class="line">		self._arg=val</span><br></pre></td></tr></table></figure>
<p>当想修改arg的值的时候，必须通过set_arg去修改，虽然仍然可以通过改_arg的方式，但是相对直接暴露多了一层保护；除此之外，它还可以实现一定的逻辑转化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">In [10]: class test(object):</span><br><span class="line">    ...: 	&quot;&quot;&quot;docstring for test&quot;&quot;&quot;</span><br><span class="line">    ...: 	def __init__(self, arg):</span><br><span class="line">    ...: 		self._arg = arg</span><br><span class="line">    ...: 		self._half_arg = arg/2</span><br><span class="line">    ...: </span><br><span class="line">    ...: 	@property</span><br><span class="line">    ...: 	def arg(self):</span><br><span class="line">    ...: 	    return self._arg</span><br><span class="line">    ...:</span><br><span class="line">    ...: 	@property</span><br><span class="line">    ...: 	def half_arg(self):</span><br><span class="line">    ...: 	    return self._half_arg</span><br><span class="line">    ...: </span><br><span class="line">    ...: 	@arg.setter</span><br><span class="line">    ...: 	def set_arg(self,val):</span><br><span class="line">    ...: 		self._arg=val</span><br><span class="line">    ...: 		self._half_arg = val/2</span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [11]: t=test(10)</span><br><span class="line"></span><br><span class="line">In [12]: t.arg</span><br><span class="line">Out[12]: 10</span><br><span class="line"></span><br><span class="line">In [13]: t.half_arg</span><br><span class="line">Out[13]: 5.0</span><br><span class="line"></span><br><span class="line">In [14]: t.set_arg=20</span><br><span class="line"></span><br><span class="line">In [15]: t.arg</span><br><span class="line">Out[15]: 20</span><br><span class="line"></span><br><span class="line">In [16]: t.half_arg</span><br><span class="line">Out[16]: 10.0</span><br></pre></td></tr></table></figure>
<hr>
<p>利用None!=None的性质可以进行快速去None，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = df[df.label_y==df.label_y]</span><br></pre></td></tr></table></figure>
<p>可以快速的把label_y=None的列进行剔除，比dropna要快一些。</p>
<hr>
<p>traceback可以很好的把错误信息进行暴露，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">try:</span><br><span class="line">     1/0</span><br><span class="line">except Exception,e:</span><br><span class="line">    print e</span><br></pre></td></tr></table></figure></p>
<p>如果这样包起来错误的花,输出结果是integer division or modulo by zero，只知道是报了这个错，但是却不知道在哪个文件哪个函数哪一行报的错</p>
<p>如果使用traceback:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">try:</span><br><span class="line">     1/0</span><br><span class="line">except Exception  as e:</span><br><span class="line">    traceback.print_exc()</span><br></pre></td></tr></table></figure>
<p>输出结果是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last): </span><br><span class="line"></span><br><span class="line">File “test_traceback.py”, line 3, in  </span><br><span class="line"></span><br><span class="line">1/0 </span><br><span class="line"></span><br><span class="line">ZeroDivisionError: integer division or modulo by zero</span><br></pre></td></tr></table></figure></p>
<p>这个还不是精髓的，如果说在一个服务中，或者大型项目中，我们不会时时刻刻去盯着进程，还可以进行落日志，方便后续查看：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">traceback.print_exc(file=open(&apos;tb.txt&apos;,&apos;w+&apos;)) </span><br><span class="line">import time</span><br><span class="line">try:</span><br><span class="line">     1/0</span><br><span class="line">except Exception  as e:</span><br><span class="line">    traceback.print_exc(file=open(str(time.time())+&apos;.txt&apos;,&apos;w+&apos;))</span><br></pre></td></tr></table></figure></p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过邮箱发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[transformer工程实现笔记]]></title>
      <url>/2019/07/01/transformer%E5%B7%A5%E7%A8%8B%E5%AE%9E%E7%8E%B0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>上线形式：<br>tensorflow直接加载上线<br>基于openblas重新实现<br>基于cublas库重新实现<br>优点：<br>tensorflow：有谷歌开源代码，不需要另外实现<br>cublas/openblas：可以定制化优化，耗时更短，可微调<br>缺点：<br>tensorflow：耗时长，内部黑盒无法微调<br>cublas/openblas：部分核心功能要重写，只支持基础包</p>
<hr>
<p>以下为工程实现细节：</p>
<p>input/output为序列：<br><img src="/2019/07/01/transformer工程实现笔记/1.png" alt=""></p>
<p>基础架构：<br><img src="/2019/07/01/transformer工程实现笔记/2.png" alt=""></p>
<p>每个Encoders中分别由6个Encoder组成（论文中是这样配置的）。而每个Decoders中同样也是由6个Decoder组成：<br><img src="/2019/07/01/transformer工程实现笔记/3.png" alt=""></p>
<p><code>假设现在存在一个输出vocabulary为32768，输入vocabulary53000</code><br>流程：</p>
<ul>
<li>切词/embedding<ul>
<li>0位置不可用，1位置为EOS，作为空格等其他非文本符号的替换</li>
<li>输入词embedding 将词转化为下标 通过映射来查找</li>
<li>输入词embedding维度：1024float</li>
<li>输入词embedding初始化结果来源：w2v</li>
<li>输入词位置信息embedding：左半部分的值是一由一个正弦函数生成的，而右半部分是由另一个函数（余弦）生成。其中，<a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py" target="_blank" rel="noopener">get_timing_signal_1d()源码地址</a><br><img src="/2019/07/01/transformer工程实现笔记/4.png" alt=""></li>
<li>input Embedding = word Embedding + position Embedding</li>
</ul>
</li>
<li>Encode（假设每个batch有15个input词）<ul>
<li>multi-headed attention<ul>
<li>self-attention<ul>
<li>创建一个Query/Key/Value的矩阵，单个矩阵size：1024(16(头数量)x64(q,k,v向量的长度))x1024(词向量的长度)</li>
<li>Query/Key/Value的矩阵合并后为15x3x1024x1024</li>
<li>将query向量和key向量点击来对相应的单词打分</li>
<li>再将得到的输出通过softmax函数标准化，使得最后的列表和为1</li>
<li>将每个Value向量乘以softmax后的得分。这里实际上的意义在于保存对当前词的关注度不变的情况下，降低对不相关词的关注。</li>
<li>QK的点击为15x64x64x15，结果15x15，这样的矩阵有16个，再对此进行sotfmax压缩</li>
<li>如果batch小于15，则填充负无穷</li>
<li>KQ的15x15x16再点积V=XxWv15x64x16，16个15x64的矩阵</li>
<li>KQxV得16x(15x15x15x64)=16x15x64</li>
<li>后接一个dense，1024x1024，得到15x1024</li>
<li>add一个bias，再normalization</li>
</ul>
</li>
<li>resudual feed forward neural network<ul>
<li>multi-headed attention前的矩阵设为in</li>
<li>multi-headed attention后的矩阵设为out</li>
<li>残差处理：Z = sqrt(∑(out-in)xx2)</li>
<li>第一层的激活函数是ReLU，第二层是一个线性变换，W1和W2分别为1024x4096，4096x1024<br><img src="/2019/07/01/transformer工程实现笔记/5.png" alt=""></li>
<li>encode out = resudual feed forward neural network in + resudual feed forward neural network out</li>
</ul>
</li>
</ul>
</li>
<li>Encoder结束</li>
</ul>
</li>
<li>Decode过程与Encode过程保持一致，不同点如下：<ul>
<li>Decode出来靠近的词更重要，则把对应的input的矩阵的上三角矩阵进行mask，保证尽量减少对已经decode出来的词的影响</li>
<li>self-attention – &gt; resudual —&gt; in/out attention —&gt; resudual —&gt; FFN —&gt; resudual，一次解码完成</li>
<li>每次计算Decode完成后把decode出来的词进行储存，下一次的input直接拼接之前m次的带入下一次decode即可</li>
<li>参数矩阵：KV来自encode部分计算好的（1024x2048），Q来自decode部分新建的（1024x1024）</li>
<li>Decode:Q:nx16x64,K/V:15x16x64,<br>QK:16x(nx64)x(15x64)= 16xnx15<br>QKxV=16x(nx15x15x64)=nx1024，n即为decode得词数</li>
<li>Q的行数是Decode的词的数量，第一轮是1，第二轮是2…，解多少个词就有多少轮</li>
<li>Decode出来的结果矩阵点乘vocabulary_sizex1024，得出vocabulary_size的向量后softmax（所有vocabulary_size和为1），得到top8</li>
<li>beam search，beam=4，一直算到eosid；如果下一个就是eosid的话，用top8其他的其他4个</li>
</ul>
</li>
<li>trick： <ul>
<li>Embedding采取半float，绝大多数都是0，所以不会对最后的结果有很大影响</li>
<li>出现eosid及finished的时候，记一次综合score，当后续继续往下解beam search的时候score不如之前的话就以综合score出现的地方为截止点</li>
<li>train的过程中，input_size+50为最长decode长度，降低学习耗时</li>
</ul>
</li>
</ul>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过邮箱发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[语音转译文本后的意图识别(YMMNlpUtils)]]></title>
      <url>/2019/06/25/%E8%AF%AD%E9%9F%B3%E8%BD%AC%E8%AF%91%E6%96%87%E6%9C%AC%E5%90%8E%E7%9A%84%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB-YMMNlpUtils/</url>
      <content type="html"><![CDATA[<p>上个月由于业务需要，定制化了一个中文语境下的手机号码识别的功能，并开源为<strong>YMMNlpUtils</strong></p>
<ul>
<li><a href="http://www.shataowei.com/2019/05/13/中文语境下的手机号识别/" target="_blank" rel="noopener">DEMO解析</a></li>
<li><a href="https://github.com/sladesha/machine_learning/tree/master/YMMNlpUtils" target="_blank" rel="noopener">Github地址</a></li>
</ul>
<p>现在由于业务需求，又新增了一个语音对话过程中是否存在手机号交换行为意图的识别，所以更新了一个版本YMMNlpUtils==0.1.1</p>
<p>实际拿来用的数据比想象中的要更加混乱，主要是由于我们的用户方言很重且经过了一轮科大讯飞语音转文本的信息转译，所以不少信息丢失，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">你等会让我jj#等会儿。是名额的香车翻起来！好，你说6.2。有三，有牛有。U0150508。6050508。50568号。</span><br></pre></td></tr></table></figure>
<p>我们设计的算法流程如下：<br><img src="/2019/06/25/语音转译文本后的意图识别-YMMNlpUtils/1.png" alt=""></p>
<p>内容解释如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">我们认为语音文本中存在手机号为正样本，</span><br><span class="line">text training data：基础本文信息</span><br><span class="line">text features：本wiki中整理出来的features</span><br><span class="line">P-Learn（全量）：正样本</span><br><span class="line">N-Learn（采样）：黄色背景为纳入计算的采样负样本，蓝色背景为未纳入计算的采样负样本</span><br><span class="line">outliers：去异常点，采取了概率分布越界原则</span><br><span class="line">OneHotEncoder：离散化</span><br><span class="line">standardize：标准化</span><br><span class="line">1-3_grams：朴素贝叶斯+2-grams+3-grams</span><br><span class="line">Predict1_4：为concat形式[predict1,predict2,predict3,predict4]</span><br></pre></td></tr></table></figure>
<p>实际线上效果如下：<br><img src="/2019/06/25/语音转译文本后的意图识别-YMMNlpUtils/2.png" alt=""></p>
<p>使用Demo如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from YMMNlpUtils import YMMPhoneDistinguish</span><br><span class="line">obj = YMMPhoneDistinguish(show_reason=False, user_dict=None, stop_words=None)</span><br><span class="line">#:param show_reason:是否需要展示被识别出来的原因</span><br><span class="line">#:param user_dict:用户自定义词典，默认调用自带词典</span><br><span class="line">#:param stop_words:自定义停顿词</span><br><span class="line">In [8]: obj.predict(&quot;你等会让我jj#等会儿。是名额的香车翻起来！好，你说6.2。有</span><br><span class="line">   ...: 三，有牛有。U0150508。6050508。50568号。&quot;,0.92)</span><br><span class="line">Out[8]: (0.9527009149362651, 1)</span><br><span class="line"># 0.92为默认阈值，可以自行修改</span><br></pre></td></tr></table></figure></p>
<p>Reason解析如下：<br><code>-1:&quot;无原因&quot;, 0: &quot;逻辑拼接&quot;, 1: &quot;命中敏感词&quot;, 2: &quot;疑似电话数字&quot;, 3: &quot;数字过长&quot;, 4: &quot;涉及微信号码敏感&quot;</code></p>
<p>阈值调整建议如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阈值</th>
<th style="text-align:center">精确率</th>
<th style="text-align:center">召回率</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.5</td>
<td style="text-align:center">99.40%</td>
<td style="text-align:center">95.20%</td>
</tr>
<tr>
<td>0.6</td>
<td style="text-align:center">99.50%</td>
<td style="text-align:center">94.50%</td>
</tr>
<tr>
<td>0.7</td>
<td style="text-align:center">99.60%</td>
<td style="text-align:center">93.30%</td>
</tr>
<tr>
<td>0.8</td>
<td style="text-align:center">99.60%</td>
<td style="text-align:center">90.80%</td>
</tr>
<tr>
<td>0.9</td>
<td style="text-align:center">99.70%</td>
<td style="text-align:center">83.60%</td>
</tr>
<tr>
<td>0.91</td>
<td style="text-align:center">99.70%</td>
<td style="text-align:center">82.40%</td>
</tr>
<tr>
<td>0.92</td>
<td style="text-align:center">99.70%</td>
<td style="text-align:center">80.60%</td>
</tr>
<tr>
<td>0.93</td>
<td style="text-align:center">99.80%</td>
<td style="text-align:center">78.00%</td>
</tr>
<tr>
<td>0.94</td>
<td style="text-align:center">99.80%</td>
<td style="text-align:center">75.70%</td>
</tr>
<tr>
<td>0.95</td>
<td style="text-align:center">99.80%</td>
<td style="text-align:center">72.80%</td>
</tr>
</tbody>
</table>
</div>
<p>开源代码详见<a href="https://github.com/sladesha/machine_learning" target="_blank" rel="noopener">Github</a></p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过邮箱发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python踩坑指南（第二季）]]></title>
      <url>/2019/05/28/Python%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%AD%A3%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>本期围绕jieba讲一个我遇到的实际问题，在同一个服务里，存在两个不同接口A和B，都用到了jieba分词，区别在于两者需要调用不同的词库，巧合中，存在以下情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">词库A：&quot;干拌面&quot;</span><br><span class="line">词库B：&quot;干拌&quot;,&quot;面&quot;</span><br></pre></td></tr></table></figure>
<p>在服务启动的时候，由于词库A优先被加载了，再去加载词库B的时候发现，并没有加载成功：</p>
<p>接口A中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jieba.load_userdict(&quot;A.txt&quot;)</span><br></pre></td></tr></table></figure></p>
<p>接口B中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jieba.load_userdict(&quot;B.txt&quot;)</span><br></pre></td></tr></table></figure></p>
<p>结果发现，在切干拌面这个词的时候，接口B中还是没有切成功。其实每次在我们加载jieba的时候，可以注意一下会出现以下info：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Building prefix dict from the default dictionary ...</span><br><span class="line">Dumping model to file cache /var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/jieba.cache</span><br><span class="line">Loading model cost 0.824 seconds.</span><br><span class="line">Prefix dict has been built succesfully.</span><br></pre></td></tr></table></figure></p>
<p>显而易见，先进行了Building prefix dict，再Dumping model to file cache，后续Loading model都会来自这，所以这个地方导致以上问题。</p>
<p>我是这么处理的：<br>接口A中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jieba1 = jieba.Tokenizer(dictionary=&quot;A.txt&quot;)</span><br></pre></td></tr></table></figure></p>
<p>接口B中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jieba2 = jieba.Tokenizer(dictionary=&quot;B.txt&quot;)</span><br></pre></td></tr></table></figure></p>
<p>案例如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [1]: import jieba</span><br><span class="line"></span><br><span class="line">In [2]: jieba1=jieba.Tokenizer(dictionary=&quot;A.txt&quot;)</span><br><span class="line"></span><br><span class="line">In [3]: jieba2=jieba.Tokenizer(dictionary=&quot;B.txt&quot;)</span><br><span class="line"></span><br><span class="line">In [4]: jieba1.lcut(&quot;干拌面&quot;)</span><br><span class="line">Building prefix dict from /Users/slade/Desktop/A.txt ...</span><br><span class="line">Dumping model to file cache /var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/jieba.u5221c1b70f06b36e44bc519f39715c96.cache</span><br><span class="line">Loading model cost 0.006 seconds.</span><br><span class="line">Prefix dict has been built succesfully.</span><br><span class="line">Out[4]: [&apos;干拌面&apos;]</span><br><span class="line"></span><br><span class="line">In [5]: jieba2.lcut(&quot;干拌面&quot;)</span><br><span class="line">Building prefix dict from /Users/slade/Desktop/B.txt ...</span><br><span class="line">Dumping model to file cache /var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/jieba.uc4f38d90bf7ce748744ff94fb2863fe4.cache</span><br><span class="line">Loading model cost 0.003 seconds.</span><br><span class="line">Prefix dict has been built succesfully.</span><br><span class="line">Out[5]: [&apos;干拌&apos;, &apos;面&apos;]</span><br></pre></td></tr></table></figure></p>
<p>需要注意的是，去看Tokenizer源码，里面有这么一段读取调用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def gen_pfdict(self, f):</span><br><span class="line">    lfreq = &#123;&#125;</span><br><span class="line">    ltotal = 0</span><br><span class="line">    f_name = resolve_filename(f)</span><br><span class="line">    for lineno, line in enumerate(f, 1):</span><br><span class="line">        try:</span><br><span class="line">            line = line.strip().decode(&apos;utf-8&apos;)</span><br><span class="line">            word, freq = line.split(&apos; &apos;)[:2]</span><br><span class="line">            freq = int(freq)</span><br><span class="line">            lfreq[word] = freq</span><br><span class="line">            ltotal += freq</span><br><span class="line">            for ch in xrange(len(word)):</span><br><span class="line">                wfrag = word[:ch + 1]</span><br><span class="line">                if wfrag not in lfreq:</span><br><span class="line">                    lfreq[wfrag] = 0</span><br><span class="line">        except ValueError:</span><br><span class="line">            raise ValueError(</span><br><span class="line">                &apos;invalid dictionary entry in %s at Line %s: %s&apos; % (f_name, lineno, line))</span><br><span class="line">    f.close()</span><br><span class="line">    return lfreq, ltotal</span><br></pre></td></tr></table></figure></p>
<p>在load_userdict的时候词库的词频可以省略不写，<code>word, freq = line.split(&#39; &#39;)[:2]</code>决定了这边需要加上，这个依赖于版本，我并没有实验不同版本。</p>
<p>A.txt:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">干拌面 1</span><br></pre></td></tr></table></figure></p>
<p>B.txt:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">干拌 1</span><br><span class="line">面 1</span><br></pre></td></tr></table></figure></p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过邮箱发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构]]></title>
      <url>/2019/05/28/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
      <content type="html"><![CDATA[<p>我一直觉得算法工程师，两把刷子，过硬的算法基础知识，可靠的代码工程能力即可；</p>
<p>最近和一些老朋友交流了一下，发现理想很丰满，现实很残酷，在自己觉得自己会了点机器学习知识的时候，结果连和别人沟通讲自己会的东西的机会都没有，还没开始就被卡死在了数据结构这一关上；能想到的，只有时间或者空间复杂度最差的那种解法。一言不合就是遍历，不行就o(n**2)，真的是弱鸡的不行</p>
<p>所以我觉得，是时候把大学做的leetcode捡起来再做一做了，其实在很早的时候，我就在GitHub分享过一些我遇到过的数据结构的问题，但是当时可能觉得不重要，更新了一段时间之后就停止了，现在仔细想想真的是有所失误，好在一切还不晚。</p>
<p><img src="/2019/05/28/数据结构/1.png" alt=""></p>
<p>关于刷leetcode这事，你可以往三个方向上走。一是按照从easy到medium到hard的方向。二是按照分类走，比如先刷树相关的，再刷数组相关的，依次类推。最后一种是我用的就是完全随机的取刷；我觉得自己疯狂刷完dp问题，过了两周又忘完了这样就是非常蠢的，做着做着来一题强化一下记忆，这样才会记得更清楚。</p>
<p>在语言选择上，Python的小trick还是有点多的，就比如字符串表达式的值，我们用eval函数就可以得到，但是请不要这么去用，这样非常的投机取巧，在真的需要去讲去面试的时候，绝对不可能让你这么去用的；如果忍不住，就用java或者C。</p>
<p>除了在Leetcode上面刷题外，有两本书可以给大家参考，一是《剑指offer》，二是《算法导论》，大家可以自行选择。</p>
<p><strong>最后贴上我现在做题及做题心得的<a href="https://github.com/sladesha/LeetCode" target="_blank" rel="noopener">GitHub地址</a>，如果有比较困难理解的地方，我都在代码里面注释了，希望能帮到大家，就这样了。补充一点，在README里面我在有些题目下面注释了一些心得，建议可以优先把这些题目看下，我觉得还是很有意思的。</strong></p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过邮箱发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python踩坑指南（第一季）]]></title>
      <url>/2019/05/14/Python%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97%EF%BC%88%E7%AC%AC%E4%B8%80%E5%AD%A3%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>最近在python开发的过程中，发现了一些比较有意思的问题，确实让自己在开发过程中被恶心了一把，所以开了这个连续的更新博文，之后会持续的按第一第二第三这种版本下去，更新一些比较有意思的python代码问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;/path/to/file&apos;, &apos;r&apos;) as f:</span><br><span class="line">    print(f.read())</span><br></pre></td></tr></table></figure>
<p>IO读取采用with形式，避免忘记close，否则会产生很神奇的结果</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">embereding_data.drop_duplicates(keep=&apos;first&apos;,inplace=True)</span><br></pre></td></tr></table></figure>
<p>python 中删除重复项 如果在原来数据上删除重复项，就要加入inplace=True<br>否则 只是保存一个副本！！！</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.read_csv(filename,error_bad_lines=False)</span><br></pre></td></tr></table></figure>
<p>pandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 3, saw 2解决办法<br>在使用pandas读取csv文件时报以上错误，解决办法如左<br>加上error_bad_lines=False就可以完美解决了</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str = &quot; &quot;.join(list(map(str, l)))</span><br></pre></td></tr></table></figure>
<p>myList = [‘a’,’b’,’c’,’d’] myString = “,”.join(myList ) 当myList含有数字的时候这种方法就会报错 因此需要转为str</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">json.loads(&apos;key&apos;:value)</span><br></pre></td></tr></table></figure>
<p>value不可以为set、tuple</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d3=&apos;&#123;&quot;Message&quot;:&quot;已注销帐户。\r\n\r\n使用者:\r\n\t安全&quot;&#125;&apos;</span><br><span class="line">j=json.loads(d3,strict=False,encoding=&apos;utf-8&apos;)</span><br><span class="line">print(type(j)) #返回值：&lt;type &apos;dict&apos;&gt;</span><br><span class="line">print(j[&apos;Opcode&apos;].encode(&apos;u8&apos;)) #返回值：信息</span><br></pre></td></tr></table></figure>
<p>使用json.loads时，如果原字符串中包含有 \r\n\t等字符，则会提示报错,<br>修改参数strict=False即可</p>
<p>说明：”If strict is False (True is the default), then control characters will be allowed inside strings. Control characters in this context are those with character codes in the 0-31 range, including ‘\t’ (tab), ‘\n’, ‘\r’ and ‘\0’.”</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data[&apos;county_route_index&apos;] = data.apply(lambda x: str(x.start_county_cd) + &apos;_&apos; + str(x.end_county_cd),</span><br><span class="line">axis=1)</span><br></pre></td></tr></table></figure>
<p>生成county_route_index字段示例310112.0_320583.0，而start_county_cd、end_county_cd为整型（int64）<br>解决办法：<br>1.str(x.start_county_cd)改为str(int(x.start_county_cd))<br>2.df[‘col3’] = df[‘col1’].map(str) + ‘_’ + df[‘col2’].map(str)</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">arr = (12,[1,3,3,4],&apos;b&apos;)</span><br><span class="line">arr[1]+=[6]</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">TypeError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-15-6bc7f041d391&gt; in &lt;module&gt;()</span><br><span class="line">----&gt; 1 arr[1]+=[6]</span><br><span class="line">TypeError: &apos;tuple&apos; object does not support item assignment</span><br></pre></td></tr></table></figure>
<p>但是实际上却操作成功：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [16]: arr</span><br><span class="line">Out[16]: (12, [1, 3, 3, 4, 6], &apos;b&apos;)</span><br></pre></td></tr></table></figure></p>
<p>不要混合使用list和tuple的时候使用concat操作</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def func(a,b):</span><br><span class="line">if a ==1:</span><br><span class="line">    return b</span><br><span class="line">else:</span><br><span class="line">    return 1-b</span><br><span class="line"></span><br><span class="line">bbs_data[&quot;real_label&quot;] = bbs_data.apply(lambda row:func(row[2],row[3]),axis = 1)</span><br></pre></td></tr></table></figure>
<p>dataframe 某列的值根据其他列的值计算得来</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pRec = &#123;&#125;</span><br><span class="line">import functools</span><br><span class="line"></span><br><span class="line">def check_is_key_param(f):</span><br><span class="line">    global pRec</span><br><span class="line">    @functools.wraps(f)</span><br><span class="line">    def wrapper(*args, **kwargs):</span><br><span class="line">        pRec.update(f(*args, **kwargs))</span><br><span class="line">        return f(*args, **kwargs)</span><br><span class="line">    return wrapper</span><br></pre></td></tr></table></figure>
<p>收集计算过程中的中间数据</p>
<p>我觉得python确实是比较容易入门的语言，代价是它带来的不稳定性确实也是比较高，需要积累踩坑</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[中文语境下的手机号识别]]></title>
      <url>/2019/05/13/%E4%B8%AD%E6%96%87%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E6%89%8B%E6%9C%BA%E5%8F%B7%E8%AF%86%E5%88%AB/</url>
      <content type="html"><![CDATA[<p>最近在做一个关于中文大段文本中的手机号码识别，由于属于对抗性的一个文本，发现传统的手机号码识别方法，比如正则匹配并不是很适用。</p>
<p>理论情况下文本中的手机号码出现方式应该如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9*6箱车转让，连线路一起打包，带线路转让，固定货源联系13802131234,手机号，非诚勿扰2+1合同</span><br></pre></td></tr></table></figure></p>
<p>对于这种情况，只要需要进行一下正则就行了：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text = &apos;9*6箱车转让，连线路一起打包，带线路转让，固定货源联系13802131234,手机号，非诚勿扰2+1合同&apos;</span><br><span class="line">mobilephone_pattern = &quot;1\d&#123;10&#125;&quot;</span><br><span class="line">phoneNumbers = re.findall(mobilephone_pattern, text, flags=0)</span><br></pre></td></tr></table></figure></p>
<p>得到了如下结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [36]: phoneNumbers</span><br><span class="line">Out[36]: [&apos;13802131234&apos;]</span><br></pre></td></tr></table></figure></p>
<p>但是实际情况下，第三方会进行逃避规则的操作，出现了比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9*6箱车转让，连线路一起打包，带线路转让，固定货源联系138-02##131234,手机号，非诚勿扰2+1合同</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9*6箱车转让，连线路一起打包，带线路转让，固定货源联系138-洞2##幺3幺234,手机号，非诚勿扰2+1合同</span><br></pre></td></tr></table></figure>
<p>甚至还会有：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9*6箱车转让，连线路一起打包，带线路转让，固定货源联系1衫8-洞2##幺散幺2删4,手机号，非诚勿扰2+1合同</span><br></pre></td></tr></table></figure></p>
<p>这样的情况下，继续通过规则就难以实现了：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [38]: re.findall(mobilephone_pattern, text, flags=0)</span><br><span class="line">Out[38]: []</span><br></pre></td></tr></table></figure></p>
<p>所以，我们做了一个拼音转译+循环判断的逻辑进行了优化，会先把文本处理一边：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [37]: text = &apos;9*6箱车转让，连线路一起打包，带线路转让，固定货源联系1衫8-洞2##幺散幺2删4,手机号，非诚勿扰2+1合同&apos;</span><br><span class="line"></span><br><span class="line">In [38]: re.findall(mobilephone_pattern, text, flags=0)</span><br><span class="line">Out[38]: []</span><br><span class="line"></span><br><span class="line">In [39]: from YMMNlpUtils import YMMNlpUtils</span><br><span class="line"></span><br><span class="line">In [40]: obj = YMMNlpUtils(strict=True)</span><br><span class="line"></span><br><span class="line">In [41]: obj.get_all_phone_number(text)</span><br><span class="line">Out[41]: &apos;96箱车转让连线路17打包带线路转让固定货源联系13802131234手机号非诚5扰21合同&apos;</span><br><span class="line"></span><br><span class="line">In [42]: re.findall(mobilephone_pattern,obj.get_all_phone_number(text), flags=0)</span><br><span class="line">Out[42]: [&apos;13802131234&apos;]</span><br></pre></td></tr></table></figure></p>
<p>这样处理完就可以得到手机号码了。</p>
<p>由于是定制化的功能，后期可能还会维护出微信号，身份证号这些，更多的可以去git看一下，放上地址链接<a href="https://github.com/sladesha/machine_learning/tree/master/YMMNlpUtils" target="_blank" rel="noopener">手机号码小工具</a>，安装方法在<a href="https://github.com/sladesha/machine_learning" target="_blank" rel="noopener">README</a>里面有解释。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过邮箱发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[PKUseg在货运领域的评测]]></title>
      <url>/2019/01/14/PKUseg%E5%9C%A8%E8%B4%A7%E8%BF%90%E9%A2%86%E5%9F%9F%E7%9A%84%E8%AF%84%E6%B5%8B/</url>
      <content type="html"><![CDATA[<p>先说结论，再和大家闲聊，对比jieba与PKUseg在公路货运切词能力上：</p>
<ul>
<li>默认模型下，jieba效果优于PKUseg</li>
<li>PKUseg提供场景精细化的预训练（还没有提供入口），长远来讲适合专业领域使用</li>
<li>PKUseg在特定的场景下有令人惊喜的效果（地址切分）</li>
</ul>
<p>给大家的建议就是，如果大家赶时间求稳定适应范围需要非常广的时候，目前来说jieba是非常好的选择，如果说在面临一些精细化领域的特殊需求的时候，可以用PKUseg进行一波尝试，有意外惊喜。</p>
<hr>
<p>那是一个风和日丽的早上，突然群里老大发出一条消息：<br><img src="/2019/01/14/PKUseg在货运领域的评测/boss.png" alt=""><br>我感觉我的心脏有一丝隐隐作痛的感觉，人在办公室坐，活从天上来，虽然身后站着一堆催上线的产品，我还是屈服于老大的正义（淫威，开玩笑的，别认真），简单测评了新出来的PKUseg与Jieba在公路货运/运输行业上的效果对比。</p>
<p>在我们的热词数据库中已经有人工切词完成的2万多条货运的词条：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">description	standard</span><br><span class="line">高博集团装货卸宝华	高博 集团 装货 卸 宝华</span><br><span class="line">北安到吉林农安饲料90吨每吨105	北安 到 吉林 农安 饲料 90吨 每吨 105</span><br><span class="line">需要4个车	需要 4个 车</span><br><span class="line">叶张公路装香闵路曲吴路两卸	叶张公路 装 香闵路 曲吴路 两卸</span><br><span class="line">从福通物流到吴滩镇	从 福通 物流 到 吴滩镇</span><br><span class="line">霞浦宏霞路到中通物流	霞浦宏霞路 到 中通物流</span><br><span class="line">石大路3场到德兴西门山	石大路 3场 到 德兴 西门山</span><br><span class="line">公园西路装	公园 西路 装</span><br><span class="line">不押车每吨150	不 押车 每吨 150</span><br><span class="line">速订价钱好商量	速订 价钱 好商量</span><br><span class="line">慈溪胜山装	慈溪 胜山装</span><br><span class="line">好装好卸高价急走	好装好卸 高价急走</span><br><span class="line">九顶山路与东方大道位置装货可以配货	九顶 山路 与 东方 大道 位置 装货 可以 配货</span><br><span class="line">要二部	要 二部</span><br><span class="line">青浦工业园区久远路提货到奉贤新杨公路进仓	青浦 工业园区 久远路 提货 到 奉贤 新杨公路 进仓</span><br><span class="line">园光路装博学南路卸	园光路 装 博学南路 卸</span><br><span class="line">公兴装卸荣昌广顺	公兴 装卸 荣昌 广顺</span><br><span class="line">打备注电话18458331112	打 备注 电话 18458331112</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>首先看，不加任何词库，预训练下的，最后的效果对比：<br><img src="/2019/01/14/PKUseg在货运领域的评测/1.png" alt=""><br>可以看到，<strong>在默认的分词模型下，jieBa分词还是拥有绝对优势的</strong>，但是在pkuSeg的git里面<img src="/2019/01/14/PKUseg在货运领域的评测/2.png" alt=""></p>
<p>所以我想看看能不能进行一下预训练下后再对比一下，可惜的是我在git（<a href="https://github.com/sladesha/pkuseg-python#相关论文" target="_blank" rel="noopener">git地址传送门</a>）上找了半天也没有找到预训练的入口，只有已经被官方预训练好的词库<br><img src="/2019/01/14/PKUseg在货运领域的评测/3.png" alt=""><br>等有时间了，可以邮件沟通一下再补充这个部分的效果对比，我觉得，应该还是有提升的。</p>
<p>但是，在我们实际去测的过程中，我们发现了一些差异话的东西比较有意思。我们其实现在在做一个语音发货的产品，涉及到把一串地址切分开的需求：<br><img src="/2019/01/14/PKUseg在货运领域的评测/4.png" alt=""></p>
<p>其中涉及到<strong>地址切分</strong>的时候，jieba的能力会比如PKUseg要弱不少，比如“山西大同”，“上海浦东”，我们需要把一级二级地址切开的时候，PKUseg可以做到，而jieba并不能按照需求切块。所以，我们已经打算在地址模块切换PKUseg的模型来适应了。<br><img src="/2019/01/14/PKUseg在货运领域的评测/5.png" alt=""></p>
<p>最后吐槽一下，虽然我知道PKUseg需要加载模型，但是一加载就是一二十秒也是有点夸张了。酒浆，各位下回见。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 开源项目 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[pandas中的问题记录]]></title>
      <url>/2018/10/23/pandas%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
      <content type="html"><![CDATA[<p>最近发现pandas的一个问题，记录一下：<br>有一组数据（test.txt）如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">20181016</span>    <span class="number">14830680298903273</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953069</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953079</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953089</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953099</span></span><br><span class="line"><span class="number">20181016</span>    <span class="number">14839603473953019</span></span><br></pre></td></tr></table></figure></p>
<p>剖析出来看，数据是按照<code>\t</code>进行分隔的：<code>&#39;20181016\t14830680298903273\n&#39;</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'test.txt'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">line = f.readline()</span><br><span class="line">print(line)</span><br></pre></td></tr></table></figure></p>
<p>我平时一直在用pandas去读数据，所以我很熟练的写下来如下的代码：<br><code>pd.read_table(&#39;test.txt&#39;,header=None)</code><br>然后发现，第一列变成了科学记数法的方式进行存储了：</p>
<p><img src="/2018/10/23/pandas中的问题记录/1.png" alt=""></p>
<p>很明显，科学记数法是可以转换的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def as_number(value):</span><br><span class="line">try:</span><br><span class="line">return &apos;&#123;:.0f&#125;&apos;.format(value)</span><br><span class="line">except:</span><br><span class="line">return value</span><br><span class="line"></span><br><span class="line"># 应用到目标列去即可</span><br><span class="line">data.uid.apply(as_number)</span><br></pre></td></tr></table></figure></p>
<p>诡异的事情发生了，对于14830680298903273在<code>as_number</code>函数转换下变成了14830680298903272，理论上讲14830680298903273没有小数部分不存在四舍五入的原因，网上搜了也没有很明确的解释，初步讨论后猜测应该是pandas在用float64去存这种长度过长的数字的时候有精度丢失的问题。</p>
<p>要解决也是很简单的：</p>
<ul>
<li>用open的形式打开，在切割逐步去用list进行append，在合并</li>
<li>用read_table的函数的时候，默认是用float64去存在的，<a href="http://pandas.pydata.org/pandas-docs/stable/basics.html?highlight=astype#selecting-columns-based-on-dtype" target="_blank" rel="noopener">改成object去存(dtype=object)</a></li>
<li>在生产数据的时候，对于这种过长的数据采取str的形式去存</li>
</ul>
<p>也是给自己提个醒，要规范一下自己的数据存储操作，并养成数据核对的习惯。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。<br><img src="https://upload-images.jianshu.io/upload_images/1129359-654dc61c581d94e1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[YoutubeNet的数据答疑]]></title>
      <url>/2018/10/16/YoutubeNet%E7%9A%84%E6%95%B0%E6%8D%AE%E7%AD%94%E7%96%91/</url>
      <content type="html"><![CDATA[<p>实在是太忙了，抽空给大家解析一下之前写的<a href="https://github.com/sladesha/deep_learning/tree/master/YoutubeNetwork" target="_blank" rel="noopener">YoutubeNet</a>的数据是怎么构造的，协助大家可以自行构造一下。</p>
<p>这边和大家说一下，我没有上传数据的原因有两个：</p>
<ul>
<li>涉及公司的数据财产，不方便上传</li>
<li>懒得做脱敏处理</li>
<li>数据一共有1300多万条，传输实在不方便</li>
</ul>
<p>主要数据处理的部分在map_id_idx.py脚本下，其中包含all_item_20180624.txt和click_thirty_day_data_20180609.txt两个数据集合。</p>
<p>其中，all_item_20180624.txt是当日所有的商品集合：包含’Prd_Id’, ‘ItemId’, ‘BrandId’, ‘MsortId’和‘GenderId’五列，分别代表着商品id，skuid，低级品牌id，中级品牌id，产品性别，最后形如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5675</span>    <span class="number">50000055</span>    <span class="number">175</span>    <span class="number">1500</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2577</span>    <span class="number">50000056</span>    <span class="number">187</span>    <span class="number">66</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2002</span>    <span class="number">50000057</span>    <span class="number">63</span>    <span class="number">11</span>    <span class="number">2</span></span><br><span class="line"><span class="number">2007</span>    <span class="number">50000058</span>    <span class="number">137</span>    <span class="number">58</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2075</span>    <span class="number">50000060</span>    <span class="number">80</span>    <span class="number">50</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2348</span>    <span class="number">50000061</span>    <span class="number">138</span>    <span class="number">16</span>    <span class="number">2</span></span><br><span class="line"><span class="number">423</span>    <span class="number">50000062</span>    <span class="number">162</span>    <span class="number">237</span>    <span class="number">3</span></span><br><span class="line"><span class="number">469</span>    <span class="number">50000063</span>    <span class="number">10</span>    <span class="number">1500</span>    <span class="number">3</span></span><br><span class="line"><span class="number">1102</span>    <span class="number">50000064</span>    <span class="number">176</span>    <span class="number">11</span>    <span class="number">1</span></span><br><span class="line"><span class="number">1896</span>    <span class="number">50000066</span>    <span class="number">37</span>    <span class="number">27</span>    <span class="number">1</span></span><br><span class="line"><span class="number">2489</span>    <span class="number">50000067</span>    <span class="number">27</span>    <span class="number">44</span>    <span class="number">1</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>click_thirty_day_data_20180609.txt为近三十天的用户点击流，包含’UId’, ‘ItemId’, ‘clickTime’三列，分别代表着uid、点击的skuid，点击时间，最后形如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">34</span>    <span class="number">51668064</span>    <span class="number">1528602406</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51890512</span>    <span class="number">1528788389</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51884724</span>    <span class="number">1528788393</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51884720</span>    <span class="number">1528788399</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51884718</span>    <span class="number">1528788414</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51580974</span>    <span class="number">1528788442</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51854970</span>    <span class="number">1528788487</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51514910</span>    <span class="number">1528788499</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51855000</span>    <span class="number">1528788535</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51854990</span>    <span class="number">1528788569</span></span><br><span class="line"><span class="number">34</span>    <span class="number">51854998</span>    <span class="number">1528788572</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>通过map_id_idx.py对所有的商品进行标序号，然后带入用户的点击流中，方便后期做embedding操作，就酱。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[GloVe向量化做文本分类]]></title>
      <url>/2018/09/25/GloVe%E5%90%91%E9%87%8F%E5%8C%96%E5%81%9A%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
      <content type="html"><![CDATA[<h1 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h1><p>在之前，我对向量化的方法一直局限在两个点，</p>
<p>第一种是常规方法的one-hot-encoding的方法，常见的比如tf-idf生成的0-1的稀疏矩阵来代表原文本：<br><img src="/2018/09/25/GloVe向量化做文本分类/1.png" alt=""></p>
<p>这种方法简单暴力，直接根据文本中的单词进行one-hot-encoding，但是数据量一但大了，这个单句话的one-hot-encoding结果会异常的长，而且没办法得到词与词之间的关系。</p>
<p>第二种是基于神经网络的方法，常见的比如word2vec，YouTubeNet：<br><img src="/2018/09/25/GloVe向量化做文本分类/2.png" alt=""></p>
<p>这种方法（这边以CBOW为例子）都是初始一个固定长度的随机向量作为每个单词的向量，制定一个目标词的向量，以上下文词向量的sum结果作为input进行前向传递，使得传递的结果和目标词向量尽可能一致，以修正初始的随机向量。<br>换句话说，就是刚开始，我随意定义生成一个vector代表一个词，然后通过上下文的联系去修正这个随机的vector。好处就是我们可以得到词与词之间的联系，而且单个词的表示不复杂，坏处就是需要大量的训练样本，毕竟涉及到了神经网络。</p>
<p>最近，我们突然发现了第三种方法，GloVe向量化。它也是开始的时候随机一个vector作为单词的表示，但是它不利用神经网络去修正，而是利用了一个自己构造的损失函数：<br><img src="/2018/09/25/GloVe向量化做文本分类/3.png" alt=""></p>
<p>通过我们已有的文章内容，去是的这个损失函数最小，这就变成了一个机器学习的方法了，相比较暴力的前馈传递，这也高快速和高效的多。同时，它还兼具了word2vec最后结果里面vector方法的优点，得到词与词之间的联系，而且单个词的表示不复杂。</p>
<p>这边就不展开GloVe算法的细节了，后面有空和大家补充，这个算法的构造非常巧妙，值得大家借鉴一下。</p>
<h1 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h1><p>刚才开门见山的聊了蛮久向量化，看起来和文本分类没什么关系，确实在通常意义上来讲，我们的最简单最常用的方法并不是向量化的方法，比如通过朴素贝叶斯，N-Grams这些方法来做分类识别。</p>
<h2 id="tfidf-N-grams"><a href="#tfidf-N-grams" class="headerlink" title="tfidf+N-grams"></a>tfidf+N-grams</h2><p>1.其实很简单，首先对语料库进行切词，维护自己的词典，做高频词的人工复审，将无意词进行stop_words归总<br><img src="/2018/09/25/GloVe向量化做文本分类/4.png" alt="对公司内部信息进行了一下处理，主要看分布趋势"></p>
<p>可以看到，高频词其实是非常非常少的，而且如果你真的去做了，你就会发现，”了”、“的”、“啊”这种语气词，和一些你公司相关的领域词汇会非常靠前，这些词作为stop_words会有效的降低训练成本、提高模型效果。</p>
<p>2.进行tf-idf，将词进行重赋权，字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降，有效的将向量化中的one hot encoding结果进行了修正。但是依然存在问题：在TFIDF算法中并没有体现出单词的位置信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sublinear_tf：replace tf with 1 + log(tf)</span></span><br><span class="line"><span class="comment"># max_df：用来剔出高于词频0.5的词</span></span><br><span class="line"><span class="comment"># token_pattern：(?u)\b\w+\b是为了匹配出长度为1及以上的词，默认的至少需要词长度为2</span></span><br><span class="line"><span class="comment"># ngram_range：这边我做了3-grams处理，如果只想朴素计算的话(1,1)即可</span></span><br><span class="line"><span class="comment"># max_features：随着我做了各种宽松的条件，最后生成的词维度会异常大，这边限制了前3万</span></span><br><span class="line">vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=<span class="keyword">True</span>, max_df=<span class="number">0.5</span>, token_pattern=<span class="string">r"(?u)\b\w+\b"</span>,ngram_range=(<span class="number">1</span>, <span class="number">3</span>), max_features=<span class="number">30000</span>)</span><br></pre></td></tr></table></figure>
<p>不得不说，python处理机器学习，深度学习的便捷程度是异常的高。</p>
<p>3.在经过TfidfVectorizer处理之后的结果是以稀疏矩阵的形式来存的，如果想看内容的话，可以用todense()转化为matrix来看。接下来，用贝叶斯来训练刚才得到的矩阵结果就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mnb_tri = MultinomialNB(alpha=<span class="number">0.001</span>)</span><br><span class="line">mnb_tri.fit(tri_train_set.tdm, tri_train_set.label)</span><br></pre></td></tr></table></figure>
<h2 id="tf-idf-n-grams-naive-bayes-lr"><a href="#tf-idf-n-grams-naive-bayes-lr" class="headerlink" title="tf-idf + n-grams + naive-bayes + lr"></a>tf-idf + n-grams + naive-bayes + lr</h2><p>这种方法是上面方法的升级版本，我们先看下架构：<br><img src="/2018/09/25/GloVe向量化做文本分类/5.png" alt="对公司内部信息进行了一下处理，主要看算法架构"></p>
<p>其实主要差异在于右侧的算法模型详细部分，我们做了一个由3-grams到3-grams+naive-bayes+lr的扩充，提升精度。</p>
<p>在模型的过程中，上面的第一步，都是一样的，在第二、三步有所差异：<br>2.在第二步中，我们除了要构造出一个3-grams的sparse matrix也需要构造出一个朴素的sparse matrix</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 朴素结果</span></span><br><span class="line">vectorizerby = TfidfVectorizer(stop_words=stpwrdlst, token_pattern=<span class="string">r"(?u)\b\w+\b"</span>, max_df=<span class="number">0.5</span>, sublinear_tf=<span class="keyword">True</span>,ngram_range=(<span class="number">1</span>, <span class="number">1</span>), max_features=<span class="number">100000</span>)</span><br></pre></td></tr></table></figure>
<p>3.不仅仅用bayes进行一次分类，而是根据3-grams和朴素情况下的sparse matrix进行预测，再用logistics regression来合并两个的结果做个stack进行0-1压缩。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造出一个3-grams的sparse matrix也需要构造出一个朴素的sparse matrix</span></span><br><span class="line">mnb_tri = MultinomialNB(alpha=<span class="number">0.001</span>)</span><br><span class="line">mnb_tri.fit(tri_train_set.tdm, tri_train_set.label)</span><br><span class="line">mnb_by = MultinomialNB(alpha=<span class="number">0.001</span>)</span><br><span class="line">mnb_by.fit(by_train_set.tdm, by_train_set.label)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加bias，cv选择最优正则结果，lbfgs配合l2正则</span></span><br><span class="line">lr = LogisticRegressionCV(multi_class=<span class="string">"ovr"</span>, fit_intercept=<span class="keyword">True</span>, Cs=np.logspace(<span class="number">-2</span>, <span class="number">2</span>, <span class="number">20</span>), cv=<span class="number">2</span>, penalty=<span class="string">"l2"</span>,solver=<span class="string">"lbfgs"</span>, tol=<span class="number">0.01</span>)</span><br><span class="line">re = lr.fit(adv_data[[<span class="string">'f1'</span>, <span class="string">'f2'</span>]], adv_data[<span class="string">'rep_label'</span>])</span><br></pre></td></tr></table></figure>
<p>总结一下上面两种方法，我觉得是入门快，效果也不错的小练手，也是完全可以作为我们开始一个项目的时候，用来做baseline的方法，主要是快啊～/斜眼笑</p>
<h2 id="GloVe-lr"><a href="#GloVe-lr" class="headerlink" title="GloVe+lr"></a>GloVe+lr</h2><p>因为我目前的带标签数据比较少，所以之前一直没有敢用word2vec去向量化作死，但是GloVe不存在这个问题啊，我就美滋滋的进行了一波。<br>首先，先讲下GloVe的使用：</p>
<ul>
<li><a href="https://github.com/stanfordnlp/GloVe" target="_blank" rel="noopener">https://github.com/stanfordnlp/GloVe</a> 在最大的代码抄袭网站下载(git clone)坦福大佬的代码，友情提醒，不要作死自己看了理论就觉得自己会写，自己搞个GloVe。(别问我是怎么知道的)</li>
<li>cd到对应目录下，vim demo.sh这个文件<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"></span><br><span class="line"><span class="comment"># Makes programs, downloads sample data, trains a GloVe model, and then evaluates it.</span></span><br><span class="line"><span class="comment"># One optional argument can specify the language used for eval script: matlab, octave or [default] python</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 请把make这边注释掉，这个是让你去下个demo，我们直接改成自己的数据</span></span><br><span class="line"><span class="comment"># make</span></span><br><span class="line"><span class="comment"># if [ ! -e text8 ]; then</span></span><br><span class="line"><span class="comment">#   if hash wget 2&gt;/dev/null; then</span></span><br><span class="line"><span class="comment">#     wget http://mattmahoney.net/dc/text8.zip</span></span><br><span class="line"><span class="comment">#   else</span></span><br><span class="line"><span class="comment">#     curl -O http://mattmahoney.net/dc/text8.zip</span></span><br><span class="line"><span class="comment">#   fi</span></span><br><span class="line"><span class="comment">#   unzip text8.zip</span></span><br><span class="line"><span class="comment">#   rm text8.zip</span></span><br><span class="line"><span class="comment"># fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CORPUS需要对应自己的欲训练的文档</span></span><br><span class="line">CORPUS=content.txt</span><br><span class="line">VOCAB_FILE=vocab.txt</span><br><span class="line">COOCCURRENCE_FILE=cooccurrence.bin</span><br><span class="line">COOCCURRENCE_SHUF_FILE=cooccurrence.shuf.bin</span><br><span class="line">BUILDDIR=build</span><br><span class="line">SAVE_FILE=vectors</span><br><span class="line">VERBOSE=2</span><br><span class="line">MEMORY=4.0</span><br><span class="line"><span class="comment"># 单词至少出现几次</span></span><br><span class="line">VOCAB_MIN_COUNT=3</span><br><span class="line"><span class="comment"># 向量长度</span></span><br><span class="line">VECTOR_SIZE=128</span><br><span class="line"><span class="comment"># 迭代次数</span></span><br><span class="line">MAX_ITER=30</span><br><span class="line"><span class="comment"># 窗口长度</span></span><br><span class="line">WINDOW_SIZE=15</span><br><span class="line">BINARY=2</span><br><span class="line">NUM_THREADS=8</span><br><span class="line">X_MAX=10</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ <span class="variable">$BUILDDIR</span>/vocab_count -min-count <span class="variable">$VOCAB_MIN_COUNT</span> -verbose <span class="variable">$VERBOSE</span> &lt; <span class="variable">$CORPUS</span> &gt; <span class="variable">$VOCAB_FILE</span>"</span></span><br><span class="line"><span class="variable">$BUILDDIR</span>/vocab_count -min-count <span class="variable">$VOCAB_MIN_COUNT</span> -verbose <span class="variable">$VERBOSE</span> &lt; <span class="variable">$CORPUS</span> &gt; <span class="variable">$VOCAB_FILE</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ <span class="variable">$BUILDDIR</span>/cooccur -memory <span class="variable">$MEMORY</span> -vocab-file <span class="variable">$VOCAB_FILE</span> -verbose <span class="variable">$VERBOSE</span> -window-size <span class="variable">$WINDOW_SIZE</span> &lt; <span class="variable">$CORPUS</span> &gt; <span class="variable">$COOCCURRENCE_FILE</span>"</span></span><br><span class="line"><span class="variable">$BUILDDIR</span>/cooccur -memory <span class="variable">$MEMORY</span> -vocab-file <span class="variable">$VOCAB_FILE</span> -verbose <span class="variable">$VERBOSE</span> -window-size <span class="variable">$WINDOW_SIZE</span> &lt; <span class="variable">$CORPUS</span> &gt; <span class="variable">$COOCCURRENCE_FILE</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ <span class="variable">$BUILDDIR</span>/shuffle -memory <span class="variable">$MEMORY</span> -verbose <span class="variable">$VERBOSE</span> &lt; <span class="variable">$COOCCURRENCE_FILE</span> &gt; <span class="variable">$COOCCURRENCE_SHUF_FILE</span>"</span></span><br><span class="line"><span class="variable">$BUILDDIR</span>/shuffle -memory <span class="variable">$MEMORY</span> -verbose <span class="variable">$VERBOSE</span> &lt; <span class="variable">$COOCCURRENCE_FILE</span> &gt; <span class="variable">$COOCCURRENCE_SHUF_FILE</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ <span class="variable">$BUILDDIR</span>/glove -save-file <span class="variable">$SAVE_FILE</span> -threads <span class="variable">$NUM_THREADS</span> -input-file <span class="variable">$COOCCURRENCE_SHUF_FILE</span> -x-max <span class="variable">$X_MAX</span> -iter <span class="variable">$MAX_ITER</span> -vector-size <span class="variable">$VECTOR_SIZE</span> -binary <span class="variable">$BINARY</span> -vocab-file <span class="variable">$VOCAB_FILE</span> -verbose <span class="variable">$VERBOSE</span>"</span></span><br><span class="line"><span class="variable">$BUILDDIR</span>/glove -save-file <span class="variable">$SAVE_FILE</span> -threads <span class="variable">$NUM_THREADS</span> -input-file <span class="variable">$COOCCURRENCE_SHUF_FILE</span> -x-max <span class="variable">$X_MAX</span> -iter <span class="variable">$MAX_ITER</span> -vector-size <span class="variable">$VECTOR_SIZE</span> -binary <span class="variable">$BINARY</span> -vocab-file <span class="variable">$VOCAB_FILE</span> -verbose <span class="variable">$VERBOSE</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$CORPUS</span>"</span> = <span class="string">'text8'</span> ]; <span class="keyword">then</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$1</span>"</span> = <span class="string">'matlab'</span> ]; <span class="keyword">then</span></span><br><span class="line">matlab -nodisplay -nodesktop -nojvm -nosplash &lt; ./<span class="built_in">eval</span>/matlab/read_and_evaluate.m 1&gt;&amp;2 </span><br><span class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$1</span>"</span> = <span class="string">'octave'</span> ]; <span class="keyword">then</span></span><br><span class="line">octave &lt; ./<span class="built_in">eval</span>/octave/read_and_evaluate_octave.m 1&gt;&amp;2</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"$ python eval/python/evaluate.py"</span></span><br><span class="line">python <span class="built_in">eval</span>/python/evaluate.py</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这边多说一下，<code>CORPUS=content.txt</code>这边content.txt里面的格式需要按照空格为分隔符进行存储，我之前一直以为是<code>\t</code>。</p>
<ul>
<li><p>直接sh demo.sh，你会得到vectors.txt，这个里面就对应每个词的向量表示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">天气 -0.754142 0.386905 -1.200074 -0.587121 0.758316 0.373824 0.342211 -1.275982 -0.300846 0.374902 -0.548544 0.595310 0.906426 0.029255 0.549932 -0.650563 -0.425185 1.689703 -1.063556 -0.790254 -1.191287 0.841529 1.080641 -0.082830 1.062107 -0.667727 0.573955 -0.604460 -0.601102 0.615299 -0.470923 0.039398 1.110345 1.071094 0.195431 -0.155259 -0.781432 0.457884 1.093532 -0.188207 -0.161646 0.246220 -0.346529 0.525458 0.617904 -0.328059 1.374414 1.020984 -0.959817 0.670894 1.091743 0.941185 0.902730 0.609815 0.752452 1.037880 -1.522382 0.085098 0.152759 -0.562690 -0.405502 0.299390 -1.143145 -0.183861 0.383053 -0.013507 0.421024 0.025664 -0.290757 -1.258696 0.913482 -0.967165 -0.131502 -0.324543 -0.385994 0.711393 1.870067 1.349140 -0.541325 -1.060084 0.078870 0.773146 0.358453 0.610744 0.407547 -0.552853 1.663435 0.120006 0.534927 0.219279 0.682160 -0.631311 1.071941 -0.340337 -0.503272 0.150010 1.347857 -1.024009 -0.181186 0.610240 -0.218312 -1.120266 -0.486539 0.264507 0.266192 0.347005 0.172728 0.613503 -0.131925 -0.727304 -0.504488 1.773406 -0.700505 -0.159963 -0.888025 -1.358476 0.540589 -0.243272 -0.236959 0.391855 -0.133703 -0.071120 1.050547 -1.087613 -0.467604 1.779341 -0.449409 0.949411</span><br><span class="line">好了 1.413075 -0.226177 -2.024229 -0.192003 0.628270 -1.227394 -1.054946 -0.900683 -1.958882 -0.133343 -1.014088 -0.434961 0.026207 -0.066139 0.608682 -0.362021 0.314323 0.261955 -0.571414 1.738899 -1.013223 0.503853 -0.536511 -0.212048 0.611990 -0.627851 0.297657 -0.187690 -0.565871 -0.234922 -0.845875 -0.767733 0.032470 1.508012 -0.204894 -0.495031 -0.159262 0.181380 0.050582 -0.333469 0.454832 -2.091174 0.448453 0.940212 0.882077 -0.617093 0.616782 -0.993445 -0.385087 0.251711 0.259918 -0.222614 -0.595131 0.661472 0.194740 0.619222 -1.253610 -0.838179 0.781428 -0.396697 -0.530109 0.022801 -0.558296 -0.656034 0.842634 -0.105293 0.586823 -0.603681 -0.605727 -0.556468 0.924275 -0.299228 -1.121538 0.237787 0.498935 -0.045423 0.171536 -1.026385 -0.262225 0.390662 1.263240 0.352172 0.261121 0.915840 1.522183 -0.498536 2.046169 0.012683 -0.073264 -0.361662 0.759529 -0.713268 0.281747 -0.811104 -0.002061 -0.802508 0.520559 0.092275 -0.623098 0.199694 -0.134896 -1.390617 0.911266 -0.114067 1.274048 1.108440 -0.266002 1.066987 0.514556 0.144796 -0.606461 0.197114 0.340205 -0.400785 -0.957690 -0.327456 1.529557 -1.182615 0.431229 -0.084865 0.513266 -0.022768 -0.092925 -0.553804 -2.269741 -0.078390 1.376199 -1.163337</span><br><span class="line">随意 0.410436 0.776917 -0.381131 0.969900 -0.804778 -0.785379 -0.887346 -1.463543 -1.574851 0.313285 0.685253 -0.918359 0.199073 -0.305374 -0.642721 0.098114 -0.723331 0.353159 0.042807 0.369208 -1.534930 -0.084871 0.020417 -0.384782 0.276833 -0.160028 1.107051 0.884343 -0.204381 -0.459738 -0.387128 0.125867 0.093569 1.192471 -0.473752 -0.314541 -1.029249 0.481447 1.358753 -1.688778 -0.113080 -0.401443 -0.958206 0.605638 1.083126 0.131617 0.092507 0.476506 0.801755 1.096883 -0.102036 0.461804 0.820297 -0.104053 -0.126638 0.957708 -0.722038 0.223686 0.583582 0.201246 -1.254708 0.770717 -1.271523 -0.584094 -1.142426 1.066567 0.071951 -0.182649 0.014365 -0.577141 0.037340 -0.166832 -0.247827 0.165994 1.143665 -0.258421 -0.335195 0.170218 -0.212838 0.013709 0.088847 0.663238 -0.597439 0.632847 0.370871 0.652707 0.306935 0.195127 -0.252443 0.588479 0.191633 -1.587564 0.564600 -0.306158 -0.648177 -0.488595 1.532795 -0.462473 -0.643878 1.292369 -0.051494 -1.032738 0.453587 0.411327 -0.469373 0.428398 -0.020839 0.307422 0.518331 -0.860913 -2.170098 -0.277532 -0.966210 0.615336 -0.924783 0.042679 1.289640 1.272992 1.367773 0.426600 -0.187254 -0.781009 1.331301 -0.088357 -1.113550 -0.262879 0.300137 0.437905</span><br><span class="line">..</span><br></pre></td></tr></table></figure>
</li>
<li><p>有了每个词的向量，我们这边采取了借鉴YoutubeNet网络的想法：<br><img src="/2018/09/25/GloVe向量化做文本分类/6.png" alt=""></p>
</li>
</ul>
<p>举个例子：存在一句话”我爱中国”，“我”的向量是[0.3,0.2,0.3]，”爱”的向量是[0.1,0.2,0.3]，“中国”的向量是[0.6,0.6,0.4]，那么average后就是[0.33,0.33,0.33]，然后这就类似一个特征为三的input。</p>
<p>这种方法的好处就是快捷，预处理的工作代价要小，随着数据量的增多，模型的效果要更加的好。</p>
<h1 id="效果对比"><a href="#效果对比" class="headerlink" title="效果对比"></a>效果对比</h1><p>最后这边粗略的给出一下业务数据对比：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>experiment</th>
<th>date</th>
<th>intercepted_recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>3-grams</td>
<td>20180915</td>
<td>79.3%</td>
</tr>
<tr>
<td>3-grams</td>
<td>20180917</td>
<td>78.7%</td>
</tr>
<tr>
<td>3-grams+bayes+lr</td>
<td>20180915</td>
<td>83.4%</td>
</tr>
<tr>
<td>3-grams+bayes+lr</td>
<td>20180917</td>
<td>88.6%</td>
</tr>
<tr>
<td>gloVe+lr</td>
<td>20180915</td>
<td>93.1%</td>
</tr>
<tr>
<td>gloVe+lr</td>
<td>20180917</td>
<td>93.9%</td>
</tr>
</tbody>
</table>
</div>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，相关代码已经上传到我的<a href="https://github.com/sladesha/machine_learning" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Google团队在DNN的实际应用方式的整理]]></title>
      <url>/2018/08/29/Google%E5%9B%A2%E9%98%9F%E5%9C%A8DNN%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E6%96%B9%E5%BC%8F%E7%9A%84%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>很荣幸有机会和论文作者<a href="https://www.linkedin.com/in/mehmet-emre-sargin-0665047/" target="_blank" rel="noopener">Emre Sargin</a>关于之前发的<a href="https://zhuanlan.zhihu.com/p/38638747" target="_blank" rel="noopener">Deep Neural Networks for YouTube Recommendations</a>进行交流，梳理如下：</p>
<p><strong>提问对话汇总：</strong></p>
<ul>
<li>如何进行负采样的？</li>
</ul>
<p>构造了千万量级热门视频集合，每个用户的负采样结果来源于这个集合，会有一些筛选的tricks，比如剔除浏览过的商品，负采样的数量Google在200万条。（也就是说，在计算loss的时候，google的label是一个200万长度的向量，瑟瑟发抖.jpg。）</p>
<ul>
<li>推荐算法应用上，有什么评估方式和评估指标？</li>
</ul>
<p>主要基于线上进行小批量的abtest进行对比，在考虑ctr指标的同时也会综合全站的信息加以分析，同时对新颖程度和用户兴趣变换也是我们考察的对象。</p>
<ul>
<li>冷启动的解决方式？从来没有被点击过的video如何处理？新上的video如何处理？</li>
</ul>
<p>google的推荐基于多种推荐算法的组合，YouTubeNet主要解决的是热门商品的一个推荐问题，冷启动或者没有被点击的video会有其他算法进行计算。换句话说，解决不了。</p>
<ul>
<li>example age如何定义？</li>
</ul>
<p>user+vedio的组合形式，train过程中，是用户点击该vedio的时间距离当前时间的间隔；predict过程中，为0。该部分对模型的鲁棒性非常重要。</p>
<ul>
<li>是否遇到神经元死亡的问题？</li>
</ul>
<p>有，解决方案很常规，都是大家了解的，降低learning_rate，使用batchnormalization。</p>
<ul>
<li>是否预到过拟合？</li>
</ul>
<p>没，youtube的用户上亿，可以构造出上千亿的数据，过拟合的情况不明显。但是会存在未登录用户，我们会通过一些其他CRM类的算法补充构造出他们的基本信息，比如gender、age…</p>
<ul>
<li>vedio vector在哪边进行构造和修正？</li>
</ul>
<p>history click部分进行vedio embedding，并进行修正。另外，50是我们尝试的历史点击长度，20-30也有不错的效果。</p>
<ul>
<li>会有工程计算压力么？</li>
</ul>
<p>不存在，建议在GPU上计算，后面由于VPN网络信号抖动没听清，大概就说Google在训练模型的时候会有大量GPU支持，每天大概更新2-3模型，没有遇到什么计算瓶颈。</p>
<p><em>(以上为我个人针对提问结果的理解及总结)</em></p>
<p><strong>个人感想如下：有钱任性</strong></p>
<p><strong>最后，我觉得算法还是要适应实际情况，大公司的方法可以借鉴但是可能很多时候抄不来，也没条件抄。</strong></p>
<p><em>原问题如下（实际有删改）：<br>How to do video embedding?<br>Is there any pre-training?<br>How to use example age in the model?<br>How to deal dead ReLU neurons？<br>How to sample negative classes？<br>How does the video embedding generated？<br>How to recommend the video never been clicked and new uploaded videos？<br>How to do ab testing? What’s the metrics?<br>Have you facing overfitting？How to solve it?<br>There is any difficulty in calculating the embedding for millions of videos and users.<br>During input embedding generation, are they simply averaged?</em></p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。<br><img src="https://upload-images.jianshu.io/upload_images/1129359-654dc61c581d94e1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 论文解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Stanford Word Segmenter问题整理]]></title>
      <url>/2018/08/27/Segmenter%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86/</url>
      <content type="html"><![CDATA[<p>最近在做一些nlp相关的项目，在涉及到Stanford CoreNLP工具包处理中文分词的时候，发现耗时问题很严重：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Item</th>
<th style="text-align:center">time(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">jieba</td>
<td style="text-align:center">0.4</td>
</tr>
<tr>
<td style="text-align:center">snownlp</td>
<td style="text-align:center">7.4</td>
</tr>
<tr>
<td style="text-align:center">pynlpir</td>
<td style="text-align:center">0.8</td>
</tr>
<tr>
<td style="text-align:center"><strong>StanfordCoreNLP</strong></td>
<td style="text-align:center"><strong>21.5</strong></td>
</tr>
<tr>
<td style="text-align:center">pyltp</td>
<td style="text-align:center">5.3</td>
</tr>
</tbody>
</table>
</div>
<p>因为Stanford CoreNLP调用的是这个pipeline，而我们实际用的是切词功能，所以尝试只用它的切词部分功能，但是在做的过程中发现一些问题，整理如下：</p>
<p>官网给出的方法<a href="http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.stanford_segmenter" target="_blank" rel="noopener">nltk.tokenize.stanford_segmenter module</a>是这么写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize.stanford_segmenter <span class="keyword">import</span> StanfordSegmenter</span><br><span class="line">seg = StanfordSegmenter()</span><br><span class="line">seg.default_config(<span class="string">'zh'</span>)</span><br></pre></td></tr></table></figure>
<p>但是这个缺少各种数据路径的，是完全不通的。</p>
<p>然后度娘的top1的答案给出的解决方案是：<a href="http://www.52nlp.cn/python自然语言处理实践-在nltk中使用斯坦福中文分词器" target="_blank" rel="noopener"></a>`<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">segmenter = StanfordSegmenter(path_to_jar=<span class="string">"stanford-segmenter-3.4.1.jar"</span>, path_to_sihan_corpora_dict=<span class="string">"./data"</span>, path_to_model=<span class="string">"./data/pku.gz"</span>, path_to_dict=<span class="string">"./data/dict-chris6.ser.gz"</span>)</span><br></pre></td></tr></table></figure></p>
<p>如果你的nltk的版本比较新，恭喜你，你会遇到下面这个问题：<br><code>TypeError: expected str, bytes or os.PathLike object, not NoneType</code></p>
<p>我在<a href="https://stackoverflow.com/questions/45663121/about-stanford-word-segmenter/45668849" target="_blank" rel="noopener">stackoverflow</a>上找了半天，发现有如下的解决方案：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.parse.corenlp <span class="keyword">import</span> CoreNLPParser </span><br><span class="line">corenlp_parser = CoreNLPParser(<span class="string">'http://localhost:9001'</span>, encoding=<span class="string">'utf8'</span>)</span><br><span class="line">result = corenlp_parser.api_call(text, &#123;<span class="string">'annotators'</span>: <span class="string">'tokenize,ssplit'</span>&#125;)</span><br><span class="line">tokens = [token[<span class="string">'originalText'</span>] <span class="keyword">or</span> token[<span class="string">'word'</span>] <span class="keyword">for</span> sentence <span class="keyword">in</span> result[<span class="string">'sentences'</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以完美解决，原因之前作者也说了，据称升级版本后不兼容，各位看看就好<a href="https://github.com/nltk/nltk/issues/1808" target="_blank" rel="noopener">“TypeError: expected str, bytes or os.PathLike object, not NoneType” about Stanford NLP </a>。</p>
<p>这个坑花了我两个多小时（主要在下载各种gz包），希望大家能够避免。</p>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，<a href="https://www.zhihu.com/people/sladesha/activities" target="_blank" rel="noopener">知乎</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[logistic regression一点理解]]></title>
      <url>/2018/08/14/regression%E4%B8%80%E7%82%B9%E7%90%86%E8%A7%A3/</url>
      <content type="html"><![CDATA[<p>Hexo没有办法用Latex，所以采取了截图的方式，更舒适的阅读体验可以参见<a href="https://www.jianshu.com/p/61ac39a57f9d" target="_blank" rel="noopener">logistic regression一点理解</a>。</p>
<p><img src="/2018/08/14/regression一点理解/1.png" alt=""></p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Tensorflow实现FFM]]></title>
      <url>/2018/08/06/%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0FFM/</url>
      <content type="html"><![CDATA[<p>没错，这次登场的是FFM。各大比赛中的“种子”算法，中国台湾大学Yu-Chin Juan荣誉出品，美团技术团队背书，Michael Jahrer的论文的field概念灵魂升华，土豪公司鉴别神器。通过引入field的概念，FFM把相同性质的特征归于同一个field，相当于把FM中已经细分的feature再次拆分，可不可怕，厉不厉害？好，让我们来看看怎么一个厉害法。</p>
<h1 id="FFM理论"><a href="#FFM理论" class="headerlink" title="FFM理论"></a>FFM理论</h1><h2 id="特征交互"><a href="#特征交互" class="headerlink" title="特征交互"></a>特征交互</h2><p>网上已经说烂了的美团技术团队给出的那张图：<br><img src="/2018/08/06/基于Tensorflow实现FFM/1.png" alt=""><br>针对Country这个变量，<br>FM的做法是one-hot-encoding，生成country_USA，country_China两个稀疏的变量，再进行embedding向量化。<br>FFM的做法是cross-one-hot-encoding，生成country_USA_Day_26/11/15，country_USA_Ad_type_Movie…M个变量，再进行embedding向量化。<br><img src="/2018/08/06/基于Tensorflow实现FFM/2.png" alt=""><br>就和上图一样，fm做出来的latent factor是二维的，就是给每个特征一个embedding结果；而暴力的ffm做出的latent factor是三维的，出来给特征embedding还考虑不同维度特征给不同的embedding结果，也是FFM中“field-aware”的由来。<br><img src="/2018/08/06/基于Tensorflow实现FFM/3.png" alt=""><br>同时从公式中看，对于xi这个特征为什么embedding的latent factor向量的V是Vifj，其实就是因为xi乘以的是xj，所以latent factor向量的信息提取才是field j，也就是fj。多说一句，网上很多给出的现成的代码，这边都是写错了的，都写着写着变成了vifi，可能是写的顺手。</p>
<p>都说到这里了，我再多说一句，为什么说ffm是土豪公司鉴别神器呢？我们看下仅仅是二次项，ffm需要计算的参数有 nfk 个，远多于FM模型的 nk个，而且由于每次计算都依赖于乘以的xj的field，所以，无法用fm的那个计算技巧(ab = 1/2(a+b)^2-a^2-b^2)，所以计算复杂度是 O(kn^2)。这种情况下，没有GPU就不要想了，有GPU的特征多于50个，而且又很离散的，没有个三位数的GPU也算了。之前我看美团说他们在用，我想再去看看他们的实际用的过程的时候，发现文章被删了，真的是可惜，我其实一直也想知道如何减轻这个变态的计算量的方法。</p>
<p>给个实例给大家看下以上的这些的应用：<br>依旧来自美团技术研发团队中给出的案例，有用户数据如下：<br><img src="/2018/08/06/基于Tensorflow实现FFM/4.png" alt=""><br>这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。<br><img src="/2018/08/06/基于Tensorflow实现FFM/5.png" alt=""><br>红色部分对应的是field，来自于原始特征的个数；蓝色部分对应的是feature，来自于原始特征onehot之后的个数。<br>对于特征Feature:<code>User=YuChin</code>而言，有<code>Movie=3Idiots</code>、<code>Genre=Comedy</code>、<code>Genre=Drama</code>、<code>Price</code>四项要交互：<br><img src="/2018/08/06/基于Tensorflow实现FFM/6.png" alt=""><br><code>User=YuChin</code>与<code>Movie=3Idiots</code>交互是<v1,2,v2,1>·1·1，也就是第一项，为什么是V1,2呢？因为<code>User=YuChin</code>是Featureindex=1，而交互的<code>Movie=3Idiots</code>是Fieldindex=2，同理V2,1也是这样的，以此类推，那么，FFM的组合特征有10项，如下图所示：<br><img src="/2018/08/06/基于Tensorflow实现FFM/7.png" alt=""><br>这就是一个案例的实际操作过程。</v1,2,v2,1></p>
<h2 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h2><p>为什么要把这个单拎出来说呢？我看了网上不少的对于特征的处理过程，版本实在是太多了，而且差异化也蛮大，这边就和大家一起梳理一下：<br>1.feature index * feature value<br>这个就是上面我这个实际案例的方式，对于分类变量采取onehot，对于连续变量之间进行值的点积，不做处理。优点是快速简单，不需要预处理，但是缺点也很明显，离群点影响，值的波动大等。</p>
<p>2.连续值离散化<br>这个方法借鉴了Cart里面对连续值的处理方式，就是把所有的连续值都当成一个分类变量处理。举例，现在有一个年龄age的连续变量[10,19,20,22,22,34]，这种方法就生成了age_10,age_19,age_20,age_22,age_34这些变量，如果连续值一多，这个方法带来的计算量就直线上升。</p>
<p>3.分箱下的连续值离散化<br>这种方法优化了第二种方法，举例解释，现在有一个年龄age的连续变量[10,19,20,22,22,34]，我们先建立一个map，[0,10):0,[10,20):1,[20,30):2,[30,100):3。原始的数据就变成了[1,1,2,2,2,3]，再进行2的连续值离散化方法，生成了age_1,age_2,age_3这几个变量，优化了计算量，而且使得结果更具有解释性。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="logisitc-loss"><a href="#logisitc-loss" class="headerlink" title="logisitc loss"></a>logisitc loss</h3><p>这个是官方指定的方法，是-1/1做二分类的时候常用的loss计算方法：<br><img src="/2018/08/06/基于Tensorflow实现FFM/8.png" alt=""><br>这边需要注意的是，在做的时候，需要把label拆分成-1/1而不是0/1，当我们预测正确的时候，pred<em>label&gt;0且越大正确的程度越高，相应的log项是越小的，整体loss越小；相反，如果我们预测的越离谱，pred</em>label&lt;0且越小离谱的程度越高，相应的log项是越大的，整体loss越大。</p>
<h3 id="交互熵"><a href="#交互熵" class="headerlink" title="交互熵"></a>交互熵</h3><p>我看到很多人的实现依旧用了<code>tf.nn.softmax_cross_entropy_with_logits</code>，其实就是多分类中的损失函数，和大家平时的图像分类、商品推荐召回一模一样：<br><img src="/2018/08/06/基于Tensorflow实现FFM/9.png" alt=""><br>这边需要注意的是，在做的时候，需要把label拆分成[1,0]和[0,1]进行计算。不得不说，大家真的是为了省事很机智(丧心病狂)啊！</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>我这边只给一些关键地方的代码，更多的去GitHub里面看吧。</p>
<h2 id="embedding-part"><a href="#embedding-part" class="headerlink" title="embedding part"></a>embedding part</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.v = tf.get_variable(<span class="string">'v'</span>, shape=[self.p, self.f, self.k], dtype=<span class="string">'float32'</span>,initializer=tf.truncated_normal_initializer(mean=<span class="number">0</span>, stddev=<span class="number">0.01</span>))</span><br></pre></td></tr></table></figure>
<p>看到了，这边生成的v就是上面Vffm的形式。</p>
<h2 id="inference-part"><a href="#inference-part" class="headerlink" title="inference part"></a>inference part</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(self.p):</span><br><span class="line"><span class="comment"># 寻找没有match过的特征，也就是论文中的j = i+1开始</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, self.p):</span><br><span class="line">print(<span class="string">'i:%s,j:%s'</span> % (i, j))</span><br><span class="line"><span class="comment"># vifj</span></span><br><span class="line">vifj = self.v[i, self.feature2field[j]]</span><br><span class="line"><span class="comment"># vjfi</span></span><br><span class="line">vjfi = self.v[j, self.feature2field[I]]</span><br><span class="line"><span class="comment"># vi · vj</span></span><br><span class="line">vivj = tf.reduce_sum(tf.multiply(vifj, vjfi))</span><br><span class="line"><span class="comment"># xi · xj</span></span><br><span class="line">xixj = tf.multiply(self.X[:, i], self.X[:, j])</span><br><span class="line">self.field_cross_interaction += tf.multiply(vivj, xixj)</span><br></pre></td></tr></table></figure>
<p>我这边强行拆开了写，这样看起来更清晰一点，注意这边的vifj和vjfi的生成，这边也可以看到找对于的field的方法是用了filed这个字典，这就是为什么不能用fm的点击技巧。</p>
<h2 id="loss-part"><a href="#loss-part" class="headerlink" title="loss part"></a>loss part</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -1/1情况下的logistic loss</span></span><br><span class="line">self.loss = tf.reduce_mean(tf.log(<span class="number">1</span> + tf.exp(-self.y * self.y_out)))</span><br></pre></td></tr></table></figure>
<p>这边记得论文中的负号，如果有batch的情况下记得求个平均再进行bp过程。</p>
<h1 id="论文结论"><a href="#论文结论" class="headerlink" title="论文结论"></a>论文结论</h1><p>原始的ffm论文中给出了一些结论，我们在实际使用中值得参考：</p>
<ul>
<li>k值不用太大，没啥提升<br><img src="/2018/08/06/基于Tensorflow实现FFM/10.png" alt=""></li>
<li>正则项lambda和学习率alpha需要着重调参<br><img src="/2018/08/06/基于Tensorflow实现FFM/11.png" alt=""></li>
<li>epoch别太大，既会拖慢速度，而且造成过拟合；在原论文中甚至要考虑用early-stopping避免过拟合，所以epoch=1，常规的来讲就可以了，论文中提到的early-stopping操作：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> Split the data set into a training set <span class="keyword">and</span> a validation set.</span><br><span class="line"><span class="number">2.</span> At the end of each epoch, use the validation set to calcu-</span><br><span class="line">late the loss.</span><br><span class="line"><span class="number">3.</span> If the loss goes up, record the number of epochs. Stop <span class="keyword">or</span></span><br><span class="line">go to step <span class="number">4.</span></span><br><span class="line"><span class="number">4.</span> If needed, use the full data set to re-train a model <span class="keyword">with</span></span><br><span class="line">the number of epochs obtained <span class="keyword">in</span> step <span class="number">3.</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>FFM是一个细化隐向量非常好的方法，虽然很简单，但还是有很多细节之处值得考虑，比如如何线上应用，如何可解释，如何求稀疏解等等。在部署实现FFM之前，我还是建议大家先上线FM，当效果真的走投无路的时候再考虑FFM，FFM在工业界的影响着实不如学术界那么强大，偷偷说一句，<strong>太慢了，真的是太慢了，慢死了</strong>，我宁可去用deepfm。</p>
<p>最后，给出代码实现的Github地址<a href="https://github.com/sladesha/machine_learning/tree/master/FFM" target="_blank" rel="noopener">FFM</a>，这边是我自己写的，理解理解算法可以，但是实际用的时候建议参考FFM的实现比较好的项目比如libffm，最近比较火的xlearn。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 特征交叉 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Tensorflow实现DeepFM]]></title>
      <url>/2018/07/30/%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0DeepFM/</url>
      <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>DeepFM，Ctr预估中的大杀器，哈工大与华为诺亚方舟实验室荣耀出品，算法工程师面试高频考题，有效的结合了神经网络与因子分解机在特征学习中的优点：同时提取到低阶组合特征与高阶组合特征，这样的称号我可以写几十条出来，这也说明了DeepFM确实是一个非常值得手动撸一边的算法。</p>
<p>当然，早就有一票人写了一车封装好的deepFM的模型，大家随便搜搜肯定也能搜到，既然这样，我就不再搞这些东西了，今天主要和大家过一遍，deepFM的代码是咋写的，手把手入门一下，说一些我觉得比较重要的地方，方便大家按需修改。（只列举了一部分，更多的解释参见GitHub代码中的注释）</p>
<p>本文的数据和部分代码构造参考了nzc大神的<a href="https://github.com/sladesha/dnn_ctr/blob/master/model/DeepFM.py" target="_blank" rel="noopener">deepfm</a>的Pytorch版本的写法，改成tensorflow的形式，需要看原版的自取。</p>
<h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="/2018/07/30/基于Tensorflow实现DeepFM/1.png" alt=""></p>
<p>DeepFM包含两部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取，两部分权重共享。<br>DeepFM的预测结果可以写为：<code>y = sigmoid(y(fm)+y(DNN))</code></p>
<h2 id="FM部分"><a href="#FM部分" class="headerlink" title="FM部分"></a>FM部分</h2><p><img src="/2018/07/30/基于Tensorflow实现DeepFM/2.png" alt=""></p>
<p>FM公式为：<br><img src="/2018/07/30/基于Tensorflow实现DeepFM/3.png" alt=""></p>
<p>我很久之前一篇文章细节讲过，这边就不多扯了，更多详见<a href="http://shataowei.com/2017/12/04/FM理论解析及应用/" target="_blank" rel="noopener">FM理论解析及应用</a>。</p>
<h2 id="DNN部分"><a href="#DNN部分" class="headerlink" title="DNN部分"></a>DNN部分</h2><p><img src="/2018/07/30/基于Tensorflow实现DeepFM/4.png" alt=""></p>
<p>这边其实和我上篇文章说的MLPS差距不大，也就是简单的全链接，差就差在input的构造，这边采取了embedding的思想，将每个feature转化成了embedded vector作为input，同时此处的input也是上面计算FM中的V，更多的大家看代码就完全了解了。</p>
<h1 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h1><p>我一共写了两个script，build_data.py和deepfm.py，也很好理解。build_data.py用来预处理数据，deepfm.py用来跑模型。</p>
<h2 id="build-data-py"><a href="#build-data-py" class="headerlink" title="build_data.py"></a>build_data.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, data.shape[<span class="number">1</span>]):</span><br><span class="line">target = data.iloc[:, I]</span><br><span class="line">col = target.name</span><br><span class="line">l = len(set(target))</span><br><span class="line"><span class="keyword">if</span> l &gt; <span class="number">10</span>:</span><br><span class="line">target = (target - target.mean()) / target.std()</span><br><span class="line">co_feature = pd.concat([co_feature, target], axis=<span class="number">1</span>)</span><br><span class="line">feat_dict[col] = cnt</span><br><span class="line">cnt += <span class="number">1</span></span><br><span class="line">co_col.append(col)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">us = target.unique()</span><br><span class="line">print(us)</span><br><span class="line">feat_dict[col] = dict(zip(us, range(cnt, len(us) + cnt)))</span><br><span class="line">ca_feature = pd.concat([ca_feature, target], axis=<span class="number">1</span>)</span><br><span class="line">cnt += len(us)</span><br><span class="line">ca_col.append(col)</span><br><span class="line">feat_dim = cnt</span><br><span class="line">feature_value = pd.concat([co_feature, ca_feature], axis=<span class="number">1</span>)</span><br><span class="line">feature_index = feature_value.copy()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> feature_index.columns:</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">in</span> co_col:</span><br><span class="line">feature_index[i] = feat_dict[I]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">feature_index[i] = feature_index[i].map(feat_dict[I])</span><br><span class="line">feature_value[i] = <span class="number">1.</span></span><br></pre></td></tr></table></figure>
<p>核心部分如上，重要的是做了两件事情，生成了feature_index和feature_value。</p>
<p>feature_index是把所有特征进行了标序，feature1，feature2……featurem，分别对应0，1，2，3，…m，但是，请注意<strong>分类变量需要拆分</strong>！就是说如果有性别：男|女|未知，三个选项。需要构造feature男，feature女，feature未知三个变量，而连续变量就不需要这样。</p>
<p>feature_value就是特征的值，连续变量按真实值填写，<strong>分类变量</strong>全部填写1。</p>
<p>更加形象的如下：<br><img src="/2018/07/30/基于Tensorflow实现DeepFM/5.png" alt=""></p>
<h2 id="deepfm-py"><a href="#deepfm-py" class="headerlink" title="deepfm.py"></a>deepfm.py</h2><h3 id="特征向量化"><a href="#特征向量化" class="headerlink" title="特征向量化"></a>特征向量化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征向量化，类似原论文中的v</span></span><br><span class="line">self.weight[<span class="string">'feature_weight'</span>] = tf.Variable(</span><br><span class="line">tf.random_normal([self.feature_sizes, self.embedding_size], <span class="number">0.0</span>, <span class="number">0.01</span>),</span><br><span class="line">name=<span class="string">'feature_weight'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次项中的w系数，类似原论文中的w</span></span><br><span class="line">self.weight[<span class="string">'feature_first'</span>] = tf.Variable(</span><br><span class="line">tf.random_normal([self.feature_sizes, <span class="number">1</span>], <span class="number">0.0</span>, <span class="number">1.0</span>),</span><br><span class="line">name=<span class="string">'feature_first'</span>)</span><br></pre></td></tr></table></figure>
<p>可以对照下面的公式看，更有感觉。<br><img src="/2018/07/30/基于Tensorflow实现DeepFM/6.png" alt=""></p>
<h3 id="deep网络部分的weight"><a href="#deep网络部分的weight" class="headerlink" title="deep网络部分的weight"></a>deep网络部分的weight</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># deep网络初始input：把向量化后的特征进行拼接后带入模型，n个特征*embedding的长度</span></span><br><span class="line">input_size = self.field_size * self.embedding_size</span><br><span class="line">init_method = np.sqrt(<span class="number">2.0</span> / (input_size + self.deep_layers[<span class="number">0</span>]))</span><br><span class="line">self.weight[<span class="string">'layer_0'</span>] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(input_size, self.deep_layers[<span class="number">0</span>])), dtype=np.float32</span><br><span class="line">)</span><br><span class="line">self.weight[<span class="string">'bias_0'</span>] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(<span class="number">1</span>, self.deep_layers[<span class="number">0</span>])), dtype=np.float32</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 生成deep network里面每层的weight 和 bias</span></span><br><span class="line"><span class="keyword">if</span> num_layer != <span class="number">1</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_layer):</span><br><span class="line">init_method = np.sqrt(<span class="number">2.0</span> / (self.deep_layers[i - <span class="number">1</span>] + self.deep_layers[I]))</span><br><span class="line">self.weight[<span class="string">'layer_'</span> + str(i)] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(self.deep_layers[i - <span class="number">1</span>], self.deep_layers[i])),</span><br><span class="line">dtype=np.float32)</span><br><span class="line">self.weight[<span class="string">'bias_'</span> + str(i)] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(<span class="number">1</span>, self.deep_layers[i])),</span><br><span class="line">dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># deep部分output_size + 一次项output_size + 二次项output_size</span></span><br><span class="line">last_layer_size = self.deep_layers[<span class="number">-1</span>] + self.field_size + self.embedding_size</span><br><span class="line">init_method = np.sqrt(np.sqrt(<span class="number">2.0</span> / (last_layer_size + <span class="number">1</span>)))</span><br><span class="line"><span class="comment"># 生成最后一层的结果</span></span><br><span class="line">self.weight[<span class="string">'last_layer'</span>] = tf.Variable(</span><br><span class="line">np.random.normal(loc=<span class="number">0</span>, scale=init_method, size=(last_layer_size, <span class="number">1</span>)), dtype=np.float32)</span><br><span class="line">self.weight[<span class="string">'last_bias'</span>] = tf.Variable(tf.constant(<span class="number">0.01</span>), dtype=np.float32)</span><br></pre></td></tr></table></figure>
<p>input的地方需要注意一下，这边用了个技巧，直接把把向量化后的特征进行拉伸拼接后带入模型，原来的v是<code>batch*n个特征*embedding的长度</code>，直接改成了<code>batch*（n个特征*embedding的长度）</code>，这样的好处就是全值共享，又快又有效。</p>
<h3 id="网络传递部分"><a href="#网络传递部分" class="headerlink" title="网络传递部分"></a>网络传递部分</h3><p>都是一些正常的操作，稍微要注意一下的是交互项的计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># second_order</span></span><br><span class="line">self.sum_second_order = tf.reduce_sum(self.embedding_part, <span class="number">1</span>)</span><br><span class="line">self.sum_second_order_square = tf.square(self.sum_second_order)</span><br><span class="line">print(<span class="string">'sum_square_second_order:'</span>, self.sum_second_order_square)</span><br><span class="line"><span class="comment"># sum_square_second_order: Tensor("Square:0", shape=(?, 256), dtype=float32)</span></span><br><span class="line"></span><br><span class="line">self.square_second_order = tf.square(self.embedding_part)</span><br><span class="line">self.square_second_order_sum = tf.reduce_sum(self.square_second_order, <span class="number">1</span>)</span><br><span class="line">print(<span class="string">'square_sum_second_order:'</span>, self.square_second_order_sum)</span><br><span class="line"><span class="comment"># square_sum_second_order: Tensor("Sum_2:0", shape=(?, 256), dtype=float32)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1/2*((a+b)^2 - a^2 - b^2)=ab</span></span><br><span class="line">self.second_order = <span class="number">0.5</span> * tf.subtract(self.sum_second_order_square, self.square_second_order_sum)</span><br><span class="line"></span><br><span class="line">self.fm_part = tf.concat([self.first_order, self.second_order], axis=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'fm_part:'</span>, self.fm_part)</span><br></pre></td></tr></table></figure></p>
<p>直接实现了下面的计算逻辑：<br><img src="/2018/07/30/基于Tensorflow实现DeepFM/7.png" alt=""></p>
<h3 id="loss部分"><a href="#loss部分" class="headerlink" title="loss部分"></a>loss部分</h3><p>我个人重写了一下我认为需要正则的地方，和一些loss的计算方式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># loss</span></span><br><span class="line">self.out = tf.nn.sigmoid(self.out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss = tf.losses.log_loss(label,out) 也行，看大家想不想自己了解一下loss的计算过程</span></span><br><span class="line">self.loss = -tf.reduce_mean(</span><br><span class="line">self.label * tf.log(self.out + <span class="number">1e-24</span>) + (<span class="number">1</span> - self.label) * tf.log(<span class="number">1</span> - self.out + <span class="number">1e-24</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则：sum(w^2)/2*l2_reg_rate</span></span><br><span class="line"><span class="comment"># 这边只加了weight，有需要的可以加上bias部分</span></span><br><span class="line">self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg_rate)(self.weight[<span class="string">"last_layer"</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.deep_layers)):</span><br><span class="line">self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg_rate)(self.weight[<span class="string">"layer_%d"</span> % I])</span><br></pre></td></tr></table></figure></p>
<p>大家也可以直接按照我注释掉的部分简单操作，看个人的理解了。</p>
<h3 id="梯度正则"><a href="#梯度正则" class="headerlink" title="梯度正则"></a>梯度正则</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line">opt = tf.train.GradientDescentOptimizer(self.learning_rate)</span><br><span class="line">trainable_params = tf.trainable_variables()</span><br><span class="line">print(trainable_params)</span><br><span class="line">gradients = tf.gradients(self.loss, trainable_params)</span><br><span class="line">clip_gradients, _ = tf.clip_by_global_norm(gradients, <span class="number">5</span>)</span><br><span class="line">self.train_op = opt.apply_gradients(</span><br><span class="line">zip(clip_gradients, trainable_params), global_step=self.global_step)</span><br></pre></td></tr></table></figure>
<p>很多网上的代码跑着跑着就NAN了，建议加一下梯度的正则，反正也没多复杂。</p>
<h3 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a>执行结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">/Users/slade/anaconda3/bin/python /Users/slade/Documents/Personalcode/machine-learning/Python/deepfm/deepfm.py</span><br><span class="line">[<span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">3</span> <span class="number">4</span> <span class="number">6</span> <span class="number">5</span> <span class="number">7</span>]</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line">[<span class="number">6</span> <span class="number">0</span> <span class="number">8</span> <span class="number">2</span> <span class="number">4</span> <span class="number">1</span> <span class="number">7</span> <span class="number">3</span> <span class="number">5</span> <span class="number">9</span>]</span><br><span class="line">[<span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use SSE4<span class="number">.2</span> instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn'</span>t compiled to use AVX2 instructions, but these are available on your machine <span class="keyword">and</span> could speed up CPU computations.</span><br><span class="line">W tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">45</span>] The TensorFlow library wasn<span class="string">'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span></span><br><span class="line"><span class="string">embedding_part: Tensor("Mul:0", shape=(?, 39, 256), dtype=float32)</span></span><br><span class="line"><span class="string">first_order: Tensor("Sum:0", shape=(?, 39), dtype=float32)</span></span><br><span class="line"><span class="string">sum_square_second_order: Tensor("Square:0", shape=(?, 256), dtype=float32)</span></span><br><span class="line"><span class="string">square_sum_second_order: Tensor("Sum_2:0", shape=(?, 256), dtype=float32)</span></span><br><span class="line"><span class="string">fm_part: Tensor("concat:0", shape=(?, 295), dtype=float32)</span></span><br><span class="line"><span class="string">deep_embedding: Tensor("Reshape_2:0", shape=(?, 9984), dtype=float32)</span></span><br><span class="line"><span class="string">output: Tensor("Add_3:0", shape=(?, 1), dtype=float32)</span></span><br><span class="line"><span class="string">[&lt;tensorflow.python.ops.variables.Variable object at 0x10e2a9ba8&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112885ef0&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3c18&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3da0&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3f28&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3c50&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112a03dd8&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112a03b38&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x16eae5c88&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112b937b8&gt;]</span></span><br><span class="line"><span class="string">time all:7156</span></span><br><span class="line"><span class="string">epoch 0:</span></span><br><span class="line"><span class="string">the times of training is 0, and the loss is 8.54514</span></span><br><span class="line"><span class="string">the times of training is 100, and the loss is 1.60875</span></span><br><span class="line"><span class="string">the times of training is 200, and the loss is 0.681524</span></span><br><span class="line"><span class="string">the times of training is 300, and the loss is 0.617403</span></span><br><span class="line"><span class="string">the times of training is 400, and the loss is 0.431383</span></span><br><span class="line"><span class="string">the times of training is 500, and the loss is 0.531491</span></span><br><span class="line"><span class="string">the times of training is 600, and the loss is 0.558392</span></span><br><span class="line"><span class="string">the times of training is 800, and the loss is 0.51909</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>
<p>看了下没啥大问题。</p>
<h1 id="还有一些要说的"><a href="#还有一些要说的" class="headerlink" title="还有一些要说的"></a>还有一些要说的</h1><ul>
<li>build_data.py中我为了省事，只做了标准化，没有进行其他数据预处理的步骤，这个是错误的，大家在实际使用中请按照我在公众号里面给大家进行的数据预处理步骤进行，<strong>这个非常重要</strong>！</li>
<li>learing_rate是我随便设置的，在实际大家跑模型的时候，请务必按照1.0，1e-3，1e-6，三个节点进行二分调优。</li>
<li>如果你直接搬上面代码，妥妥过拟合，请在真实使用过程中，务必根据数据量调整batch的大小，epoch的大小，建议在每次传递完成后加上<code>tf.nn.dropout</code>进行dropout。</li>
<li>如果数据量连10万量级都不到，我还是建议用机器学习的方法，xgboost+lr，mixed logistics regression等等都是不错的方法。</li>
</ul>
<p>好了，最后附上全量代码的地址<a href="https://github.com/sladesha/deep_learning" target="_blank" rel="noopener">Github</a>，希望对大家有所帮助。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 特征交叉 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Tensorflow实现多层感知机网络MLPs]]></title>
      <url>/2018/07/25/%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%BD%91%E7%BB%9CMLPs/</url>
      <content type="html"><![CDATA[<p>之前在<a href="https://www.jianshu.com/p/f26e232bff57" target="_blank" rel="noopener">基于Tensorflow的神经网络解决用户流失概率问题</a>写了一个MLPs的网络，很多人在问，其实这个网络看起来很清晰，但是却写的比较冗长，这边优化了一个版本更方便大家修改后直接使用。</p>
<p><img src="/2018/07/25/基于Tensorflow实现多层感知机网络MLPs/1.png" alt="多层感知机网络"></p>
<p>直接和大家过一遍核心部分：</p>
<p>上次我们计算过程中，通过的是先定义好多层网络中每层的weight，在通过<code>tf.matual</code>进行层与层之间的计算，最后再通过tf.contrib.layers.l2_regularizer进行正则；而这次我们直接通过图像识别中经常使用的全连接（FC）的接口，只需要确定每层的节点数，通过<code>layers_nodes</code>进行声明，自动可以计算出不同层下的weight，更加清晰明了。另外，还增加了dropout的部分，降低过拟合的问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">din_all = tf.layers.batch_normalization(inputs=din_all, name=<span class="string">'b1'</span>)</span><br><span class="line">layer_1 = tf.layers.dense(din_all, self.layers_nodes[<span class="number">0</span>], activation=tf.nn.sigmoid,use_bias=<span class="keyword">True</span>,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name=<span class="string">'f1'</span>)</span><br><span class="line">layer_1 = tf.nn.dropout(layer_1, keep_prob=self.drop_rate[<span class="number">0</span>])</span><br><span class="line">layer_2 = tf.layers.dense(layer_1, self.layers_nodes[<span class="number">1</span>], activation=tf.nn.sigmoid,use_bias=<span class="keyword">True</span>,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name=<span class="string">'f2'</span>)</span><br><span class="line">layer_2 = tf.nn.dropout(layer_2, keep_prob=self.drop_rate[<span class="number">1</span>])</span><br><span class="line">layer_3 = tf.layers.dense(layer_2, self.layers_nodes[<span class="number">2</span>], activation=tf.nn.sigmoid,use_bias=<span class="keyword">True</span>,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name=<span class="string">'f3'</span>)</span><br></pre></td></tr></table></figure>
<p><code>tf.layers.dense</code>接口信息如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.dense(</span><br><span class="line">inputs,</span><br><span class="line">units,</span><br><span class="line">activation=<span class="keyword">None</span>,</span><br><span class="line">use_bias=<span class="keyword">True</span>,</span><br><span class="line">kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">bias_initializer=tf.zeros_initializer(),</span><br><span class="line">kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">trainable=<span class="keyword">True</span>,</span><br><span class="line">name=<span class="keyword">None</span>,</span><br><span class="line">reuse=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<ul>
<li>inputs：必需，即需要进行操作的输入数据。</li>
<li>units：必须，即神经元的数量。</li>
<li>activation：可选，默认为 None，如果为 None 则是线性激活。</li>
<li>use_bias：可选，默认为 True，是否使用偏置。</li>
<li>kernel_initializer：可选，默认为 None，即权重的初始化方法，如果为 None，则使用默认的 Xavier 初始化方法。</li>
<li>bias_initializer：可选，默认为零值初始化，即偏置的初始化方法。</li>
<li>kernel_regularizer：可选，默认为 None，施加在权重上的正则项。</li>
<li>bias_regularizer：可选，默认为 None，施加在偏置上的正则项。</li>
<li>activity_regularizer：可选，默认为 None，施加在输出上的正则项。</li>
<li>kernel_constraint，可选，默认为 None，施加在权重上的约束项。</li>
<li>bias_constraint，可选，默认为 None，施加在偏置上的约束项。</li>
<li>trainable：可选，默认为 True，布尔类型，如果为 True，则将变量添加到 GraphKeys.TRAINABLE_VARIABLES 中。</li>
<li>name：可选，默认为 None，卷积层的名称。</li>
<li>reuse：可选，默认为 None，布尔类型，如果为 True，那么如果 name 相同时，会重复利用。</li>
</ul>
<p>除此之外，之前我们定义y和y_的时候把1转化为[1,0]，转化为了[0,1]，增加了工程量，这次我们通过：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy_mean = -tf.reduce_mean(self.y_ * tf.log(self.output + <span class="number">1e-24</span>))</span><br><span class="line">self.loss = cross_entropy_mean</span><br></pre></td></tr></table></figure>
<p>直接进行计算，避免了一些无用功。</p>
<p>最后，之前对于梯度的值没有进行限制，会导致整体模型的波动过大，这次优化中也做了修改，如果大家需要也可以参考一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们用learning_rate_base作为速率η，来训练梯度下降的loss函数解，对梯度进行限制后计算loss</span></span><br><span class="line">opt = tf.train.GradientDescentOptimizer(self.learning_rate_base)</span><br><span class="line">trainable_params = tf.trainable_variables()</span><br><span class="line">gradients = tf.gradients(self.loss, trainable_params)</span><br><span class="line">clip_gradients, _ = tf.clip_by_global_norm(gradients, <span class="number">5</span>)</span><br><span class="line">self.train_op = opt.apply_gradients(zip(clip_gradients, trainable_params), global_step=self.global_step)</span><br></pre></td></tr></table></figure></p>
<p>MLPs是入门级别的神经网络算法，实际的工业开发中使用的频率也不高，<strong>后面我准备和大家过一下常见的FM、FFM、DeepFM、NFM、DIN、MLR等在工业开发中更为常见的网络，欢迎大家持续关注。</strong></p>
<p>完整代码已经上传到Github中。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。<br><img src="https://upload-images.jianshu.io/upload_images/1129359-654dc61c581d94e1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[伪标签半监督学习]]></title>
      <url>/2018/07/24/%E4%BC%AA%E6%A0%87%E7%AD%BE%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
      <content type="html"><![CDATA[<p>之前在训练YoutubeNet和DCN的时候，我都发现平台用户中基础用户的信息数据缺失率特别高，比如性别一栏准确填写的不足60%，所以我一直想调研一下有没有什么更好的填充方法，要保证既不能太复杂太耗时，也要有足够好的效果。</p>
<p>其实这个问题就是一个缺失值填充，之前的文章中也写过很多办法，常规的也总结过：</p>
<ul>
<li>均值、众数填充<br>最简单的填充，效果也惨不忍睹</li>
<li>根据没有缺失的数据线性回归填充<br>填充的好会造成共线性错误，填充的不好就没价值，很矛盾</li>
<li>剔除<br>丢失信息量</li>
<li>设置哑变量<br>会造成数据分布有偏</li>
<li>smote<br>连续值有效，离散值就无法实施了</li>
</ul>
<p>我在Google上看imbalance问题的时候，偶然看到了这个<a href="http://course.fast.ai/?spm=a2c4e.11153940.blogcont215222.9.378a3aadIvrayn" target="_blank" rel="noopener">视频教程</a>，上面讲了图像的缺失处理，提到了伪标签处理的半监督学习方式。我就在国内的论坛上找了下，阿里云技术论坛也同样注意到了这个问题，但是只给出了如下的粗糙的构思图：<br><img src="/2018/07/24/伪标签半监督学习/1.png" alt=""></p>
<p>有一份整理了的流程图，具体执行步骤总结，和大家一起看一下：<br><img src="/2018/07/24/伪标签半监督学习/2.png" alt=""></p>
<ul>
<li>将有标签部分数据分为两份：train_set&amp;validation_set，并训练出最优的model1</li>
<li>用model1对未知标签数据(test_set)进行预测，给出伪标签结果pseudo-labeled</li>
<li>将train_set中抽取一部分做新的validation_set，把剩余部分与pseudo-labeled部分融合作为新的train_set，训练出最优的model2</li>
<li>再用model2对未知标签数据(test_set)进行预测，得到最终的final result label</li>
</ul>
<p>我利用了已知标签的数据对这个方法进行测试，用了最简单的mixed logistic regression模型作为Basic Model，得到结果如下：<br><img src="/2018/07/24/伪标签半监督学习/3.png" alt=""><br>利用伪标签半监督的方式，同样的mixed logistic regression模型AUC值会提高0.1pp左右，效果还不错，而且实施并不复杂，大家可以在缺失值处理或者分类问题中应用尝试一下。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。</p>
]]></content>
      
        <categories>
            
            <category> 特征刻画 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据处理 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[热传导-物质扩散算法应用于推荐]]></title>
      <url>/2018/07/19/%E7%83%AD%E4%BC%A0%E5%AF%BC-%E7%89%A9%E8%B4%A8%E6%89%A9%E6%95%A3%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E4%BA%8E%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<p>没有大量的数据，没有大量的人力就不能做好推荐么？当然不是，热传导/物质扩散推荐算法就是作为冷启动及小规模团队非常实用的推荐召回部分的算法。</p>
<p>目标是为a图中标有星号（不妨记为用户1）的用户推荐商品，该用户已经购买过的两件商品是我们可以利用的信息，用来给目标用户进行推荐。</p>
<p>物质扩散算法：<br><img src="/2018/07/19/热传导-物质扩散算法应用于推荐/1.png" alt=""><br>初始，我们认为每件被目标用户购买过的商品的信息量为1。<br>商品把自己的信息平均分给所有购买过它的用户，用户的信息值则是从所有商品所得到的信息值得总和，比如上图(b)中的第一个节点的信息就等于第一个商品平均分给三个用户的的平均信息1/3，再加上第四个商品平均分给两个用户的平均信息1/2，即为1/3+1/2=5/6；接下来，每一个用户再把自己的信息平均分给所有购买过的商品，商品的信息则是从所有用户收到的信息值得总和，如对于图(c)中的第一个商品，它的信息值就等于第一个用户信息值的一半，为5/12，加上第二个用户信息值的1/4，为5/24，再加上第三个用户信息值得一半，为1/6，总的能量值即为:5/12+5/24+1/6=19/24。</p>
<p>以上两个步骤加起来为从商品到商品信息扩散一步。针对大规模系统的推荐，为了保持实时性和效率，往往只需扩散三步以内。如果以一步为界，基于图(c)中的结果，则在目标用户没有购买过的所有商品中，第三个商品的信息值最大，因此基于物质扩散算法的推荐系统则会将此商品推荐给目标用户，同时可以得到对于用户1的商品得分排序，自然可以得到用户召回集。值得注意的是物质扩散这种算法得到的所有商品最后的信息值之和就等于初始时所有商品的信息值，即能量是守恒的，图(c)中所有商品的信息之和仍为2。</p>
<p>热传导算法：<br><img src="/2018/07/19/热传导-物质扩散算法应用于推荐/2.png" alt=""></p>
<p>初始，我们认为目标用户购买过的每件商品的信息量为1。<br>目标用户的信息等于所有他购买过的商品信息的平均值，如图(d)所示，目标用户购买了商品1和商品4，则该用户的信息值即为(1 + 1) / 2 = 1。再根据目标用户浏览过的商品给所以商品计算信息，第一个商品、第四个商品信息量为1/2，其他商品的信息量为0（因为目标用户没有买过），接下来根据每一个商品的信息计算其他的用户的信息，如图(d)中的第二个用户的信息就为商品1,2,3,4的信息的平均值（1/2 + 1/2）/4 = 1/2；再根据每个用户的信息量平均分配信息到每个商品，如图(e)中的第一个商品来自第一个、第二个、第三个用户的信息的和，即为1/2<em>1/2+1/2</em>1/3+1/2*/12=2/3。</p>
<p>以上两个步骤加起来为从商品到商品热传导一步。因此基于热传导算法的推荐系统则会将此信息量大的商品推荐给目标用户，同时可以得到对于用户1的商品得分排序，自然可以得到用户召回集。与物质扩散不同的是这种算法得到的所有商品最后的信息值之和就不一定等于初始时所有商品的信息值，即不满足守恒定律，这是因为在信息传到的第二步过程中，有的用户的信息可能会被多次计算，从而导致不守恒。</p>
<p>基于物质扩散和基于热传导的推荐算法的区别在于： 基于物质扩散的方法在进行个性化推荐时，系统的总信息是守恒的；而热传导在推荐过程中，目标用户（即被推荐用户）的收藏品将被视作信息初始点，负责提供能量，所以系统的总信息量随着传递步骤的增加是在不断增加的。</p>
<p>如果对物理比较熟悉的朋友很容易联想到凸透镜和凹透镜，是的，我个人在理解的时候也是这样迁移理解，原理上确实一致。 基于物质扩散的方法相当于凸透镜一样把用户历史点击的信息聚焦到了少量优势的skn上了； 基于热传导的方法相当于是凹透镜一样把用户的历史点击信息发散到了那些较不流行的物品上，从而提高了推荐的新颖多样性。</p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[量化评估推荐系统效果]]></title>
      <url>/2018/07/19/%E9%87%8F%E5%8C%96%E8%AF%84%E4%BC%B0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%95%88%E6%9E%9C/</url>
      <content type="html"><![CDATA[<p>推荐系统最有效的方法就是A/B test进行模型之间的对比，但是由于现实原因的局限，存在现实实时的困难性，所以，梳理了一些可以补充替代的指标如下，但是离线评估也存在相应的问题：<br>1) 数据集的稀疏性限制了适用范围，用户之间的交集稀疏。<br>2) 评价结果的客观性，由于用户的主观性，不管离线评测的结果如何，都不能得出用户是否喜欢某推荐系统的结论，只是一个近似的评估。<br>3) 深度评估指标的缺失。(如点击深度、购买客单价、购买商品类别、购买偏好)之间的关联关系。<br>4）冷启动<br>5）Exploration 和 Exploitation问题</p>
<h1 id="离线模型之间的评估"><a href="#离线模型之间的评估" class="headerlink" title="离线模型之间的评估"></a>离线模型之间的评估</h1><h2 id="召回集测试"><a href="#召回集测试" class="headerlink" title="召回集测试"></a>召回集测试</h2><ul>
<li><p>recall<br>命中skn个数/用户真实点击skn个数</p>
</li>
<li><p>precision<br>命中skn个数/所有预测出来的skn总数</p>
</li>
<li><p>F1-Measure<br>2/(1/recall+1/precison)</p>
</li>
<li><p>交互熵 </p>
</li>
<li>MAE</li>
<li>RMSE</li>
<li>相关性<br>常见的比如：Pearson、Spearman和Kendall’s Tau相关，其中Pearson是更具数值之间的相似度，Spearman是根据数值排序之间的相似度，Kendall’s Tau是加权下的数值排序之间的相似度。</li>
<li>基尼系数</li>
<li>信息熵</li>
</ul>
<h2 id="排序部分测试"><a href="#排序部分测试" class="headerlink" title="排序部分测试"></a>排序部分测试</h2><ul>
<li>NDCG（Normalize DCG）</li>
<li>RBP（rank-biased precision）</li>
</ul>
<p>RBP和NDCG指标的唯一不同点在于RBP把推荐列表中商品的浏览概率p按等比数列递减，而ND CG则是按照log调和级数形式。</p>
<h1 id="离线模型与在线模型之间的评估"><a href="#离线模型与在线模型之间的评估" class="headerlink" title="离线模型与在线模型之间的评估"></a>离线模型与在线模型之间的评估</h1><p>很多时候，我们需要确定离线模型的效果足够的健壮才能允许上线进行线上测试，那如何进行离线模型与线上模型的评估对比就是一个比较复杂的问题。</p>
<h2 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h2><ul>
<li>缺乏公平的测试数据<br>实际处理过程中，我们发现，所有的已知点击都是来自线上模型推荐的结果，所以极端情况下，线上的recall是100%</li>
<li>缺乏公认的衡量指标<br>在线下对比中，我们发现比如recall、precision、F1-Measure等指标都是大家约定俗成的，不存在很大的争议，而离线在线模型对比却没有一个准确公认的衡量指标</li>
</ul>
<h2 id="指标设计"><a href="#指标设计" class="headerlink" title="指标设计"></a>指标设计</h2><ul>
<li>online_offline_cover_rate&amp;first_click_hit_rate</li>
</ul>
<p>这一组指标是结合在一起看的，其中online_offline_cover_rate是指针对每一个用户计算理线模型推荐的商品与在线模型推荐的商品的重合个数/在线模型的推荐商品个数，online_offline_cover_rate越低代表离线模型相对在线模型越独立；first_click_hit_rate是指offline模型对用户每天第一次点击的命中率，也就是命中次数/总统计用户数。<br>结合这两个指标，我们可以得到在online_offline_cover_rate越低的情况下，却能覆盖线上用户真实点击的次数越多，代表offline模型的效果优于线上模型。</p>
<ul>
<li>online_precision_rate/offline_precision_rate</li>
</ul>
<p>离线模型的准确率和在线模型的准确率。<br>这边在实际计算的时候采取了一个技巧，针对某个推荐位计算在线模型准确率的时候，用的是从来没有浏览过这个推荐位的用户的浏览历史匹配这个用户这个推荐位的推荐结果。这样可以避免用户的点击结果受到推荐位推荐结果影响的问题。</p>
<p>举个例子：用户在推荐位A上没有浏览过，他的点击是不受推荐位A推荐的商品影响的，拿这个用户推荐位A我们给他线上推荐的结果作为线上模型的推荐结果去计算，这样才更加合理。</p>
<ul>
<li>online_recall_rate/offline_recall_rate</li>
</ul>
<p>离线模型的召回率和在线模型的召回率。<br>同上解释。</p>
<ul>
<li>roi_reall/roi_precision</li>
</ul>
<p>同上解释，只是把未来的点击作为match源更换成了加购物车、购买、收藏这些数据。</p>
<h1 id="其他评估方向"><a href="#其他评估方向" class="headerlink" title="其他评估方向"></a>其他评估方向</h1><h2 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h2><p>推荐覆盖率越高， 系统给用户推荐的商品种类就越多 ，推荐多样新颖的可能性就越大。如果一个推荐算法总是推荐给用户流行的商品，那么它的覆盖率往往很低，通常也是多样性和新颖性都很低的推荐。</p>
<h2 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h2><p>采用推荐列表间的相似度（hamming distance、Cosine Method），也就是用户的推荐列表间的重叠度来定义整体多样性。</p>
<h2 id="新颖性"><a href="#新颖性" class="headerlink" title="新颖性"></a>新颖性</h2><p>计算推荐列表中物品的平均流行度。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>用户满意度、用户问卷、信任度、鲁棒性、实时性、</p>
<h1 id="评测维度"><a href="#评测维度" class="headerlink" title="评测维度"></a>评测维度</h1><p>最后说一下评测维度分为如下3种，多角度评测：</p>
<ul>
<li>用户维度<br>主要包括用户的人口统计学信息、活跃度以及是不是新用户等。</li>
<li>物品维度<br>包括物品的属性信息、流行度、平均分以及是不是新加入的物品等。</li>
<li>时间维度<br>包括季节，是工作日还是周末，是白天还是晚上等。</li>
</ul>
<p>附常规评价指标的整理结果(来自论文Evaluation Metrics for Recommender Systems)：<br><img src="/2018/07/19/量化评估推荐系统效果/1.png" alt=""></p>
<hr>
<p>欢迎大家关注我的个人<a href="http://shataowei.com" target="_blank" rel="noopener">bolg</a>，更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐评估方法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow在GPU下的Poolallocator Message]]></title>
      <url>/2018/06/28/Tensorflow%E5%9C%A8GPU%E4%B8%8B%E7%9A%84Poolallocator%20Message/</url>
      <content type="html"><![CDATA[<p><img src="/2018/06/28/Tensorflow在GPU下的Poolallocator Message/1.jpg" alt=""><br><a id="more"></a></p>
<p>我在在用GPU跑我一个深度模型的时候，发生了以下的问题：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">2018-06-27 18:09:11.701458: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 63521 get requests, put_count=63521 evicted_count=1000 eviction_rate=0.0157428 and unsatisfied allocation rate=0.0173171</span><br><span class="line">2018-06-27 18:09:11.701503: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110</span><br><span class="line">Global_step 2000        Train_loss: 0.0758</span><br><span class="line">Global_step 3000        Train_loss: 0.0618</span><br><span class="line">Global_step 4000        Train_loss: 0.0564</span><br><span class="line">Global_step 5000        Train_loss: 0.0521</span><br><span class="line">Global_step 6000        Train_loss: 0.0492</span><br><span class="line">Global_step 7000        Train_loss: 0.0468</span><br><span class="line">Global_step 8000        Train_loss: 0.0443</span><br><span class="line">Global_step 9000        Train_loss: 0.0422</span><br><span class="line">Global_step 10000       Train_loss: 0.0410</span><br><span class="line">Global_step 11000       Train_loss: 0.0397</span><br><span class="line">Global_step 12000       Train_loss: 0.0383</span><br><span class="line">2018-06-27 18:13:59.743133: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 71532 get requests, put_count=71532 evicted_count=1000 eviction_rate=0.0139798 and unsatisfied allocation rate=0.0143013</span><br><span class="line">2018-06-27 18:13:59.743167: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>除了常规的loss数据之外，我看到穿插在之间的warming informations ，虽然最后的结果没有任何问题，但是我抱着好奇的心态在stackoverflow找到了原因：</p>
<blockquote>
<p>TensorFlow has multiple memory allocators, for memory that will be used in different ways. Their behavior has some adaptive aspects.<br><strong>In your particular case, since you’re using a GPU, there is a PoolAllocator for CPU memory that is pre-registered with the GPU for fast DMA. A tensor that is expected to be transferred from CPU to GPU, e.g., will be allocated from this pool.</strong><br>The PoolAllocators attempt to amortize the cost of calling a more expensive underlying allocator by keeping around a pool of allocated then freed chunks that are eligible for immediate reuse. Their default behavior is to grow slowly until the eviction rate drops below some constant. (The eviction rate is the proportion of free calls where we return an unused chunk from the pool to the underlying pool in order not to exceed the size limit.) In the log messages above, you see <strong>“Raising pool_size_limit_” lines that show the pool size growing. Assuming that your program actually has a steady state behavior with a maximum size collection of chunks it needs, the pool will grow to accommodate it, and then grow no more.</strong> It behaves this way rather than simply retaining all chunks ever allocated so that sizes needed only rarely, or only during program startup, are less likely to be retained in the pool.<br>These messages should only be a cause for concern <strong>if you run out of memory</strong>. In such a case the log messages may help diagnose the problem. Note also that peak execution speed may only be attained after the memory pools have grown to the proper size.</p>
</blockquote>
<p>加粗部分解释机制、处理方式和原因。总结起来就是，PoolAllocator会有一个内存分配机制，GPU和CPU之间不是独立的可以相互传输，如果你使用的空间太多，他就会提高原有的预设的空间大小，如果够用了，就没有什么影响了，但是，需要注意的是，兄弟你的数据加载量太大了，看看是不是改改batch size，一次性少加载点数据，或者干掉隔壁同事的任务。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow代码解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[关于'Deep Neural Networks for YouTube Recommendations'的一些思考和实现]]></title>
      <url>/2018/06/26/%E5%85%B3%E4%BA%8EDeep-Neural-Networks-for-YouTube-Recommendations%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83%E5%92%8C%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/1.png" alt=""></p>
<a id="more"></a>
<p>论文 Deep Neural Networks for YouTube Recommendations 来自google的YouTube团队，发表在16年9月的RecSys会议。我想应该很多人都读过，之前参与了公司的推荐系统优化的项目，本来想从各大搜索引擎中寻找到现成的分析，但是出人意料的一无所获。Github上的代码实现也出奇的少以及不清晰，所以就借着这个机会和大家分享一下自己做的过程中的一些理论心得、工程坑、代码实现等等。</p>
<p><em>本文基于大家对Deep Neural Networks for YouTube Recommendations已经完成通读的基础上，不会做细致的论文解析，只会涉及到自己实现过程中的一些总结，如果没有论文了解，会非常不易理解。</em></p>
<h2 id="系统概览"><a href="#系统概览" class="headerlink" title="系统概览"></a>系统概览</h2><p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/2.png" alt=""><br>上面这张图可以说是比较详细的涵盖了基础框架部分，整体的模型的优点我就不详述了，包括规模容纳的程度大啊、鲁棒性好啊、实时性优秀啊、延展性好啊等等，网上很多水字数的文章很多，我们主要总结几个愿论文上的亮点和实际去做的时候需要注意的地方：</p>
<ul>
<li>DNN网络可以怎么改</li>
<li>负采样的“避坑”</li>
<li>example age有没有必要构造</li>
<li>user feature的选择方向</li>
<li>attention 机制的引入</li>
<li>video vectors的深坑</li>
<li>实时化的选择</li>
</ul>
<p>整体上来说，G厂的这套算法基于的是两个部分：matching+ranking，这个的也给我们带来了更大的工作量，在做的时候，分成两个部分，我们在实际处理的时候，通过recall rate来判断matching部分的好坏，通过<a href="https://www.cnblogs.com/eyeszjwang/articles/2368087.html" target="_blank" rel="noopener">NDCG</a>来判断排序部分的好坏。总体如下：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/3.png" alt=""></p>
<p>candidate generation就是我们matching的模块，目的是把百万级的商品、视频筛选出百级、千级的可排序的量级；再通过ranking模块，选出十位数的展示商品、视频作为最后的推送内容。之所以把推荐系统划分成Matching和Ranking两个阶段，主要是从性能方面考虑的。Matching阶段面临的是百万级，而Ranking阶段的算法则非常消耗资源，不可能对所有目标都算一遍，而且就算算了，其中大部分在Ranking阶段排名也很低，也是浪费计算资源。</p>
<h2 id="Matching-amp-Ranking-Problems"><a href="#Matching-amp-Ranking-Problems" class="headerlink" title="Matching &amp; Ranking Problems"></a>Matching &amp; Ranking Problems</h2><p>首先，我们都知道，G厂给出的这个解决方案用的就是基于DNN的超大规模多分类思想，即在时刻t，为用户U（上下文信息C）在视频库V中精准的预测出视频i的类别（每个具体的视频视为一个类别，i即为一个类别），用数学公式表达如下：</p>
<p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/4.png" alt=""></p>
<p>很显然上式为一个softmax多分类器的形式。向量u是<user, context="">信息的高纬“embedding”，而向量v则是视频 j 的embedding向量，通过u与v的点积大小来判断用户与对应视频的匹配程度。所以DNN的目标就是在用户信息和上下文信息为输入条件下学习视频的embedding向量v，从而得到用户的向量u。</user,></p>
<p>说完基本思想，让我们看看实际的效果对比：</p>
<h3 id="DNN网络可以怎么改：softmax及revise的考虑"><a href="#DNN网络可以怎么改：softmax及revise的考虑" class="headerlink" title="DNN网络可以怎么改：softmax及revise的考虑"></a>DNN网络可以怎么改：<strong>softmax及revise</strong>的考虑</h3><p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/5.png" alt=""></p>
<p>如我图中两处红色标记，论文中虽然给出了模型的整体流程，但是却没有指明，1处的video vectors需要单独的embedding还是沿用最下方的embedded video watches里面的已经embedding好的结果，我们称之为softmax问题；2处论文没有提及一个问题，就是在固定好历史watch的长度，比如过去20次浏览的video，可能存在部分用户所有的历史浏览video数量都不足20次，在average的时候，是应该除以固定长度（比如上述例子中的20）还是选择除以用户真实的浏览video数量，我们称之为revise问题。</p>
<p>根据我们的数据实测，效果对比如下：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/6.png" alt=""></p>
<p><code>nosoft:沿用最下方的embedded video watches里面的已经embedding好的结果</code><br><code>revise:除以用户真实的浏览video数量</code></p>
<p>我们尝试的去探求原因发现，nosoft比softmax好的原因在于user vector是由最下方的embedded video watches里面的已经embedding好的结果进行多次FC传递得来的，如果新增一个video embedded vector 的话，和FC传递得到的u vector的点积的意义就难以解释；revise比norevise好的原因是，实际在yoho!buy的购物场景下，用户的点击历史比较我们实际选取的window size要短不少，如果所有的用户都除以固定长度的话，大量的用户history click average的vector大小接近于0。</p>
<h3 id="DNN网络可以怎么改：神经元死亡及网络的内部构造"><a href="#DNN网络可以怎么改：神经元死亡及网络的内部构造" class="headerlink" title="DNN网络可以怎么改：神经元死亡及网络的内部构造"></a>DNN网络可以怎么改：<strong>神经元死亡及网络的内部构造</strong></h3><p>这是一个异常恶心还有没什么好方法的问题，在刚开始做的时候，我们遇到了一个常见的问题，神经元批量死亡的问题。在增加了batch normalization、clip_by_global_norm和exponential_decay learning rate 后有所缓解。</p>
<p>网络结构的变化比较常规，对比场景的激活函数，参考了论文中推荐的深度、节点数，效果对比如下：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/7.png" alt=""></p>
<p>虽然我们看到增加网络的深度（3—&gt;4）一定程度上会提高模型的命中率，增加leakyrelu的一层网络也可以有些许的提升，但是总的来说，对模型没有啥大的影响，所以在之后的实际模型中，我们选择了原论文中relu+relu+relu，1024+512+256的框架。</p>
<h3 id="负采样的“避坑”"><a href="#负采样的“避坑”" class="headerlink" title="负采样的“避坑”"></a><strong>负采样的“避坑”</strong></h3><p>我们都知道，算法写起来小半天就可以搞定，但是前期的数据处理要搞个小半个月都不一定能出来。作为爱省事的我，为了快速实现算法，没有重视负采样的部分，采取了列表页点击为label=1，未点击为label=0的方式，详情如下：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/8.png" alt=""></p>
<p>看上去没什么问题，省略了从全量样本中抽样作为负样本的复杂过程，实际上，我把代码狂改了n边效果也一直维持在1.57%，可以说是没有任何提升，在此过程期间，我还是了拿用户的尾次点击（last_record）进行训练，拿了有较多行为的用户的尾次点击（change_last_record）进行训练，效果很感人：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/9.png" alt=""></p>
<p>在我孤注一掷一致，选择按照原论文中说的，每次label=0的我不拿展现给用户但是用户没有点击的商品，而是随机从全量商品中抽取用户没有点击过的商品作为label=0的商品后，奇迹发生了：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/10.png" alt=""></p>
<p>事后我仔细分析了原因：<br>a.在当次展现的情况下，虽然用户只点击了click商品，其他商品没有点击，但是很多用户在后续浏览的时候未click的商品也在其他非列表页的地方进行click，我实际上将用户感兴趣的商品误标记为了负样本<br>b.后来我咨询看了论文，也发现了原论文中也提及到，展现商品极有可能为热门商品，虽然该商品该用户未点击，但是我们不能降低热门商品的权重（通过label=0的方式），实际上最后的数据也证明了这一点<br>c.“偷窥未来的行为”，如下图，原论文中指出input构造时候不能拿还未发生的点击，只能拿label=1产生时之前的所有历史点击作为input；同理，在构造label=0的时候，只能拿在label=0的时候已经上架的商品，由于训练时间的拉长，不能偷窥label=1发生时还未上架的商品作为label=0的负样本</p>
<p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/11.png" alt=""></p>
<h3 id="example-age有没有必要构造"><a href="#example-age有没有必要构造" class="headerlink" title="example age有没有必要构造"></a><strong>example age有没有必要构造</strong></h3><p>首先，先稍微解释一下我对example age的概念的理解。所有的训练数据其实都是历史数据，距离当前的时刻都过去了一段时间，站在当前来看，距离当前原因的数据，对当前的影响应该是越小的。就比如1年前我买了白色的铅笔，对我现在需要不需要再买一支黑色的钢笔的影响是微乎其微的。而example age其实就是给了每一条数据一个权重，引用一下原论文的描述<code>In (5b), the example age is expressed as tmax − tN where tmax is the maximum observed time in the training data</code>，我这边采取了(tmax − tN)/tmax的赋权方式：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/12.png" alt=""></p>
<p>很悲催的是，直观的离线训练数据并没有给出很直观的效果提升，但是由于评估机制的问题（我们后面会说到），我会在实际上线 做abtest的时候重新验证我的这个example age的点，但是可以肯定的是，理论和逻辑上，给样本数据进行权重的更改，是一个可以深挖的点，对线上的鲁棒性的增强是正向的。</p>
<h3 id="user-feature的选择方向"><a href="#user-feature的选择方向" class="headerlink" title="user feature的选择方向"></a><strong>user feature的选择方向</strong></h3><p>很不幸的是，在这一块的提升，确实没有论文中说的那么好，对于整个网络的贡献，以我做的实际项目的结果来说，history click embedded item &gt; history click embedded brand &gt; history click embedded sort &gt; user info &gt; example age &gt; others。不过，因为时间、数据质量、数据的真实性的原因，可能作为原始input的数据构造的就没有那么好。这边主要和大家说两个点：</p>
<p>1.topic数据<br>原论文中在第四节的RANKING中指出:<br><code>We observe that the most important signals are those that describe a user’s previous interaction with the item itself and other similar items, matching others’ experience in ranking ads</code><br>论文中还举出了比如用户前一天的每个频道（topic）的浏览视频个数，最后一次浏览距今时间，其实说白了就是强调了过去的行为汇总对未来的预测的作用，认为过去的行为贯穿了整体的用户点击轨迹。<br>除此之外，G厂大佬还认为一些用户排序性质的描述特征对后面的ranking部分的提高也是蛮重要的，这边还举出了用户视频评分的例子，更多的内容大家可以自己去看一下原论文的部分，应该都会有自己的体会。</p>
<p>回到我们的项目，因为yoho!buy是电商，我类比着做了用户每个类目（裤子、衣服、鞋子…）的历史浏览点击购买次数、最后一次点击距今时长等等的topic信息，提升不是很明显。但是在大家做G厂这边论文，准确率陷入困境的时候，可以尝试一下这边的思路。<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/13.png" alt=""></p>
<p>2.query infomation<br>相比于论文中的user information的添加，在实际模型测试中，我们发现，query的information的部分有更多的”遐想”。</p>
<p>原论文中点名指出user language and video language 做为basic info的重要性，这边给出的提升也是相对于user info有明显的增长的：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/14.png" alt=""></p>
<p>有提升也自然有该部分的缺点：<br>1.语言模型的处理复杂，耗时久<br>在该部分的处理中，我强行拖着隔壁组的nlp博士和我一起搞了一周，每天都加班的搞去做数据清理，句法分析，语句树解析。如果需要让一个常规做推荐的人去弄，会有各种各样的坑，而且耗时还久<br>2.语言新增问题<br>商品的标题这类的文本处理还好，毕竟每日更新的数据存在一个可控的范围，但是用户搜索内容的变化是巨大的，粗略估测一下，一周时间间隔后，原提纯文本数据和新提纯文本数据的交集覆盖率不到78%，这意味着要重复的做nlp工作</p>
<h3 id="attention-机制的引入"><a href="#attention-机制的引入" class="headerlink" title="attention 机制的引入"></a><strong>attention 机制的引入</strong></h3><p>attention 机制的引入是我老大的硬性需求，我这边也就做了下，如果不了解attention 机制的朋友，可以阅读以下这边文章：<a href="https://blog.csdn.net/qq_21190081/article/details/53083516" target="_blank" rel="noopener">Attention model</a>。</p>
<p>我通俗的解释一下，不准确但是方便理解，Attention model就是让你每一个input与你的output计算一个similarity，再通过这些similarities给出每个input的权重。但是，很明显，我们离线训练还好，既有input也有output，但是线上预测的时候，就没有output了，所以，我们采取了lastclick替代的方式：<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/15.png" alt=""></p>
<p><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/16.png" alt=""></p>
<p>不得不说，老祖宗传下来的东西确实有独到之处，但是在提升了近1pp的rate代价之下，会有一个让人头疼的问题耗时。因为每一个input的weight需要和output进行一次相似度计算，而且后续还要对计算出的相似度进行处理，原本只需要6-7小时训练完的模型，在我加了3层Multihead Attention后被拖到了一天。数据量还只采样了一半，确实需要斟酌带来的提升与投入的成本之间的平衡问题。</p>
<h3 id="video-vectors的深坑"><a href="#video-vectors的深坑" class="headerlink" title="video vectors的深坑"></a><strong>video vectors的深坑</strong></h3><p>G厂一句话，我们测断腿。这句话不是瞎说的，大家应该还记得一开始我给出的那张图，在最上面有一行不是很明显的小字：video vectors。G厂的大佬们既没有说这些video vectors该怎么构造，也没有说video vectors需不需要变动，留下了一个乐趣点让大家体验。</p>
<p>刚开始我很傻的用了我们最开始的embedded item作为video vectors，与模型FC出来的user vectors进行点击，计算top items。我来来回回测了一个月，老命都快改没了，最后提升rate到4pp。然而RNN随便跑跑就能到达3pp，我说很不服气的，所以拉着同事一起脑洞了一下，我们之前做图片相似度匹配的时候，喜欢把图片的向量拆成颜色+款式+性别，所以我们就借用了一下，改成了embedded item + embedded brand + embedded sort作为video vectors，历史总是给我们惊喜，效果上一下子就能大到5.2pp左右，这个点的提升应该是得来的最意外的，建议大家在用的时候考虑一下。<br><img src="/2018/06/26/关于Deep-Neural-Networks-for-YouTube-Recommendations的一些思考和实现/17.png" alt=""></p>
<h3 id="实时化的选择"><a href="#实时化的选择" class="headerlink" title="实时化的选择"></a><strong>实时化的选择</strong></h3><p>实时部署上，我们用了tensor flow serving，没什么好说的，给一下关键代码，大家看下自己仿一下就行，一般自己做做demo不需要，企业级上线才需要，企业级上线的那些大佬可能也比我有更多想法，所以就不展开了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">部署及用python作为Client进行调用的测试：</span><br><span class="line">#1.编译服务</span><br><span class="line">bazel build //tensorflow_serving/model_servers:tensorflow_model_server</span><br><span class="line">#2.启动服务</span><br><span class="line">bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9005 --model_name=test --model_base_path=/Data/sladesha/tmp/test/ </span><br><span class="line">#3.编译文件 </span><br><span class="line">bazel build //tensorflow_serving/test:test_client</span><br><span class="line">#4.注销报错的包</span><br><span class="line">注销：/Data/muc/serving/bazel-bin/tensorflow_serving/test/test_client.runfiles/org_tensorflow/tensorflow/contrib/image/__init__.pyc中的from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms</span><br><span class="line">参考：https://github.com/tensorflow/serving/issues/421</span><br><span class="line">#5.运行</span><br><span class="line">bazel-bin/tensorflow_serving/test/test_client --server=localhost:9005</span><br></pre></td></tr></table></figure>
<p>相关的问题，有大佬已经梳理好了，自取其他可选的一些参数设置：<a href="https://blog.csdn.net/langb2014/article/details/54317490" target="_blank" rel="noopener">tensorflow serving 参数设置</a>。</p>
<p>还有一些评估技巧，模型之间的对比技巧，这边就不细讲了，可借鉴的意义也不大。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>虽然早就读过这篇文章，但是实现之后，发现新收获仍然不少。我特别赞成清凇的一句话:’对于普通的学术论文，重要的是提供一些新的点子，而对于类似google这种工业界发布的paper，特别是带有practical lessons的paper，很值得精读。’<br>G厂的这个推荐代码和attention model的代码之前是准备放GitHub的，想想还是算了。一是之前也放过很多此代码，也没什么反馈，二是这两个代码自己写也不是很难，可以作为练手项目。</p>
<h2 id="鸣谢"><a href="#鸣谢" class="headerlink" title="鸣谢"></a>鸣谢</h2><p>以上我个人在Yoho!Buy团队在实践中的一点总结，不代表公司的任何言论，仅仅是我个人的观点。最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！溜了溜了。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 论文解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tf.nn.embedding_lookup]]></title>
      <url>/2018/06/11/Tensorflow%E4%B8%93%E9%A2%98tf-nn-embedding-lookup/</url>
      <content type="html"><![CDATA[<p><img src="/2018/06/11/Tensorflow专题tf-nn-embedding-lookup/1.jpg" alt=""><br><a id="more"></a></p>
<p><img src="/2018/06/11/Tensorflow专题tf-nn-embedding-lookup/2.jpg" alt=""><br>我觉得这张图就够了，实际上tf.nn.embedding_lookup的作用就是找到要寻找的embedding data中的对应的行下的vector。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup(params, ids, partition_strategy=<span class="string">'mod'</span>, name=<span class="keyword">None</span>, validate_indices=<span class="keyword">True</span>, max_norm=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p><strong><a href="https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup" target="_blank" rel="noopener">官方文档位置</a></strong>，其中，params是我们给出的，可以通过：<br>1.<code>tf.get_variable(&quot;item_emb_w&quot;, [self.item_count, self.embedding_size])</code>等方式生产服从[0,1]的均匀分布或者标准分布<br>2.<code>tf.convert_to_tensor</code>转化我们现有的array<br>然后，ids是我们要找的params中对应位置。</p>
<p>举个例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">data = np.array([[[<span class="number">2</span>],[<span class="number">1</span>]],[[<span class="number">3</span>],[<span class="number">4</span>]],[[<span class="number">6</span>],[<span class="number">7</span>]]])</span><br><span class="line">data = tf.convert_to_tensor(data)</span><br><span class="line">lk = [[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]]</span><br><span class="line">lookup_data = tf.nn.embedding_lookup(data,lk)</span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure></p>
<p>先让我们看下不同数据对应的维度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">76</span>]: data.shape</span><br><span class="line">Out[<span class="number">76</span>]: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">In [<span class="number">77</span>]: np.array(lk).shape</span><br><span class="line">Out[<span class="number">77</span>]: (<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">In [<span class="number">78</span>]: lookup_data</span><br><span class="line">Out[<span class="number">78</span>]: &lt;tf.Tensor <span class="string">'embedding_lookup_8:0'</span> shape=(<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>) dtype=int64&gt;</span><br></pre></td></tr></table></figure></p>
<p>这个是怎么做到的呢？关键的部分来了，看下图：<br><img src="/2018/06/11/Tensorflow专题tf-nn-embedding-lookup/3.png" alt=""><br>lk中的值，在要寻找的embedding数据中下找对应的index下的vector进行拼接。永远是look(lk)部分的维度+embedding(data)部分的除了第一维后的维度拼接。很明显，我们也可以得到，<strong>lk里面值是必须要小于等于embedding(data)的最大维度减一的</strong>。</p>
<p>以上的结果就是：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">79</span>]: data</span><br><span class="line">Out[<span class="number">79</span>]:</span><br><span class="line">array([[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>],</span><br><span class="line">[<span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">6</span>],</span><br><span class="line">[<span class="number">7</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">80</span>]: lk</span><br><span class="line">Out[<span class="number">80</span>]: [[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># lk[0]也就是[0,1]对应着下面sess.run(lookup_data)的结果恰好是把data中的[[2],[1]],[[3],[4]]</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">81</span>]: sess.run(lookup_data)</span><br><span class="line">Out[<span class="number">81</span>]:</span><br><span class="line">array([[[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>],</span><br><span class="line">[<span class="number">4</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[[[<span class="number">3</span>],</span><br><span class="line">[<span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]],</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>],</span><br><span class="line">[<span class="number">1</span>]]]])</span><br></pre></td></tr></table></figure></p>
<p>最后，partition_strategy是用于当len(params) &gt; 1，params的元素分割不能整分的话，则前(max_id + 1) % len(params)多分一个id.<br>当partition_strategy = ‘mod’的时候，13个ids划分为5个分区：[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]，也就是是按照数据列进行映射，然后再进行look_up操作。<br>当partition_strategy = ‘div’的时候，13个ids划分为5个分区：[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]，也就是是按照数据先后进行排序标序，然后再进行look_up操作。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow代码解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tf.scan]]></title>
      <url>/2018/06/04/Tensorflow%E4%B8%93%E9%A2%98tf-scan/</url>
      <content type="html"><![CDATA[<p><img src="/2018/06/04/Tensorflow专题tf-scan/1.jpg" alt=""><br><a id="more"></a></p>
<p>tf.scan(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=None)</p>
<p>fn：计算函数<br>elems：以elems的第一维度的变量list作函数计算直到遍历完整个elems<br>initializer：fn计算的初始值，替代elems做第一次计算</p>
<p>举个好理解的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">z = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">x = tf.convert_to_tensor(x)</span><br><span class="line">z = tf.convert_to_tensor(z)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x,y)</span>:</span></span><br><span class="line"><span class="keyword">return</span> x+y</span><br><span class="line"></span><br><span class="line">g = tf.scan(fn=f,elems = x,initializer=z)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer)</span><br><span class="line"></span><br><span class="line">sess.run(g)</span><br></pre></td></tr></table></figure></p>
<p>会得到：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">97</span>]: sess.run(g)</span><br><span class="line">Out[<span class="number">97</span>]: array([<span class="number">11</span>, <span class="number">13</span>, <span class="number">16</span>], dtype=int32)</span><br></pre></td></tr></table></figure></p>
<p>详细的计算逻辑如下：<br>11 = 10(初始值initializer)+ 1(x[0])<br>13 = 11(上次的计算结果)+2(x[1])<br>16 = 13(上次的计算结果)+3(x[2])</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow代码解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[写给想转行机器学习深度学习的同学]]></title>
      <url>/2018/03/18/%E5%86%99%E7%BB%99%E6%83%B3%E8%BD%AC%E8%A1%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%8C%E5%AD%A6/</url>
      <content type="html"><![CDATA[<p><img src="/2018/03/18/写给想转行机器学习深度学习的同学/1.jpg" alt=""></p>
<a id="more"></a>
<p>update 1:很多同学还是私信我，让我推荐或者提供一些电子书给他们，我这边也打包了一些我认为比较重要的，如果有需要的同学可以「邮箱」联系我。申明，我所发送的书个人均已购买正版实体书，建议大家也支持正版，谢谢。</p>
<hr>
<p>自从我毕业以来，先是火机器学习，然后火大数据，之后火深度学习，现在火人工智能这些算法领域。越来越多的朋友想从工业，金融等等行业转行到算法相关的行业，我一年前在知乎上写了一个答案<a href="https://www.zhihu.com/question/56050487/answer/148428497" target="_blank" rel="noopener">本科生怎样通过努力拿到较好的机器学习/数据挖掘相关的offer？</a>，当时拿了不少的赞，所以也一直有同学找我咨询相关的问题，确确实实也有相当一批人拿到了不错的offer。</p>
<p>我个人不是很喜欢更新非技术的文章，但是我还是觉得如果能帮助到一些人，其实也是另一种技术输出的展现，所以我就写下了下面这篇短文，希望对迷茫的人有所帮助。</p>
<h2 id="评估转行难度"><a href="#评估转行难度" class="headerlink" title="评估转行难度"></a>评估转行难度</h2><p>今天一大早，我在刷知乎的时候，刷到这个题目<a href="https://www.zhihu.com/question/265041005/answer/344257277" target="_blank" rel="noopener">非计算机专业学生如何转行AI，并找到算法offer?</a>，我看到这个叫做<a href="https://www.zhihu.com/people/wang-rui-meng-43/activities" target="_blank" rel="noopener">BrianRWang</a>的答主的一个“10问检验你的基础水平”，我觉得是至少我看来非常全面考验数学基础的，所以这边就和大家分享一下（答案我会在最后给出，有兴趣的最好自己做一下，括号里面的我个人觉得没有意义所以没有给出解释，有兴趣的却又解不出来的同学可以私信我）：</p>
<blockquote>
<p>1.什么是贝叶斯定理？请简述其公式？现分别有 A，B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器 A 的概率是多少?</p>
</blockquote>
<p>这题考了概率论的基础，虽然考了贝叶斯，但是后面的容器问题完全可以不用贝叶斯也可以算出来，算是一题数学敏感度的测试题，看看自己适不适合去努力切入这个方向。</p>
<blockquote>
<p> 2.请简述卡方分布和卡方检验的定义？(给你一个2*2的列表让你算卡方分布，你会怎么做？)</p>
</blockquote>
<p>这题考了梳理统计的基础，括号里面的我个人觉得没有意义，有兴趣的可以查表算一下。</p>
<blockquote>
<p>3.在概率统计学里，自由度是如何被定义的，又该怎样去应用？</p>
</blockquote>
<p>原作者BrianRWang认为这题比较偏，属于冷门题目。个人看法：其实我觉得如果是任何一个理工科的同学，这题都应该能答出来，大学的课程里，自由度的理解直接决定了统计科目大家的学习质量。</p>
<p>以上的三题考了概率论与数理统计的基础，在机器学习理论中，概率论和数理统计的基础是否扎实直接决定了能否很好的理解各个理论的前置条件，适用场景，提升方向等，着实重要。</p>
<blockquote>
<p>4.请简述什么是线性代数里的矩阵特征值和特征向量？(求矩阵:A=np.array([[1,2],[3,4]])的特征值，特征向量，写出其运算公式)</p>
</blockquote>
<p>线性代数题目，很简单给出对应的公式即可，我在SVD介绍的时候就完全讲过。如果换成，如何理解特征值及特征向量在空间中的实际意义，这题就会变得非常卡人。</p>
<blockquote>
<p>5.如何使用级数分解的方法求解e^x?(并给出在数值计算中可能遇到的问题。)</p>
</blockquote>
<p>数学分析的题目，一个公式。</p>
<p>以上的题目都是线性代数，数学分析的题目，都是比较考验大学的基本功，如果不记得也很正常，只要能说出大概的思想就行，比如空间选择啊，点导数展开。</p>
<blockquote>
<p>6.数据结构的定义是什么？运用数据结构的意义是什么？</p>
</blockquote>
<p>计算机题，这题应该是几个问答中最简单的了。</p>
<blockquote>
<p>7.请说明至少两种用于数据可视化（data visualization）的package。并且说明，在数据分析报告里用数据可视化的意义是什么？</p>
</blockquote>
<p>前一问如果主动接触过计算科学的人这题比较好答，如果是纯新手，这题就是无从下手的。后面一小问也是属于考察你的数据敏感度的，如果能够match到一些点，很加分。</p>
<blockquote>
<p>8.假如让你用编程方法，比如python，处理一个你没见过的数学问题，比如求解一个pde或者整快速傅里叶变换，你应该查什么东西，找哪一个package的参考资料？</p>
</blockquote>
<p>同上一条前一部分。</p>
<blockquote>
<p>9.请简述面向对象编程和函数式编程分别的定义，并举出其案例。</p>
</blockquote>
<p>计算机题，考了基础的编程的一些风格的了解程度，说实话，这题我第一次看到也很懵，还去Google了一下。</p>
<p>原作者还有一个第10题，不涉及技术，我就没放。以上四题更偏向coding的能力，虽然说算法工程师、数据挖掘工程师、NLP工程师，等等，都是挂着科研的title，但是过硬的coding能力是完全不能缺少的，要其他人把很复杂的数学理论用代码帮你实现出来的交流成本巨大，我觉得精通或者熟悉至少一门语言还是非常重要的。</p>
<p>原作者认为：</p>
<blockquote>
<p>以上提问如果能闭卷对7个及以上，证明一个学生的基础还是比较好的。只要聪明肯学，一定是有所裨益的。在7个，到3个之间，不妨提高一下自己的数学水平；努努力还是可以学会机器学习的。如果写对不了两个（“这都啥啊？”），郴州勃学院复读班欢迎你过去。</p>
</blockquote>
<p>其实我还是比较认同的，答对3个或者2.5个以上的同学，完全可以试一试转一转，我觉得不存在说入不了门的情况。能答对7个或者7.5以上的同学，我觉得可以投简历了，如果我收到你的简历，即便是你没有历史的工作经验，我很愿意让你试一试的。</p>
<h2 id="一些资料"><a href="#一些资料" class="headerlink" title="一些资料"></a>一些资料</h2><p>很多转行的朋友会问我，到底看什么书会比较好，我刚开始会推荐一堆，后来自己想了想发现，还是太天真，大家工作忙的要死，看一本就很难了，别说一堆。</p>
<p>我最后就浓缩了三本:：周志华老师的西瓜书（《机器学习》周志华 清华大学出版社），李航的带你玩转基础理论（《统计学习方法》李航 清华大学出版社），经典厕所读物（《数学之美》吴军 人民邮电出版社）。</p>
<p>确实是很经典很经典的书，我现在基本上每次必回答以上三本。</p>
<p>除此之外，在coursera上找吴恩达（Andrew Ng）教授的机器学习课程，他把要用到的数学知识也做了简单的讲解，机器学习方面的理论和算法讲的也很详细，而且很基础，肯定可以看懂。<a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Machine Learning | Coursera</a>，应该是最适合看的视频类的资料没有之一。</p>
<p>我不反对也不支持大家去参加几千几万的速成班，几十几百的live课程，但是我觉得你不妨先看完以上的书和视频再做决定，一定不会让你失望。之前我一直在给team做吴恩达（Andrew Ng）在线课程的分享，一直到最近我发现不如整理出来给team以外的大家一起看算了，所以在Gradient Checking(9-5)这节课之后的所有课程，如果有价值的地方，我都做了笔记后面会分享在我的GitHub中，希望给大家一些帮助。</p>
<p>最后，希望我们都不负自己的青春。</p>
<p><em>附录：</em><br><em>1.<a href="https://github.com/sladesha/machine_learning/blob/master/Knowledge%20Summary/Questions%20and%20Answers.md" target="_blank" rel="noopener">BrianRWang的十条问题的答案链接</a></em><br><em>2.<a href="https://github.com/sladesha/machine_learning/blob/master/Knowledge%20Summary/Notes%20of%20Andrew%20Ng&#39;s%20Lessons.md" target="_blank" rel="noopener">吴恩达（Andrew Ng）Gradient Checking(9-5)这节课之后的课程整理（持续更新中）</a></em></p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 公告 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[yoho!buy注册概率预估]]></title>
      <url>/2018/03/04/yoho!buy%E6%B3%A8%E5%86%8C%E6%A6%82%E7%8E%87%E9%A2%84%E4%BC%B0/</url>
      <content type="html"><![CDATA[<p><img src="/2018/03/04/yoho!buy注册概率预估/1.jpeg" alt=""></p>
<a id="more"></a>
<hr>
<p>GRU 部分的demo代码：<a href="https://github.com/sladesha/deep_learning/tree/master/RNN_applied_classification/src" target="_blank" rel="noopener">model.py</a>，注意，其中上传的lr_classification.py并没有做wide&amp;deep设计</p>
<hr>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文主要介绍yoho!buy大数据团队在深度学习传统应用方向上的一些实践和思考。传统用户行为预估方向上，如何根据用户的行为数据，对用户行为建模，进而预测用户的购买行为，点击行为，注册行为等等一直以来都受到工业界及学术界的关注。相对而言，就用户注册概率的预测受限数据获取的局限性、传统的计算模型的时效性等原因并没有很多可参考的研究案例。我们想和大家分享的「yoho!buy基于GRU+LR算法下的用户注册概率预估」，基于循环神经网络的框架，充分的利用了用户在app上的行为信息，保证了高效的结果反馈速度，兼备算法框架良好的延拓性能。</p>
<h1 id="注册概率预估定义"><a href="#注册概率预估定义" class="headerlink" title="注册概率预估定义"></a>注册概率预估定义</h1><p><strong>注册概率预估</strong>，即预估用户下载app后，浏览app过程中主动注册的可能性。通过识别出有注册倾向的人群，辅助以人为介入的方式（优惠、折扣，关怀等），可以提高用户实际注册的概率。</p>
<p>基本的注册概率预估算法设计的流程如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/2.png" alt=""></p>
<p>数据整理节点：在于收集用户行为信息，包括地理位置，当时的时间，用户来源的渠道，用户点击行为等等；模型计算节点：在于根据数据整理节点的结果判断用户的注册概率高低；计算结果推送节点：在于根据不同注册概率用户采取不同的营销策略，个性化引导用户注册。</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/3.jpeg" alt=""></p>
<p>目前注册概率预估主要有两大难点：</p>
<ul>
<li>传统模型难以实时预测：因为缺乏平台忠诚度，用户第一次也有可能是最后一次登陆app之前这段时间是相对暂短，如何压缩用户每一次步操作下的模型计算时间，提高反馈频率是需要考虑的重要问题，而传统模型在这方面的表现比较平庸。</li>
<li>传统模型特征加工复杂：因为用户可能是第一次接触app，在没有注册信息，历史行为信息，完成订单信息等等数据下，利用有限的数据进行的特征处理，如果想要有不错的效果相对而言特征加工过程的复杂程度和难度要比普通的项目更加具有挑战性。</li>
</ul>
<p>我们通过以Recurrent Neural Network 及 Logistic Regression为基模型，通过Stacking方式，针对性的尝试去解决以上两个注册预估中的难点。其中Recurrent Neural Network最本质的能够work的地方在于，实际上在没有过多特征的时候，对于新用户来说，他的浏览路径实际上就是反映了他对这个app的喜好。</p>
<h1 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h1><h2 id="Recurrent-Neural-Network基模型"><a href="#Recurrent-Neural-Network基模型" class="headerlink" title="Recurrent Neural Network基模型"></a>Recurrent Neural Network基模型</h2><h3 id="数据整理部分"><a href="#数据整理部分" class="headerlink" title="数据整理部分"></a>数据整理部分</h3><p>无论在Kaggle还是天池大赛上，数据特征工程是非常重要而且繁琐的一个过程：<br>关于预处理，通常我们会采取：</p>
<ul>
<li>数据检查，提出异常字符、乱码等数据</li>
<li>缺失值处理，剔除、填充、拟合构造等</li>
<li>方差衡量，剔除方差低的低贡献特征</li>
<li>共线性检查，提高泛化能力</li>
<li>异常检验，剔除错误异常数据</li>
<li>…</li>
</ul>
<p>在数据预处理结束之后，我们还会在更新完的数据集上进行特征筛选：</p>
<ul>
<li>基于自变量与因变量之间的交互熵</li>
<li>基于模型中的特征贡献程度（Xgboost里面的importance/Lasso、Ridge中的参数绝对大小）</li>
<li>基于预训练模型中的特征参数的显著性</li>
<li>基于自变量之间的相关性</li>
<li>…</li>
</ul>
<p>在数据特征筛选结束之后，我们还需要进行特征组合寻找最大方差下的新的特征，还会通过PCA/LDA/t-SNE/FM等寻找是否可以进行降维或者升维，交叉特征构造等等。通常无论是在离线训练还是线上预测中，对特征的加工处理过程都是非常耗时的，极有可能在用户已经离开app后，用户的注册概率还没有算出来。</p>
<p>我们利用Recurrent Neural Network来解决注册预估的时候，我们需要做的数据整理就非常的轻松，只需根据用户浏览的顺序，将用户浏览的页面编号Item_Page,同时记录用户浏览的先后顺序Time_Rank：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/4.jpg" alt=""></p>
<p>构造如下的数据形式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>User_Imme</th>
<th style="text-align:center">Item_Page</th>
<th>Time_Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td>012939003331092</td>
<td style="text-align:center">92374573284354</td>
<td>1518422132</td>
</tr>
<tr>
<td>012939003331092</td>
<td style="text-align:center">82374573771273</td>
<td>1518422142</td>
</tr>
<tr>
<td>012939003331092</td>
<td style="text-align:center">92374573284354</td>
<td>1518422147</td>
</tr>
<tr>
<td>078939002221093</td>
<td style="text-align:center">66774573284354</td>
<td>1518422247</td>
</tr>
<tr>
<td>078939002221093</td>
<td style="text-align:center">66774573284442</td>
<td>1518422249</td>
</tr>
<tr>
<td>…</td>
<td style="text-align:center">…</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
<p>数据处理过程，只需要按照用户的浏览先后顺序进行排序即可，大大的降低了耗时，对整体算法的实效性上不会产生任何影响。因此，我们甚至可以在用户每一次产生动作之后就对其的注册概率进行重新判定，得到用户的浏览流对应的注册概率波动情况。</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/5.png" alt=""></p>
<p>我们的用户可以大致可分为铁粉用户和普通用户两种。从用户的注册概率分布就可以很清晰的看出：铁粉用户浏览目的明确，寻找自己关注的商品，一旦找到立马注册下单，所以铁粉用户的时间流较短，注册概率呈现上升趋势，注册概率均值较高，需要我们运营手段干涉的情况较少；而普通用户属于无明确目的的浏览，所以注册概率的波动较大，注册概率均值较低，但是一旦察觉到用户有高概率注册的行为却未注册且注册概率持续下降时立刻进行营销引导，多次营销尝试后用户成功注册。</p>
<h3 id="模型计算部分"><a href="#模型计算部分" class="headerlink" title="模型计算部分"></a>模型计算部分</h3><p>整体的模型框架概览如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/6.png" alt=""></p>
<p>我们试图通过循环网络部分提取出用户的浏览行为的汇总信息，再通过Logistic Regression部分融合用户基础信息，以行为特点+基础特征猜测用户每次浏览是属于随便点点还是认真挑选。</p>
<h4 id="Recurrent-Neural-Network部分"><a href="#Recurrent-Neural-Network部分" class="headerlink" title="Recurrent Neural Network部分"></a>Recurrent Neural Network部分</h4><p><strong>1.循环单元结构</strong></p>
<p>在循环神经网络模块中，我们采取了Gated Recurrent Unit (GRU)代替普通RNN作为最小循环单元进行计算，以此来避免梯度消失等问题：</p>
<p>隐藏状态计算如下：<br><img src="/2018/03/04/yoho!buy注册概率预估/7.jpeg" alt=""><br><img src="/2018/03/04/yoho!buy注册概率预估/8.jpeg" alt=""><br><img src="/2018/03/04/yoho!buy注册概率预估/9.jpeg" alt=""><br><img src="/2018/03/04/yoho!buy注册概率预估/10.jpeg" alt=""></p>
<p>对比LSTM，GRU只用了两个gates，将LSTM中的输入门和遗忘门合并成了更新门。并且并不把线性自更新建立在额外的memory cell上，而是直接线性累积建立在隐藏状态上，并靠gates来调控，这样就可以大大的加快离线训练的速度，同时在RNN的官方论文中，我们看到了实测的效果如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/11.jpeg" alt=""></p>
<p>很明显的可以看到:虽然GRU减少了一个门的存在，但是效果与LSTM相当，但是几乎每次测试的test效果都要优秀于传统方法，同时GRU是真的肉眼可见的比LSTM快。综合考虑了计算速度及最后计算结果的准确程度，我们选择了多层GRU模型而并非是以输入门、输出门、遗忘门为信息传递的LSTM模型。</p>
<p><strong>2.循环网络结构</strong></p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/12.png" alt=""></p>
<p>整体上看，网络结构也是非常简单的。如上图，先将用户浏览的所有商品进行embedding操作，然后根据用户每个商品的浏览顺序构建时序序列，进行多层的GRU模型训练，最后再以前馈网络传递，softmax后得到用户下次点击每个商品的概率，再根据预测结果Output Second Items 和 Real Second Items修正多层GRU layer中的参数。</p>
<p>通过已经训练好的循环网络，我们根据新用户浏览商品的顺序，得到用户每次浏览的后一次浏览每个商品的概率（output scores）及用户前N次浏览信息的trend，seasonality的汇总（隐藏状态GRU States）。与此同时，我们可以通过控制GRU layer的层数及优化多次隐藏状态GRU States的拼接方式控制整体模型框架的复杂程度。</p>
<p><strong>3.用户数据构造</strong></p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/13.png" alt=""></p>
<p>用户数据训练过程常常采用如上图这样的最小批处理。每一个Session就可以看作是一个用户，每一个i可以看作是一个商品，商品i的下标代表着商品的浏览顺序。</p>
<p>input中，每一行表示的为多个用户正常的浏览顺序中的起始商品，比如Session1用户浏览顺序：i1.1—&gt;i1.2—&gt;i1.3—&gt;1.4中的i1.1，i1.2，i1.3；对应的，在output中，每一行代表多个用户在input位置的下一次浏览内容，比如Session1用户浏览顺序：i1.1—&gt;i1.2，i1.2—i1.3，i1.3—&gt;i1.4中的i1.2，i1.3，i1.4；当input中的一个的用户或者说是一个的Session的点击信息被全部使用后，追加一个新的用户或者说是Session的点击信息，同时通过控制同时计算的用户或者说Session的个数，直到所有的Session信息都被使用完一遍，这样就构造完成了一个由用户的上次点击结果预测用户的下次点击结果的循环神经网络。</p>
<p>这样设计避免了通常的神经网络构造在用户行为上应用中的两个问题：</p>
<ul>
<li>固定滑动窗口导致大量用户信息不能获取完整</li>
<li>拆分用户的浏览行为计算导致循环网络在信息理解上的歧义</li>
</ul>
<p><strong>4.损失函数选取</strong></p>
<p>关于损失函数的选取，我们这边主要推荐两种方案：基于贝叶斯后验优化(BayesianPersonalizedRanking,BPR)和第一准则(TOP1)：</p>
<ul>
<li>贝叶斯后验优化(BayesianPersonalizedRanking,BPR)是一个近似矩阵分解的方法，我们分别选取当前用户当前商品的下一个真实浏览的商品作为Positive Item，随机抽样的商品作为Negative Item，具体表达形如：<img src="/2018/03/04/yoho!buy注册概率预估/14.jpeg" alt=""><br>，其中Ns即为随机采样个数，<img src="/2018/03/04/yoho!buy注册概率预估/15.jpeg" alt=""><br>中k为i时对应Positive Item的真实计算得分，K为j时对应Negative Item，保持i=Positive Item不变，计算所有抽样出来的Negative Item作为j进行计算即可。</li>
<li>第一准则(TOP1)是我们自己设计的一个近似排序的方法，我们想要真实的Positive Item所计算出来的结果尽可能的接近1，Negative Item计算出来的结果尽可能的接近0，所以我们要保证采样出来的Negative Item比Positive Item在当前计算方式下的得分要低，所以，我们可以设计损失函数如下：<img src="/2018/03/04/yoho!buy注册概率预估/16.jpeg" alt=""><br>，最末项增加了个正则项修正拟合程度。</li>
</ul>
<p>除此之外，《SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS》中还提到了POP，S-POP，Item-KNN，BPR-MF等方法，可单独了解。</p>
<h4 id="Logistics-Regression部分"><a href="#Logistics-Regression部分" class="headerlink" title="Logistics Regression部分"></a>Logistics Regression部分</h4><p><strong>通过Logistics Regression部分提高融合Recurrent Neural Network的潜藏层和传统的用户基础特征，进行一次重排序的操作，个性化的提供用户的注册激励也是非常重要的一环。</strong><br>整个Recurrent Neural Network部分在一定程度上帮助我们获取到了用户的浏览操作行为中的trend和seasonality（隐藏状态GRU States），但是缺乏考虑外部信息，比如热点爆品，用户区域，用户性别，用户需求等等。最典型的一个例子就是，我们不能向一个浏览了多个中性黑色太阳眼镜的正在试图走酷雅风格的东北女性推送潮牌男性短裤优惠券作为注册激励，很多时候会起到相反的作用。</p>
<p>如何兼顾Recurrent Neural Network部分的潜藏数据与为数不多的用户基础特征数据，并加以融合快速反馈出结果，是需要多方面考虑的：<br><strong>数据应用</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据类别</th>
<th style="text-align:center">数据详解</th>
</tr>
</thead>
<tbody>
<tr>
<td>基础用户画像</td>
<td style="text-align:center">人口属性，地点，性别，消费力等</td>
</tr>
<tr>
<td>主动行为数据</td>
<td style="text-align:center">品类偏好、品牌偏好、行为性别等</td>
</tr>
<tr>
<td>文本偏好数据</td>
<td style="text-align:center">浏览商品文字描述特征</td>
</tr>
<tr>
<td>反馈数据</td>
<td style="text-align:center">停留时长，复停留行为，当前时间段等</td>
</tr>
<tr>
<td>…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>基础用户画像&amp;主动行为数据：我们可以在用户原始日志中快速清洗出用户的地址，环境，设备等基础信息，结合用户浏览商品的性别+价格+品牌加权预估出用户的性别，消费力等价值属性，品类偏好、品牌偏好、行为性别等基础汇总属性。</li>
<li>文本偏好数据：根据用户的浏览商品，去匹配是否命中了我们预先提取出的注册用户浏览高频关键词，比如“鬼洗”，“典藏”等等及当前的一些热点词汇“小白鞋”，“华莱士”等。</li>
</ul>
<p><img src="/2018/03/04/yoho!buy注册概率预估/17.jpeg" alt=""></p>
<ul>
<li>反馈数据：在整个Recurrent Neural Network部分我们考虑的是用户的浏览顺序，但是忽略了用户的浏览质量，用户进入平台后的15s内，A商品重复浏览了3次，停留了9S，B商品重复浏览了1次，停留了1S，商品A的注册激励价值是远远高于商品B的。</li>
</ul>
<p>通过以上的方式获取到的“用户画像”好处在于快速，再扩充了用户基本属性的同时还能够在规定的时间内完成所有的重排序计算，但是缺点在于一定程度上降低了用户特征刻画的精度。</p>
<p><strong>数据融合</strong></p>
<p>在实际的应用过程中，我们发现，在一定程度上交叉部分高价值的用户特征有助于提高最后的预测结果的准确性，构造的框架图如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/18.jpeg" alt=""></p>
<p>这边借鉴但是没有完全采用wide&amp;deep的方法，借鉴了对原始用户特征需要通过embedding layer进行处理，比如通过简单的one hot encoding的形式，然后采取特征交叉的方式获取新的用户特征，最后再进行前向传播或者logistics regression；但是此处，在embedding layer的过程中会采取人为限制分箱逻辑去噪（剔除了比如地点归属000ex00f这样的错误数据），在交叉过程中只选取了部分对最后的用户注册影响较大的因素进行交叉，在提升了模型对用户拟合的能力的同时也保证了模型的实效性。</p>
<p><strong>数据流设计</strong></p>
<p>简单的数据执行流如下：<br><img src="/2018/03/04/yoho!buy注册概率预估/19.jpeg" alt=""></p>
<p>主要步骤如下：</p>
<ul>
<li>Kafka+Flume解析实时点击、搜索、浏览等用户操作日志流，在线进行用户操作数据的抽取</li>
<li>实时解析基础用户环境信息，获取环境特征：手机型号，网络，地址等，实时写入到线上Hadoop/Spark中的HDFS里</li>
<li>根据离线存储在HDFS中的用户操作数据、用户点击流数据和用户是否真实注册的结果离线更新循环网络GRU及LR模型参数</li>
<li>将新的模型参数应用于线上用户数据的预测</li>
</ul>
<p>最后可得到个人及全站的注册概率变化可视化如下：</p>
<p><img src="/2018/03/04/yoho!buy注册概率预估/20.png" alt=""></p>
<h1 id="可优化方向"><a href="#可优化方向" class="headerlink" title="可优化方向"></a>可优化方向</h1><ul>
<li>GRU卷积神经网络的层数优化，由多层隐藏层替代单层隐藏层提高对用户行为的汇总效果</li>
<li>Logistics Regression部分可以由多模型bagging替换，降低过拟合的可能</li>
<li>反馈数据清洗，对于有强烈意愿注册的用户进行识别，避免干扰正样本池</li>
<li>推荐内容干扰，那些热门爆款更多的用户看到，而且“被看到”这个行为也加深了它接下去被接连看到的可能性</li>
<li>GRU卷积神经网络的构造中，修改上一次操作预测下一次操作为上一次操作预测<strong>目标行为</strong>（常停留时长的点击、收藏点击等等高价值的行为节点）</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>传统的机器学习方案给用户行为预估的项目一个基准水平线，而深度学习的出现，一定程度上使得这个上限有所提高，但是以数据为基础，用算法去雕琢，只有将二者有机结合，才会带来更好的效果提升。</p>
<p>以上是yoho!buy团队在实践中的一点总结，当然我们还有还多事情要做，keep learning！最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[浅入浅出深度学习理论实践]]></title>
      <url>/2018/02/07/%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E5%AE%9E%E8%B7%B5/</url>
      <content type="html"><![CDATA[<p><img src="http://upload-images.jianshu.io/upload_images/1129359-c6cc20032d7d0314.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br><a id="more"></a></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前在知乎上看到这么一个问题：<a href="https://www.zhihu.com/question/266368852/answer/307130218" target="_blank" rel="noopener">在实际业务里，在工作中有什么用得到深度学习的例子么？用到 GPU 了么？</a>，回头看了一下自己写了这么多东西一直围绕着traditional machine learning，所以就有了一个整理出深度学习在我熟悉的风控、推荐、CRM等等这些领域的用法的想法。</p>
<p>我想在这边篇文章浅入浅出的谈谈这几个方面，当然深度学习你所要了解必然不仅仅如此，后面如果有机会我会一篇篇的完善：</p>
<ul>
<li>CNN/RNN理解</li>
<li>Attention理解</li>
<li>深度学习（CNN和RNN）传统领域的简单应用</li>
<li>关于深度学习的一些想法</li>
</ul>
<p>大概会将全文分为以上几块，大家可以跳读，因为本文理论上应该会冗长无比，肯定也包括数据块+代码块+解析块，很多有基础的同学没有必要从头在了解一遍。好了，让我们正式开始。</p>
<h1 id="CNN-RNN理解"><a href="#CNN-RNN理解" class="headerlink" title="CNN/RNN理解"></a>CNN/RNN理解</h1><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>CNN，卷积神经网络，让我们先从一个简单的网络结构来梳理一下：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-772dffb29dc4d9c0.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h3 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h3><p>假如，我们有car,plane,desk,flower等等<strong>一共十类的图片</strong>，需要让电脑识别图片是哪种的话，自然需要把图片变成电脑理解的了的一种方式，比如：RxGxB（图片的高度、宽度和深度）也就是上面<strong>输入层</strong>32x32x3，至于什么是RGB，请自行阅读<a href="https://baike.baidu.com/item/RGB/342517?fr=aladdin" target="_blank" rel="noopener">RGB百度百科</a>。</p>
<h3 id="卷积层一-卷积层二"><a href="#卷积层一-卷积层二" class="headerlink" title="卷积层一/卷积层二"></a>卷积层一/卷积层二</h3><p><img src="http://upload-images.jianshu.io/upload_images/1129359-47d1b5a6fb64a4f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这张图片我觉得形象的不能再形象了，让我们结合代码和图形来理解这个卷积到底是什么意思。tensorflow中卷积的集成代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, <span class="number">1</span>, filter_num], stddev=<span class="number">0.1</span>),name=<span class="string">"filter_weights"</span>)</span><br><span class="line"></span><br><span class="line">conv_layer = tf.nn.conv2d(item_embed_layer_expand, filter_weights, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"VALID"</span>,name=<span class="string">"conv_layer"</span>)</span><br></pre></td></tr></table></figure>
<p>filter_weights的shape是window_sizexembed_dimx1xfilter_num，window_sizexembed_dimgx1就是类似于gif图中的黄色区域的大小，这边就可以看作window_size=embed_dim=3，filter_weights第三维的1是指HRG中的第三维度：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-071901b190573269.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>filter_weights第三维如果是2的话：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-d2bd2d98ce6fd07d.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>想到于多了一维的并行处理，接下来看filter_weights的filter_num，最上方的gif的动图解释了卷积层的计算形式：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-0ed7c1c4734f9206.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>黑色字体的1/0矩阵是原来图像的像素值，红色的1/0是上面设置filter_weights值，他们的分别计算后的累计值即为一次扫描计算结果，比如黄色区域即为1x1+1x0+0x1+1x0+1x1+1x0+0x1+1x0+1x1.将所有的像素值所在的位置都进行一次扫描后就可以得到：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-a79b2232f850395b.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>当然，除了随机生产filter_weights，图像操作中，指定不同的filter_weights会起到不同的作用：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-1ce76cadc5718922.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>但是这里面存在两个问题：</p>
<ul>
<li>边界扫描</li>
<li>扫描速度</li>
</ul>
<h4 id="边界扫描（padding-”VALID”）"><a href="#边界扫描（padding-”VALID”）" class="headerlink" title="边界扫描（padding=”VALID”）"></a>边界扫描（padding=”VALID”）</h4><p><img src="http://upload-images.jianshu.io/upload_images/1129359-a1a5d8e2b907771f.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>VALID模式如上图所示，对原始图像进行卷积，卷积后的矩阵只有3×3阶，比原来的图片要小了。</p>
<p>SAME模式要求卷积后的feature map与输入的矩阵大小相同，因此需要对输入矩阵的外层包裹n层0，然后再按照VALID的卷积方法进行卷积。</p>
<p>n的求法如下式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SAME：</span><br><span class="line">edge_row = (kernel_row - 1) / 2</span><br><span class="line">edge_cols = (kernel_cols - 1) / 2</span><br><span class="line">VALID：edge_row = edge_cols = 0</span><br></pre></td></tr></table></figure>
<p>其中，edge_row是包裹0的行数，edge_cols是包裹0的列数 , kernel_row就卷积核的行数。</p>
<h4 id="扫描速度（tf-nn-conv2d中的-1-1-1-1-）"><a href="#扫描速度（tf-nn-conv2d中的-1-1-1-1-）" class="headerlink" title="扫描速度（tf.nn.conv2d中的[1,1,1,1]）"></a>扫描速度（tf.nn.conv2d中的[1,1,1,1]）</h4><p>这个概率也是最好理解的了，就是图中的黄色方框位移的速度：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-56235bdd16fba64e.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>回到最上面filter_num，filter_num的值就是重复上述流程的次数，随着次数的增加，会增加后面pooling层的基础数据层数：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-a186f2410acb046d.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>每次黄色箭头后的pool层的层数就是filter_num。</p>
<p>到此为止，卷积层就简单的梳理完了，主要是要清楚几个概念：filter_weights的作用，filter_num的定义，padding的差异，还有扫描的速度。这些主要是围绕着下面我要实际应用的场景梳理的卷积神经网络的知识点，如果要深刻透彻的了解还需要更多更深入的解读。</p>
<h3 id="池化层一-池化层二"><a href="#池化层一-池化层二" class="headerlink" title="池化层一/池化层二"></a>池化层一/池化层二</h3><p>先从数学角度来看，它的操作步骤：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-1277229fc560f408.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这张图看起来，和卷积层中的image —&gt; Convolved feature非常类似，也是确定一个shape之后，对shape内的数据进行操作，但是差异就在：<strong>卷积层中是采取对image里面的像素点逐点计算后汇总，相当于加权了每个像素点的作用；而池化层通常采用最大/最小/均值/求和等方式汇总Convolved feature</strong>。</p>
<p>罗列出来就是：</p>
<ul>
<li>对象不同，一个是image，一个是Convolved feature</li>
<li>计算方式不同，一个matrix点对点乘积后求和，一个是直接求和（或者其他聚合操作）</li>
</ul>
<p>池化层数学的操作比较简单，在实际工程中的理解比较让人困惑，其实，意义主要在三点：</p>
<ul>
<li>其中一个显而易见，就是减少参数。通过对 Feature Map（通过的手段是聚合计算） 降维，有效减少后续层需要的参数</li>
<li>一个是 Translation Invariance。它表示对于 Input，当其中像素在邻域发生微小位移时，Pooling Layer 的输出是不变的。这就使网络的鲁棒性增强了，有一定抗扰动的作用</li>
<li>另一个是以区块的角度代替逐个点进行计算，降低每个点对最后结果的影响，避免了过拟合的现象</li>
</ul>
<h3 id="全连接层-输出层"><a href="#全连接层-输出层" class="headerlink" title="全连接层/输出层"></a>全连接层/输出层</h3><p>这个就比较简单了，全连接层（FC）构造如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.dense(brand_embed_layer, embed_dim, name=<span class="string">"brand_fc_layer"</span>, activation=tf.nn.relu)</span><br></pre></td></tr></table></figure>
<p>简单的来说，通过activation增加了特征的非线性的拟合能力；如果不设置activation的话，就增加了特征的线性拟合能力。</p>
<p>但是，我们要知道，全连接层会有很多缺陷：</p>
<p>在一定程度上，可以通过增加全连接的层数提高train data的准确率，但是如果过分的增加，会造成过拟合，所以如果是自己写的网络，一定程度上，如何控制还好全连接层的数量决定了valid data了准确率的波动。其实，完全可以通过pool层代替全连接层，17年年初很多论文指出：GAP（Global Average Pooling）的方法可以代替FC（全连接）。思想就是：用 feature map 直接表示属于某个类的 confidence map，比如有10个类，就在最后输出10个 feature map，每个feature map中的值加起来求平均值，然后把得到的这些平均值直接作为属于某个类别的 confidence value，再输入softmax中分类， 更重要的是实验效果并不比用 FC 差，所以全连接层的分类器的作用就可以被pool层合理代替掉。</p>
<p>而且，全连接层参数冗余（仅全连接层参数就可占整个网络参数60%-80%），计算量会集中在这些参数的计算上，而且随着你的层数的增加，你的计算成本越来越大，如果是非GPU的机器在计算的过程中会非常非常吃亏。</p>
<p>之所以，现在的很多很多流行网络还是以FC参与计算的原因：</p>
<ul>
<li>简单，很方便了解。而且当前的各个计算框架tensorflow，caffe等等对FC的封装即成也是非常的完善</li>
<li>借鉴非常容易。举个例子，如果已经有了一个通过输出层产出10类的模型，我现在要做一个5类的模型的话，我只需要在最后一次FC层后面增加一个embed_dim=5的FC层即可。</li>
</ul>
<p>上面就简单了梳理了CNN里面的简单的网络结构，是不是真的就是这么简单呢？让我们看看上面叫做相对成熟的网络：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-33c3f5aa3d6445a7.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>图像中几年前的技术，Alexnet，SSD，Yolo，还有去年的RCNN，fast-RCNN等等，网络结构都远远比我们想象中的要复杂，在对数据操作的行为中，无非也是上面这些操作的一些组合。</p>
<p>再次强调，本文重点不在介绍CNN，而是利用CNN作为传统机器学习做协助，所以，如果想要深入了解CNN的同学建议从头开始学习，不建议阅读我这种跳讲的内容作为入门。</p>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>相比CNN而言，RNN要简单而又有趣一些：</p>
<p>几乎所有的讲RNN的技术文章都会有下面这张图，无法免俗，因为确实囊括了RNN的核心：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-246995854fe4a3f1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>不得不说，nlp是RNN非常优秀的应用场景，我们从nlp的角度去切入，观察RNN在其中所起的作用也是非常好的一个方式：<br>假设有一句话：“今天天气真的不是很好，让我们去____吧。”</p>
<p>如果用朴素贝叶斯来解决这个填空问题，它的解决思路是：</p>
<ul>
<li>先分词，今天，天气，真的，不是很好，让，我们，去，吧</li>
<li>去除语气词等，剩余天气，不是很好，我们，去</li>
<li>再根据贝叶斯公式得到概率最大的值</li>
</ul>
<p>如果用N-Grams来解决这个填空问题，它的解决思路是：</p>
<ul>
<li>先分词，今天，天气，真的，不是很好，让，我们，去，吧</li>
<li>去除语气词等，剩余天气，不是很好，我们，去</li>
<li>根据前缺失词的前N个词的条件概率来计算出具有最大概率的缺失值</li>
</ul>
<p>朴素贝叶斯的方法只考虑每个词出现的结果没有考虑先后顺序，可能导致由意外的非真实排序决定缺失值的问题；N-Grams的方法虽然考虑了每个词出现的可能的同时，也考虑缺失值前N个词的内容，但是由于计算能力的约束，并不能够完整的保留全部的前置语句的信息。</p>
<p>而RNN的出现，利用state层来存储前面t-1刻的信息，并循环传递在每次输出计算中，解决ngram做不到的完整信息保存的问题，如下图：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-0fb4f9504f5a996a.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>很明显，在对Yt+2的结果预测的时候，考虑到了前面所有前置的信息。<img src="http://upload-images.jianshu.io/upload_images/1129359-123ce6c502bff9d0.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这张图很好的解释了RNN的传递逻辑，将所有前期的信息以state的形式进行传递，在第t次的输出结果计算的过程中，不仅仅考虑第t次的输入值，同时考虑t-1次的state，也就是前t-1层的信息的流动汇总结果。我们知道，最简单最基础的RNN里面，可以通过tanh层来合并t-1时刻的state和t时刻的xt信息的。虽然理论上来说，无论信息是各多远，RNN都能够记得，但是！但是！实际上，我们发现，RNN随着tanh的重复操作，是无法稍远的信息就无法合理的被记忆，幸运的是后面优化出来的LSTM和GRU就能一定程度上缓解这些的问题。</p>
<p>下面让我们以GRU为例子，具体看看RNN是怎么进行一次循环神经网络的计算的：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-06870b92163db024.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这边大家需要注意，与LSTM不同，GRU将LSTM中的输入门和遗忘门合并成了更新门。而且没有建立中间过渡键memory cell，而是直接通过更新门和重置门来更新state。这样做的好处就是大大的降低了计算的成本，加快了整个RNN训练的速度。同时通过各种Gate将重要特征保留，保证其在long-term传播的时候也不会被丢失，也有利于BP的时候不容易造成梯度消失。</p>
<p>在RNN的官方论文中，我们看到了实测的效果如下：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-1effb405a319dacc.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>很明显的可以看到，1.虽然GRU减少了一个门的存在，但是效果与LSTM相当，但是几乎每次测试的test效果都要优秀于传统方法。2.GRU是真的肉眼可见的比LSTM快，证实了我们上述说的内容。也是因为这些原因，在后面为实际应用的过程中，我也是选择了GRU来代替了LSTM做向量化及state层提取等等操作。</p>
<p>问题来了，虽然我知道LSTM和GRU在最后实测的效果上是比直接用tanh的简单RNN效果要好的，但是我也无法解释和理解为什么这样的构造就能够有这样的提升，这就比较尴尬了。</p>
<p>另外要提的一点就是，在GRU实际计算的过程中，采取了学习参数拼接的方式，比如上面的Wz，Wr等是通过拼接的方式存储的，在需要的计算的时候再拆分开进行计算：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-7589255974e45aba.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这也是让我在学习GRU过程中眼前一亮的点，非常值得玩味的地方。</p>
<h1 id="Attention理解"><a href="#Attention理解" class="headerlink" title="Attention理解"></a>Attention理解</h1><p>在篇幅如此冗长的情况下，我依然坚持要和大家讨论一下关于Attention的一些看法和观点，我觉得正是有attention的存在，才让我们能够想到如何更好的去扩展应用这些深度学习的方法。</p>
<p>我之前一直没有找到很好的通俗易懂的解释attention的文章，这边我尝试以业务的角度为大家分析一下，尽可能的抛开数学的角度让大家浅入浅出一下。</p>
<p>假设存在用户A，及他的各种行为A_actions，如果我不做任何操作简单的把他的各种行为A_actions当成变量进行模型训练可以得到模型AM。但是，如果我知道，他可能是一个2年前流失近期活跃的用户，我选择剔除他两年前的A_actions，而只考虑他近期的行为，这样的过程其实就是一个Attention的过程，因为我们要预测他近期可能买什么，所以我们应该把关注点集中在了他近期的部分信息而不是全部信息上。</p>
<p>而在深度学习运用的过程中，我们也应该考虑attention的问题，比如用户商品点击流为A—&gt;B—&gt;A—&gt;C—&gt;D，我们常规操作是什么样的？无非是：</p>
<ul>
<li>生成4xembed_dim的embedding层</li>
<li>将ABCD四个商品编号为0123</li>
<li>找到对应商品在embedding层中的向量表示</li>
<li>Encoder过程完成</li>
<li>通过RNN或者其他深度学习网络进行非线性Decoder输出对应的可能结果</li>
</ul>
<p>以上就是一个非常简单的Encoder-Decoder过程。</p>
<p>仔细想想其实就会发现很多不合理的地方，比如我在B商品停留了2mins，而其他商品均只停留了不到10s；再比如，我有购买C商品的历史，ABD商品均为第一次接触等等。其实，对于ABCD而言，简单的理解就是它们为不是等权的。而且我们发现，随着你的信息量的增加，也就是item点击流的长度增加，encoder的信息丢失就会变得非常严重，decoder的难度会大大的提升。</p>
<p>回过头来看上述的流程，如果变成：</p>
<ul>
<li>生成4xembed_dim的embedding层</li>
<li>将ABCD四个商品编号为0123</li>
<li>找到对应商品在embedding层中的向量表示</li>
<li>Xa,Xb,Xc,Xd = ∑(aiA,biB,ciC,diD)</li>
<li>通过RNN或者其他深度学习网络进行非线性Decoder输出对应的可能结果</li>
</ul>
<p>换句话说，就是在坑爹的Encoder到Decoder过程中，增加了缓冲计算Ci，通过构造Ci代替Encoder结果进行Decoder的过程，让深度学习的过程更加的合理。<br><img src="http://upload-images.jianshu.io/upload_images/1129359-4d833425ca8bff8b.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>数学的形式就是:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y1=f1(C1)</span><br><span class="line">y2=f1(C2,y1)</span><br><span class="line">y3=f1(C3,y1,y2)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>比如我在B商品停留了2mins，而其他商品均只停留了不到10s时，我们就可以构造缓冲C=g(0.1xf(A)+0.6xf(B)+0.1xf(C)+0.1xf(D))，这意味着A—&gt;B—&gt;A—&gt;C—&gt;D—&gt;?，对于？的判断，B起了比ACD都要重要的作用。</p>
<p>大名鼎鼎的RNN在attention的机制下就会变成：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-3eada4a94d4275f4.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么具体如何构造缓冲C呢，我们看下面这个流程：<br>首先，在RNN最初参数设置的时候，我们会确定init memory，不妨记为z0；hi为当前时刻输出的隐层输出向量，所以对每个商品ABCD都有一个z0与hi的相似度∂0i：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-3cfbf5662c5ce7e9.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>在每次循环之前，相当于考虑了当前所有的输入（比如此刻的ABCD）与initmemory的匹配度，至于匹配度match在论文中的计算方式为：矩阵变换α=hTWz (Multiplicative attention，Luong et al., 2015) ，其实简化为余弦相似度也是可以的，只要能判断两者之间的相似程度都行。算出所有的∂0a,∂0b,∂0c,∂0d后归一化后的值即可作为ABCD对应的隐层ha，hb，hc，hd的权重：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-eafbed902bac58a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>c0即可作为rnn的输入，有c0和z0，我们非常容易可以算出z1，得到z1后，重复上述的过程可以得到c1…，如此循环，直到结束。<a href="https://arxiv.org/abs/1412.7449" target="_blank" rel="noopener">论文中的计算方式</a>如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-b5d7a1337ffa209a.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>和nlp中构造方式对比起来，还是有一定的差异，nlp的训练集往往是确定的。比如：“我爱学习”翻译为“i love studying”，我翻译为i，所以我确定一定要对“i”进行翻译的时候，需要提高对应i的权重。而我在商品点击流预测购买概率的时候，只能通过停留时长，历史是否购买过来建立约等的关系，但是这个约等的关系是不存在强成立的前提的。</p>
<p>attention的机制最初理解起来有点绕，但是如果能够搞懂并在我们做深度网络设计中应用起来，理论上收益还是非常之大的，建议大家把上述为贴的论文详读一边，真的是写的非常不错的一篇文章。</p>
<h1 id="深度学习传统领域的应用"><a href="#深度学习传统领域的应用" class="headerlink" title="深度学习传统领域的应用"></a>深度学习传统领域的应用</h1><p>我们先来回想一下，我们做传统有监督是怎么做的，如果记不得了，可以回顾这篇文章：<a href="http://shataowei.com/2018/01/20/提升有监督学习效果的实战解析/" target="_blank" rel="noopener">提升有监督学习效果的实战解析</a>，我认为有几点传统有监督学习不是很友好：</p>
<ul>
<li>特征工程</li>
<li>实效性</li>
<li>数据解析能力</li>
</ul>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>想必有过特征工程项目经验的同学可能是对数据预处理及特征筛选过程心有余悸：</p>
<ul>
<li>是不是用户信息，商品信息，用户历史信息，商品信息统计属性刻画，用户行为整合每一块写hive都要很久很久？跑数据的时间更久？</li>
<li>是不是数据好不容易跑出来了，各种垃圾信息，各种格式问题，pandas，numpy来回折腾到几百行的代码？</li>
<li>是不是好不容易数据处理完，一跑结果auc0.6？修改都不知道怎么修改？</li>
<li>是不是四处看别人整理的调参心得，比如这个家伙的<a href="http://shataowei.com/2017/12/30/Kaggle-TianChi分类问题相关纯算法理论剖析/" target="_blank" rel="noopener">Kaggle&amp;TianChi分类问题相关纯算法理论剖析</a>,然后发现优化后就提升了1个点？</li>
<li>是不是上线之后发现数据量一旦一大，你本地跑的脚本全部都报出：<code>MemoryError</code>?</li>
<li>是不是立项一周后产品经理过来问什么时候上线的时候，你连数据还没整理完？</li>
</ul>
<p>诸如这样坑爹的事情实在多的不能再多，相对而言，无论是是CNN还是RNN或者其他深度学习网络的input都是非常简单很清晰的，我这边给出一些简单的例子：</p>
<p>你在构造卷积神经网络的时候，只需拿出商品的基础属性，然后用不同性质的向量化方法embedding成不同的向量对象进行channel叠加就行了：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-e62bbbd6b61adaf8.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>你在构造循环神经网络的时候，只需拿出用户商品的点击流，然后构造一个流通的点对点的循环网络即可：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d482bcf05c779604.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>卷积网络的原始数据只需要整理item与attribute对应关系，循环网络的原始数据只需要整理item与clickflow对应关系，相比复杂的传统方法的各种技巧，特征工程的提取整理的时间会大大减少，而且在线上数据处理过程中发生<code>Memory Error</code>的可能也无限变小。</p>
<h2 id="实效性"><a href="#实效性" class="headerlink" title="实效性"></a>实效性</h2><p>这个就比较好理解了，如果我们需要知道用户在app上每一刻的下单概率分布，如果用传统方法实现难度比较大，比如汇总前若干长时间内的信息再处理加工成模型需要的形式，再通过模型判断概率，可能就不是实时概率预估了。</p>
<p>而如果按照上述深度学习特征梳理方法，离线训练训练好用户的点击信息商品信息，再利用训练好的模型加用户在app上的实时行为，去预测用户在app上每一步操作对目标变量的影响,虽然我在离线训练的时间会付出的更多，但是我在线上预测会更加快捷。</p>
<p>具体效果我们以订单预估为例，深度学习预估方法下我们会很容易看到<strong>一个用户</strong>从开始一个session到结束一个session的过程中，购买欲望的分布：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-3defe3531234b706.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在用户购买欲望特别高涨的时候，通过相应的push或者文案提醒，促进用户下单，提高成单率。</p>
<p>除此之外，我们还可以观察到每一刻全平台用户的购买欲望分布：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-7096379764d513bf.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="数据解析能力"><a href="#数据解析能力" class="headerlink" title="数据解析能力"></a>数据解析能力</h2><p>在围绕构造特征的时候，我们在对过去的数据整理的过程中，其实构造的最多的就是“过去N天”，“历史上”，“最后一次”，“第一次至今”，等等。其实，这些构造方法要么是汇总整合一段时间的信息，要么是单点的考虑某个时刻的信息量。但是，深度学习一定程度上会选择的汇总过去的信息的累积，根据实际对最终结果的影响，改变单次行为上的权重，避免单次行为对因变量的错误影响。比如RNN中的state，上面RNN中的文章我也介绍了，它的生成其实就是保留了前t-1次中的部分信息。</p>
<p>知乎上有这么一个问题<a href="https://www.zhihu.com/question/266479309" target="_blank" rel="noopener">RNN方法能够捕捉到 传统时间序列回归中的 trend ，seasonality么？</a>,其实我也很好奇，在引入深度学习的fc层到machine learing做stack的时候，确实绝大部分都能提高auc，但是是不是因为这些深度学习方法能捕捉到传统的数据里面的类似trend这些难以统计描述的性质？</p>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>说了这么多，我觉得还是以具体的例子来剖析比较有说服力，因为深度学习的模型相对比传统的模型代码要长很多很多，我这边只截取我认为比较重要的地方解释一下，想要看demo的去看我的<a href="https://github.com/sladesha/deep_learning" target="_blank" rel="noopener">GitHub</a>吧。我给出的例子都是最简单的网络设计，如果实际要应用大家可以按照自己业务的需求增加网络的深度，改变网络的结构，这边只是给大家一个方向。<strong>此外，数据的处理也并没有因为深度学习模型的出现而变得不重要，Garbage In, Garbage Out!!!</strong></p>
<p>RNN方案的思路是来自于Domonkos Tikk和Alexandros Karatzoglou在《Session-based Recommendations with Recurrent Neural Networks》</p>
<p>它构造了embedding层来把原始的输入item映射为一个长度定义好的向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">embedding = tf.get_variable(<span class="string">'embedding'</span>, [self.n_risks, self.rnn_size], initializer=initializer)</span><br></pre></td></tr></table></figure>
<p>通过把user浏览或者点击过的item进行index编号X，然后根据编号去embedding层去找对应的vector，后续只要用用户接触到了该item就重复以上的embedding过程就行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = tf.nn.embedding_lookup(embedding, self.X)</span><br></pre></td></tr></table></figure>
<p>再构造了简单的GRU层来学习每次用户的点击先后顺序之间的关系：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多层简单GRU层定义</span></span><br><span class="line">cell = rnn_cell.GRUCell(self.rnn_size, activation=self.hidden_act)</span><br><span class="line">drop_cell = rnn_cell.DropoutWrapper(cell, output_keep_prob=self.dropout_p_hidden)</span><br><span class="line">stacked_cell = rnn_cell.MultiRNNCell([drop_cell] * self.layers)</span><br></pre></td></tr></table></figure>
<p>它的网络结构一点也不复杂：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-4239c751af300c04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>首先，需要把数据集构造成中间的uid+item+time的格式：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-d482bcf05c779604.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>然后通过用户自身点击item的顺序，以时间靠前的item项预测时间靠后的item项，训练完成后记录每条数据对应的out和state。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-d5b28a58edc73240.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>但是我实测了多层GRU和单层GRU，因为我们需要进行stacking的过程，不建议做多层的GRU，层数越多每层的信息量越稀薄，我通过sum，mean处理后仍不如单层：<br><img src="http://upload-images.jianshu.io/upload_images/1129359-f124151538779717.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<p>CNN的方案通常可以采取以下通用的网络形式：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1129359-9620ea0cbce2fad9.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>所以可优化的点我均在网络结构中标注了，但是我一直没有找到CNN再传统学习中比较好的应用方式，如果拿最后一个FC层的向量stacking实测效果并不理想，相关代码我也放在了GitHub中了，大家可以作为一个尝试性的demo去看。</p>
<p>一样的item的向量化处理方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cate_embed_matrix = tf.Variable(tf.random_uniform([cate_max, embed_dim], <span class="number">-1</span>, <span class="number">1</span>),name=<span class="string">"cate_embed_matrix"</span>)</span><br><span class="line">cate_embed_layer = tf.nn.embedding_lookup(cate_embed_matrix, cate, name=<span class="string">"cate_embed_layer"</span>)</span><br></pre></td></tr></table></figure>
<p>网络结构中主要是通过构造全连接层和卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全连接</span></span><br><span class="line">cate_fc_layer = tf.layers.dense(cate_embed_layer, embed_dim, name=<span class="string">"cate_fc_layer"</span>, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积层</span></span><br><span class="line">filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, <span class="number">1</span>, filter_num], stddev=<span class="number">0.1</span>),name=<span class="string">"filter_weights"</span>)</span><br><span class="line">filter_bias = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[filter_num]), name=<span class="string">"filter_bias"</span>)</span><br><span class="line">conv_layer = tf.nn.conv2d(item_embed_layer_expand, filter_weights, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"VALID"</span>,name=<span class="string">"conv_layer"</span>)</span><br><span class="line">relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer, filter_bias), name=<span class="string">"relu_layer"</span>)</span><br><span class="line">maxpool_layer = tf.nn.max_pool(relu_layer, [<span class="number">1</span>, sentences_size - window_size + <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],padding=<span class="string">"VALID"</span>, name=<span class="string">"maxpool_layer"</span>)</span><br></pre></td></tr></table></figure>
<p>然后再把所有全连接完和卷积完的vector拼接：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一层全连接</span></span><br><span class="line">cate_fc_layer = tf.layers.dense(cate_embed_layer, embed_dim, name=<span class="string">"cate_fc_layer"</span>,activation=tf.nn.relu)</span><br><span class="line">brand_fc_layer = tf.layers.dense(brand_embed_layer, embed_dim, name=<span class="string">"brand_fc_layer"</span>,activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二层全连接</span></span><br><span class="line">bc_combine_layer = tf.concat([cate_fc_layer, brand_fc_layer, dropout_layer], <span class="number">2</span>) bc_combine_layer = tf.contrib.layers.fully_connected(bc_combine_layer, <span class="number">200</span>, tf.tanh)</span><br></pre></td></tr></table></figure>
<p>最后通过全连接输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inference_layer = item_combine_layer_flat</span><br><span class="line">inference = tf.layers.dense(inference_layer, <span class="number">2</span>, kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),kernel_regularizer=tf.nn.l2_loss, name=<span class="string">"inference"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="关于深度学习一些想法"><a href="#关于深度学习一些想法" class="headerlink" title="关于深度学习一些想法"></a>关于深度学习一些想法</h1><p>这篇文章终于要结束了，漫漫长文。图像、语音、自然语言处理这三个领域，深度学习的性能就是比传统方法好得多，无可辩驳。但是传统领域，比如点击率预估，风控概率预估，金融风险预估等等，我不赞成非得和深度学习扯上关系，我们应该想想：</p>
<ul>
<li>我们有足够大量的数据支撑计算么？</li>
<li>我们的业务需求允许我们进行大量黑盒计算么？</li>
<li>带来的“提高”允许你所付出的成本么？</li>
<li>使用者真的知道自己在做什么么？</li>
</ul>
<p>最后，我以血和泪的教训知道自己写的网络对模型的效果提升是非常非常小的，建议大家先熟知现有的成熟的网络：</p>
<ul>
<li>谷歌的wide&amp;deep思想，<a href="https://github.com/sladesha/deep_learning/tree/master/Wide%20%26%20Deep" target="_blank" rel="noopener">论文及快速上手的demo</a>。</li>
<li>Youtube的推荐，<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf" target="_blank" rel="noopener">论文地址</a>。</li>
<li>网易考虑的考拉，<a href="http://cfm.uestc.edu.cn/~zhangdongxiang/papers/ICDE16_industry_231.pdf" target="_blank" rel="noopener">论文地址</a>。</li>
</ul>
<p>参考文献</p>
<p>[1] <a href="http://www.cnblogs.com/rgvb178/p/6017991.html" target="_blank" rel="noopener">Foundation of Convolutional Neural Networks</a></p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/25518711" target="_blank" rel="noopener">YJango的循环神经网络</a></p>
<p>[3] <a href="http://cs224d.stanford.edu" target="_blank" rel="noopener">Deep Learning for Natural Language Processing</a></p>
<p>[4] <a href="http://blog.csdn.net/wuzqChom/article/details/75792501" target="_blank" rel="noopener">Attention机制</a></p>
<p>[5] <a href="https://arxiv.org/abs/1606.07792" target="_blank" rel="noopener">Wide &amp; Deep Learning for Recommender Systems</a></p>
<p>[6] <a href="https://research.google.com/pubs/archive/45530.pdf" target="_blank" rel="noopener">Deep Neural Networks for YouTube Recommendations</a></p>
<p>[7] <a href="http://cfm.uestc.edu.cn/~zhangdongxiang/papers/ICDE16_industry_231.pdf" target="_blank" rel="noopener">Personal Recommendation Using Deep Recurrent Neural Networks in NetEase</a></p>
<p>[8] <a href="https://www.researchgate.net/publication/284579100_Session-based_Recommendations_with_Recurrent_Neural_Networks" target="_blank" rel="noopener">Session-based Recommendations with Recurrent Neural Networks</a></p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[提升有监督学习效果的实战解析]]></title>
      <url>/2018/01/20/%E6%8F%90%E5%8D%87%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%95%88%E6%9E%9C%E7%9A%84%E5%AE%9E%E6%88%98%E8%A7%A3%E6%9E%90/</url>
      <content type="html"><![CDATA[<p><img src="/2018/01/20/提升有监督学习效果的实战解析/1.png" alt=""><br><a id="more"></a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近很长时间没有和大家分享东西了，最近一直在忙公司的项目，先说一声抱歉。</p>
<p>之前写过<a href="http://shataowei.com/2017/11/01/交叉销售算法/" target="_blank" rel="noopener">销售预估算法</a>,但是被诸多大佬吐槽有监督学习部分毫无深度，其实我是想写给一些刚入门的朋友看的，这边我boss最近也想让我总结一些相对”上档次”的一点的东西，我做了一些稍微深入一点的总结，希望能够给新人朋友有稍微深入的方法介绍。</p>
<hr>
<p>去年年末的那段时间里，看了很多天池大赛里面得高分的选手的算法思路，大概总结了有监督学习中的一些核心流程及重要细节：</p>
<ul>
<li>feature processing tricks</li>
</ul>
<p>这个是老生常谈的问题，但是我还是看到了一些不错的点，比如<strong>根据high importance feature剔除高度缺失的cases</strong>这些等等</p>
<ul>
<li>single feature + crossing feature</li>
</ul>
<p><strong>交叉特征组合原始特征</strong>，可以显著的提升auc，提高命中的准确程度，这边除了FM，我们也可以在常规的算法中去实现这个trick</p>
<ul>
<li>有监督学习架构思路</li>
</ul>
<p>下面，我们来看看针对每个点，具体是如何实现的，及我们需要注意哪些相关的东西：</p>
<h2 id="feature-processing-tricks"><a href="#feature-processing-tricks" class="headerlink" title="feature processing tricks"></a>feature processing tricks</h2><h3 id="case-and-feature-selection"><a href="#case-and-feature-selection" class="headerlink" title="case and feature selection"></a>case and feature selection</h3><p>我们在做模型训练之前通常会对模型的feature做一些删减，比如共线性检验，去除掉相似度过高的连续feature；比如变异度检验，去除掉一些数据变化差异过小的feature等等。然而，在常规的样本处理中，我们通常只会根据初始的数据分布去看，比如用户在feature上缺失大于来某个阈值才回去剔除这个用户；其实，在深入的思考一下这个问题，会发现，如果用户在高重要性的feature缺失程度高去剔除才更合理一些，这样想可能不是很清晰，这边看下面这个feature flow：</p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/2.png" alt=""></p>
<p>针对uid来看，如果普通的统计的话，uid3的null的个数5个，uid5的null的个数4个，我们应该优先剔除uid3，再考虑剔除uid5，因为null过多的用户所能提供的信息量会相对的少，会增大泛化误差。</p>
<p>但如果我们提前知道，对于判断label的能力，feature3&gt;feature5&gt;feature6&gt;feature8&gt;其他，那么uid5在高重要性的缺失情况极度严重于uid3，所以我们应该优先剔除uid5，相对于上面一种情况，我们预先知道feature的重要性排序就显得很重要了。关于如何判断提供了几种简单的方法：</p>
<ul>
<li>方差膨胀系数：我们认为，在数据归一化之后，数据波动的更大的feature能够提供的信息量相对而言也是更大的，举个很明显的例子，如果feature1全都是1的话，它对我们判断用户是否下单这样结果毫无意义。</li>
<li>互信息：我一直认为，互信息是判断feature重要性的非常好的方法。方差膨胀系数只单纯了考虑feature本身的特征，而互信息在考虑feature的同时也考虑了label之间的关系，H(X,Y) = H(X) - H(X/Y)，这个信息量的公式很好的解释了这一点。</li>
<li>xgb’s importance：如果互信息是方差膨胀系数的进阶，那么xgb’s importance则是互信息的进阶，在考虑label与feature之间的关系的时候，同时还考虑了feature与feature之间的关系，这样得出来的重要性排序更加全面了一些。</li>
</ul>
<p>除此之外：</p>
<ul>
<li>Logistic regression的params的参数</li>
<li>Recursive feature elimination（递归参数选择方法）</li>
<li>Pearson Correlation</li>
<li>Distance correlation</li>
<li>Mean decrease impurity/Mean decrease accuracy</li>
<li>…</li>
</ul>
<p>诸如这样的方法很多，需要根据数据的形式，目标变量的形式，时间成本，效率等等综合考虑，这边只是给大家梳理一下常规的方法，至于实际使用的情况，需要大家累积项目经验。</p>
<h3 id="null-feature-treatment-method"><a href="#null-feature-treatment-method" class="headerlink" title="null-feature treatment method"></a>null-feature treatment method</h3><p>在空值或者异常值的处理上，基本上分为2个派别，要么剔除这个feature或者case，要么填充这个feature或者case，它们的缺点也显而易见，随意剔除会减少判断的信息，如果数据较少的时候，会降低模型的效果；填充的话会造成困惑，到底是众数？平均数？中位数？最大值？最小值？现在很多人的处理方法都是观察数据的分布，如果偏态分布就考虑分位数填充，如果是正态分布就考虑均值或者众数填充，相对而言，这样处理的时间成本会更高，而且很多时候解释的说服力不是很强。</p>
<p>我在看了17年3月份JD的订单预估赛，17年的天池工业赛等等的高分答案中，不得不说，有一个分箱的方法确实能够提高0.5-1.5的auc，我之前思考过，可能存在的原因：</p>
<ul>
<li>保存了原始的信息，没有以填充或者删除的方式改变真实的数据分布</li>
<li>让feature存在的形式更加合理，比如age这个字段，其实我们在乎的不是27或者28这样的差别，而是90后，80后这样的差别，如果不采取分箱的形式，一定程度上夸大了27与26之前的差异</li>
<li>在数据计算中，不仅仅加快了计算的速度而且消除了实际数据记录中的随机偏差，平滑了存储过程中可能出现的噪音</li>
</ul>
<p>这边就直接给大家分享一下我的梳理：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/3.png" alt=""><br>这边涉及到一个问题，连续数值特征是否一定要切为离散特征，建议综合考虑以下几个问题:a.所使用的算法是否为knn、svm这样的距离计算的算法b.是否在实际业务中依赖于离散判断c.连续数值特征的实际意义是否支持离散化。如果以上问题都没有问题的话，我建议优先考虑离散化连续特征，在一定程度上，离散完的feature有更好的解释意义。</p>
<h2 id="single-feature-crossing-feature"><a href="#single-feature-crossing-feature" class="headerlink" title="single feature + crossing feature"></a>single feature + crossing feature</h2><p>我们在之前的<a href="http://shataowei.com/2017/12/04/FM理论解析及应用/" target="_blank" rel="noopener">FM理论解析及应用</a>中提到过特征交叉这个概念，当时的文章中紧接着通过矩阵的计算技巧：</p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/4.png" alt=""></p>
<p>构造了全部feature的C(n,2)的形式，后面追加了线性模型，这样一定程度上可以提高分类算法的准确度。这是一个非常好的将低维特征向高维转化的方式，所以在我们其他算法的过程中也可以借鉴这种思路，但是假设我们初始的feature量特别多，比如我在日常的CTR预估或者feature梳理的过程中，很容易就整理500以上的feature集合，如果仅考虑C(n,2)的形式的话，就有250<em>499个feature的新增组合，这个是不可能接受的，所以回到我们上面一节feature processing tricks中提到的case and feature selection就是一个非常好的解决办法，我们可以先通过比如xgboost中的importance：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/5.png" alt=""><br>我这边实际的画出了我做下单概率预测时初始筛选完成后的417个feature经过xgboost初步分类后的importance，可以很明显看前37，前53，前94个feature对应了三次importance的拐点，我们可以在这些拐点中选择一个既能够涵盖绝大多数的信息量，又不会造成后续交叉特征个数过多的值，比如我这边选择的是60，那么我接下来会生成的新的的交叉feature就是30`</em><code>59个，比不做处理下的417</code>*`208要小很多倍，而且相对而言不会减少很多的信息量。</p>
<p>整体的流程我这边也画出来了，希望能够给大家一个比较清晰的认识：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/6.png" alt=""></p>
<p>可以看到，样本cases在经过了最初的空值筛选及第一轮高重要性feature后的空值筛选后，就保持不变了，而特征feature的筛选过程则贯穿了整个交叉特征生成流。</p>
<h2 id="bagging及stacking的思路架构"><a href="#bagging及stacking的思路架构" class="headerlink" title="bagging及stacking的思路架构"></a>bagging及stacking的思路架构</h2><p>我相信在读的各位，不论是机器学习从业者抑或是算法工程师甚至是其他研发工程师，一定看过类似如下的快速拖动的模块流：</p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/7.jpg" alt="机器学习工具Clementine截图"></p>
<p>它相当于把每个功能封装到一个固定的盒子中，当我们需要使用某个模块的时候，进行模块的操作，不需要的时候直接切断模块的流向即可，我们甚至可以空值每个模块的var及bias的偏向程度，在bagging和stacking的思路框架中，我非常常用的就是类似这样的思想：确定好我要进行的组合模块的组合方式（stacking还是bagging还是blending），再确定这次为想要做的子模块是什么，在根据组合形式及子模块细微调节每个子模块。</p>
<h3 id="首先，子模块可以有哪些？"><a href="#首先，子模块可以有哪些？" class="headerlink" title="首先，子模块可以有哪些？"></a><strong>首先，子模块可以有哪些？</strong></h3><ul>
<li>svm分类/回归</li>
<li>logistic分类/回归</li>
<li>神经网络分类/回归</li>
<li>xgboost分类/回归</li>
<li>gbdt分类/回归</li>
<li>xgboost叶子节点index</li>
<li>gbdt叶子节点index</li>
<li>randomforest分类/回归</li>
<li>elastic net</li>
</ul>
<p>除了这些，还有么？当然，如果你愿意的话，每一个你自己构造出来的分类或者回归的single model都可以成为你bagging或者stacking或者blending之前的子模块。</p>
<h3 id="如何训练子模块？"><a href="#如何训练子模块？" class="headerlink" title="如何训练子模块？"></a><strong>如何训练子模块？</strong></h3><p>这边的方法可谓是多种多样，百花齐放，很大程度上来讲，你在天池也好，kaggle也好，你能前十还是前十开外决定因素是你的feature处理的好坏，但是你能拿第一还是第十很大程度上就是依赖你的子模块构造及子模块组合上。这边给大家分享我最近看到的比较有意思的三个子模块形式：</p>
<p><strong>1.<a href="https://github.com/wepe" target="_blank" rel="noopener">wepon</a>的Large-Scale SVM</strong></p>
<p>读过我之前写的<a href="http://shataowei.com/2017/12/01/SVM理论解析及python实现/" target="_blank" rel="noopener">SVM理论解析及python实现</a>这篇文章的朋友应该还记得，我当时说过svm在10.7%的数据集中取得第一，算是传统的机器学习方法中非常值得一学的算法，但是实际应用中，在处理大规模数据问题时存在训练时间过长和内存空间需求过大的问题比较让人头疼，wepon同学采取的方法如下：<img src="/2018/01/20/提升有监督学习效果的实战解析/8.png" alt=""></p>
<p>这种方法看似增加了计算复杂度，实际上是却是减小的，假设原始训练数据大小是n，则在原始数据上训练的复杂度是o(n^2)，将数据集n分成p份，则每份数据量是(n/p)，每一份训练一个子svm，复杂度是o((n/p)^2)，全加起来o(n^2/p)，复杂度比在原始数据上训练减小了p倍。变向的解决了在量大的数据集合上使用svm，提高速度同时保证质量这个问题。论文支持建议参考<a href="http://jmlr.org/papers/volume15/claesen14a/claesen14a.pdf" target="_blank" rel="noopener">Ensemble SVM</a>。</p>
<p><strong>2.<a href="https://mp.weixin.qq.com/s/lUP2BehOh7KczR3WRnOqFw" target="_blank" rel="noopener">爱奇艺 Gbdts’ Node Leafs</a></strong></p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/9.png" alt=""></p>
<p>我们分别来解释一下左右的Dense features 和 Spare Features。</p>
<p>首先，左侧这块很好理解，在<a href="http://shataowei.com/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/" target="_blank" rel="noopener">上一次的文章中</a>,我们已经讲了如何利用xgboost或者gbdt获得用户的数据落在的每棵树上面的叶子节点的index值：<img src="/2018/01/20/提升有监督学习效果的实战解析/10.jpeg" alt=""><br>如果有不清楚的同学，请回顾一下上次讲的内容。</p>
<p>右侧这块分别写了user preference 和video content，当然这是因为它是视频公司的原因，在我实际的使用中，我用的是user preference 和 item content，这里的preference和content其实就是你个人信息及行为的向量化的形式。</p>
<p>最简单的表示就是把你的基本信息和item信息先onehotencoding，再首尾相接成一个超长的vector，这就是一个稀疏的Spare Features。</p>
<p>当然除了这种粗暴的办法，还有比如我们在若干天之前讲过的<a href="http://shataowei.com/2017/08/19/深度学习下的电商商品推荐/" target="_blank" rel="noopener">深度学习下的电商商品推荐</a>中的word2vec的技巧，先将所有的用户随机生成为我们需要的长度N维下一一对应的向量，在通过huffman编码的形式找到每个item对应的Huffman树子的唯一路径，再通过在每个节点上生成一个logsitic分类的办法，使得所有该路径成立的概率最高，以此来修正我们最初随便生成的N维向量，最后这个N维向量就可以看作是一个Spare Features。</p>
<p>还有么？当然，我私下问了我之前在该公司任职的同学，他们还有一种思路就是划分数据集到M个子集，每个子集上面生成一个xgboost，然后每个子集取xgboost的叶子节点，相当于把左侧的Dense features复制了M份Dense features放在了右边的Spare Features，最后会得到一个M+1个Dense features。实际使用起来的效果完全不比word2vec的结果差。</p>
<p><strong>3.基于GRU的潜藏层</strong></p>
<p>Domonkos Tikk和Alexandros Karatzoglou在《Session-based Recommendations with Recurrent Neural Networks》文章中提到了可以用循环神经网络RNN来预估用户的行为，如下图：</p>
<p><img src="/2018/01/20/提升有监督学习效果的实战解析/11.png" alt=""></p>
<p>我们可以清晰的看到，针对每个用户Session1，他的行为由i1.1变化至i1.4其实是一个有序的过程，我们可以设计一个从i1.1——&gt;i1.2,i1.2——&gt;i1.3,i1.3——&gt;i1.4这样的一个循环流程。同时在他的文章中还解释了这样的设计解决的两个问题：</p>
<ul>
<li>the length of sessions can be very different</li>
<li>breaking down into fragments</li>
</ul>
<p>一来通过了首尾相接，解决不同用户的session不同长度；二来通过了embedding layer，解决了不完整session下预测的可能。具体网络设计如下：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/12.png" alt=""></p>
<p>模型的更新流可以参考下面：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/13.png" alt=""></p>
<p>我们只需要拿到每个用户的item流下所对应的state即可，这state就包含了这个用前M次的操作潜藏信息，同时我们还可以随意定义这个信息向量的长度，这个就可以看作用户状态向量，作为子模块的输出。</p>
<p>这个思路的缺点就是，要预测的基础数据不存在时序性，效果极差。比如滴滴打车的下单过程，从登陆到打到车的时间最短在20s，最长在1分钟，否则用户就退出了app，这样的情况下，时序性质就显得格外薄弱，强行用这样的RNN获得的用户属性非常不存在代表性。</p>
<h3 id="如何组合子模块？"><a href="#如何组合子模块？" class="headerlink" title="如何组合子模块？"></a>如何组合子模块？</h3><h4 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h4><p><img src="/2018/01/20/提升有监督学习效果的实战解析/14.png" alt=""></p>
<p>这个是我们<a href="http://shataowei.com/2017/12/30/Kaggle-TianChi分类问题相关纯算法理论剖析/" target="_blank" rel="noopener">Kaggle&amp;TianChi分类问题相关纯算法理论剖析</a>就强调过的bagging的最简单的形式，在每个子模块的设计选择过程中要尽可能的保证：</p>
<ul>
<li>low biase </li>
<li>high var</li>
</ul>
<p>也就是说子模块可以适当的过拟合，增加子模型拟合准确程度，通过加权平均的时候可以降低泛化误差</p>
<h4 id="stacking"><a href="#stacking" class="headerlink" title="stacking"></a>stacking</h4><p><img src="/2018/01/20/提升有监督学习效果的实战解析/15.png" alt=""></p>
<p>这个是我们<a href="http://shataowei.com/2017/12/30/Kaggle-TianChi分类问题相关纯算法理论剖析/" target="_blank" rel="noopener">Kaggle&amp;TianChi分类问题相关纯算法理论剖析</a>就强调过的stacking的最简单的形式，在每个子模块1、子模块2的设计选择过程中要尽可能的保证：</p>
<ul>
<li>high biase </li>
<li>low var</li>
</ul>
<p>在子模块3的时候，要保证：</p>
<ul>
<li>low biase </li>
<li>high var</li>
</ul>
<p>也就是说，在子模块1，2的选择中，我们需要保证可稍欠拟合，在子模块3的拟合上再保证拟合的准确度及强度</p>
<h4 id="blending"><a href="#blending" class="headerlink" title="blending"></a>blending</h4><p>我们知道单个组合子模块的结果不够理想，如果想得到更好的结果，需要把很多单个子模块的结果融合在一起：<br><img src="/2018/01/20/提升有监督学习效果的实战解析/16.png" alt=""><br>这种方法也可以提高我们最后的预测的效果。</p>
<p>关于有监督学习的方法大概就梳理到这边，最后希望能够给一些新人同学对有监督的理解和实战有一些帮助。</p>
<p>没啥广告要打，就这样吧。</p>
<hr>
<p><em>Reference:</em><br><em>[1] 周志华。《机器学习》，清华大学出版社，3.7，2016</em><br><em>[2] wepon。 《<a href="https://github.com/wepe/PPD_RiskControlCompetition" target="_blank" rel="noopener">PPD_RiskControlCompetition</a>》</em><br><em>[3] 爱奇艺技术产品团队。 《<a href="https://mp.weixin.qq.com/s/lUP2BehOh7KczR3WRnOqFw" target="_blank" rel="noopener">爱奇艺个性化推荐排序实践</a>》</em><br><em>[4] slade。 《<a href="http://shataowei.com/2017/12/30/Kaggle-TianChi分类问题相关纯算法理论剖析/" target="_blank" rel="noopener">Kaggle&amp;TianChi分类问题相关纯算法理论剖析</a>》</em><br><em>[5] E Cernadas，D Amorim。 《<a href="http://xueshu.baidu.com/s?wd=paperuri%3A%28491eb32b0997a8dde16c12fe69bf3eac%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2697065%26amp%3Bdl%3Dacm%26amp%3Bcoll%3Ddl%26amp%3Bpreflayout%3Dflat&amp;ie=utf-8&amp;sc_us=5730949819175219868" target="_blank" rel="noopener">Do we need hundreds of classifiers to solve real world classification problems?</a>》</em><br><em>[6] slade。 《<a href="http://shataowei.com/2017/08/19/深度学习下的电商商品推荐/" target="_blank" rel="noopener">深度学习下的电商商品推荐</a>》</em><br><em>[7] Domonkos Tikk，Alexandros Karatzoglo。 《Session-based Recommendations with Recurrent Neural Networks》</em><br><em>[8] slade. 《<a href="http://shataowei.com/2017/12/04/FM理论解析及应用/" target="_blank" rel="noopener">FM理论解析及应用</a>》</em></p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 模型设计 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kaggle&TianChi分类问题相关纯算法理论剖析]]></title>
      <url>/2017/12/28/Kaggle&amp;TianChi%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9B%B8%E5%85%B3%E7%BA%AF%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E5%89%96%E6%9E%90/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/1.png" alt=""></p>
<a id="more"></a>
<hr>
<p>17/12/30-update ：很多朋友私密我想要代码，甚至利用金钱诱惑我，好吧，我沦陷了。因为原始代码涉及到公司的特征工程及一些利益trick，所以我构造了一个数据集后复现了部分算法流程，需要看详细代码实现朋友可以移步<a href="https://github.com/sladesha/machine_learning/tree/master/Ensemble" target="_blank" rel="noopener">Ensemble_Github</a></p>
<hr>
<p>更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过stw386@sina.com联系我，知无不答。</p>
<hr>
<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><p>在<a href="http://shataowei.com/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/" target="_blank" rel="noopener">上一次的文章</a>中，我们讲了，如何快速的利用bagging、boosting、stacking、ensemble的形式实现一个分类算法，当时我们直接看了代码以及核心的理论注意点。如果需要有更加优异的结果表现，对整套算法的设计及相关的理论了解是必不可少的。本文将从数学、工程、领域经验的角度去剖析如何用好bagging、boosting、stacking、ensemble去训练一个相对完善的模型。</p>
<p>再次提醒，本文中的数据公式较多，抽象概念较多，需要一定的高等代数、泛函分析、机器学习基础作为前置条件。如果你只需要知道如何运行或者完成分类识别，请参考<a href="http://shataowei.com/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/" target="_blank" rel="noopener">Kaggle-TianChi分类问题相关算法快速实现</a>。如果需要更详尽的理论解析或者有哪些地方不明白的同学，建议私下联系我stw386@sina.com。如果你想skip read本文，请直接阅读最后一个小节：调参流程梳理。</p>
<p>那么接下来让我们开始正文，虽然本文写的很冗长，我依旧建议阅读完此文，即便是处于懵懂的状态，对后续模型调整的理解也是有一定益处的，而且我会尽可能的用通俗易懂的语言来讲，很多地方会存在解释不严谨的地方但是更易于理解。</p>
<h1 id="Bias-Variance-Tradeof"><a href="#Bias-Variance-Tradeof" class="headerlink" title="Bias-Variance-Tradeof"></a>Bias-Variance-Tradeof</h1><p>在上次的文章中，我们就提到了一个好的模型应该有着非常好的拟合能力，就是说我的偏差要尽可能的小；同时，也要保证方差尽可能的小，这样我们才能在泛化能力上有很不错的表现。</p>
<p>设样本容量为n的训练集为随机变量的集合(X1, X2, …, Xn)，那么模型是以这些随机变量为输入的随机变量函数（这边的F虽然上函数，但是也是随机变化的）：F(X1, X2, …, Xn)。抽样的随机性带来了模型的随机性。那如何定义一个模型的Bias和Variance呢？这边我们采取的是基模型的加权均值E(F)=E(∑(γ<code>*</code>fi))移动来替代bias、基模型的加权方差Var(F)=Var(∑(γ<code>*</code>fi))替代Variance，更详细的数学定义如下：<br><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/2.png" alt=""><br>这边在Variance推导过程中用到了这个性质：<br>Cov(X<code>*</code>Y) = E(X<code>*</code>Y) - E(X)<code>*</code>E(Y)，同时将方差拆分成协方差的形式。我们可以看到，组合后的模型的Bias和基模型的Bias的权重γ相关，组合后的模型的Var和基模型的权重γ、基模型个数m、基模型相关性ρ相关。<br><strong>请务必深刻的记得上述bias和var的数学式子，在后续无论是调参数还是模型设计，我们都是围绕着，降低bias和var的角度去做的。</strong></p>
<h2 id="Bagging-Bias-and-Variance"><a href="#Bagging-Bias-and-Variance" class="headerlink" title="Bagging Bias and Variance"></a>Bagging Bias and Variance</h2><p><strong>很多同学在没看这篇文章之前就知道，bagging算法和stacking算法是需要基模型保持强基模型（偏差低方差高）的</strong>，但是不知道大家有没有想过为什么？阿里15年校招的时候，我有幸回答过一个题目就是这个问题，下面让我们看看为什么是这样的？</p>
<p>对于bagging算法而言，每次的抽样都是以尽可能使得基模型相互独立为前提的，为了维持这样的假设，我们做了三件事：</p>
<ul>
<li>样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样</li>
<li>子抽样：整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量</li>
<li>弱抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）下的feature的抽样</li>
</ul>
<p>同时，由此我们由此也可以得到bias和var公式中的γ=1/m，基模型的权重一定程度是可以看作是相等的，<br>所以原来的E和Var公式就变成：<br><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/3.png" alt=""><br>组合模型F的bias和基模型fi的bias一致，这就是我们为什么要求基模型fi的bias要低的原因，因为组合模型F的拟合能力E(F)不随着基模型个数的增加而上升。</p>
<p>组合模型F的var与基模型fi的var、基模型fi的个数m、基模型的相关性ρ相关，很明显可以看出，随着基模型的个数上升Var(F)第二项是在下降的，所以基模型的个数上升会降低组合模型的方差，这就是为什么基模型的方差可以高一些。</p>
<p>除此之外，我们还可以看出，如果基模型相关性ρ越低，整体的方差是越小的，所以我们才去做样本抽样，子抽样，弱抽样等等行为。还有，基模型的个数上升一定程度上会降低组合模型F的方差，但是不是无限递减的，公式中它只能降低第二项的方差值，第一项的方差值不随基模型的个数而增减。</p>
<p>从这个角度看，是不是对Bagging算法的理解又深刻了一些？接下来让我们看看Boosting。</p>
<h2 id="Boosting-Bias-and-Variance"><a href="#Boosting-Bias-and-Variance" class="headerlink" title="Boosting Bias and Variance"></a>Boosting Bias and Variance</h2><p>依旧的是<strong>很多同学在没看这篇文章之前就知道，boosting算法是需要基模型保持弱基模型（偏差高方差低）的</strong>，让我们一探究竟。<br><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/4.png" alt=""><br>熟悉boosting算法的同学都知道，boosting算法的基模型的相关性几乎≈1的，后续模型强依赖于前模型，所以我们可以认为ρ=1，得到如上的简化式子。组合模型F的bias是基模型的bias的累加，基模型的准确度都不是很高（因为其在训练集上的准确度不高），随着基模型数的增多，整体模型的期望值增加，更接近真实值。而站在方差的角度，组合模型F的方差是随着基模型的fi个数上升而平方上升的，这就要求我们的基模型的方差不能太高，否则组合模型的F就会增长爆炸。这就是为什么我们在boosting模型设计的时候，需要基模型保持弱模型（偏差高方差低）的原因。</p>
<p>多说一句，大家也看到了，boosting的Var(F)是依赖于基模型的权重γ的，所以在后续的gbdt、xgboost，各位数据科学家选择了类似bagging的采样模式，降低模型的γ，控制方差，所以说，了解原理再去重新或者优化还是很重要的。</p>
<h2 id="Bagging、Boosting、Stacking-Bias-amp-Variance总结"><a href="#Bagging、Boosting、Stacking-Bias-amp-Variance总结" class="headerlink" title="Bagging、Boosting、Stacking Bias&amp;Variance总结"></a>Bagging、Boosting、Stacking Bias&amp;Variance总结</h2><p>纵观刚才说了的这么多，我们在Bagging、Boosting、Stacking的模型设计中所围绕的就是降低bias的同时，降低Variance。而我们所做的sklearn里面的参数调整就可以说用来：</p>
<ul>
<li>a.构建基模型，变化基模型的Bias和Variance </li>
<li>b.组合模型构建，控制基模型后，如何把这个基模型很好的组合成一个优秀的ensemble模型。</li>
</ul>
<h1 id="GBDT-理论剖析"><a href="#GBDT-理论剖析" class="headerlink" title="GBDT 理论剖析"></a>GBDT 理论剖析</h1><h2 id="模型过程推导"><a href="#模型过程推导" class="headerlink" title="模型过程推导"></a>模型过程推导</h2><p>其实random forest和gbdt、xgboost都是非常好的Bagging、Boosting、Stacking算法的优化升级版本，我个人用的gbdt稍多，所以就以gbdt为例子给大家梳理一遍，从理论，到调参数，到trick分享。</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/5.png" alt=""></p>
<p>我最讨厌很多博主贴个上面的伪代码就跑了，我念书的时候，老师讲题目也是，我靠，我要是看得懂还需要你贴？所以，我们选择一步一步的来看这个伪代码。</p>
<p>让我们先概览一下整个流程，gbdt有递归设计如下：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/6.jpeg" alt=""></p>
<p>如果y(i)代表着第i个基模型，第i+1个基模型其实是基于第i个基模型的结果而追加了一个New function去修正前i个基模型的误差。如果以F代替y，h代替f的话，我们可以得到下面这个递归函数：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/7.png" alt=""></p>
<p>第i个基模型是由前i-1个基模型中的h(x)累计得到的，我们最后想要得到的分类器即为：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/8.png" alt=""></p>
<p>每一轮迭代中，只要集中精力使得Fi(x)每次loss下降的最快即可：每次构建一个残差负梯度作为更新方向的New function(即hi(x))，就可以解决这个复杂的递归问题，从而得到F(x)的解析式。</p>
<p>接下来，我们再看看更加详细的做法：</p>
<ul>
<li>初始化部分，在这次梳理之前，我也一直认为是随机构造的，这边看完伪代码我才知道，在初始值设置的时候，考虑了直接使得损失函数极小化的常数值，它是只有一个根节点的树，即是一个c常数值。</li>
</ul>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/9.png" alt=""></p>
<ul>
<li>构建回归树，这边就稍许复杂，让我们拆开一步一步来看：</li>
</ul>
<p><strong>首先，先求整体损失函数的反向梯度。</strong>先举个mse的例子，如果现在我们考虑的是<strong>mse(着重注意，只有mse的情况下以下的梯度才是这样的)</strong>的形式，我们要做的就是在每一步的时候让我们的预测值Fi(x)与真实值y的损失函数：1/2<code>*</code>(y-Fi(x))^2最小(前面的1/2是为了方便求导计算加上去的)，如果对梯度下降有了解的朋友就知道，此刻要求它的最小就是去求偏导：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/10.png" alt=""></p>
<p>按照这个方向去更新Fi(x)，可以保证，组合模型F(x)的每次都是按照最优的方向去优化。</p>
<p>MSE的损失函数确实是残差形式，不代表所有的损失函数下更新的方向都满足这样的残差形式。<strong>kaggle master 在<a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/" target="_blank" rel="noopener">blog</a>里面提到Although we can minimize this function directly, gradient descent will let us minimize more complicated loss functions that we can’t minimize directly</strong>，我们就需要设计出一种更快速能提升泛化能力且不失一般性的解决方案，所以有大神提出了以梯度下降的值直接代替因变量y，也就是我每次预测不去比预测值与真实值y的差异，我们比较的是预测的梯度方向与真实的梯度方向。</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/11.jpeg" alt=""></p>
<p><strong>再基于已经预估出来的负梯度去计算最优的更新步长，使得预测值与真实值更加靠近</strong>，因为每层的负梯度的方向不固定，所以每层i的步长都是变化的。</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/12.png" alt=""></p>
<p>最后通过缩减率v（这边就是类似logistics里面的∂）控制速率。</p>
<p>综上，假设test集合第i轮预测中，根据训练集训练出来的负梯度拟合模型不妨记为fi(x)、最优步长γi、缩减率v，可得到最终的递归公式为：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/13.png" alt=""></p>
<h2 id="损失函数介绍"><a href="#损失函数介绍" class="headerlink" title="损失函数介绍"></a>损失函数介绍</h2><p>刚才上面我举了一个mse作为损失函数的例子，其实还有很多其他的，参考如下：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/14.png" alt=""></p>
<p>这边说个有意思的东西，就是如果有兴趣的朋友可以把exponential：指数损失函数计算一下，反向梯度为：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/15.png" alt=""></p>
<p>则有第i轮损失函数：</p>
<p><img src="/2017/12/28/Kaggle&TianChi分类问题相关纯算法理论剖析/16.png" alt=""></p>
<p>这货就是adaboost的第i轮损失函数的非归一化的结果，是不是很有趣，虽然知道了没啥用，但是起码得到了我们在用gbdt的时候，loss=’exponential’即为adaboost的结论啊，哈哈～所以说，我觉得去推推公式，还是很有意思的。</p>
<p>到此为止，gbdt是怎么构造得来的就讲完了，其实这个和bias&amp;variance关系不大，但是为了铺垫后续的GBDT实战剖析，我觉得还是非常有必要梳理一遍的，但就比起Bias-Variance-Tradeof这节的内容，我觉得各位还是着重理解Bias-Variance-Tradeof，这节可以看作是’甜品’缓解下气氛。</p>
<h1 id="GBDT-实战剖析"><a href="#GBDT-实战剖析" class="headerlink" title="GBDT 实战剖析"></a>GBDT 实战剖析</h1><p>我们以python下的sklearn.ensemble中的GradientBoosting及RandomForest为例子，实战分析一下，如何能够理性的调好参数而并非玄学的gridsearch。</p>
<p>在Bias-Variance-Tradeof我们提到了参数设置分为两块：a.构建基模型,b.构建组合模型，我们分GradientBoosting参数如下：</p>
<h2 id="构建组合模型："><a href="#构建组合模型：" class="headerlink" title="构建组合模型："></a>构建组合模型：</h2><p>1) n_estimators:<br>基模型的个数，对于gbdt来说，因为我们需要通过基模型的个数来提升准确率所以n_estimators一般都会大于random forest的n_estimators的个数，实际上RandomForestClassifier默认为10，GradientBoostingClassifier默认为100也证明了这点。</p>
<p>2）learning_rate：<br>步长，对于gbdt来说，步长依赖于n_estimators，100=2<code>*</code>50=5<code>*</code>20，就是这个道理。而random forest里面不存在步长这个概念。在gbdt里面，关于步长的优化一般是伴随着基模型的变化而变化的。</p>
<p>3）subsample：<br>子采样率，还记得我们上面对子采样的描述么？一般来说，降低“子采样率”（subsample），也会造成子模型间的关联度降低，整体模型的方差减小，但是当子采样率低到一定程度时，子模型的偏差增大，将引起整体模型的准确度降低。</p>
<p>4) init：初始化，更多见GBDT 理论剖析中，我们对初始化的描述。</p>
<p>5）loss：<br>对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。<br>分类模型不说了，刚才在GBDT 理论剖析中讲了，一般用”deviance”比较多；回归模型中，”ls”我们也在GBDT 理论剖析中讲了，异常点多的情况下”huber”,训练集分段预测的话用”quantile”，但是我个人建议异常点或者分段预测还是在数据已处理中完成。</p>
<p>6) alpha：这个参数只有Huber损失”huber”和分位数损失”quantile”下的GradientBoostingRegressor，alpha越小对噪声处理的力度越强，alpha越小分位数的值越小。</p>
<h2 id="构建基模型："><a href="#构建基模型：" class="headerlink" title="构建基模型："></a>构建基模型：</h2><p>1）max_features：<br>每次划分最大特征数，有log2，sqrt，None等等，默认的是sqrt，该值越小，我们每次能获得信息越少，造成偏差时变大的，同时方差是变小的，所以当我们模型拟合能力不足的时候，可以考虑提升该值。</p>
<p>2）max_depth:<br>基模型最大深度，深度越大，模型的拟合能力越强，bias越小。根据Bias-Variance-Tradeof我们对bagging和boosting里面的Var和Bias的描述可知，如果在boost（gbdt）采用了过深的基模型，组合模型的var会很大，在泛化能力会降低，造成训练集效果优秀，测试集差；如果在bagging（random forest）采取了过浅的基模型，组合模型的拟合能力会不足，我们可以考虑增加深度，甚至不控制生长。</p>
<p>3）min_samples_split：<br>内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。随着分裂所需的最小样本数的增加，子模型的结构变得越来越简单，极端情况下，方差减小导致整体模型的拟合能力不足。</p>
<p>4）min_weight_fraction_leaf：<br>叶节点最小权重总值，这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和其他子叶节点一起被剪枝，会使得模型变得简单，降低了方差，提高了偏差，如果正负样本不一致，需要考虑调整这个值。</p>
<p>5）max_leaf_nodes：<br>最大叶子节点数，通过限制最大叶子节点数，可以防止过拟合，会使得模型变得简单，降低了方差，提高了偏差。<strong>这边需要注意如果设置了max_leaf_nodes，会忽略max_depth参数值</strong>。</p>
<p>梳理完以上每个参数的对模型拟合的能力及对Vaild集合泛化能力的影响，我们可以根据项目训练中，实际模型的训练集拟合效果，检验集的泛化效果进行优化参数。</p>
<h2 id="调参流程梳理"><a href="#调参流程梳理" class="headerlink" title="调参流程梳理"></a>调参流程梳理</h2><p>ok，问题来了，很多同学看了之后说，你说了这么多参数作用，你还是没告诉我改如何调参数？我就喜欢这种很关注结果的同学，以下干货来自个人及个人朋友及我从知乎等网站”剽窃”来的观点，不负任何理论责任。</p>
<p>我第一任老大，现在在阿里做算法专家，他根据24个数据集合上以不同的调参流程去训练相同的测试集得出的效果对比，总结出以下一个流程：</p>
<ul>
<li>先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值</li>
<li>再确定合适的subsample</li>
<li>再调优最大树深度（max_depth）</li>
<li><strong>再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）</strong>,这边是不一定使用的</li>
<li>再调优最大叶节点数（max_leaf_nodes）</li>
<li>再组合优化分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf） </li>
<li>最后，优化分裂时考虑的最大特征数（max_features）</li>
<li>组合调整n_estimators和learning_rate</li>
</ul>
<p>但是，我今年在逛知乎的时候偶然看到一个帖子，里面讲的就是调参数的困扰，提到了一个点，就是先确定了max_depth=3后，无论怎么优化min_samples_split和min_samples_leaf对结果都没有任何影响了。当时我想了很久，最后是一位知友解答了这疑惑，其实这样的：</p>
<p>假设原始数据中正负样本比是1:1000，在做max_depth=3的时候，因为样本不均衡，已经可以通过非常简单的少量feature对正负样本进行区分，所以，在之后怎么调节分裂所需要最小样本树和子节点最小样本数都不能够影响到回归树的构造，然而该区分的回归树是没有泛化能力的。</p>
<p>要解决这个问题要么平衡数据，要么就是先确定回归决策树每个叶子结点最小的样本数(min_samples_leaf),再确定分裂所需最小样本数（min_samples_split），才能确定最大深度,这样就能保证不会出现某棵树通过一个feature将数量较少的的正类以较过拟合的简单浅层树拟合出来，而是优先保证了每一次我构造树都尽可能的平衡满足了数据量合理，数据具有样本具有代表性，不会过拟合这样的假设。所以，可以优化为：</p>
<ul>
<li>先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值</li>
<li>再确定合适的subsample</li>
<li>再组合调优最大树深度（max_depth）和叶节点最小样本数（min_samples_leaf） </li>
<li>再调优最大叶节点数（max_leaf_nodes）</li>
<li><strong>再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）</strong>,这边是不一定使用的</li>
<li>再组合优化分裂所需最小样本数（min_samples_split）</li>
<li>最后，优化分裂时考虑的最大特征数（max_features）</li>
<li>组合调整n_estimators和learning_rate</li>
</ul>
<p>去年Aarshay Jain大神总结的<a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/" target="_blank" rel="noopener">调参数整理</a>也给出了一种调优思路：</p>
<ul>
<li>优先，调整最大叶节点数和最大树深度</li>
<li>其次，分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf）及叶节点最小权重总值（min_weight_fraction_leaf）</li>
<li>然后，分裂时考虑的最大特征数（max_features）</li>
</ul>
<p>容我多嘴一句，我们思考了这么多，其实如果能在最开始做一个正负样本平衡就会避免很多问题，所以，再次强调数据预处理的重要性。</p>
<p>除此在外，很多人会选择在以上模型调优结束后再以10<code>*</code>learning_rate进行”鞍点逃逸”，以0.1<code>*</code>learning_rate进行”极限探索”。至于random forest及xgboost的更多调参数的细节与gbdt类似，我就不赘述了，有问题可以问我。</p>
<p>终于结束了，这篇文章真的是又繁琐又冗长，希望能够给一些同学对gbdt更深刻的理解。</p>
<p>没啥广告要打，就这样吧。</p>
<p><em>另求一个比较好的公式编辑器，鬼知道我现在在excel里面写完公式截图过来有多扯淡，而且图片质量超差，谢谢了。</em></p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 模型设计 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kaggle&TianChi分类问题相关算法快速实现]]></title>
      <url>/2017/12/28/Kaggle-TianChi%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95%E5%BF%AB%E9%80%9F%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/1.png" alt=""><br><a id="more"></a></p>
<hr>
<p>17/12/30-update ：很多朋友私密我想要代码，甚至利用金钱诱惑我，好吧，我沦陷了。因为原始代码涉及到公司的特征工程及一些利益trick，所以我构造了一个数据集后复现了部分算法流程，需要看详细代码实现朋友可以移步<a href="https://github.com/sladesha/machine_learning/tree/master/Ensemble" target="_blank" rel="noopener">Ensemble_Github</a></p>
<hr>
<p>更多代码内容欢迎follow我的个人<a href="https://github.com/sladesha" target="_blank" rel="noopener">Github</a>，如果有任何算法、代码疑问都欢迎通过stw386@sina.com联系我。</p>
<hr>
<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><p>无论是在TianChi，还是在Kaggle上，通常会出现类似0-1分类，多分类这样的问题，比如：</p>
<p><a href="http://www.chioka.in/kaggle-competition-solutions/" target="_blank" rel="noopener">Kaggle Competition Past Solutions</a><br><a href="http://blog.csdn.net/bryan__/article/details/53907292" target="_blank" rel="noopener">O2O优惠券使用预测</a><br><a href="https://space.dingtalk.com/c/gQHOEnXdXw?spm=5176.9876270.0.0.7f003e68VLX8Bb" target="_blank" rel="noopener">移动推荐算法</a></p>
<p>除此之外，在金融、风控、交通领域，也会有比较相近的分类问题：<br><a href="http://blog.csdn.net/bryan__/article/details/51190452" target="_blank" rel="noopener">Kesci“魔镜杯”风控算法大赛</a><br><a href="http://blog.csdn.net/bryan__/article/details/50977513" target="_blank" rel="noopener">DataCastle微额借款用户人品预测</a></p>
<p>如果仔细阅读就会发现，很多很多case，但是总结下来都是一个套路：</p>
<ul>
<li>Bagging</li>
<li>Boosting</li>
<li>Stacking</li>
<li>Ensemble</li>
</ul>
<p>我想写这篇文章的目的就是<strong>让大家在比如竞赛、项目push的情况下，在较短时间内，快速的构造出一个效果中上的算法集合。</strong></p>
<h1 id="理论解析"><a href="#理论解析" class="headerlink" title="理论解析"></a>理论解析</h1><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging通过构造一个预测函数系列，然后以一定的方式将它们组合成一个预测函数。更加形象的理解就是，我们考试，大家都有各种擅长的科目，都有自己弱势的科目，我们每个人都尽力把我们擅长的科目考好，然后最后把各自答案互相借鉴，这样每一科都能有不错的结果。<br>如果以图形理解就是：<br><img src="/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/2.jpeg" alt=""></p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting通过改变训练样本的权重（增加分错样本的权重，减小分对样本的权重），学习多个分类器，并将这些分类器线性组合，提高分类器性能。更加形象的理解就是，我们考试，每次我们数学都考的不好，然后我们在平时学习的时候投入更多精力去学数学，然后再看什么考的不好，再去投入精力学什么。</p>
<h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>Stacking通过用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。更加形象的理解就是，我们考试，先让学霸去考，然后把每个学霸的答案按照他们历史上这科目考的成绩加权打分，最后确定每个学霸的各科目成绩加权的变化，重复再让学霸去考试修正这个各科目成绩加权。<br>注意：其实，Stacking和Boosting的思想是很近的，其中很大的差异在于Stacking一般用的都是完善的模型中间件结果当作入参，生成了新的数据分布，算法设计核心应该考虑的Bias-Vars-Balance；而Boosting用的简单的模型结果当做权重分布优化原始入参，并没有生成新数据，算法核心的目的还是在考虑降低Bias。<br>所以，在多数情况下，Stacking的模型应该更稳定，有更好的泛化性能。<br><img src="/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/3.png" alt=""></p>
<h2 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h2><p>Ensemble通过对新的数据实例进行分类的时候，通过训练好多个分类器，把这些分类器的的分类结果进行某种组合（比如投票）决定分类结果。更加形象的理解就是，先让学霸去考试，尽可能的让每个学霸都考好，然后根据历史考的分数高低投票出每个科目对应的考的最好的那个学霸，决定每科抄对应的那个学霸。<br>注意：对的，正如你们所想，Ensemble的思想就和Bagging的思想近似。Bagging的关键集中在，我不管我每个弱鸡的分类器的效果，只要我数量足够多，一定能得到不错的组合效果，相对而言简单，效果稍弱；而我们在Ensemble的时候，设计更加复杂而且要注意：a.尽可能让每个子分类器能识别出不同分布的数据，如果针对同一份数据做再多的子分类器，也只是只能起到堆砌的作用。b.加权方式的不同：简单平均（Simple Average），加权平均（Weight Average），概率投票（Soft vote）等等。</p>
<p><strong>说了这么多，我们其实有个前提，就是加权或者组合或者堆砌能够提升准确度，不然等于以上都不合理，下面简单证明下：</strong></p>
<p>假设，我们有5个分类器的正例率分别是{0.7,0.7,0.7,0.9,0.9}：<br>a.如果直接判断的话，最好的一个子分类器的准确率也只能达到0.9<br>b.采用简单投票方法（必须有三个以上分类正确），那么根据二项分布：<br>3个分类器正确：0.73<code>*</code>0.1<code>*</code>0.1+3<code>*</code>0.72<code>*</code>2<code>*</code>0.9<code>*</code>0.1+3<code>*</code>0.7<code>*</code>0.32<code>*</code>0.9<code>*</code>0.9<br>4个分类器正确：0.73<code>*</code>0.9<code>*</code>0.1<code>*</code>2+3<code>*</code>0.72<code>*</code>0.3<code>*</code>0.92<br>5个分类器正确：0.73<code>*</code>0.92<br>把这几个相加可得p≈0.933&gt;0.9。</p>
<p>这些都是是非常基础的，也不是本文的重点，如果有任何疑问，请自行百度了解或者邮件我。</p>
<p>Bagging的代表作有randomforest，Boosting的代表作有Adaboost及GBDT，Stacking和Ensemble就需要我们自己设计了。</p>
<h1 id="如何设计一个Stacking-Ensemble的模型？"><a href="#如何设计一个Stacking-Ensemble的模型？" class="headerlink" title="如何设计一个Stacking|Ensemble的模型？"></a>如何设计一个Stacking|Ensemble的模型？</h1><p>先看一个Stacking的经典之作：<a href="http://www.cbdio.com/BigData/2015-08/27/content_3750170.htm" target="_blank" rel="noopener">FaceBook基于gbdt+lr下stacking的CTR预估</a>。不得不说，虽然实测下来对Valid的准确率提升只有2-3pp，但是在泛化性能上稳定程度上升了若干个档次，后面我们细讲。Ensemble就更不用举例子了，随便打开一个互联网公司的算法库，99.9%的都已经有完善的成功案例了。我呆过的滴滴、电信、hp等等，还没有一个公司不用的。</p>
<p>讲理论之前，先看一个概念：<a href="https://en.wikipedia.org/wiki/Bias–variance_tradeoff" target="_blank" rel="noopener">Bias–variance tradeoff</a>。我知道每多一个公式，会少一半的读者，我尽可能的用叙述的方式才阐述。<br>首先，<strong>Error = Bias + Variance</strong>，这个公式请刻在脑子里。<br>如果记不住，上面这个公式有个对应的图：<br><img src="/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/4.png" alt=""></p>
<p>我们来解释<strong>Error = Bias + Variance</strong>，公式中的Error就是我们需要考虑的我们设计模型中的Loss；Bias就是我们预测数据距离真实数据的距离程度，比如你今年25，我预测你23，BIas=2；Variance就是我预测的结果的波动程度，比如，我预测三个人的年龄A、B组分别为（23，22，24）及（10，20，80），很明显B组波动程度要狠狠大于A。我们形象的来看，如果<strong>Error = Bias + Variance</strong>中Bias过大是什么样？<br><img src="/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/5.jpeg" alt=""><br><strong>结果就是红色线条，预测的结果几乎没有任何参考意义，偏离正常值非常远。</strong></p>
<p>如果<strong>Error = Bias + Variance</strong>中Variance过大是什么样？<br><img src="/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/6.jpeg" alt=""><br><strong>结果就是红色线条，预测的结果几乎完全拟合了所有数据，预测的结果波动非常不稳定。</strong></p>
<p>所以，我们在设计Stacking和Ensemble的过程需要避免上述的两个问题，最简单的举个例子：<br>如果，我们在做Stacking模型，Model_1我们用的Adaboost的算法，我们improve了正例的权重，纠正了正负样本比，训练出了叶子结点。接下来，我们是选择做XGBOOST的时候，参数设置如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">estimator=XGBClassifier(</span><br><span class="line">learning_rate=0.1,</span><br><span class="line">n_estimators=500,</span><br><span class="line">max_depth=3,</span><br><span class="line">min_child_weight=7,</span><br><span class="line">objective=&apos;binary:logistic&apos;,</span><br><span class="line">scale_pos_weight=0.707653,</span><br><span class="line">gamma=0.6,</span><br><span class="line">seed=27)</span><br></pre></td></tr></table></figure></p>
<p>参数中scale_pos_weight是否应该更改为1？答案是yes的，这边建议大家自己思考一下。如果再做一次scale_pos_weight  = negative /positive ，是不是相当于我们2次提高了正样本的权重？对正样本的拟合过度是的bias下降，Variance上升，Vaild的泛化能力就会非常的弱，极其不稳定。</p>
<p>除此之外，如果我们用了random forest的作为Model_1，后接一个Xgboost中的subsample还有必要设置为0.4或者0.5么？这边考虑到randomforest已经控制每棵树，随机采样的比例，也控制了每个feature，每个样本的被随机选取的前提，在后面追加模型stacking的过程就需要更注重拟合，此时就算接一个nernual network 都是可以的，所以后面可以但是不建议再追加注重Bagging的算法。</p>
<p><strong>其实，核心在于不论我们如何组合一个stacking或者ensemble模型，需要时时刻刻考虑的是平衡bias和variable。</strong>上面这些描述很抽象，我自己返回的阅读也觉得不是解释的很清晰，但是建议各位自己好好想一下，如何搭建一个stacking和ensemble不需要考虑上面这些，但是要如何搭建<strong>好</strong>一个stacking和ensemble模型，最核心的就是上面这些。</p>
<h1 id="案例复现"><a href="#案例复现" class="headerlink" title="案例复现"></a>案例复现</h1><p>先看结果，我借着公司case，训练了常规的方法其中stacking中基于tensorflow下的deepFM和FNN当时没有记录，就没有留下，其他的都如下：<br><img src="/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/7.png" alt=""><br>整体上，我写了包括sample、ensemble、stacking、deepFM、TF-FNN前后一共花了3天的时间，所以真的可以说是高效快速的方法而且可复制性极高。效果上，基本上比简单的处理完直接random forest，accuracy要高10-15pp，如果愿意深挖，效果应该还可以提升。</p>
<p>这边就着重和大家捋一遍Facebook15年出品的xgboost+sparse+lr这个思路吧，这边只贴了核心的代码段，后面看大家需求再考虑是不是GitHub共享吧，如果想要知道其他的模型或者其他什么想法，可以邮件我～</p>
<ul>
<li>数据预处理</li>
</ul>
<p>修复一些DBA没有处理好的数据，这样需要在做数据处理之前纵览整体数据质量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">make_new_data = []</span><br><span class="line">for i in range(train_data[&apos;crm__crm_user_wechat_info__we_chat&apos;].shape[0]):</span><br><span class="line">    if train_data[&apos;crm__crm_user_wechat_info__we_chat&apos;][i].replace(&apos;\&quot;&apos;, &apos;&apos;).replace(&quot;[&quot;, &apos;&apos;).replace(&apos;]&apos;, &apos;&apos;) == str(</span><br><span class="line">    0):</span><br><span class="line">        make_new_data.append(0)</span><br><span class="line">    else:</span><br><span class="line">        make_new_data.append(1)</span><br><span class="line">train_data[&apos;crm__crm_user_wechat_info__we_chat&apos;] = make_new_data</span><br></pre></td></tr></table></figure></p>
<p>离散化连续特征，这边也可以保留一些连续变量，我这边两种都尝试了，离散化的效果是要优于保留连续变量的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># separate the classification data : define that if the set is under 10,the columns can be treated as classification</span></span><br><span class="line">class_set = []</span><br><span class="line">continue_set = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> arrange_data_col:</span><br><span class="line">    <span class="keyword">if</span> arrange_data_col[key] &gt;= <span class="number">10</span> <span class="keyword">and</span> key != <span class="string">'uid'</span>:</span><br><span class="line">        continue_set.append(key)</span><br><span class="line">        class_set = [x <span class="keyword">for</span> x <span class="keyword">in</span> train_data.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> continue_set <span class="keyword">and</span> x != <span class="string">'uid'</span> <span class="keyword">and</span> x != <span class="string">'label'</span> <span class="keyword">and</span> arrange_data_col[x] &gt; <span class="number">1</span>]</span><br></pre></td></tr></table></figure></p>
<p>删除低方差的feature，我这边用的是我之前写的一个包，理论：<a href="http://shataowei.com/2017/12/01/python开发：特征工程代码模版-二/" target="_blank" rel="noopener">特征工程代码模版</a>，包地址：<a href="https://github.com/sladesha/tool-box" target="_blank" rel="noopener">data_preprocessing</a>，这个是我自己写的，也不难，大家嫌麻烦也可以自己写。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># remove the low variance columns</span><br><span class="line">meaningful_col = [&apos;uid&apos;, &apos;label&apos;]</span><br><span class="line">for i in cbind_classed_data_columns:</span><br><span class="line">    if i != &apos;uid&apos; and i != &apos;label&apos;:</span><br><span class="line">        if arrange_data_col[i] &gt;= 2:</span><br><span class="line">            meaningful_col.append(i)</span><br><span class="line">            meaningful_data = cbind_classed_data[meaningful_col]</span><br></pre></td></tr></table></figure></p>
<p>同理，计算了互信量，删除低贡献的feature，也是上面包<code>data_preprocessing.feature_filter()</code>里面有的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ff = data_preprocessing.feature_filter()</span><br><span class="line">res = ff.mic_entroy(reshaped_data.iloc[:, 1:], &apos;label&apos;)</span><br></pre></td></tr></table></figure></p>
<p>然后，我自己定义了评价函数，根据importance删选了feature，特征由最开始的243个减少到最后的64个。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">metrics_spec</span><span class="params">(actual_data, predict_data, cutoff=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    actual_data = np.array(actual_data)</span><br><span class="line">    predict_data = np.array(predict_data)</span><br><span class="line">    bind_data = np.c_[actual_data, predict_data]</span><br><span class="line">    res1 = <span class="number">1.0</span> * (bind_data[bind_data[:, <span class="number">0</span>] == <span class="number">1</span>][:, <span class="number">1</span>] &gt;= cutoff).sum() / bind_data[bind_data[:, <span class="number">0</span>] == <span class="number">1</span>].shape[<span class="number">0</span>]</span><br><span class="line">    res2 = <span class="number">1.0</span> * (</span><br><span class="line">    (bind_data[bind_data[:, <span class="number">0</span>] == <span class="number">1</span>][:, <span class="number">1</span>] &gt;= cutoff).sum() + (</span><br><span class="line">    bind_data[bind_data[:, <span class="number">0</span>] == <span class="number">0</span>][:, <span class="number">1</span>] &lt; cutoff).sum()) / \</span><br><span class="line">    bind_data.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">return</span> res1, res2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the initial param</span></span><br><span class="line">clf = XGBClassifier(</span><br><span class="line">learning_rate=<span class="number">0.01</span>,</span><br><span class="line">n_estimators=<span class="number">500</span>,</span><br><span class="line">objective=<span class="string">'binary:logistic'</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># best cutoff : 223 , more details follow the train_doc_guide</span></span><br><span class="line">filter_columns = [<span class="string">'uid'</span>, <span class="string">'label'</span>] + [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> res[<span class="number">-223</span>:]]</span><br><span class="line">reshaped_data = reshaped_data[filter_columns]</span><br><span class="line">X_train = reshaped_data.iloc[:, <span class="number">2</span>:]</span><br><span class="line">y_train = reshaped_data.iloc[:, <span class="number">1</span>]</span><br><span class="line">model_sklearn = clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the importance ,best cutoff : 0.0022857142612338 , more details follow the train_doc_guide</span></span><br><span class="line">importance = np.c_[X_train.columns, model_sklearn.feature_importances_]</span><br><span class="line">train_columns = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> importance <span class="keyword">if</span> x[<span class="number">1</span>] &gt; <span class="number">0.0022857142612338</span>]</span><br></pre></td></tr></table></figure></p>
<p>这样，数据预处理就完成了，接下来就是模型设计部分了，但是上面的过程很重要，请务必重视！</p>
<hr>
<ul>
<li>xgboost叶子结点获取<br>核心在于参数调优，没什么特别多的技术壁垒：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># update the values in the model</span></span><br><span class="line"><span class="comment"># scale_weight_suggestion = (Y_train.count() - Y_train.sum()) / Y_train.sum()</span></span><br><span class="line">param_test = &#123;</span><br><span class="line"><span class="string">'n_estimators'</span>: [<span class="number">100</span>, <span class="number">250</span>, <span class="number">500</span>, <span class="number">750</span>]</span><br><span class="line">&#125;</span><br><span class="line">gsearch = GridSearchCV(</span><br><span class="line">estimator=XGBClassifier(</span><br><span class="line">learning_rate=<span class="number">0.1</span>,</span><br><span class="line">objective=<span class="string">'binary:logistic'</span>,</span><br><span class="line">scale_pos_weight=<span class="number">0.707653</span>,</span><br><span class="line">seed=<span class="number">27</span>),</span><br><span class="line">param_grid=param_test,</span><br><span class="line">scoring=<span class="string">'roc_auc'</span>,</span><br><span class="line">n_jobs=<span class="number">4</span>,</span><br><span class="line">iid=<span class="keyword">False</span>,</span><br><span class="line">cv=<span class="number">5</span>)</span><br><span class="line">gsearch.fit(X_train, Y_train)</span><br><span class="line">print(gsearch.best_params_)</span><br><span class="line"><span class="comment"># &#123;'n_estimators': 500&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the final param</span></span><br><span class="line">clf = XGBClassifier(</span><br><span class="line">learning_rate=<span class="number">0.01</span>,</span><br><span class="line">n_estimators=<span class="number">500</span>,</span><br><span class="line">max_depth=<span class="number">3</span>,</span><br><span class="line">min_child_weight=<span class="number">7</span>,</span><br><span class="line">objective=<span class="string">'binary:logistic'</span>,</span><br><span class="line">scale_pos_weight=<span class="number">0.707653</span>,</span><br><span class="line">gamma=<span class="number">0.6</span>,</span><br><span class="line">reg_alpha=<span class="number">1</span>,</span><br><span class="line">seed=<span class="number">27</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train the values</span></span><br><span class="line">model_sklearn = clf.fit(X_train, Y_train)</span><br><span class="line">y_bst = model_sklearn.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">metrics_spec(Y_train, model_sklearn.predict_proba(X_train)[:, <span class="number">1</span>])</span><br><span class="line">metrics_spec(Y_test, y_bst)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>为了避免冗长，我删除了调参数细节，留了一个case做guide，下面就是拿出xgboost的叶子结点，并enhotencoding的过程。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 叶子结点获取</span></span><br><span class="line">train_new_feature = clf.apply(X_train)</span><br><span class="line">test_new_feature = clf.apply(X_test)</span><br><span class="line"><span class="comment"># enhotcoding</span></span><br><span class="line">enc = OneHotEncoder()</span><br><span class="line">enc.fit(train_new_feature)</span><br><span class="line">train_new_feature2 = np.array(enc.transform(train_new_feature).toarray())</span><br><span class="line">test_new_feature2 = np.array(enc.transform(test_new_feature).toarray())</span><br><span class="line">res_data = pd.DataFrame(np.c_[Y_train, train_new_feature2])</span><br><span class="line">res_data.columns = [<span class="string">'f'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(res_data.shape[<span class="number">1</span>])]</span><br><span class="line">res_test = pd.DataFrame(np.c_[Y_test, test_new_feature2])</span><br><span class="line">res_test.columns = [<span class="string">'f'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(res_test.shape[<span class="number">1</span>])]</span><br></pre></td></tr></table></figure></p>
<p>到此为止，将叶子结点获取过程就结束了，这边细心的人会发现，这个是一个非常稀疏的矩阵，我这边追加的是常规的LR，但是如果就单纯从数据特征的角度来讲，神经网络和FFM对这类数据类型有更好的表现，如果需要写FM收尾的同学，可以参考我写的这个FM包，理论：<a href="http://shataowei.com/2017/12/04/FM理论解析及应用/" target="_blank" rel="noopener">FM理论解析及应用</a>，代码在：<a href="https://github.com/sladesha/machine_learning/tree/master/FM" target="_blank" rel="noopener">FM快速实现Github</a>。</p>
<hr>
<ul>
<li>logistics模块python实现<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(C=<span class="number">1</span>, penalty=<span class="string">'l2'</span>, max_iter=<span class="number">1000</span>, solver=<span class="string">'sag'</span>, multi_class=<span class="string">'ovr'</span>)</span><br><span class="line">model_lr = lr.fit(res_data.iloc[:,<span class="number">1</span>:], res_data[<span class="string">'f0'</span>])</span><br><span class="line">y_train_lr = model_lr.predict_proba(res_data.iloc[:,<span class="number">1</span>:])[:, <span class="number">1</span>]</span><br><span class="line">y_test_lr = model_lr.predict_proba(res_test.iloc[:,<span class="number">1</span>:])[:, <span class="number">1</span>]</span><br><span class="line">res = metrics_spec(Y_test, y_test_lr)</span><br><span class="line">correct_rank = X_train.columns</span><br><span class="line"><span class="comment"># (0.80, 0.71)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>简单易上手，实现了下图的流：<br><img src="/2017/12/28/Kaggle-TianChi分类问题相关算法快速实现/8.jpeg" alt=""></p>
<p>顺带附上ks值计算逻辑：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 算法评估</span></span><br><span class="line"><span class="comment"># ks_xgb_lr = np.c_[Y_test,y_test_lr]</span></span><br><span class="line"><span class="comment"># ks_xgb_lr = sorted(ks_xgb_lr , key = lambda x : x[1],reverse = True)</span></span><br><span class="line"><span class="comment"># ks_xgb_lr = pd.DataFrame(ks_xgb_lr)</span></span><br><span class="line"><span class="comment"># for i in range(9):</span></span><br><span class="line"><span class="comment">#     end = (i+1)*break_cut</span></span><br><span class="line"><span class="comment">#     res1 = 1.0*ks_xgb_lr.iloc[:end,:][ks_xgb_lr.iloc[:end,0]==0].shape[0]/ks_xgb_lr[ks_xgb_lr.iloc[:,0]==0].shape[0]</span></span><br><span class="line"><span class="comment">#     res2 = 1.0*ks_xgb_lr.iloc[:end,:][ks_xgb_lr.iloc[:end,0]==1].shape[0]/ks_xgb_lr[ks_xgb_lr.iloc[:,0]==1].shape[0]</span></span><br><span class="line"><span class="comment">#     res = res2-res1</span></span><br><span class="line"><span class="comment">#     print(res1,res2,res)</span></span><br></pre></td></tr></table></figure></p>
<p>只要重复上述的案例浮现中的code流程，可以快速复刻出一份效果还不错的分类算法，但是如果想能够拿到很好的名次或者有更优秀的表现，等待下一篇文章中的原理剖析。</p>
<p>最后，给大家分享一下之前和Kaggle大神在算法竞赛或者解决项目问题的时候总结出来需要尤其注意的点：</p>
<ul>
<li>请务必重视数据集构造，你能不能上榜或者得到leader的重视，这一点最关键，没有之一</li>
<li>如果条件允许，尽可能的离散化数据尝试一下，多做两次特征筛选这些预处理的步骤，收益是非常大的</li>
<li>请善于使用gridsearch，最后能进前十还是前三很大程度上相差的就是那零点几</li>
<li>在集群或者资源充足的情况下，利用交叉检验代替Valid test，管中窥豹的结果会让自己更加固步自封，离真相越走越远</li>
<li>乐于分享，表达出自己的观点，反过来在驳斥自己的观点，直到可以完全说服自己</li>
</ul>
<p>没啥广告要打，就这样吧。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 模型设计 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[风控用户识别方法]]></title>
      <url>/2017/12/09/%E9%A3%8E%E6%8E%A7%E7%94%A8%E6%88%B7%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/09/风控用户识别方法/1.jpg" alt=""><br><a id="more"></a></p>
<p><strong>update:<br>18.1.1 :<a href="https://github.com/sladesha/Frcwp" target="_blank" rel="noopener">Frcwp</a>已如期上线，满足本文中的所有方法，欢迎拍砖</strong></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>因为工作方向相关，之前我也尝试着在Google、arXiv、wikipedia等等地方搜一些风控识别的资料或者思路，但是事与愿违的是，绝大多数的与风控算法都毫无关系，基本上都是推销自己家的产品的，所以，我之前也尝试着写了一些方法的梳理，如:</p>
<ul>
<li><a href="http://shataowei.com/2017/12/01/多算法识别撞库刷券等异常用户/" target="_blank" rel="noopener">多算法识别撞库刷券等异常用户</a></li>
<li><a href="http://shataowei.com/2017/08/09/数据预处理-异常值识别/" target="_blank" rel="noopener">异常值识别与处理</a></li>
</ul>
<p>但是在我前几天再回过头去看自己写的这些东西的时候，作为一个老司机来说，我都不想去看一篇又一篇动则上千字的文章，理论交错，文笔粗陋，正巧现在公司内部也有一个风控的项目，所以，我准备做一个开源的项目<strong><a href="https://github.com/sladesha/Frcwp" target="_blank" rel="noopener">Frcwp</a></strong>，核心在于：</p>
<ul>
<li>简单操作，几乎不用多少调参，自动识别异常点</li>
<li>理论清晰，支持的方法多，兼容性好</li>
<li>集成数据预处理的过程，减轻前置工作量</li>
</ul>
<p>“纠结”了几个朋友的情况下，一期已经完工，主要是搭建了最简单的框架，我相信，这只是一个开始，欢迎大家试用，也欢迎每一个人来批评，更希望有想法的同学一起来做这个事情。</p>
<hr>
<h1 id="接下来，让我们来讲讲，一期我们做了什么？"><a href="#接下来，让我们来讲讲，一期我们做了什么？" class="headerlink" title="接下来，让我们来讲讲，一期我们做了什么？"></a>接下来，让我们来讲讲，一期我们做了什么？</h1><p>核心我们一期做的异常点识别中，核心是利用的14年周志华教授提出的isolation forest算法进行识别，详细的理论部分请参见：<strong><a href="http://shataowei.com/2017/08/09/数据预处理-异常值识别/" target="_blank" rel="noopener">Isolation Forest</a></strong>，重复说一个事情的意义也不大。这边需要解释几点：</p>
<ul>
<li>具体是怎么得到当前的算法流程的呢？</li>
<li>为什么用当前的算法进行识别而不用其他的识别算法？</li>
<li>当前的设计下存在哪些问题？</li>
<li>未来的方向会在哪边？</li>
</ul>
<p>让我们来一一来回答这些问题。</p>
<h1 id="为了用Isolation-Forest而不用其他的识别算法？"><a href="#为了用Isolation-Forest而不用其他的识别算法？" class="headerlink" title="为了用Isolation Forest而不用其他的识别算法？"></a>为了用Isolation Forest而不用其他的识别算法？</h1><p>在设计这套算法之前，我们其实是遇到了一个实际的业务问题，<strong>黑产撞库</strong>。相信大家毫不陌生这个词，无论是阿里、京东、滴滴还是腾讯，被撞库是一件普通了不能再普通的事情，“黑产”的人从第三方渠道，获取到你历史上的手机号和一些你曾经用的密码，重复的登陆，暴力的尝试，如果你的密码设置的比较简单，比如：“123456”，“qwerty”…非常容易被破解，然后再根据你历史下单的情况，进行假冒“客服”退款，进行诈骗，百度一搜就有一堆这样的新闻：</p>
<ul>
<li><a href="http://tech.huanqiu.com/gundong/2017-11/11369192.html" target="_blank" rel="noopener">频发假冒电商客服</a></li>
<li><a href="http://www.qhnews.com/newscenter/system/2016/05/24/012012151.shtml" target="_blank" rel="noopener">当心!近期有人冒充假客服行骗 几分钟骗走市民八九万元</a><br>…</li>
</ul>
<p>所以，我们需要阻止“黑产”人员进行这样的暴力破解，获取用户的资料，由此而引发了我们对这个问题的思考。我们在对这个问题分析的时候，巧妙的发现了如下的一些信息：<br><img src="/2017/12/09/风控用户识别方法/2.png" alt=""><br>因为涉及公司机密，这边隐去了具体坐标和值，很容易发现以下问题：</p>
<ul>
<li>正常扇面内数据分布密集，未知扇面内数据分布松散，异常扇面内数据分布稀疏</li>
<li>正常扇面内的数据量占全量数据的绝大多数</li>
<li>不存在明显的分割线，正常扇面和异常扇面存在过度地带</li>
</ul>
<p>这个给了我们一些启发，我们做了如下的分析：</p>
<ul>
<li>我们观察了异常扇面内的用户黑白比，如我们预计的黑白比为20:3，也就是说分布远离大量数据点的用户绝大多数存在问题</li>
<li>为止区域的用户黑白比为1:2，这说明在黑白用户之间不存在明显的界限，有交错地带</li>
<li>正常区域内也存在黑名单用户，比例在504:1，也就是说，我们划分有一定识别能力，但是还是不能做到全量识别</li>
</ul>
<p>综合上述这些预先的处理，我们要用算法完成三件事情：<br>1.切分全量用户，做到识别出<strong>正常，未知，异常用户</strong><br>2.识别出异常用户和正常用户之间的<strong>差异约束切割</strong><br>3.在异常用户+未知用户里面，找出利用差异约束切割出黑名单</p>
<h1 id="为什么用当前的算法进行识别而不用其他的识别算法？"><a href="#为什么用当前的算法进行识别而不用其他的识别算法？" class="headerlink" title="为什么用当前的算法进行识别而不用其他的识别算法？"></a>为什么用当前的算法进行识别而不用其他的识别算法？</h1><p>切分数据的时候，我们这边采用的是切比雪夫切割。非理工科的同学可能比较疑惑什么是切比雪夫切割，这边如果数据是正态下，箱式图的Q3+3/2xQI作为上top点进行切割，大家就应该很熟悉了，其实利用的就是数据出现的概率。<br><img src="/2017/12/09/风控用户识别方法/3.jpeg" alt="来源于百度百科"><br>上面这张图很好的解释了，在数据服从正态分布的情况下，出现数据值比均值+3x标准差要大的概率不足0.1%，所以，我们可以认为这些数据是异常点了。那现在出现了一个问题，日常数据分布都不一定是正态的，所以引出来了类似的切比雪夫理论，它用的是马氏距离距离中心点的程度，详细的马氏距离理论见<a href="http://shataowei.com/2017/08/09/数据预处理-异常值识别/" target="_blank" rel="noopener">马氏距离分布</a>。</p>
<p>切分完成数据之后，我们要做寻找差异约束切割逻辑。从最上面的扇面图，我们很容易发现，正常数据与异常数据之间的密度差异很明显，所以如何识别密度差异的算法就是我们需要的，这边我大概找了6、7种常见的切分方法，这边主要讲三种：isolation forest，lof，distance similarity。理论我之前也讲过，贴上地址，不废话了：<a href="http://shataowei.com/2017/12/01/多算法识别撞库刷券等异常用户/" target="_blank" rel="noopener">密度算法</a>。这边主要展示效果差异：<br><img src="/2017/12/09/风控用户识别方法/4.png" alt=""><br>通过68个数据集，很明显的可以看出LOF的识别出来的用户的异常用户异常程度是低于Isolation Forest和Distince Similarity的，起码在我们这些数据集样本中，Isolation Forest和Distince Similarity识别效果差异不大，所以，我们再考虑了另一个性能问题：<br><img src="/2017/12/09/风控用户识别方法/5.png" alt=""><br>我们用了CV=10的交叉检验，发现，平均下来，Isolation Forest识别速度是Distince Similarity的1/3以下。综合上述，还有一些其他因素，最后我们选择了Isoation Forest的方法。</p>
<h1 id="当前的设计下存在哪些问题？"><a href="#当前的设计下存在哪些问题？" class="headerlink" title="当前的设计下存在哪些问题？"></a>当前的设计下存在哪些问题？</h1><p>上面说的都是比较正面的问题，让我们看看，有哪些缺点。<br>首先，从头到尾，我们一直在围绕密度差异这个问题，但是就我平时做的一些小爬虫都知道，降低暴力获取的速度，慢慢搞，这时候就以上的方法就无法做到有效的识别。除此之外，因为我们用了切比雪夫不等式，所以对其有概念的同学知道，算马氏距离的时候需要算协方差矩阵，当数据量异常异常大(我测算的是12mx100)的时候计算资源紧张，可能算不出来；数据量异常异常小的时候feature严重共线性，也可能计算不出来。</p>
<h1 id="未来的方向会在哪边？"><a href="#未来的方向会在哪边？" class="headerlink" title="未来的方向会在哪边？"></a>未来的方向会在哪边？</h1><p>所以，后续我们会新增其他算法，支持过大过小情况下的识别方法。针对数据量过小的识别情况，我在V0.0.3版本下更新了一个简单识别的方法，之后会优化更好的算法替代掉的。只要数据量太大无法计算的问题，我之后会采取矩阵切割分块计算的方法，这个是后话了。</p>
<p>最后，我们以当前算法包的使用来结束整篇介绍：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装</span></span><br><span class="line">pip install Frcwp</span><br></pre></td></tr></table></figure></p>
<p>自动识别过程：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Frcwp <span class="keyword">import</span> Frcwp</span><br><span class="line">traindata = pd.read_table(<span class="string">'../路径'</span>)<span class="comment">#数据可以在https://github.com/sladesha/machine_learning/tree/master/data下的data_all.csv获取</span></span><br><span class="line">frc = Frcwp()</span><br><span class="line">traindata = frc.changeformat(traindata, index=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># You can define your own outlier size , the details of these params can be got from ../Frcwp/Frcwp.py:</span></span><br><span class="line">params = &#123;</span><br><span class="line"><span class="string">'na_rate'</span>: <span class="number">0.4</span>,</span><br><span class="line"><span class="string">'single_dealed'</span>: <span class="number">1</span>,</span><br><span class="line"><span class="string">'is_scale'</span>: <span class="number">0</span>,</span><br><span class="line"><span class="string">'distince_method'</span>: <span class="string">'Maha'</span>,</span><br><span class="line"><span class="string">'outlier_rate'</span>: <span class="number">0.05</span>,</span><br><span class="line"><span class="string">'strange_rate'</span>: <span class="number">0.15</span>,</span><br><span class="line"><span class="string">'nestimators'</span>: <span class="number">150</span>,</span><br><span class="line"><span class="string">'contamination'</span>: <span class="number">0.2</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># train the frc model</span></span><br><span class="line">frc.fit(traindata, **params)</span><br></pre></td></tr></table></figure></p>
<p>相关的结果显示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict outliers with the trained frc model</span></span><br><span class="line">predict_params = &#123;</span><br><span class="line"><span class="string">'output'</span>: <span class="number">20</span>,</span><br><span class="line"><span class="string">'is_whole'</span>: <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">frc.predict(frc.potentialdata_set, **predict_params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># if you want get the whole probability of your potential outliers</span></span><br><span class="line">frc.similarity_label</span><br></pre></td></tr></table></figure></p>
<p>以上部分内容截取自我的<a href="https://github.com/sladesha/Frcwp" target="_blank" rel="noopener">github</a>，希望对大家有一些帮助。</p>
<p>最后，谢谢大家的阅读，欢迎大家关注我的<a href="http://shataowei.com" target="_blank" rel="noopener">个人博客</a>。</p>
<p><strong>本文拒绝任何形式的转载，若要转载请联系stw386@sina.com</strong></p>
]]></content>
      
        <categories>
            
            <category> 开源项目 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 风控 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[FM理论解析及应用]]></title>
      <url>/2017/12/04/FM%E7%90%86%E8%AE%BA%E8%A7%A3%E6%9E%90%E5%8F%8A%E5%BA%94%E7%94%A8/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/04/FM理论解析及应用/1.jpg" alt=""><br><a id="more"></a></p>
<h1 id="FM的产生背景"><a href="#FM的产生背景" class="headerlink" title="FM的产生背景"></a>FM的产生背景</h1><p>我其实没有做过很多CTR预估的事情，但是我在工作中常常遇到CRM流失预估、订单预估这些依赖于特征工程的事情，其中就涉及到特征的组合问题。</p>
<h1 id="one-hot过程"><a href="#one-hot过程" class="headerlink" title="one-hot过程"></a>one-hot过程</h1><p>在feature选取过程中，不可避免的会出现，学历这种高中、大学、研究生等多分类的feature，在实际应用中，我们对单个feature需要进行一种one hot过程，就是将原来的学历拆解为 是否为高中，是否为大学，注意，可以不用加是否为研究生一列，因为是否为高中，是否为大学的两列已经可以推导这个用户是否为研究生，加上这一列有时候反而会共线性。但是这样做，看起来没什么问题，想想看要是100个这样的特征，每个特征有100个这样单独的feature value的话，整体数据将是一个非常庞大的稀疏矩阵，无论是计算还是分析都是会存在巨大的问题的，所以看看我们能不能组合一些特征降低维度。</p>
<h1 id="什么叫做组合问题"><a href="#什么叫做组合问题" class="headerlink" title="什么叫做组合问题"></a>什么叫做组合问题</h1><p>现在有一组数据，其中特征包含性别（男女），学历（高中，大学，研究生），想要判断这两个feature对是否对化妆品感谢兴趣。单独的观察性别这一栏，发现有一定相关性，但是比较弱，并不是所有的女性都对化妆品感兴趣；单独的观察学历这一栏也发现，学历与对化妆品感兴趣的程度并没有显著的相关性。其实，我们可以从自己的感知理解，首先，数据中女生可能比男生对化妆品更感兴趣，但是女生数据中存在大量的高中生，相对于高中生而言，大学生和研究生可能对化妆品更加感兴趣一点，所以原来的两个feature：性别，学历就组合成了是否为性别女+学历大于高中一个feature，这就是特征组合的过程。如果feature总个数少还可以，要是要有上千上万个，光两两组和就有n*(n-1)/2种可能，所以我们需要想一个其他办法。</p>
<h1 id="组合特征后的表达形式"><a href="#组合特征后的表达形式" class="headerlink" title="组合特征后的表达形式"></a>组合特征后的表达形式</h1><p>首先，我们都知道一般的线性模型为：<br><img src="/2017/12/04/FM理论解析及应用/2.jpeg" alt=""></p>
<p>为了考虑组合特征的作用，我们采用多项式来代表，形如特征xi与xj的组合用xixj表示，具体的表达式如下：<br><img src="/2017/12/04/FM理论解析及应用/3.jpeg" alt=""><br>其中，wij为组合特征xixj的权重，n表示样本的feature个数，xi为第i个feature。</p>
<h1 id="方程定义完成了，下面就要开始数学定义"><a href="#方程定义完成了，下面就要开始数学定义" class="headerlink" title="方程定义完成了，下面就要开始数学定义"></a>方程定义完成了，下面就要开始数学定义</h1><p>对每一个特征xi引入辅助向量Vi=(vi1,vi2,…vik),这边的k就是矩阵拆解的规模值，利用ViVj.T对交叉项的系数wij进行估计,<br>及<img src="/2017/12/04/FM理论解析及应用/4.jpeg" alt=""><br>则<br><img src="/2017/12/04/FM理论解析及应用/5.jpeg" alt=""><br><strong>这边需要注意一点，k理论上讲，越大越能强化拟合的能力，但是实际在运算过程中，一来受限于计算能力，二来受限于数据量，过大的k只会带来过拟合的问题。我实测了40w左右的数据，观察到k值在6-8左右，valid集合数据拟合效果最优，仅供参考</strong></p>
<p>很明显，上面这么多未知数：1+n是线性未知数个数，nxfeature是组合特征的未知数个数，常规求解的效率可想而知。但是看到xixj这样的形式，我们很容易联想到：2ab = (a+b)^2 -a^2 -b^2，所以在解决这个wij、xi、xj点积的问题上，我们采用了：1/2 * ( (a+b+c)^2  -  a^2  -  b^2  -  c^2)的方式<br><img src="/2017/12/04/FM理论解析及应用/6.jpeg" alt=""></p>
<h1 id="下面让我们来解这个式子"><a href="#下面让我们来解这个式子" class="headerlink" title="下面让我们来解这个式子"></a>下面让我们来解这个式子</h1><p>这边需要一点导数功底，我们先来看对w0也就是bias求导，这个毫无意外，梯度为1；再对wi求导，这个也很简单，xi即可，这个也很简单，少许繁琐的就是wij求导，让我来仔细看看：<br><img src="/2017/12/04/FM理论解析及应用/7.jpg" alt=""><br>ok，我知道我的字很丑，别说话，看问题，所以我们可以总结为下面这个网上到处都有的式子：<br><img src="/2017/12/04/FM理论解析及应用/8.jpeg" alt=""><br>这个式子就是上面这么来的。<br>把上面的那个点积形式代入求解及为：<br><img src="/2017/12/04/FM理论解析及应用/9.jpeg" alt=""></p>
<h1 id="引申一个FFM概念"><a href="#引申一个FFM概念" class="headerlink" title="引申一个FFM概念"></a>引申一个FFM概念</h1><p>在FM模型中，每一个特征会对应一个隐变量，但在FFM模型中，认为应该将特征分为多个field，每个特征对应每个field分别有一个隐变量。</p>
<p>举个例子，我们的样本有3种类型的字段：qualifications, age, gender，分别可以代表学历，年龄段，性别。其中qualifications有3种数据，age有5种数据，gender有男女2种，经过one-hot编码以后，每个样本有7个特征，其中只有3个特征非空。<br>如果使用FM模型，则7个特征，每个特征对应一个隐变量。<br>如果使用FFM模型，则7个特征，每个特征对应3个隐变量，即每个类型对应一个隐变量，及对应qualifications, age, gender各占一个。</p>
<p>我看了Yu-Chin Juan实现了一个C++版的FFM模型的源码，倒过来想他的表达式应该是这样的：<br><img src="/2017/12/04/FM理论解析及应用/10.jpeg" alt=""><br>其他模块都与fm差不多，主要看Vj1f2Vj2f1这个东西。我们假设j1特征属于f1这个field，j2特征属于f2这个feild，则Vj1f2表示j1这个特征对应j2所属的field的隐变量。很恶心的解释，通俗的来讲就是，性别为女与学历这个field的组合有个隐变量，性别女与年龄这个field的组合又有一个不一样的隐变量，而却不考虑到底是什么学历是啥，年龄具体到什么细节。<br>Yu-Chin Juan大神在实际写code的过程中，干掉来常数和一次项，可能是为了方便计算，保留的如下：<br><img src="/2017/12/04/FM理论解析及应用/11.jpeg" alt=""><br>整理的最优化损失函数如下：<br><img src="/2017/12/04/FM理论解析及应用/12.jpeg" alt=""><br>前面为l2正则，后面为交互熵形式，我们看到了y*Φ(V,x)这个及其类似hinge loss里面的1−t⋅y部分，所以注意这边的y属于{-1，1}<br>这边的求导，我算了一个小时都没搞出来，等哪天有空了，再仔细的去算一下，去翻了原论文，最后的迭代形式如下：<br><img src="/2017/12/04/FM理论解析及应用/13.jpeg" alt=""><br>η是常规的速率，V是初始均匀分布即可</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>我这边完成了FM的代码实现，详细见我的github：<a href="https://github.com/sladesha/machine_learning/tree/master/FM" target="_blank" rel="noopener">fm代码</a><br>为了方便不想看细节，只想撸代码的同学，我打包上传到了pypi，你只需要<code>pip install Fsfm</code>即可体验<br>至于ffm，我下午实在没写出来，对不起彭老师，丢脸了，后续看什么时候有空再研究一下。</p>
<p>最后，着重提示，本文很多思路很解析都参考的Yu-Chin Juan的源代码，附上<a href="https://github.com/guestwalk/libffm" target="_blank" rel="noopener">github地址</a>，欢迎去关注原作者的内容，感谢大神带路，谢谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 特征交叉 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SMOTE算法]]></title>
      <url>/2017/12/01/SMOTE%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/01/SMOTE算法/1.jpg" alt=""><br><a id="more"></a></p>
<hr>
<p>17.11.28更新一下：最近把这个算法集成到了数据预处理的python工程代码中了，不想看原理想直接用的，有简易版的<a href="https://sladesha.github.io/2017/12/01/python开发：特征工程代码模版-一/">python开发：特征工程代码模版
</a>，进入页面后ctrl+F搜smote就行，请自取</p>
<hr>
<p>之前一直没有用过python，最近做了一些数量级比较大的项目，觉得有必要熟悉一下python，正好用到了smote，网上也没有搜到，所以就当做一个小练手来做一下。</p>
<p>首先，看下Smote算法之前，我们先看下当正负样本不均衡的时候，我们通常用的方法：</p>
<ul>
<li>抽样<br>常规的包含过抽样、欠抽样、组合抽样<br>过抽样：将样本较少的一类sample补齐<br>欠抽样：将样本较多的一类sample压缩<br>组合抽样：约定一个量级N，同时进行过抽样和欠抽样，使得正负样本量和等于约定量级N</li>
</ul>
<p>这种方法要么丢失数据信息，要么会导致较少样本共线性，存在明显缺陷</p>
<ul>
<li>权重调整<br>常规的包括算法中的weight，weight matrix<br>改变入参的权重比，比如boosting中的全量迭代方式、逻辑回归中的前置的权重设置</li>
</ul>
<p>这种方式的弊端在于无法控制合适的权重比，需要多次尝试</p>
<ul>
<li>核函数修正<br>通过核函数的改变，来抵消样本不平衡带来的问题</li>
</ul>
<p>这种使用场景局限，前置的知识学习代价高，核函数调整代价高，黑盒优化</p>
<ul>
<li>模型修正<br>通过现有的较少的样本类别的数据，用算法去探查数据之间的特征，判读数据是否满足一定的规律<br>比如，通过线性拟合，发现少类样本成线性关系，可以新增线性拟合模型下的新点</li>
</ul>
<p>实际规律比较难发现，难度较高</p>
<p><strong>SMOTE（Synthetic minoritye over-sampling technique,SMOTE）是Chawla在2002年提出的过抽样的算法，一定程度上可以避免以上的问题</strong></p>
<p>下面介绍一下这个算法：<br><img src="/2017/12/01/SMOTE算法/2.png" alt="正负样本分布"></p>
<p>很明显的可以看出，蓝色样本数量远远大于红色样本，在常规调用分类模型去判断的时候可能会导致之间忽视掉红色样本带了的影响，只强调蓝色样本的分类准确性，这边需要增加红色样本来平衡数据集</p>
<p>Smote算法的思想其实很简单，先随机选定n个少类的样本，如下图</p>
<p><img src="/2017/12/01/SMOTE算法/3.png" alt="找出初始扩展的少类样本"></p>
<p>再找出最靠近它的m个少类样本，如下图</p>
<p><img src="/2017/12/01/SMOTE算法/4.png" alt=""></p>
<p>再任选最临近的m个少类样本中的任意一点，</p>
<p><img src="/2017/12/01/SMOTE算法/5.png" alt=""></p>
<p>在这两点上任选一点，这点就是新增的数据样本</p>
<hr>
<p>R语言上的开发较为简单，有现成的包库，这边简单介绍一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rm(list=ls())</span><br><span class="line">install.packages(“DMwR”,dependencies=T)</span><br><span class="line">library(DMwR)<span class="comment">#加载smote包</span></span><br><span class="line">newdata=SMOTE(formula,data,perc.over=,perc.under=)</span><br><span class="line"><span class="comment">#formula:申明自变量因变量</span></span><br><span class="line"><span class="comment">#perc.over：过采样次数</span></span><br><span class="line"><span class="comment">#perc.under：欠采样次数</span></span><br></pre></td></tr></table></figure></p>
<p>效果对比：<br><img src="/2017/12/01/SMOTE算法/6.png" alt=""><br>简单的看起来就好像是重复描绘了较少的类<br>这边的smote是封装好的，直接调用就行了，没有什么特别之处</p>
<hr>
<p>这边自己想拿刚学的python练练手，所有就拿python写了一下过程：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#读数据</span></span><br><span class="line">data = pd.read_table(<span class="string">'C:/Users/17031877/Desktop/supermarket_second_man_clothes_train.txt'</span>, low_memory=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#简单的预处理</span></span><br><span class="line">test_date = pd.concat([data[<span class="string">'label'</span>], data.iloc[:, <span class="number">7</span>:<span class="number">10</span>]], axis=<span class="number">1</span>)</span><br><span class="line">test_date = test_date.dropna(how=<span class="string">'any'</span>)</span><br></pre></td></tr></table></figure></p>
<p>数据大致如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">test_date.head()</span><br><span class="line">Out[<span class="number">25</span>]:</span><br><span class="line">label  max_date_diff  max_pay  cnt_time</span><br><span class="line"><span class="number">0</span>      <span class="number">0</span>           <span class="number">23.0</span>  <span class="number">43068.0</span>        <span class="number">15</span></span><br><span class="line"><span class="number">1</span>      <span class="number">0</span>           <span class="number">10.0</span>   <span class="number">1899.0</span>         <span class="number">2</span></span><br><span class="line"><span class="number">2</span>      <span class="number">0</span>          <span class="number">146.0</span>   <span class="number">3299.0</span>        <span class="number">21</span></span><br><span class="line"><span class="number">3</span>      <span class="number">0</span>           <span class="number">30.0</span>  <span class="number">31959.0</span>        <span class="number">35</span></span><br><span class="line"><span class="number">4</span>      <span class="number">0</span>            <span class="number">3.0</span>  <span class="number">24165.0</span>        <span class="number">98</span></span><br><span class="line">test_date[<span class="string">'label'</span>][test_date[<span class="string">'label'</span>]==<span class="number">0</span>].count()/test_date[<span class="string">'label'</span>][test_date[<span class="string">'label'</span>]==<span class="number">1</span>].count()</span><br><span class="line">Out[<span class="number">37</span>]: <span class="number">67</span></span><br></pre></td></tr></table></figure></p>
<p>label是样本类别判别标签，1:0=67:1，需要对label=1的数据进行扩充</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 筛选目标变量</span></span><br><span class="line">aimed_date = test_date[test_date[<span class="string">'label'</span>] == <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 随机筛选少类扩充中心</span></span><br><span class="line">index = pd.DataFrame(aimed_date.index).sample(frac=<span class="number">0.1</span>, random_state=<span class="number">1</span>)</span><br><span class="line">index.columns = [<span class="string">'id'</span>]</span><br><span class="line">number = len(index)</span><br><span class="line"><span class="comment"># 生成array格式</span></span><br><span class="line">aimed_date_new = aimed_date.ix[index.values.ravel(), :]</span><br></pre></td></tr></table></figure>
<p>随机选取了全量少数样本的10%作为数据扩充的中心点</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 自变量标准化</span></span><br><span class="line">sc = StandardScaler().fit(aimed_date_new)</span><br><span class="line">aimed_date_new = pd.DataFrame(sc.transform(aimed_date_new))</span><br><span class="line">sc1 = StandardScaler().fit(aimed_date)</span><br><span class="line">aimed_date = pd.DataFrame(sc1.transform(aimed_date))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义欧式距离计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist</span><span class="params">(a, b)</span>:</span></span><br><span class="line">a = array(a)</span><br><span class="line">b = array(b)</span><br><span class="line">d = ((a[<span class="number">0</span>] - b[<span class="number">0</span>]) ** <span class="number">2</span> + (a[<span class="number">1</span>] - b[<span class="number">1</span>]) ** <span class="number">2</span> + (a[<span class="number">2</span>] - b[<span class="number">2</span>]) ** <span class="number">2</span> + (a[<span class="number">3</span>] - b[<span class="number">3</span>]) ** <span class="number">2</span>) ** <span class="number">0.5</span></span><br><span class="line"><span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<p>下面定义距离计算的方式，所有算法中，涉及到距离的地方都需要标准化去除冈量，也同时加快了计算的速度<br>这边采取了欧式距离的方式，更多计算距离的方式参考：<br><a href="http://www.jianshu.com/p/1417fcb06797" target="_blank" rel="noopener">多种距离及相似度的计算理论介绍</a></p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计所有检验距离样本个数</span></span><br><span class="line">row_l1 = aimed_date_new.iloc[:, <span class="number">0</span>].count()</span><br><span class="line">row_l2 = aimed_date.iloc[:, <span class="number">0</span>].count()</span><br><span class="line">a = zeros((row_l1, row_l2))</span><br><span class="line">a = pd.DataFrame(a)</span><br><span class="line"><span class="comment"># 计算距离矩阵</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(row_l1):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(row_l2):</span><br><span class="line">d = dist(aimed_date_new.iloc[i, :], aimed_date.iloc[j, :])</span><br><span class="line">a.ix[i, j] = d</span><br><span class="line">b = a.T.apply(<span class="keyword">lambda</span> x: x.min())</span><br></pre></td></tr></table></figure>
<p>调用上面的计算距离的函数，形成一个距离矩阵</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 找到同类点位置</span></span><br><span class="line">h = []</span><br><span class="line">z = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(number):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(len(a.iloc[i, :])):</span><br><span class="line">ai = a.iloc[i, j]</span><br><span class="line">bi = b[i]</span><br><span class="line"><span class="keyword">if</span> ai == bi:</span><br><span class="line">h.append(i)</span><br><span class="line">z.append(j)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">new_point = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">new_point = pd.DataFrame(new_point)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(h)):</span><br><span class="line">index_a = z[i]</span><br><span class="line">new = aimed_date.iloc[index_a, :]</span><br><span class="line">new_point = pd.concat([new, new_point], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">new_point = new_point.iloc[:, range(len(new_point.columns) - <span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
<p>再找到位置的情况下，再去原始的数据集中根据位置查找具体的数据</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">r1 = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(new_point.columns)):</span><br><span class="line">r1.append(random.uniform(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">new_point_last = []</span><br><span class="line">new_point_last = pd.DataFrame(new_point_last)</span><br><span class="line"><span class="comment"># 求新点 new_x=old_x+rand()*(append_x-old_x)</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(new_point.columns)):</span><br><span class="line">new_x = (new_point.iloc[<span class="number">1</span>:<span class="number">4</span>, i] - aimed_date_new.iloc[number - <span class="number">1</span> - i, <span class="number">1</span>:<span class="number">4</span>]) * r1[i] + aimed_date_new.iloc[</span><br><span class="line">number - <span class="number">1</span> - i, <span class="number">1</span>:<span class="number">4</span>]</span><br><span class="line">new_point_last = pd.concat([new_point_last, new_x], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> new_point_last</span><br></pre></td></tr></table></figure>
<p>最后，再根据smote的计算公式<code>new_x=old_x+rand()*(append_x-old_x)</code>，计算出新的点即可，python练手到此就结束了</p>
<p>其实，在这个结果上，我们可以综合Tomek link做一个集成的数据扩充的算法，思路如下：<br>假设，我们利用上述的算法产生了两个青色方框的新数据点：<br><img src="/2017/12/01/SMOTE算法/7.png" alt=""><br><strong>我们认为，对于新产生的青色数据点与其他非青色样本点距离最近的点，构成一对Tomek link</strong>，如下图框中的青蓝两点</p>
<p><img src="/2017/12/01/SMOTE算法/8.png" alt=""><br>我们可以定义规则：<br>当以新产生点为中心，Tomek link的距离为范围半径，去框定一个空间，空间内的<code>少数类的个数/多数类的个数</code>&lt;最低阀值的时候，认为新产生点为“垃圾点”，应该剔除或者再次进行smote训练；空间内的<code>少数类的个数/多数类的个数</code>&gt;=最低阀值的时候,在进行保留并纳入smote训练的初始少类样本集合中去抽样<br>所以，剔除左侧的青色新增点，只保留右边的新增数据如下：</p>
<p><img src="/2017/12/01/SMOTE算法/9.png" alt=""></p>
<p>参考文献：</p>
<ul>
<li><a href="https://www.jair.org/media/953/live-953-2037-jair.pdf" target="_blank" rel="noopener">https://www.jair.org/media/953/live-953-2037-jair.pdf</a></li>
<li><a href="https://github.com/fmfn/UnbalancedDataset" target="_blank" rel="noopener">https://github.com/fmfn/UnbalancedDataset</a></li>
<li>Batista, G. E., Bazzan, A. L., &amp; Monard, M. C. (2003, December). Balancing Training Data for Automated Annotation of Keywords: a Case Study. In WOB (pp. 10-18).</li>
<li>Batista, G. E., Prati, R. C., &amp; Monard, M. C. (2004). A study of the behavior of several methods for balancing machine learning training data. ACM Sigkdd Explorations Newsletter, 6(1), 20-29.</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python开发：特征工程代码模版(二)]]></title>
      <url>/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%BA%8C/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/01/python开发：特征工程代码模版-二/1.jpg" alt=""><br><a id="more"></a><br><strong>update:</strong><br><strong>17.12.21 : Mutual Information互信息中mic_entroy函数里的I应该是i，已修正</strong></p>
<hr>
<p><strong>转载请注明文章来源：<a href="http://www.jianshu.com/writer#/notebooks/14156301/notes/20406464/preview" target="_blank" rel="noopener">python开发：特征工程代码模版（二）</a>，你们免费转我文章，不标注来源就算了，现在还开始写“原创”，这就过分了～</strong></p>
<p>正题开始：<br>这篇文章是入门级的特征处理的打包解决方案的python实现汇总，如果想get一些新鲜血液的朋友可以叉了，只是方便玩数据的人进行数据<strong>特征筛选</strong>的代码集合，话不多说，让我们开始。</p>
<hr>
<p>首先，让我们看一张入门级别的数据预处理的基本操作图，网上有很多版本，这个是我自己日常干活的时候必操作的行为罗列，其中<a href="http://www.jianshu.com/p/1f2f887f0811" target="_blank" rel="noopener">数据整理部分</a>已经在上一篇文章中给出了，下面我们讲一起来看看特征筛选这块。<strong>此图请尊重一下我，别拿出去传播，纯属个人的方法论，大家看看就行，谢谢。</strong>网上有其他版本的，你们去传播那些就ok了～<br><img src="/2017/12/01/python开发：特征工程代码模版-二/2.png" alt="特征工程"></p>
<hr>
<h3 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">var_filter</span><span class="params">(data, k=None)</span>:</span></span><br><span class="line">var_data = data.var().sort_values()</span><br><span class="line"><span class="keyword">if</span> k <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">new_data = VarianceThreshold(threshold=k).fit_transform(data)</span><br><span class="line"><span class="keyword">return</span> var_data, new_data</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> var_data</span><br></pre></td></tr></table></figure>
<p>这个方法的思路很明确，我们筛掉方差过小的feature，也很好理解，一列值完全或者几乎完全一致的feature对于我们去训练最后的模型没有任何好处。熵理论也同样印证了这一点。</p>
<h3 id="线性相关系数衡量"><a href="#线性相关系数衡量" class="headerlink" title="线性相关系数衡量"></a>线性相关系数衡量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearson_value</span><span class="params">(data, label, k=None)</span>:</span></span><br><span class="line">label = str(label)</span><br><span class="line"><span class="comment"># k为想删除的feature个数</span></span><br><span class="line">Y = data[label]</span><br><span class="line">x = data[[x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]]</span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">data_res = np.c_[Y, x.iloc[:, i]].T</span><br><span class="line">cor_value = np.abs(np.corrcoef(data_res)[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">res.append([label, x.columns[i], cor_value])</span><br><span class="line">res = sorted(np.array(res), key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>])</span><br><span class="line"><span class="keyword">if</span> k <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line"><span class="keyword">if</span> k &lt; len(res):</span><br><span class="line">new_c = []  <span class="comment"># 保留的feature</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(res) - k):</span><br><span class="line">new_c.append(res[i][<span class="number">1</span>])</span><br><span class="line"><span class="keyword">return</span> res, new_c</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">'feature个数越界～'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>当你明确了自变量与因变量之间存在线性关系的时候，你就需要剔除掉一些关心比较弱的变量，奥卡姆剃刀原理告诉我们，在尽可能压缩feature个数大小的情况下去得到效果最优的模型才是合理模型。</p>
<h3 id="共线性检验"><a href="#共线性检验" class="headerlink" title="共线性检验"></a>共线性检验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vif_test</span><span class="params">(data, label, k=None)</span>:</span></span><br><span class="line">label = str(label)</span><br><span class="line"><span class="comment"># k为想删除的feature个数</span></span><br><span class="line">x = data[[x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]]</span><br><span class="line">res = np.abs(np.corrcoef(x.T))</span><br><span class="line">vif_value = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">0</span>]):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(res.shape[<span class="number">0</span>]):</span><br><span class="line"><span class="keyword">if</span> j &gt; I:</span><br><span class="line">vif_value.append([x.columns[i], x.columns[j], res[i, j]])</span><br><span class="line">vif_value = sorted(vif_value, key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>])</span><br><span class="line"><span class="keyword">if</span> k <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line"><span class="keyword">if</span> k &lt; len(vif_value):</span><br><span class="line">new_c = []  <span class="comment"># 保留的feature</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line"><span class="keyword">if</span> vif_value[-i][<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> new_c:</span><br><span class="line">new_c.append(vif_value[-i][<span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">new_c.append(vif_value[-i][<span class="number">0</span>])</span><br><span class="line"><span class="keyword">if</span> len(new_c) == k:</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">out = [x <span class="keyword">for</span> x <span class="keyword">in</span> x.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> new_c]</span><br><span class="line"><span class="keyword">return</span> vif_value, out</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">'feature个数越界～'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> vif_value</span><br></pre></td></tr></table></figure>
<p>2-3年前面试必考题，什么叫做共线性？如何解决共线性？答案之一就是共线性检验啊，判断feature之间的相关性，剔除相关性较高的feature，在R语言里面有个VIF函数可以直接求的。除此之外，采用非线性函数做特征拆解也是很好的方法。共线性严重的情况下，会导致泛化误差异常大，需着重注意～</p>
<h3 id="Mutual-Information互信息"><a href="#Mutual-Information互信息" class="headerlink" title="Mutual Information互信息"></a>Mutual Information互信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MI</span><span class="params">(X, Y)</span>:</span></span><br><span class="line"><span class="comment"># len(X) should be equal to len(Y)</span></span><br><span class="line"><span class="comment"># X,Y should be the class feature</span></span><br><span class="line">total = len(X)</span><br><span class="line">X_set = set(X)</span><br><span class="line">Y_set = set(Y)</span><br><span class="line"><span class="keyword">if</span> len(X_set) &gt; <span class="number">10</span>:</span><br><span class="line">print(<span class="string">'%s非分类变量，请检查后再输入'</span> % X_set)</span><br><span class="line">sys.exit()</span><br><span class="line"><span class="keyword">elif</span> len(Y_set) &gt; <span class="number">10</span>:</span><br><span class="line">print(<span class="string">'%s非分类变量，请检查后再输入'</span> % Y_set)</span><br><span class="line">sys.exit()</span><br><span class="line"><span class="comment"># Mutual information</span></span><br><span class="line">MI = <span class="number">0</span></span><br><span class="line">eps = <span class="number">1.4e-45</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> X_set:</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> Y_set:</span><br><span class="line">indexi = np.where(X == i)</span><br><span class="line">indexj = np.where(Y == j)</span><br><span class="line">ijinter = np.intersect1d(indexi, indexj)</span><br><span class="line">px = <span class="number">1.0</span> * len(indexi[<span class="number">0</span>]) / total</span><br><span class="line">py = <span class="number">1.0</span> * len(indexj[<span class="number">0</span>]) / total</span><br><span class="line">pxy = <span class="number">1.0</span> * len(ijinter) / total</span><br><span class="line">MI = MI + pxy * np.log2(pxy / (px * py) + eps)</span><br><span class="line"><span class="keyword">return</span> MI</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic_entroy</span><span class="params">(data, label)</span>:</span></span><br><span class="line"><span class="comment"># mic_value值越小，两者相关性越弱</span></span><br><span class="line">label = str(label)</span><br><span class="line"><span class="comment"># k为想删除的feature个数</span></span><br><span class="line">x = data[[x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]]</span><br><span class="line">Y = data[label]</span><br><span class="line">mic_value = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> len(set(x.iloc[:, i])) &lt;= <span class="number">10</span>:</span><br><span class="line">res = MI(Y, x.iloc[:, i])</span><br><span class="line">mic_value.append([x.columns[i], res])</span><br><span class="line">mic_value = sorted(mic_value, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">return</span> mic_value</span><br></pre></td></tr></table></figure>
<p>本来我想偷懒，直接<code>import minepy</code>然后就得了，发现真的是特么难装，各种报错，一怒之下自己写了，这边求大佬告知，为什么<code>pip install minepy</code>会有这样的问题：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun</span><br><span class="line">error: command <span class="string">'/usr/bin/clang'</span> failed <span class="keyword">with</span> exit status <span class="number">1</span></span><br><span class="line">----------------------------------------</span><br><span class="line">Command <span class="string">"/Users/slade/anaconda3/bin/python -u -c "</span></span><br><span class="line"><span class="keyword">import</span> setuptools, tokenize;__file__=<span class="string">'/private/var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-build-hr9ej0lw/minepy/setup.py'</span>;</span><br><span class="line">f=getattr(tokenize, <span class="string">'open'</span>, open)(__file__);</span><br><span class="line">code=f.read().replace(<span class="string">'\r\n'</span>, <span class="string">'\n'</span>);f.close();</span><br><span class="line">exec(compile(code, __file__, <span class="string">'exec'</span>))<span class="string">" install --record /var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-30cn7rbs-record/install-record.txt --single-version-externally-managed --compile"</span> failed <span class="keyword">with</span> error code <span class="number">1</span> <span class="keyword">in</span> /private/var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-build-hr9ej0lw/minepy/</span><br></pre></td></tr></table></figure></p>
<p>回到正题，互信息其实很简单，我们看个公式I(X;Y)=H(X)-H(X|Y)，看完是不是超级清晰了，其实就是X发生的概率中去掉Y发生后X发生的概率，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。<br>计算公式如下，你们也可以在上面的代码里找到影子。<br><img src="/2017/12/01/python开发：特征工程代码模版-二/3.jpeg" alt="特征工程"><br>最后还是吐槽下，这个minepy太难装了，为了个互信息，不至于不至于～</p>
<h3 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapper_way</span><span class="params">(data, label, k=<span class="number">3</span>)</span>:</span></span><br><span class="line"><span class="comment"># k 为要保留的数据feature个数</span></span><br><span class="line">label = str(label)</span><br><span class="line">label_data = data[label]</span><br><span class="line">col = [x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]</span><br><span class="line">train_data = data[col]</span><br><span class="line">res = pd.DataFrame(</span><br><span class="line">RFE(estimator=LogisticRegression(), n_features_to_select=k).fit_transform(train_data, label_data))</span><br><span class="line">res_c = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> (res.iloc[:, i] - data.iloc[:, j]).sum() == <span class="number">0</span>:</span><br><span class="line">res_c.append(data.columns[j])</span><br><span class="line">res.columns = res_c</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这边开始的代码就基本上是方法梳理了，没啥亮点，我就大概和大家聊聊，递归特征消除法，用R语言里面的step()函数是一毛一样的东西，都是循环sample特征，选一个对于当前模型，特征组合最好的结果。如果数据量大，你会有非一般的感觉，这边就有小trick了，以后有空可以和大家分享～</p>
<h3 id="l1-l2正则方法"><a href="#l1-l2正则方法" class="headerlink" title="l1/l2正则方法"></a>l1/l2正则方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedded_way</span><span class="params">(data, label, way=<span class="string">'l2'</span>, C_0=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">label = str(label)</span><br><span class="line">label_data = data[label]</span><br><span class="line">col = [x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]</span><br><span class="line">train_data = data[col]</span><br><span class="line">res = pd.DataFrame(SelectFromModel(LogisticRegression(penalty=way, C=C_0)).fit_transform(train_data, label_data))</span><br><span class="line">res_c = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> (res.iloc[:, i] - data.iloc[:, j]).sum() == <span class="number">0</span>:</span><br><span class="line">res_c.append(data.columns[j])</span><br><span class="line">res.columns = res_c</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>正则理论参考：<a href="http://www.jianshu.com/p/4f91f0dcba95" target="_blank" rel="noopener">总结：常见算法工程师面试题目整理(二)</a>，这边要提一点，并不是所有情况下都需要正则预处理的，很多算法自带正则，比如logistic啊，比如我们自己去写tensorflow神经网络啊，模型会针对性的解决问题，而这边单纯用的logstic方法来筛选，相对而言内嵌的效果会更好的。</p>
<h3 id="基于树模型特征选择"><a href="#基于树模型特征选择" class="headerlink" title="基于树模型特征选择"></a>基于树模型特征选择</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tree_way</span><span class="params">(data,label)</span>:</span></span><br><span class="line">label = str(label)</span><br><span class="line">label_data = data[label]</span><br><span class="line">col = [x <span class="keyword">for</span> x <span class="keyword">in</span> data.columns <span class="keyword">if</span> x != label]</span><br><span class="line">train_data = data[col]</span><br><span class="line">res = pd.DataFrame(SelectFromModel(GradientBoostingClassifier()).fit_transform(train_data, label_data))</span><br><span class="line">res_c = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(res.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> (res.iloc[:, i] - data.iloc[:, j]).sum() == <span class="number">0</span>:</span><br><span class="line">res_c.append(data.columns[j])</span><br><span class="line">res.columns = res_c</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这边用的是决策树每次分支下，如果改变一列值为随机值，观察对整体数据效果的影响。举个通俗易懂的例子，看看你在公司的重要性，就去和你老板提离职，要是老板疯狂给你加工资做你的思想工作，代表你很重要；如果你的老板让你去财务结账，代表你没啥意义。这里你就是这个feature，你老板就是数据效果的检验指标，常见的就是oob之类的。</p>
<p>这边facebook有个非常好的拓展的思路，但是大家都吹的多实际应用很少，我最近在搞这事情，等下更完这边的特征工程和下面一个nlp的case后，我想专门聊聊这个事情，用的就是决策树的另一角度，以叶子结点代替原feature，做到了非线性的特征融入线性模型，虽然很老套，但是我稍稍做了测试，效果斐然：<br><img src="/2017/12/01/python开发：特征工程代码模版-二/4.png" alt="特征工程"></p>
<p>最后的最后，感谢大家阅读，希望能够给大家带来收获，谢谢～</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[多算法识别撞库刷券等异常用户]]></title>
      <url>/2017/12/01/%E5%A4%9A%E7%AE%97%E6%B3%95%E8%AF%86%E5%88%AB%E6%92%9E%E5%BA%93%E5%88%B7%E5%88%B8%E7%AD%89%E5%BC%82%E5%B8%B8%E7%94%A8%E6%88%B7/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/01/多算法识别撞库刷券等异常用户/1.jpg" alt=""><br>在运营业务中，绝大多数公司会面临恶意注册，恶意刷接口，恶意刷券等流量问题，此类问题的常规解决方案都是拍定单位时间内的ip访问上限次数、qps上限次数等等，会存在误伤、频繁修改阀值等问题。<br><a id="more"></a></p>
<h1 id="问题剖析："><a href="#问题剖析：" class="headerlink" title="问题剖析："></a><strong>问题剖析：</strong></h1><p>此类问题的关键在识别出与正常数据集群差异较大的离群点。所以，存在两个难点：</p>
<ul>
<li>1.难以找到一个很清晰的边界，界定什么是正常用户，什么是异常用户</li>
<li>2.维数灾难及交叉指标计算之间的高频计算性能瓶颈</li>
</ul>
<p><strong>算法概述：</strong></p>
<ul>
<li>1.图形位置分布</li>
<li>2.统计方法检测</li>
<li>3.距离位置检测</li>
<li>4.密度位置检测</li>
<li>5.无监督模型识别</li>
</ul>
<h1 id="算法详述："><a href="#算法详述：" class="headerlink" title="算法详述："></a><strong>算法详述：</strong></h1><h2 id="图形位置分布"><a href="#图形位置分布" class="headerlink" title="图形位置分布"></a><strong>图形位置分布</strong></h2><p>当我们不需要长期监控异常用户，只需要少数几次识别异常用户，且精度要求不高的时候，我们可以采取简单方便的图形识别方式，例如：箱式图。</p>
<p><img src="/2017/12/01/多算法识别撞库刷券等异常用户/2.png" alt=""></p>
<p>箱式图判断中，一般我们只需要锁定25%(Q1)分位点的用户特征值，75%(Q3)分位点的用户特征值，Q3与Q1之间的位差即为IQR，一般认定Q3+1.5个IQR外的点即为异常点，对应的用户即为异常用户。这种方法也叫做“盖帽法”，不必人为设定上限阀值，随着用户的数据变化而变化上界，避免了高频修改的问题，只是精度欠缺且绝大多数情况下识别出的异常用户较少。</p>
<p>方法比较简单，也不多加解释了。</p>
<h2 id="统计方法检测"><a href="#统计方法检测" class="headerlink" title="统计方法检测"></a><strong>统计方法检测</strong></h2><p>方法也比较简单，上线开发简单。一直是分两步：</p>
<ul>
<li>先假设全量数据服从一定的分布，比如常见的正太分布，泊松分布等</li>
<li>在计算每个点属于这个分布的概率，也就是大家常用的以平均值和方差定密度函数的问题</li>
</ul>
<p><img src="/2017/12/01/多算法识别撞库刷券等异常用户/3.png" alt=""></p>
<p>因为这边，我们前期无法知道数据服从什么样的分布，所以，我们这边可以用切比雪夫不等式来代替确定的分布形式。除此之外，也就是同时用了马氏距离来衡量了每个具体的点在整体数据集中的位置。</p>
<p><img src="/2017/12/01/多算法识别撞库刷券等异常用户/4.png" alt=""></p>
<p>核心代码就是下面这个协方差矩阵及矩阵相乘：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#两个维度之间协方差矩阵</span></span><br><span class="line">S=np.cov(X)</span><br><span class="line"><span class="comment">#协方差矩阵的逆矩阵</span></span><br><span class="line">SI = np.linalg.inv(S)</span><br><span class="line"><span class="comment">#第一次计算全量用户的维度重心</span></span><br><span class="line">XTmean = XT.mean(axis=<span class="number">0</span>)</span><br><span class="line">d1=[]</span><br><span class="line">n  = XT.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">delta = XT[i] - XTmean</span><br><span class="line">d = np.sqrt(np.dot(np.dot(delta,SI),delta.T))</span><br><span class="line">d1.append(d)</span><br></pre></td></tr></table></figure></p>
<p>这个方法的最核心的优点就是对全量数据进行了分块，可以理解为将1拆分成了必定有问题的1/m用户，可能有问题的1/n用户，必定没问题的1/w用户（1/m+1/n+1/w=1），这也奠定了后续更好的方法的基础。<br>但是问题也是很明显的，对于1/m，1/n的大小确定无法非常的精准，多了则影响正常用户，少了则无法准确拦截，还是一个划分的算法，并不能给出每个人的好坏程度。</p>
<h2 id="距离位置检测"><a href="#距离位置检测" class="headerlink" title="距离位置检测"></a><strong>距离位置检测</strong></h2><p>距离探测的方法有一个非常强的假设，正常的用户都比较集中，有较多的邻居，而异常用户都特立独行。<br>在常见的业务问题中都是满足的，比如对爬虫ip的识别，撞库的识别，这些一看那些高频访问的就不是正常用户，但是对于特别稀疏的业务场景，比如企业融资，高深度的敏感页面访问，均不是很适用，它们的频次较低无法构成一个邻居的概念。</p>
<p>这边非常常用的有2种，一个是连续特征间的欧式距离（标准化下的欧式距离（马氏距离）），另一个是名义变量下的余弦相似度。</p>
<p>这边只讨论第一种情况，连续特征下如何衡量数据是否为异常数据。前面我们也说到了，切比雪夫不等式的方法能够有效的划分出三个类别正常用户，异常用户，未知用户。所以，相应的，我们只需要在未知用户的集群里面去寻找与正常用户更不相似的，或者和异常用户更相似的用户就可以了。</p>
<p>对于单变量衡量：<br><img src="/2017/12/01/多算法识别撞库刷券等异常用户/5.png" alt=""></p>
<p>对于多变量衡量：<br><img src="/2017/12/01/多算法识别撞库刷券等异常用户/6.png" alt=""></p>
<p>核心计算相似度的方式就是以上两个公式，会有一些细节处理的问题及注意点，大家可自行研究。</p>
<h2 id="密度位置检测"><a href="#密度位置检测" class="headerlink" title="密度位置检测"></a><strong>密度位置检测</strong></h2><p>这边先等下谈原理，较为冗长，先说结论，其实，在能够使用距离位置检测的情况下，优先使用距离位置检测的方法。密度方法的前提几乎与位置方法的前提一致，但是在计算量级上而言，存在较大的差异差别。</p>
<p><img src="/2017/12/01/多算法识别撞库刷券等异常用户/7.png" alt=""><br>上述的图片是Fei Tony Liu, Kai Ming Ting, Zhi-Hua Zhou的一篇论文里面对比的常见的iForest,ORCA,LOF(也就是密度位置检测),RF方法的准确率和耗时情况，也清晰的可以看出，同为距离衡量的ORCA的耗时较大，但是LOF的耗时更高，甚至部分情况下都无法计算出结果。</p>
<p>下面让我们看下理论先，密度位置检测的方法之一，LOF：<br>概念定义：<br>1) d(p,o)：两点p和o之间的距离；<br>2) k-distance：第k距离<br>　　　　对于点p的第k距离dk(p)定义如下：<br>　　　　dk(p)=d(p,o)，并且满足：<br>　　　　　　　　　　a) 在集合中至少有不包括p在内的k个点o,∈C{x≠p}， 满足d(p,o,)≤d(p,o) ；<br>　　　　　　　　　　　　　　　　b) 在集合中最多有不包括p在内的k−1个点o,∈C{x≠p}，满足d(p,o,)<d(p,o) ；="" 　　　　　　　　　　　　　　　　="" 　　　　　　　　　　　　　　　　上面两个条件总结起来就是1.距离范围内至少满足一定数量的点数，2.最多允许有一个距离最大的非p点="" 　　　　　　　　　　　　　　　　形象的看，距离就是p的第k距离，也就是距离p第k远的点的距离，不包括p，如下图箭头的路径长度。="" 　　　　　　　　　　　　　　　　![](多算法识别撞库刷券等异常用户="" 8.png)="" 　　　　　　　　　　　　　　　　3)="" k-distance="" neighborhood="" of="" p：第k距离邻域="" 　　　　　　　　　　　　　　　　　　　　点p的第k距离邻域nk(p)，就是p的第k距离即以内的所有点，包括第k距离。="" 　　　　　　　　　　　　　　　　　　　　　　　　因此p的第k邻域点的个数记为="" |nk(p)|，且|nk(p)|≥k="" 　　　　　　　　　　　　　　　　　　　　　　　　="" 　　　　　　　　　　　　　　　　　　　　　　　　我们在定义一些衡量指标，那么lof就算是完成了：="" 　　　　　　　　　　　　　　　　　　　　　　　　1、可达距离（reach-distance）="" 　　　　　　　　　　　　　　　　　　　　　　　　点o到点p的第k可达距离定义为：="" 　　　　　　　　　　　　　　　　　　　　　　　　reach-distancek(p,o)="max{k−distance(o),d(p,o)}" 　　　　　　　　　　　　　　　　　　　　　　　　2、局部可达密度（local="" reachablility="" density）="" 　　　　　　　　　　　　　　　　　　　　　　　　点p处的局部可达密度为：="" 　　　　　　　　　　　　　　　　　　　　　　　　![](多算法识别撞库刷券等异常用户="" 9.png)="" 　　　　　　　　　　　　　　　　　　　　　　　　其中，|nk(p)|为p的第k领域点的个数，∑o∈nk(p)reach-distk(p,o)计算的是p的k领域内的点到p的可达距离，也就是1中涉及的计算方式。="" 　　　　　　　　　　　　　　　　　　　　　　　　3、局部离群因子（local="" outlier="" factor）="" 　　　　　　　　　　　　　　　　　　　　　　　　点p的局部离群因子为：="" 　　　　　　　　　　　　　　　　　　　　　　　　lof（p）="（∑o∈Nk(p)lrdk(o)/lrdk(p)）/|Nk(p)|" 　　　　　　　　　　　　　　　　　　　　　　　　其中，lrdk(o)="" lrdk(p)比值衡量了p点与附近的点之间的密切差异情况，lof值="1时，代表p与p附近的点密度一致；LOF值<1时，代表p点的密度大于p附近点的密度；LOF值">1时，代表p点的密度小于p附近点的密度，也是非常符合我们的前提假设的，异常点总是比较稀疏，正常点总是比较稠密的。<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　到此位置LOF的数学理论就完成了，让我们回顾一下它的思想。它其实就是找数据集合中的每一个点及其邻居的点，计算它和它的邻居的密度，当它的密度大于等于它邻居的密度的时候，则认为它是稠密中心，是正常用户数据；否则异常。<br>　　　　　　　　　　　　　　　　　　　　　　　　但是要计算每个点及对应的邻居的LOF值，计算成本也是非常的高的，最初我们也指出了这一点。<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　核心代码：<br>　　　　　　　　　　　　　　　　　　　　　　　　<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="function"><span class="keyword">def</span> <span class="title">k_distance</span><span class="params">(k, instance, instances, distance_function=distance_euclidean)</span>:</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　distances = &#123;&#125;</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">for</span> instance2 <span class="keyword">in</span> instances:</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　distance_value = distance_function(instance, instance2)</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">if</span> distance_value <span class="keyword">in</span> distances:</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　distances[distance_value].append(instance2)</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">else</span>:</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　distances[distance_value] = [instance2]</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　distances = sorted(distances.items())</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　neighbours = []</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　k_sero = <span class="number">0</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　k_dist = <span class="keyword">None</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">for</span> dist <span class="keyword">in</span> distances:</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　k_sero += len(dist[<span class="number">1</span>])</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　neighbours.extend(dist[<span class="number">1</span>])</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　k_dist = dist[<span class="number">0</span>]</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">if</span> k_sero &gt;= k:</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">break</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">return</span> k_dist, neighbours</span><br></pre></td></tr></table></figure></d(p,o)></p>
<p>　　　　　　　　　　　　　　　　　　　　　　　　## <strong>无监督模型识别</strong><br>　　　　　　　　　　　　　　　　　　　　　　　　其实这边说完全的无监督，我觉得不是很准确，我觉得叫“半监督”可能更好一些。<br>　　　　　　　　　　　　　　　　　　　　　　　　这边方法很多，我只介绍两种：<br>　　　　　　　　　　　　　　　　　　　　　　　　1.Iforest<br>　　　　　　　　　　　　　　　　　　　　　　　　2.RNN<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　先让我们看下Iforest：<br>　　　　　　　　　　　　　　　　　　　　　　　　算法的关键在于:对于一个有若干维的数据集合，对于其中的任一维度，如果该维度是连续属性的话，在若干次随机二分类后，边界稀疏点最容易优先达到子叶节点,如下图：<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　<img src="/2017/12/01/多算法识别撞库刷券等异常用户/10.png" alt=""><br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　算法实现详细的过程为：<br>　　　　　　　　　　　　　　　　　　　　　　　　假设数据集有N条数据，构建一颗iTree时，从N条数据中均匀抽样(一般是无放回抽样)出m(通常为256)个样本出来，作为这颗树的训练样本。在样本中，随机选一个特征，并在这个特征的所有值范围内(最小值与最大值之间)随机选一个值，对样本进行二叉划分，将样本中小于该值的划分到节点的左边，大于等于该值的划分到节点的右边,重复以上划分步骤，直到达到划分层数上限log(m)或者节点内只有一个样本，一棵树Itree的结果往往是不可信的，所以我们可以训练100-255棵树，最后整合所以树的结果取平均的深度作为输出深度，也叫做Isolation Forest。<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　有了算法逻辑，再看衡量指标：<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　<img src="/2017/12/01/多算法识别撞库刷券等异常用户/11.png" alt=""><br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　其中，h(x)为x对应的节点深度，c(n)为样本可信度，s(x,n)~[0,1]，正常数据来讲s(x,n)小于0.8，s(x,n)越靠近1，数据异常的可能性越大。（这边需要注意，在sklearn中的Isolation是取得相反的逻辑，score越小数据异常的可能性越大。）<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　这边也贴上核心代码：<br>　　　　　　　　　　　　　　　　　　　　　　　　<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None, sample_weight=None)</span>:</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　X = check_array(X, accept_sparse=[<span class="string">'csc'</span>], ensure_2d=<span class="keyword">False</span>)</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">if</span> issparse(X):</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="comment"># Pre-sort indices to avoid that each individual tree of the</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="comment"># ensemble sorts the indices.</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　X.sort_indices()</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　rnd = check_random_state(self.random_state)</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　y = rnd.uniform(size=X.shape[<span class="number">0</span>])</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="comment"># ensure that max_sample is in [1, n_samples]:</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">if</span> <span class="keyword">not</span> (self.max_samples &lt;= n_samples):</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　warn(<span class="string">"max_samples is larger than the total number of samples"</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="string">" n_samples. Corrected as max_samples=n_samples"</span>)</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　self.max_samples = n_samples</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">if</span> <span class="keyword">not</span> (<span class="number">0</span> &lt; self.max_samples):</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">raise</span> ValueError(<span class="string">"max_samples has to be positive"</span>)</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　super(IsolationForest, self).fit(X, y, sample_weight=sample_weight)</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">return</span> self</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="function"><span class="keyword">def</span> <span class="title">_cost</span><span class="params">(self, n)</span>:</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">return</span> <span class="number">1.</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">else</span>:</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　harmonic_number = np.log(n) + <span class="number">0.5772156649</span></span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　<span class="keyword">return</span> <span class="number">2.</span> * harmonic_number - <span class="number">2.</span> * (n - <span class="number">1.</span>) / n</span><br></pre></td></tr></table></figure></p>
<p>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　2.RNN<br>　　　　　　　　　　　　　　　　　　　　　　　　通常我们会以5层的卷积神经网络作为训练网络。我在这边处理之前将切比雪夫不等式划分出来的正常用户作为0-output，异常用户作为1-output，然后尽可能的降低损失函数的误差即可。<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　<img src="/2017/12/01/多算法识别撞库刷券等异常用户/12.png" alt=""><br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　第一层是常规层，将不同的input做线性组合：<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　<img src="/2017/12/01/多算法识别撞库刷券等异常用户/13.png" alt=""><br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　第二层、第四层是做数据非线性变化：<br>　　　　　　　　　　　　　　　　　　　　　　　　这边选用的是tanh函数<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　第三层是做梯度分层下的非线性变化，抹平相似特征间的ouput：<br>　　　　　　　　　　　　　　　　　　　　　　　　<img src="/2017/12/01/多算法识别撞库刷券等异常用户/14.png" alt=""><br>　　　　　　　　　　　　　　　　　　　　　　　　其中，k为3，N为想要分的梯度的个数，a3为一个阶梯跳跃到另一个阶梯的转换效率，形如：<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　<img src="/2017/12/01/多算法识别撞库刷券等异常用户/15.png" alt=""><br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　第五层，也就是最后一层通过sigmoid进行0-1之间的压缩。<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　这边的损失函数用的是常见的mse：<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　<img src="/2017/12/01/多算法识别撞库刷券等异常用户/16.png" alt=""><br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　当通过测试数据训练完成后，再将未知数据进行模型训练，观察得到结果的大小，越靠近1，越有可能为异常用户。<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　<em>**</em><br>　　　　　　　　　　　　　　　　　　　　　　　　以上就是5种常见的只基于数据下的异常用户的识别，更偏方法技术一点，但是无论是算法实现还是业务应用中，同样需要注意输入特征的问题。由于大家运用方向不同，就不细节赘述。<br>　　　　　　　　　　　　　　　　　　　　　　　　<br>　　　　　　　　　　　　　　　　　　　　　　　　详细实现demo可以私信我要，最后谢谢大家阅读了。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 风控 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python开发：特征工程代码模版(一)]]></title>
      <url>/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%B8%80/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/01/python开发：特征工程代码模版-一/1.jpg" alt=""><br><a id="more"></a><br>作为一个算法工程师，我们接的业务需求不会比数据分析挖掘工程师少，作为一个爱偷懒的人，总机械重复的完成一样的预处理工作，我是不能忍的，所以在最近几天，我正在完善一些常规的、通用的预处理的code，方便我们以后在每次分析之前直接import快速搞定，省的每次都要去做一样的事情。</p>
<p><strong>如果大家有什么想实现但是懒得去弄的预处理的步骤也可以私信我，我相对而言闲暇还是有的（毕竟工资少工作也不多，摊手：《），我开发完成后直接贴出来，大家以后一起用就行了</strong></p>
<p>我们需要预加载这些包，而且接下来所有的操作均在dataframe格式下完成，所以我们需要将数据先处理成dataframe格式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'slade_sal'</span></span><br><span class="line">__time__ = <span class="string">'20171128'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_data_format</span><span class="params">(data)</span>:</span></span><br><span class="line"><span class="comment"># 以下预处理都是基于dataframe格式进行的</span></span><br><span class="line">data_new = pd.DataFrame(data)</span><br><span class="line"><span class="keyword">return</span> data_new</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="空值处理"><a href="#空值处理" class="headerlink" title="空值处理"></a>空值处理</h1><p>接下来就开始我们的正题了，首先，我们需要判断哪些列是空值过多的，当一列数据的空值占列数的40%以上（经验值），这列能够带给我们的信息就不多了，所以我们需要把某个阀值（rate_base）以上的空值个数的列干掉，如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 去除空值过多的feature</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_remove</span><span class="params">(data, rate_base=<span class="number">0.4</span>)</span>:</span></span><br><span class="line">all_cnt = data.shape[<span class="number">0</span>]</span><br><span class="line">avaiable_index = []</span><br><span class="line"><span class="comment"># 针对每一列feature统计nan的个数，个数大于全量样本的rate_base的认为是异常feature，进行剔除</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line">rate = np.isnan(np.array(data.iloc[:, i])).sum() / all_cnt</span><br><span class="line"><span class="keyword">if</span> rate &lt;= rate_base:</span><br><span class="line">avaiable_index.append(i)</span><br><span class="line">data_available = data.iloc[:, avaiable_index]</span><br><span class="line"><span class="keyword">return</span> data_available, avaiable_index</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="离群点处理"><a href="#离群点处理" class="headerlink" title="离群点处理"></a>离群点处理</h1><p>把空值过多的列去完之后，我们需要考虑将一些特别离群的点去掉，这边需要注意两点：</p>
<ul>
<li>异常值分析类的场景禁止使用这步，比如信用卡评分，爬虫识别等，你如果采取了这步，还怎么去分离出这些异常啊</li>
<li>容忍度高的算法不建议使用这步，比如svm里面已经有了支持向量机这个东西，你如果采取了这步的离群识别的操作会改变原分布而且svm里面决定超平面的核心与离群点无关，后接函数会引发意想不到的彩蛋～</li>
</ul>
<p>这边采取盖帽法与额定的分位点方法，建议组合使用，用changed_feature_box定义需要采用盖帽法的列的index_num，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 离群点盖帽</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outlier_remove</span><span class="params">(data, limit_value=<span class="number">10</span>, method=<span class="string">'box'</span>, percentile_limit_set=<span class="number">90</span>, changed_feature_box=[])</span>:</span></span><br><span class="line"><span class="comment"># limit_value是最小处理样本个数set，当独立样本大于limit_value我们认为非可onehot字段</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">feature_change = []</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'box'</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line"><span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">return</span> data, feature_change</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'self_def'</span>:</span><br><span class="line"><span class="comment"># 快速截断</span></span><br><span class="line"><span class="keyword">if</span> len(changed_feature_box) == <span class="number">0</span>:</span><br><span class="line"><span class="comment"># 当方法选择为自定义，且没有定义changed_feature_box则全量数据全部按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 如果定义了changed_feature_box，则将changed_feature_box里面的按照box方法，changed_feature_box的feature index按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">in</span> changed_feature_box:</span><br><span class="line">q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line"><span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">return</span> data, feature_change</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="空值填充"><a href="#空值填充" class="headerlink" title="空值填充"></a>空值填充</h1><p>在此之后，我们需要对空值进行填充，这边方法就很多很多了，我这边实现的是基本的，分了连续feature和分类feature，分别针对continuous feature采取mean,min,max方式，class feature采取one_hot_encoding的方式；除此之外还可以做分层填充，差分填充等等，那个比较定制化，如果有需要，我也可以搞一套，但是个人觉得意义不大。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 空feature填充</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_fill</span><span class="params">(data, limit_value=<span class="number">10</span>, countinuous_dealed_method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">normal_index = []</span><br><span class="line">continuous_feature_index = []</span><br><span class="line">class_feature_index = []</span><br><span class="line">continuous_feature_df = pd.DataFrame()</span><br><span class="line">class_feature_df = pd.DataFrame()</span><br><span class="line"><span class="comment"># 当存在空值且每个feature下独立的样本数小于limit_value，我们认为是class feature采取one_hot_encoding；</span></span><br><span class="line"><span class="comment"># 当存在空值且每个feature下独立的样本数大于limit_value，我们认为是continuous feature采取mean,min,max方式</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> np.isnan(np.array(data.iloc[:, i])).sum() &gt; <span class="number">0</span>:</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line"><span class="keyword">if</span> countinuous_dealed_method == <span class="string">'mean'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].mean())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'max'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].max())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'min'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].min())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt; <span class="number">0</span> <span class="keyword">and</span> len(</span><br><span class="line">pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">class_feature_df = pd.concat(</span><br><span class="line">[class_feature_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line">class_feature_index.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">normal_index.append(i)</span><br><span class="line">data_update = pd.concat([data.iloc[:, normal_index], continuous_feature_df, class_feature_df], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> data_update</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="one-hot-encoding过程"><a href="#one-hot-encoding过程" class="headerlink" title="one hot encoding过程"></a>one hot encoding过程</h1><p>分类feature的one hot encoding过程，常见操作，不多说<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># onehotencoding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ohe</span><span class="params">(data, limit_value=<span class="number">10</span>)</span>:</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">class_index = []</span><br><span class="line">class_df = pd.DataFrame()</span><br><span class="line">normal_index = []</span><br><span class="line"><span class="comment"># limit_value以下的均认为是class feature，进行ohe过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">class_index.append(i)</span><br><span class="line">class_df = pd.concat([class_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">normal_index.append(i)</span><br><span class="line">data_update = pd.concat([data.iloc[:, normal_index], class_df], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> data_update</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="Smote过程"><a href="#Smote过程" class="headerlink" title="Smote过程"></a>Smote过程</h1><p>正负样本不平衡的解决，这边我写的是smote，理论部分建议参考：<a href="http://www.jianshu.com/p/ecbc924860af" target="_blank" rel="noopener">Python：SMOTE算法</a>,其实简单的欠抽样和过抽样就可以解决，建议参考这边文章：<a href="http://www.jianshu.com/p/9a3b3104776e" target="_blank" rel="noopener">Python:数据抽样平衡方法重写</a>。都是一些老生常谈的问题了，不多说了，上代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'sladesal'</span></span><br><span class="line">__time__ = <span class="string">'20171110'</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">data : 原始数据</span></span><br><span class="line"><span class="string">tag_index : 因变量所在的列数，以0开始</span></span><br><span class="line"><span class="string">max_amount : 少类别类想要达到的数据量</span></span><br><span class="line"><span class="string">std_rate : 多类:少类想要达到的比例</span></span><br><span class="line"><span class="string">#如果max_amount和std_rate同时定义优先考虑max_amount的定义</span></span><br><span class="line"><span class="string">kneighbor : 生成数据依赖kneighbor个附近的同类点，建议不超过5个</span></span><br><span class="line"><span class="string">kdistinctvalue : 认为每列不同元素大于kdistinctvalue及为连续变量，否则为class变量</span></span><br><span class="line"><span class="string">method ： 生成方法</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smote</span><span class="params">(data, tag_index=None, max_amount=<span class="number">0</span>, std_rate=<span class="number">5</span>, kneighbor=<span class="number">5</span>, kdistinctvalue=<span class="number">10</span>, method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">data = pd.DataFrame(data)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line"><span class="keyword">raise</span> ValueError</span><br><span class="line">case_state = data.iloc[:, tag_index].groupby(data.iloc[:, tag_index]).count()</span><br><span class="line">case_rate = max(case_state) / min(case_state)</span><br><span class="line">location = []</span><br><span class="line"><span class="keyword">if</span> case_rate &lt; <span class="number">5</span>:</span><br><span class="line">print(<span class="string">'不需要smote过程'</span>)</span><br><span class="line"><span class="keyword">return</span> data</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 拆分不同大小的数据集合</span></span><br><span class="line">less_data = np.array(</span><br><span class="line">data[data.iloc[:, tag_index] == np.array(case_state[case_state == min(case_state)].index)[<span class="number">0</span>]])</span><br><span class="line">more_data = np.array(</span><br><span class="line">data[data.iloc[:, tag_index] == np.array(case_state[case_state == max(case_state)].index)[<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 找出每个少量数据中每条数据k个邻居</span></span><br><span class="line">neighbors = NearestNeighbors(n_neighbors=kneighbor).fit(less_data)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(less_data)):</span><br><span class="line">point = less_data[i, :]</span><br><span class="line">location_set = neighbors.kneighbors([less_data[i]], return_distance=<span class="keyword">False</span>)[<span class="number">0</span>]</span><br><span class="line">location.append(location_set)</span><br><span class="line"><span class="comment"># 确定需要将少量数据补充到上限额度</span></span><br><span class="line"><span class="comment"># 判断有没有设定生成数据个数，如果没有按照std_rate(预期正负样本比)比例生成</span></span><br><span class="line"><span class="keyword">if</span> max_amount &gt; <span class="number">0</span>:</span><br><span class="line">amount = max_amount</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">amount = int(max(case_state) / std_rate)</span><br><span class="line"><span class="comment"># 初始化，判断连续还是分类变量采取不同的生成逻辑</span></span><br><span class="line">times = <span class="number">0</span></span><br><span class="line">continue_index = []  <span class="comment"># 连续变量</span></span><br><span class="line">class_index = []  <span class="comment"># 分类变量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(less_data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; kdistinctvalue:</span><br><span class="line">continue_index.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">class_index.append(i)</span><br><span class="line">case_update = list()</span><br><span class="line">location_transform = np.array(location)</span><br><span class="line"><span class="keyword">while</span> times &lt; amount:</span><br><span class="line"><span class="comment"># 连续变量取附近k个点的重心，认为少数样本的附近也是少数样本</span></span><br><span class="line">new_case = []</span><br><span class="line">pool = np.random.permutation(len(location))[<span class="number">1</span>]</span><br><span class="line">neighbor_group = location_transform[pool]</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'mean'</span>:</span><br><span class="line">new_case1 = less_data[list(neighbor_group), :][:, continue_index].mean(axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 连续样本的附近点向量上的点也是异常点</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'random'</span>:</span><br><span class="line">away_index = np.random.permutation(len(neighbor_group) - <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">neighbor_group_removeorigin = neighbor_group[<span class="number">1</span>:][away_index]</span><br><span class="line">new_case1 = less_data[pool][continue_index] + np.random.rand() * (</span><br><span class="line">less_data[pool][continue_index] - less_data[neighbor_group_removeorigin][continue_index])</span><br><span class="line"><span class="comment"># 分类变量取mode</span></span><br><span class="line">new_case2 = np.array(pd.DataFrame(less_data[neighbor_group, :][:, class_index]).mode().iloc[<span class="number">0</span>, :])</span><br><span class="line">new_case = list(new_case1) + list(new_case2)</span><br><span class="line"><span class="keyword">if</span> times == <span class="number">0</span>:</span><br><span class="line">case_update = new_case</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">case_update = np.c_[case_update, new_case]</span><br><span class="line">print(<span class="string">'已经生成了%s条新数据，完成百分之%.2f'</span> % (times, times * <span class="number">100</span> / amount))</span><br><span class="line">times = times + <span class="number">1</span></span><br><span class="line">less_origin_data = np.hstack((less_data[:, continue_index], less_data[:, class_index]))</span><br><span class="line">more_origin_data = np.hstack((more_data[:, continue_index], more_data[:, class_index]))</span><br><span class="line">data_res = np.vstack((more_origin_data, less_origin_data, np.array(case_update.T)))</span><br><span class="line">label_columns = [<span class="number">0</span>] * more_origin_data.shape[<span class="number">0</span>] + [<span class="number">1</span>] * (</span><br><span class="line">less_origin_data.shape[<span class="number">0</span>] + np.array(case_update.T).shape[<span class="number">0</span>])</span><br><span class="line">data_res = pd.DataFrame(data_res)</span><br><span class="line"><span class="keyword">return</span> data_res</span><br></pre></td></tr></table></figure></p>
<h1 id="总体整合"><a href="#总体整合" class="headerlink" title="总体整合"></a>总体整合</h1><p>一期的内容就这样吧，我感觉也没有啥好说的，都是数据分析挖掘的一些基本操作，我只是为了以后能够复用模版化了，下面贴一个全量我做预处理的过程，没啥差异，整合了一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'slade_sal'</span></span><br><span class="line">__time__ = <span class="string">'20171128'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_data_format</span><span class="params">(data)</span>:</span></span><br><span class="line"><span class="comment"># 以下预处理都是基于dataframe格式进行的</span></span><br><span class="line">data_new = pd.DataFrame(data)</span><br><span class="line"><span class="keyword">return</span> data_new</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除空值过多的feature</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_remove</span><span class="params">(data, rate_base=<span class="number">0.4</span>)</span>:</span></span><br><span class="line">all_cnt = data.shape[<span class="number">0</span>]</span><br><span class="line">avaiable_index = []</span><br><span class="line"><span class="comment"># 针对每一列feature统计nan的个数，个数大于全量样本的rate_base的认为是异常feature，进行剔除</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(data.shape[<span class="number">1</span>]):</span><br><span class="line">rate = np.isnan(np.array(data.iloc[:, i])).sum() / all_cnt</span><br><span class="line"><span class="keyword">if</span> rate &lt;= rate_base:</span><br><span class="line">avaiable_index.append(i)</span><br><span class="line">data_available = data.iloc[:, avaiable_index]</span><br><span class="line"><span class="keyword">return</span> data_available, avaiable_index</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 离群点盖帽</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outlier_remove</span><span class="params">(data, limit_value=<span class="number">10</span>, method=<span class="string">'box'</span>, percentile_limit_set=<span class="number">90</span>, changed_feature_box=[])</span>:</span></span><br><span class="line"><span class="comment"># limit_value是最小处理样本个数set，当独立样本大于limit_value我们认为非可onehot字段</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">feature_change = []</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'box'</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line"><span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">return</span> data, feature_change</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'self_def'</span>:</span><br><span class="line"><span class="comment"># 快速截断</span></span><br><span class="line"><span class="keyword">if</span> len(changed_feature_box) == <span class="number">0</span>:</span><br><span class="line"><span class="comment"># 当方法选择为自定义，且没有定义changed_feature_box则全量数据全部按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line">q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 如果定义了changed_feature_box，则将changed_feature_box里面的按照box方法，changed_feature_box的feature index按照percentile_limit_set的分位点大小进行截断</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">in</span> changed_feature_box:</span><br><span class="line">q1 = np.percentile(np.array(data.iloc[:, i]), <span class="number">25</span>)</span><br><span class="line">q3 = np.percentile(np.array(data.iloc[:, i]), <span class="number">75</span>)</span><br><span class="line"><span class="comment"># q3+3/2*qi为上截距点，详细百度分箱图</span></span><br><span class="line">top = q3 + <span class="number">1.5</span> * (q3 - q1)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; top] = top</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)</span><br><span class="line">data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limit</span><br><span class="line">feature_change.append(i)</span><br><span class="line"><span class="keyword">return</span> data, feature_change</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 空feature填充</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nan_fill</span><span class="params">(data, limit_value=<span class="number">10</span>, countinuous_dealed_method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">normal_index = []</span><br><span class="line">continuous_feature_index = []</span><br><span class="line">class_feature_index = []</span><br><span class="line">continuous_feature_df = pd.DataFrame()</span><br><span class="line">class_feature_df = pd.DataFrame()</span><br><span class="line"><span class="comment"># 当存在空值且每个feature下独立的样本数小于limit_value，我们认为是class feature采取one_hot_encoding；</span></span><br><span class="line"><span class="comment"># 当存在空值且每个feature下独立的样本数大于limit_value，我们认为是continuous feature采取mean,min,max方式</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> np.isnan(np.array(data.iloc[:, i])).sum() &gt; <span class="number">0</span>:</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:</span><br><span class="line"><span class="keyword">if</span> countinuous_dealed_method == <span class="string">'mean'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].mean())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'max'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].max())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> countinuous_dealed_method == <span class="string">'min'</span>:</span><br><span class="line">continuous_feature_df = pd.concat(</span><br><span class="line">[continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].min())], axis=<span class="number">1</span>)</span><br><span class="line">continuous_feature_index.append(i)</span><br><span class="line"><span class="keyword">elif</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt; <span class="number">0</span> <span class="keyword">and</span> len(</span><br><span class="line">pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">class_feature_df = pd.concat(</span><br><span class="line">[class_feature_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line">class_feature_index.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">normal_index.append(i)</span><br><span class="line">data_update = pd.concat([data.iloc[:, normal_index], continuous_feature_df, class_feature_df], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> data_update</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># onehotencoding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ohe</span><span class="params">(data, limit_value=<span class="number">10</span>)</span>:</span></span><br><span class="line">feature_cnt = data.shape[<span class="number">1</span>]</span><br><span class="line">class_index = []</span><br><span class="line">class_df = pd.DataFrame()</span><br><span class="line">normal_index = []</span><br><span class="line"><span class="comment"># limit_value以下的均认为是class feature，进行ohe过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(feature_cnt):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:</span><br><span class="line">class_index.append(i)</span><br><span class="line">class_df = pd.concat([class_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">normal_index.append(i)</span><br><span class="line">data_update = pd.concat([data.iloc[:, normal_index], class_df], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> data_update</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'sladesal'</span></span><br><span class="line">__time__ = <span class="string">'20171110'</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Parameters</span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string">data : 原始数据</span></span><br><span class="line"><span class="string">tag_index : 因变量所在的列数，以0开始</span></span><br><span class="line"><span class="string">max_amount : 少类别类想要达到的数据量</span></span><br><span class="line"><span class="string">std_rate : 多类:少类想要达到的比例</span></span><br><span class="line"><span class="string">#如果max_amount和std_rate同时定义优先考虑max_amount的定义</span></span><br><span class="line"><span class="string">kneighbor : 生成数据依赖kneighbor个附近的同类点，建议不超过5个</span></span><br><span class="line"><span class="string">kdistinctvalue : 认为每列不同元素大于kdistinctvalue及为连续变量，否则为class变量</span></span><br><span class="line"><span class="string">method ： 生成方法</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># smote unbalance dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smote</span><span class="params">(data, tag_index=None, max_amount=<span class="number">0</span>, std_rate=<span class="number">5</span>, kneighbor=<span class="number">5</span>, kdistinctvalue=<span class="number">10</span>, method=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">data = pd.DataFrame(data)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line"><span class="keyword">raise</span> ValueError</span><br><span class="line">case_state = data.iloc[:, tag_index].groupby(data.iloc[:, tag_index]).count()</span><br><span class="line">case_rate = max(case_state) / min(case_state)</span><br><span class="line">location = []</span><br><span class="line"><span class="keyword">if</span> case_rate &lt; <span class="number">5</span>:</span><br><span class="line">print(<span class="string">'不需要smote过程'</span>)</span><br><span class="line"><span class="keyword">return</span> data</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 拆分不同大小的数据集合</span></span><br><span class="line">less_data = np.array(</span><br><span class="line">data[data.iloc[:, tag_index] == np.array(case_state[case_state == min(case_state)].index)[<span class="number">0</span>]])</span><br><span class="line">more_data = np.array(</span><br><span class="line">data[data.iloc[:, tag_index] == np.array(case_state[case_state == max(case_state)].index)[<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 找出每个少量数据中每条数据k个邻居</span></span><br><span class="line">neighbors = NearestNeighbors(n_neighbors=kneighbor).fit(less_data)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(less_data)):</span><br><span class="line">point = less_data[i, :]</span><br><span class="line">location_set = neighbors.kneighbors([less_data[i]], return_distance=<span class="keyword">False</span>)[<span class="number">0</span>]</span><br><span class="line">location.append(location_set)</span><br><span class="line"><span class="comment"># 确定需要将少量数据补充到上限额度</span></span><br><span class="line"><span class="comment"># 判断有没有设定生成数据个数，如果没有按照std_rate(预期正负样本比)比例生成</span></span><br><span class="line"><span class="keyword">if</span> max_amount &gt; <span class="number">0</span>:</span><br><span class="line">amount = max_amount</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">amount = int(max(case_state) / std_rate)</span><br><span class="line"><span class="comment"># 初始化，判断连续还是分类变量采取不同的生成逻辑</span></span><br><span class="line">times = <span class="number">0</span></span><br><span class="line">continue_index = []  <span class="comment"># 连续变量</span></span><br><span class="line">class_index = []  <span class="comment"># 分类变量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(less_data.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; kdistinctvalue:</span><br><span class="line">continue_index.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">class_index.append(i)</span><br><span class="line">case_update = list()</span><br><span class="line">location_transform = np.array(location)</span><br><span class="line"><span class="keyword">while</span> times &lt; amount:</span><br><span class="line"><span class="comment"># 连续变量取附近k个点的重心，认为少数样本的附近也是少数样本</span></span><br><span class="line">new_case = []</span><br><span class="line">pool = np.random.permutation(len(location))[<span class="number">1</span>]</span><br><span class="line">neighbor_group = location_transform[pool]</span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'mean'</span>:</span><br><span class="line">new_case1 = less_data[list(neighbor_group), :][:, continue_index].mean(axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 连续样本的附近点向量上的点也是异常点</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'random'</span>:</span><br><span class="line">away_index = np.random.permutation(len(neighbor_group) - <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">neighbor_group_removeorigin = neighbor_group[<span class="number">1</span>:][away_index]</span><br><span class="line">new_case1 = less_data[pool][continue_index] + np.random.rand() * (</span><br><span class="line">less_data[pool][continue_index] - less_data[neighbor_group_removeorigin][continue_index])</span><br><span class="line"><span class="comment"># 分类变量取mode</span></span><br><span class="line">new_case2 = np.array(pd.DataFrame(less_data[neighbor_group, :][:, class_index]).mode().iloc[<span class="number">0</span>, :])</span><br><span class="line">new_case = list(new_case1) + list(new_case2)</span><br><span class="line"><span class="keyword">if</span> times == <span class="number">0</span>:</span><br><span class="line">case_update = new_case</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">case_update = np.c_[case_update, new_case]</span><br><span class="line">print(<span class="string">'已经生成了%s条新数据，完成百分之%.2f'</span> % (times, times * <span class="number">100</span> / amount))</span><br><span class="line">times = times + <span class="number">1</span></span><br><span class="line">less_origin_data = np.hstack((less_data[:, continue_index], less_data[:, class_index]))</span><br><span class="line">more_origin_data = np.hstack((more_data[:, continue_index], more_data[:, class_index]))</span><br><span class="line">data_res = np.vstack((more_origin_data, less_origin_data, np.array(case_update.T)))</span><br><span class="line">label_columns = [<span class="number">0</span>] * more_origin_data.shape[<span class="number">0</span>] + [<span class="number">1</span>] * (</span><br><span class="line">less_origin_data.shape[<span class="number">0</span>] + np.array(case_update.T).shape[<span class="number">0</span>])</span><br><span class="line">data_res = pd.DataFrame(data_res)</span><br><span class="line"><span class="keyword">return</span> data_res</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reload</span><span class="params">(data)</span>:</span></span><br><span class="line">feature = pd.concat([data.iloc[:, :<span class="number">2</span>], data.iloc[:, <span class="number">4</span>:]], axis=<span class="number">1</span>)</span><br><span class="line">tag = data.iloc[:, <span class="number">3</span>]</span><br><span class="line"><span class="keyword">return</span> feature, tag</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据切割</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_data</span><span class="params">(feature, tag)</span>:</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(feature, tag, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="keyword">return</span> X_train, X_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">path = sys.argv[<span class="number">0</span>]</span><br><span class="line">data_all = pd.read_table(str(path))</span><br><span class="line">print(<span class="string">'数据读取完成！'</span>)</span><br><span class="line"><span class="comment"># 更改数据格式</span></span><br><span class="line">data_all = change_data_format(data_all)</span><br><span class="line"><span class="comment"># 删除电话号码列</span></span><br><span class="line">data_all = data_all.iloc[:, <span class="number">1</span>:]</span><br><span class="line">data_all, data_avaiable_index = nan_remove(data_all)</span><br><span class="line">print(<span class="string">'空值列处理完毕！'</span>)</span><br><span class="line">data_all, _ = outlier_remove(data_all)</span><br><span class="line">print(<span class="string">'异常点处理完成！'</span>)</span><br><span class="line">data_all = nan_fill(data_all)</span><br><span class="line">print(<span class="string">'空值填充完成！'</span>)</span><br><span class="line">data_all = ohe(data_all)</span><br><span class="line">print(<span class="string">'onehotencoding 完成！'</span>)</span><br><span class="line">data_all = smote(data_all,tag_index=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'smote过程完成！'</span>)</span><br><span class="line">feature, tag = reload(data_all)</span><br><span class="line">X_train, X_test, y_train, y_test = split_data(feature, tag)</span><br><span class="line">print(<span class="string">'数据预处理完成！'</span>)</span><br></pre></td></tr></table></figure></p>
<p>大家自取自用，这个也没啥好转载的，没啥干货，只是方便大家日常工作，就别转了，谢谢各位编辑大哥了。</p>
<p>最后，感谢大家阅读，谢谢。</p>
]]></content>
      
        <categories>
            
            <category> 代码集合 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[常见算法工程师面试题目整理(二)]]></title>
      <url>/2017/12/01/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E6%95%B4%E7%90%86-%E4%BA%8C/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/01/常见算法工程师面试题目整理-二/1.jpg" alt=""><br><a id="more"></a><br>接着上回写的《<a href="http://www.jianshu.com/p/c3c921dca07b" target="_blank" rel="noopener">总结：常见算法工程师面试题目整理(1)</a>》,继续填接下来的坑。</p>
<h1 id="boost算法的思路是什么样的？讲一下你对adaboost-和-gbdt的了解？"><a href="#boost算法的思路是什么样的？讲一下你对adaboost-和-gbdt的了解？" class="headerlink" title="boost算法的思路是什么样的？讲一下你对adaboost 和 gbdt的了解？"></a>boost算法的思路是什么样的？讲一下你对adaboost 和 gbdt的了解？</h1><p>答：<br>boost的核心思想不同于bagging，<strong>它在基于样本预测结果对照与真实值得差距，进行修正，再预测再修正，逐步靠近正确值。</strong></p>
<p>我对adaboost和gbdt了解的也不算很全面：大概的梳理如下：<br>不足：<br>1.adaboost存在异常点敏感的问题<br>2.gbdt一定程度上优化了adaboost异常点敏感的问题，但是存在难以并行的缺点<br>3.两者的目标都是优化bias，必然导致训练出来的数据var的不稳定</p>
<p>亮点：<br>1.发现非线性的特征关系，网格化的切分feature<br>2.拟合的效果相较于其他分类器更加精准，且训练参数较少</p>
<h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost:"></a>Adaboost:</h2><p><img src="/2017/12/01/常见算法工程师面试题目整理-二/2.png" alt=""><br>adaboost初始数据权重都是1/M，然后通过训练每一个弱分类器Classifier使得在每一次y_pred误差最小，得到每一个弱Classifier的权重方法对：（αi，yi）然后提高错分了的数据的权重，降低正确分类的数据权重，循环训练，最后组合最后若干次的训练弱Classifier对，得到强分类器。<br>其中，αi由误差决定：<br><img src="/2017/12/01/常见算法工程师面试题目整理-二/3.png" alt=""><br>该弱分类器分类错误率em越大，则该若分类器作用越小。<br><strong>1.剖析了原理之后，我们发现，这样做对异常点非常敏感，异常点非常容易错分，会影响后续若干个弱分类器</strong></p>
<h2 id="gbdt"><a href="#gbdt" class="headerlink" title="gbdt:"></a>gbdt:</h2><p>gbdt的核心在于下面这个公式：<br><img src="/2017/12/01/常见算法工程师面试题目整理-二/4.png" alt=""><br>L（y，y_pred）：预测值与实际值间的误差<br>F(x):前若干个弱分类器的组合<br>关键的在于当前预测结果=对前若干个弱分类器+当前弱分类器修正，所以对前若干个分类器组合求偏导的方向进行梯度处理，保证L（x）出来的值最小。<br>这边结果在于你选取什么样的误差函数：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-二/5.png" alt=""><br>Loss即为损失函数，Derivative即为导数<br>除此之外，在每一步弱分类器构建的同时，它还考虑了正则化：<br><strong>Ω=入T+μ<code>*</code>linalg.norm(f(xi))</strong><br>T为子叶节点数目，同时也对预测结果得分的值进行了l1或者l2压缩，避免过拟合。</p>
<p>我个人更喜欢用xgboost，在求解速度上，对异常值处理上面都要比gbdt要快，而且基于R、python版本都有package。</p>
<h1 id="听说你做过用户关系，你用的什么方法？社群算法有了解，讲讲什么叫做Modularity-Q？"><a href="#听说你做过用户关系，你用的什么方法？社群算法有了解，讲讲什么叫做Modularity-Q？" class="headerlink" title="听说你做过用户关系，你用的什么方法？社群算法有了解，讲讲什么叫做Modularity Q？"></a>听说你做过用户关系，你用的什么方法？社群算法有了解，讲讲什么叫做Modularity Q？</h1><p>1.我用的是Jaccard相关。<br>比如，用户1一共收过150个红包，发了100个红包，其中20个被用户2抢过<br>用户2一共收过100个红包，发了50个红包，其中30个被用户1抢过<br>similarity(user1=&gt;user2)=(30+20)/(150+100)<br>similarity(user2=&gt;user1)=(30+20)/(50+100)<br>similarity(user2=&gt;user1)=(30+20+30+20)/(150+100+50+100)</p>
<p>2.社区算法主要是用来衡量用户关系网中，不同用户、链接、信息之间的相似程度。<br>本来这边我准备讲pagerank的，结果被打断了，说需要讲内部结构相关的，其实我觉得PageRank这边来描述更加合适。不过，无所谓，我这边谈的是一个很基本的叫做：Kernighan-Lin算法（后面简称了KL算法）<br>KL算法中，先随机切分原数据集群，得到不同社区集，随机交换不同社区集内的不同点，观察优化值得变化程度是否为正向，循环即可。</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-二/6.png" alt=""><br>共需执行次数：循环次数x集群A内点的个数x集群B内点的个数</p>
<p>感觉这边答的不行，被嫌弃了，有知道的大神可以自行去研究一下相关的社区算法，我这边只了解PageRank和LK。</p>
<p>3.Q-modularity：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-二/7.png" alt=""><br>这个简单，E：关系点连接线之间的个数，I：关系点连接线两端都在社群内的数量，O：关系点连接线有至少一端在社群外的连接线的数量</p>
<p>这个指标是用来衡量社群划分的稳定性的，讲真我也没用过，只是在周志华的算法的书上看过。</p>
<h1 id="如果让你设计一套推荐算法，请说出你的思路？"><a href="#如果让你设计一套推荐算法，请说出你的思路？" class="headerlink" title="如果让你设计一套推荐算法，请说出你的思路？"></a>如果让你设计一套推荐算法，请说出你的思路？</h1><p><img src="/2017/12/01/常见算法工程师面试题目整理-二/8.png" alt=""><br>讲真，这个点，我起码说了有25分种，对面的面试管也很耐心的听完了，并且还给予了很多点的反馈，个人觉得非常受到尊重，我下面细节梳理一下。<br>首先，我个人非常赞同阿里现在的推荐算法这边的设计思路：<br><strong>推荐＝人＋场景＋物</strong><br>其中，<br><strong>人＝新用户＋老用户＋综合特征＋…</strong><br><strong>场景＝属性偏好＋周期属性＋黏度偏好＋…</strong><br><strong>物＝相关性＋物品价值＋特殊属性＋…</strong><br>接下来，我简单的剖析三个最常见也最重要的问题：</p>
<ul>
<li>冷启动<br>很多人有一种错觉，只要业务上线时间长了就不存在所谓的冷启动问题，实则不是，新用户是持续进入的、流失用户也是在增长的、很多盲目用户（没有有价值行为）等等都可以归纳为冷启动问题，这类问题的核心在于你可用的数据很少，甚至没有，我这边采取的是热门推荐的方法。<br>然而在热门推荐的算法中，我这边推荐一些方法：<br><strong>威尔逊区间法</strong>：综合考虑总的行为用户中，支持率与支持总数的平衡<br><strong>hacker new排序</strong>：综合考虑时间对支持率的影响<br><strong>pagerank排序</strong>：考虑用户流向下的页面权重排序<br><strong>梯度效率排序</strong>：考虑商品增速下的支持率的影响<br>…<br>方法很多，但是核心的一点是热门推荐是冷启动及实时推荐必不可少的一环，优化好实时推荐的算法是占到一个好的推荐算法的30%以上的权重的，<strong>切忌0推荐</strong>。</li>
</ul>
<ul>
<li><p>不同种算法产生的推荐内容互不冲突<br><img src="/2017/12/01/常见算法工程师面试题目整理-二/9.png" alt=""><br>这个是苏宁易购的首页推荐位，1、2、3分别是三个推荐位，我们在做算法的时候常常会特别注意，不能用太多相关性比较高的变量，会产生共线性，但在推荐内容上，“58同城”的算法推荐团队之前有一份研究证明，同一个页面上由不同算法产出的推荐结果不存在相互影响。<br>所以，我非常赞同不同的算法产出不同的结果同时展示，因为我们不知道对目标用户是概率模型、距离模型、线性模型等不同模型中哪个产出的结果更加合适。<br>关于常用的推荐算法，我之前梳理过，这边也不再多加重复，需要仔细研究的可看我上面的图，或者看我之前的文章：《<a href="http://www.jianshu.com/p/00a28141521f" target="_blank" rel="noopener">深度学习下的电商商品推荐</a>》、《<a href="http://www.jianshu.com/p/a3e633e396a0" target="_blank" rel="noopener">偏RSVD及扩展的矩阵分解方法</a>》等等</p>
</li>
<li><p>你的对象是用户，不是冰冷的数字<br>我在苏宁呆的时间不长，但是我有个感觉，身边算法工程师很容易把自己陷入数字陷阱，近乎疯狂去用各种算法去拟合当前的用户数据，以求得得到高的ctr或者转化率。<br>不同的推荐场景需要使用不同的用户行为。举例假设存在经典的关系：买了炸鸡和番茄酱的用户，接下来的一周有35%的用户会来买汽水。所以，很多工程师会选择只要买了炸鸡和番茄酱的用户，就弹窗汽水，因为就35%的百分比而言，是非常高的支持度了。其实只要有用户画像的支持就会发现，这35%的用户中，80%的都是年龄在青少年，如果在推送之前做一个简单的逻辑判断只针对所有青少年进行推送汽水的话，35%轻而易举的上升到了70%，这是任何算都无法比拟的。</p>
</li>
</ul>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-二/10.png" alt=""><br>最上方的橙黄色的横条中，橙色代表原始的目标用户，黄色代表非目标用户，假设我们知道黑色方框所框选的用户的转化率达到最小置信度的时候，我们可以通过特征映射、非线性分解、用户画像刻画等不同方法得到左右完全不同的新的用户分布，在同样的用户框选占比下，效果也是完全不同的。<br>真实推荐中，比如针对用户冬装推荐，我不仅仅以用户近期的搜索、浏览、购买商品等行为判断用户的偏好，我也根据他夏天的购买风格款式、他的年龄、生理性别、浏览性别等综合判断他可能会买什么。推荐算法才不会是冷漠的。</p>
<p>至于想要了解具体实现算法及创新的一些想法可以看上方的脑图，但是我觉得那并不是最重要。</p>
<h1 id="什么是P、NP、NP-Hard、NP-Complete问题？"><a href="#什么是P、NP、NP-Hard、NP-Complete问题？" class="headerlink" title="什么是P、NP、NP-Hard、NP-Complete问题？"></a>什么是P、NP、NP-Hard、NP-Complete问题？</h1><p>P：很快可以得出解的问题<br>NP：一定有解，但可很快速验证答案的问题</p>
<p>后面两个我没答出来，网上搜了下，分享下：<br>NP-Hard：比所有的NP问题都难的问题<br>NP-Complete：满足两点：</p>
<ol>
<li>是NP-Hard的问题</li>
<li>是NP问题</li>
</ol>
<p>个人不喜欢这种问题。</p>
<h1 id="常见的防止过拟合的方法是什么？为什么l1、l2正则会防止过拟合？"><a href="#常见的防止过拟合的方法是什么？为什么l1、l2正则会防止过拟合？" class="headerlink" title="常见的防止过拟合的方法是什么？为什么l1、l2正则会防止过拟合？"></a>常见的防止过拟合的方法是什么？为什么l1、l2正则会防止过拟合？</h1><p>当被问了第一个问题的时候，我愣了下，因为我觉得挺简单的，为什么要问这个，我感觉接下来有坑。<br>我回答的是：<br>先甩出了下面的图解释了一波欠拟合、正常、过拟合：<br><img src="/2017/12/01/常见算法工程师面试题目整理-二/11.png" alt=""><br>然后举了几个例子：</p>
<ul>
<li>针对error递归的问题，l1，l2正则化</li>
<li>扩充数据量，使得数据更加符合真实分布</li>
<li>bagging等算法技巧</li>
</ul>
<p>当问到为什么的时候，我觉得自己回答的不好，有点蛋疼：<br>我说的是，l1以：<br><img src="/2017/12/01/常见算法工程师面试题目整理-二/12.png" alt=""><br>l2以：<br><img src="/2017/12/01/常见算法工程师面试题目整理-二/13.png" alt=""><br>l1中函数与约束点偏向相切于四个端点，端点处压缩了某个特征＝0；l2中函数与约束点偏向相切于圆，会造成某些特征值压缩的接近于0；<br>根据奥卡姆剃刀原理，<strong>当两个假说具有完全相同的解释力和预测力时，我们以那个较为简单的假说作为讨论依据</strong>，而通常过拟合的模型的特征维度过高过多，所以一定程度可以缓解过拟合。</p>
<p>面试管以一种奇怪的眼神看着我，然后表示他其实想让我通过先验概率解释，不过我这样说仿佛也有道理。我回来之后就研究了一下，比如l2，大致如下：<br>首先，我们确定两点：<br>l2，其实就给了系数w一个期望是0，协方差矩阵是 1/alpha的先验分布。l1对应的是Laplace先验。</p>
<p>我们相当于是给模型参数w设置一个协方差为1/alpha 的零均值高斯分布先验。<br><img src="/2017/12/01/常见算法工程师面试题目整理-二/14.png" alt=""><br>根据贝叶斯定律：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-二/15.png" alt=""><br>这一步我没看懂，我计算了半天也没由最大似然估计算出下面这个式子，有会的朋友可以私信我一下。</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-二/16.png" alt=""></p>
<p>有了上面的式子就很简单了，alpha在0-正无穷之间，如果a接近0的话，左侧及为正常的MSE也就是没有做任何的惩罚。如果alpha越大的话，说明预测值越接近真实值的同时，协方差也控制的很小，模型越平稳，Var也越小，所以整体的模型更加有效，避免了过拟合导致训练数据拟合效果很差的问题。</p>
<p>到这里，我觉得常见的算法题目都讲完了，很多简单的知识点我没有提，上面这些算是比较经典的，我没答出来的，希望对大家有所帮助，最后谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[常见算法工程师面试题目整理(一)]]></title>
      <url>/2017/12/01/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E6%95%B4%E7%90%86-%E4%B8%80/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/1.jpg" alt=""><br><a id="more"></a><br>最近抽风，出去面试了不少公司，和不少算法工程师招聘的朋友有所交流，整理了相关比较有意思的题目，供大家参考：</p>
<p>附：每题视情况给出答案或答案简介，如有疑问，欢迎私信</p>
<h1 id="基于每日用户搜索内容，假设只有少量已知商品的情况下，如何根据用户搜索内容获取平台内没有的新商品？"><a href="#基于每日用户搜索内容，假设只有少量已知商品的情况下，如何根据用户搜索内容获取平台内没有的新商品？" class="headerlink" title="基于每日用户搜索内容，假设只有少量已知商品的情况下，如何根据用户搜索内容获取平台内没有的新商品？"></a>基于每日用户搜索内容，假设只有少量已知商品的情况下，如何根据用户搜索内容获取平台内没有的新商品？</h1><p><img src="/2017/12/01/常见算法工程师面试题目整理-一/2.png" alt=""></p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/3.png" alt=""></p>
<p>答案：<strong>这是一条类似于分词“新词获取问题”，答案是基于信息熵＋聚合度。</strong></p>
<p>这边需要考虑排除，首先做stop词库，先去除形容词等。<br>信息熵：比如用户搜索“曲面显示屏 白色”，假设现在我们的商品库中没有显示屏这个商品，我们需要判断“显示屏”是否是潜在的商品，我们需要考虑“显示屏”左词、右词出现的可能。换句话说，如果大家都在搜索“显示屏”商品的话，会出现大量的“便宜显示屏”、“可旋转显示屏”、“显示屏 黑色”等搜索短语，根据信息熵计算公式<code>-p∑logp</code>，“显示屏”前后出现的词语类别越多，信息熵越大，代表用户搜索的需求越旺盛，“显示屏”越有可能是没有的商品。</p>
<p>聚合度：根据信息熵的理论也会出现“显示”等高频出现的干扰词，再用聚合度，比如先计算出p(“显示”)、p(“屏”)、或p(“显”)、p(“示屏”)的概率，如果“显示”是一个高频合理的搜索词的话，p(“显示”)*p(“屏”)应该远远大于p(“显示屏”)，p(“显”)＊p(“示屏”)应该远远大于p(“显示屏”)的概率，而实际电商搜索中，用户连贯搜索“显示屏”的概率才是远超其它。</p>
<hr>
<h1 id="为什么logistic回归的要用sigmoid函数？优缺点？"><a href="#为什么logistic回归的要用sigmoid函数？优缺点？" class="headerlink" title="为什么logistic回归的要用sigmoid函数？优缺点？"></a>为什么logistic回归的要用sigmoid函数？优缺点？</h1><p>答案：优点：<br>1.数据压缩能力，将数据规约在［0，1］之间<br>2.导数形式优秀，方便计算<br>缺点：<br>1.容易梯度消失，x稍大的情况下就趋近一条水平线<br>2.非0中心化，在神经网络算法等情况下，造成反向传播时权重的全正全负的情况。</p>
<p>为什么要用？<br>答案1:<strong>logistic是基于Bernoulli分布的假设，也就是y|X~Bernoulli分布，而Bernoulli分布的指数族的形式就是1/(1+exp(-z))</strong><br>其实还有一个答案二，我当时没想起来，如就是：<br>对于logistic多分类而言，<br>x1、x2、…、xn，属于k类的概率正比于：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/4.png" alt=""></p>
<p>我们回到2类：<br>x1、x2、…xn属于1的概率是：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/5.png" alt=""><br>分子分母同除以分子极为1/(1+exp(-z))，z＝w11-w01，个人觉得这样的证明才有说服力</p>
<hr>
<h1 id="对比牛顿法、梯度下降法的关系"><a href="#对比牛顿法、梯度下降法的关系" class="headerlink" title="对比牛顿法、梯度下降法的关系"></a>对比牛顿法、梯度下降法的关系</h1><p>讲真，大学学完牛顿法就丢了，一时没回答出来，回来整理如下：<br>答案：<strong>牛顿法快于梯度下降法，且是梯度下降法的极限。</strong></p>
<p>首先，我们有展开式：<br>f′(x+Δx)=f′(x)+f″(x)∗Δx<br>Δx=−μ∗f′(x)<br>合并两个式子，有：<br>f′(x+Δx)=f′(x)+f″(x)∗(−μ∗f′(x))<br>令f′(x+Δx)＝0，<br>μ＝1/f″(x)，极为牛顿法在随机梯度下降中的μ</p>
<hr>
<h1 id="两个盒子，50个红球，50个白球，问如何放球，抽到红球的概率最高？（每个盒子必须有球）"><a href="#两个盒子，50个红球，50个白球，问如何放球，抽到红球的概率最高？（每个盒子必须有球）" class="headerlink" title="两个盒子，50个红球，50个白球，问如何放球，抽到红球的概率最高？（每个盒子必须有球）"></a>两个盒子，50个红球，50个白球，问如何放球，抽到红球的概率最高？（每个盒子必须有球）</h1><p><strong>答案：一个盒子1个红球，另外一个盒子剩余的99个球</strong></p>
<p>先假设第一个盒子放x个红球，y个白球，另外的一个盒子里面就有50-x红球，50-y个白球.<br>求的目标函数：p＝1/2<em>(x/(x+y))+1/2</em>((50-x)/(100-x-y))<br>subject to. x+y&gt;0 &amp; 100-x-y&gt;0</p>
<p>常规解法如上，被坑了一手的是，面试的说没有常规解，我回来思考了半天，可能是盒子里面的排练顺序有差异，上层的抽取概率&gt;下层的抽取概率，所以需要通过EM算法，先得到若干次抽取的结果下，每层的最大概率密度函数，再结合上述的结果去回答。</p>
<hr>
<h1 id="常见的正则化有是么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？"><a href="#常见的正则化有是么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？" class="headerlink" title="常见的正则化有是么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？"></a>常见的正则化有是么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？</h1><p><strong>答案：<br>(1)l1,l2正则化<br>l1对应python里面numpy.linalg.norm(ord=1)<br>形如|w1|+|w2|+|w3|+…<br>l2对应python里面numpy.linalg.norm(ord=2)<br>形如w1^2+w2^2+w3^2+…</strong></p>
<p><strong>(2)防止过拟合<br>其它防止过拟合的方法还有：<br>1.增加数据量<br>2.采取bagging算法，抽样训练数据
</strong><br>(3)画图解决**</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/6.png" alt=""><br><strong>左边的l1，右边的l2，</strong><br><strong>l1在作图只要不是特殊情况下与正方形的边相切，一定是与某个顶点优先相交，那必然存在横纵坐标轴中的一个系数为0，起到对变量的筛选的作用。</strong><br><strong>l2的时候，其实就可以看作是上面这个蓝色的圆，在这个圆的限制下，点可以是圆上的任意一点，所以q＝2的时候也叫做岭回归，岭回归是起不到压缩变量的作用的，在这个图里也是可以看出来的。</strong></p>
<hr>
<h1 id="分类模型如何选择？如何判断效果？如何计算AUC？你最熟悉的ensemble-Classification-model是什么？"><a href="#分类模型如何选择？如何判断效果？如何计算AUC？你最熟悉的ensemble-Classification-model是什么？" class="headerlink" title="分类模型如何选择？如何判断效果？如何计算AUC？你最熟悉的ensemble Classification model是什么？"></a>分类模型如何选择？如何判断效果？如何计算AUC？你最熟悉的ensemble Classification model是什么？</h1><p>我这边参考了《Do we Need Hundreds of Classifiers to Solve Real World Classification Problems》里面的结论，有兴趣的自行去搜<br>答案：<br><strong>整体上讲：数据量越大，神经网络越好；维度越多，bagging算法越优秀；数据量上不多不少的情况下，SVM效果最好；<br>常用判断：roc、auc、ks、f1值、recall等；<br>AUC计算方法：roc曲线下方的面积积分即可，或者大数定律的投点实验</strong></p>
<p>最熟悉的集成分类模型，我说的是randomforest，详述了原理及实际应用的注意点，后来我问了面试管，主要在这块想了解的是实际解决的相关项目的真实性：<br>1.randomforest是由若干颗cart树构成的，每棵树尽情生长不枝剪，最后采取加权投票或者均值的方式确定输出值<br>2.每棵树的数据是采取bagging式的随机抽取特征及数据样本，两颗树之间的数据有可能会重复<br>3.一般流程会先以sqrt(feature_number)作为每次输入的特征数，采取grid_search的方法观察tree的数量由0-500，oob的变化<br>这边被打断了，解释什么叫做oob，也就是out of bag，每次抽取的数据样本进行训练，没有被抽取到的数据作为检验样本，检验样本上的误差就叫做oob<br>4.根据实际要求的精度上后期可以跟进调整：每次输入的特征个数、每棵树的最大深度、每个节点的分支方式（GINI还是信息增益率）、子节点最少数据量、父节点最少数据量等等<br>这边又被打断了，问，什么叫做信息增益率？<br>首先熵的计算如下：<br><img src="https://www.zhihu.com/equation?tex=E%28X%29%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bp_%7Bi%7Dlog_%7B2%7D%28p_%7Bi%7D+%29++%7D+" alt=""><br>信息增益如下：<br><img src="https://www.zhihu.com/equation?tex=IGain%28S%2CA%29%3DE%28S%29-E%28A%29" alt=""><br><img src="https://www.zhihu.com/equation?tex=E%28A%29%3D%5Csum_%7Bv%5Cin+value%28A%29%7D%7B%5Cfrac%7Bnum%28S_%7Bv%7D%29+%7D%7Bnum%28S%29%7DE%28S_%7Bv%7D+%29+%7D+" alt=""><br>比如14个人，好人5个坏人9个。这14个人被通过性别划分开，10个男性中3个坏人，7个好人；4个女性中2个坏人，2个好人。</p>
<p>信息增益就是:<br>IGain＝(-5/14)<em>log(5/14)+(-9/14)</em>log(9/14)-(10/14<em>(-3/10</em>log(3/10)-7/10<em>log(7/10))+4/14</em>(-1/2<em>log(1/2)-1/2</em>log(1/2)))<br>看到这样的计算方式，必然会存在问题，假设我们身份证为区分类别的化，每个身份证号码都是独一无二的，势必存在存在1/n*log(1)=0这样的最佳划分，但是这样的结果就是将所有的情况分别作为子节点，很明显没有意义，所以引出下面的信息增益率。</p>
<p>信息增益率就是:<br><img src="https://www.zhihu.com/equation?tex=Gain-ratio%3D%5Cfrac%7BIGain%28S%2CA%29%7D%7BInfo%7D+" alt=""><br><img src="https://www.zhihu.com/equation?tex=Info%3D-%5Csum_%7Bv%5Cin+value%28A%29%7D%7B%5Cfrac%7Bnum%28S_%7Bv%7D%29+%7D%7Bnum%28S%29%7Dlog_2%7B%5Cfrac%7Bnum%28S_%7Bv%7D+%29%7D%7Bnum%28S%29%7D+%7D%7D+" alt=""><br>比如上面分人的例子，Info＝-10/14log(10/14)-4/14log(4/14)<br>很明显也可以看出，当你划分的子类别越多，你的info会越大，Gain_ratio就越小，信息增益率就越低，惩罚了刚才身份证分类这种行为。</p>
<p>这也是id3和c4.5之间最大的差异，c4.5以信息增益率代替率id3里面的信息增益，除此之外，id3只能对分类变量处理而c4.5既可以分类变量也可以连续变量，还是很强的，同时他们都可以做多分类，而后续的cart等做多分类的成本会增加（叠加的方式）</p>
<p>其实，这些都很基础但是时间长了，真的很绕人，我也是先自己默默的在纸上画了挺久才和面试管聊，有点出乎我的意料。</p>
<hr>
<h1 id="循环神经网络中介绍一个你熟悉的？"><a href="#循环神经网络中介绍一个你熟悉的？" class="headerlink" title="循环神经网络中介绍一个你熟悉的？"></a>循环神经网络中介绍一个你熟悉的？</h1><p>我说的是LSTM。<br>首先，先跑出了循环的机制，同时点明了RNN潜在隐藏节点对output的影响，做了下图：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/7.png" alt=""><br>及当前的预测结果，与input及上次的layer1节点下的结果相关。</p>
<p>正向循环：<br>节点1的值 = sigmoid(np.dot(输入参数,神经元1) + np.dot(上次节点1的值,潜在神经元))<br>输出值＝sigmoid(np.dot(节点1的值,神经元2))</p>
<p>误差计算：<br>真实y－输出值</p>
<p>delta：<br>节点2处的deltas=误差计算*sigmoid(np.dot(节点1的值,神经元2))／(1-sigmoid(np.dot(节点1的值,神经元2)))</p>
<p>反向修正神经元：<br>神经元2 += (节点1的值).T.dot(节点2处的delta)<br>潜藏神经元 += (上次的节点1的值).T.dot(节点1处的delta)<br>神经元1 += 输入值.T.dot(节点1处的delta)</p>
<p><strong>核心强调了：sigmoid(np.dot(输入参数,神经元1) + np.dot(上次节点1的值,潜在神经元))，输出值与输出值及上次节点1处的输入值有关。</strong><br>然后讲了简单的在语义识别的实际作用。</p>
<hr>
<h1 id="kmeans的原理及如何选择k？如何选择初始点？"><a href="#kmeans的原理及如何选择k？如何选择初始点？" class="headerlink" title="kmeans的原理及如何选择k？如何选择初始点？"></a>kmeans的原理及如何选择k？如何选择初始点？</h1><p>原理是送分题，<br>原理：在给定K值和K个初始类簇中心点的情况下,把每个点(亦即数据记录)分到离其最近的类簇中心点所代表的类簇中，优点在于易于理解和计算，缺点也是很明显，数据一多的情况计算量极大，且标签feature定义距离的难度大。</p>
<p>K的选择，我答的一般，欢迎大家补充，<br>1.根据具体的业务需求，实际需求确定最后聚成的类的个数<br>2.grid_search去试，看那种距离下，损失函数最小（其实这样回答不好，数据量大的情况下，机会不可能）<br>这边的损失函数类别较多，可能包括组内间距和／组外间距和等<br>3.随机抽样下的层次聚类作为预参考<br>理论上，随机采样的数据分布满足原来的数据集的分布，尤其是大量采样次数下的情况，针对每一个较小的数据集合采取层次聚类确定最后的聚类个数，再针对原始的数据集合进行kmeans聚类</p>
<p><strong>如何选取初始点？</strong><br>这个问题我被问过好多次，其实，不管是r或者python里面，或者大家日常使用中都是默认的随机选取，然后通过多次k-折等方法不断的去迭代，其实这样存在的问题就是如果初始点随机选取的有误，导致无论这么迭代都得不到最优的点，如：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/8.png" alt="随机初始点"></p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/9.png" alt="修正初始点"><br>在随机初始点的情况下，红色区域的部分点被蓝色和绿色侵占为己点，修正初始点，也就是将随机初始点的聚类中心全部上移的情况下，蓝色点区收回了原属于自己的点区。<br>之前我恶补过一片论文：《K-means 初始聚类中心的选择算法》，里面提出了两个指标来衡量：<br>1.k-dist<br><img src="/2017/12/01/常见算法工程师面试题目整理-一/10.png" alt=""><br>某个点 p 到它的第k 个最近点的距离为点 p 的 k-dist 值。点的 k-dist 半径范围内至少包含k + 1 个点，理论上同一个聚类中改变k值不会引起k-dist值明显变化。将 k-dist 值由小到大排序，a、b、c表示平缓点，d，e，f为跃迁点。<br>2.DK图<br><img src="/2017/12/01/常见算法工程师面试题目整理-一/11.png" alt=""><br>k-dist 图中相邻两点的 k-dist 值之差记为 DK。k-dist 图中相邻两点pm和pm－1的 k-dist的差为DKm=k-distm －k-distm－1 ( m ＞ 1) 。由于 k-distm 非递减，显然 DKm ＞ 0。DK 值接近的连续邻近点处于 k-dist 图的同一条平缓曲线上，即处于<br>同一个密度层次; DK 值大幅跳动的点处于密度转折曲线或噪声曲线上。<br>3.选择<br>对 DK 值从小到大排序，得到 DK 标准范围δ。依据 DK 标准范围内对应的数据点的分布情况，在 k-dist 图中找出 k’ 个平缓曲线，代表 k’ 个主要密度水平。选择每个密度水平的第一个点作为初始聚类中心。<br>重复若干次，得到若干组的优化聚类中心，在根据优化聚类中心组下的组内间距和／组外间距和判断那个点组为最优点组。</p>
<p>其实这样的开销也挺大的，目前也没有看到其它比较易理解的kmeans的初始点计算的方式。</p>
<hr>
<h1 id="大致讲解一下最优化中拉格朗日乘子法的思路？KKT是什么？"><a href="#大致讲解一下最优化中拉格朗日乘子法的思路？KKT是什么？" class="headerlink" title="大致讲解一下最优化中拉格朗日乘子法的思路？KKT是什么？"></a>大致讲解一下最优化中拉格朗日乘子法的思路？KKT是什么？</h1><p>当我们求解一个函数的最小值，且这个函数也被某些确定的限制条件限制的时候：<br><img src="/2017/12/01/常见算法工程师面试题目整理-一/12.png" alt=""></p>
<p>我们可以将限制条件加入f(x)中一同进行后续的偏导计算：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/13.png" alt=""></p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/14.png" alt=""></p>
<p>至于KKT我了解的其实不多，也是回来之后恶补了一下，通过例子入手：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/15.png" alt=""><br>求解上面这个问题的化，我们需要考虑构造两个约束变量a1，b1，使得<br>h1(x，a1)＝g1(x)＋a1<code>^</code>2＝a－x＋a1<code>^</code>2=0<br>h2(x，b1)＝g2(x)＋b1<code>^</code>2＝b－x＋b1<code>^</code>2=0<br>在根据普通拉格朗日乘子的方法对下面公式的每一项求偏导：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/16.png" alt=""></p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/17.png" alt=""><br>这个条件就是KKT条件<br>其实我觉得，<a href="http://www.cnblogs.com/zhangchaoyang/articles/2726873.html，这篇文章写的挺好的，想要详细了解的可以仔细参考一下。" target="_blank" rel="noopener">http://www.cnblogs.com/zhangchaoyang/articles/2726873.html，这篇文章写的挺好的，想要详细了解的可以仔细参考一下。</a></p>
<hr>
<h1 id="听说你做过风控，异常点检测你用过什么办法？"><a href="#听说你做过风控，异常点检测你用过什么办法？" class="headerlink" title="听说你做过风控，异常点检测你用过什么办法？"></a>听说你做过风控，异常点检测你用过什么办法？</h1><p>之前正好整理过，内心大喜：<br><strong>1.6个西格玛的原理</strong><br><strong>2.箱式图大于3/2QI＋Q3，小于Q1－3/2Qi</strong><br><strong>3.基于距离离群检测（聚类），包括欧式、马氏距离、街道距离</strong><br>这边被打断了，问了马氏距离的细节，好处：<br><img src="/2017/12/01/常见算法工程师面试题目整理-一/18.png" alt=""><br>追问了协方差Sigma怎么算：<br>Cov(X,Y)=E(XY)-E(X)E(Y)<br>追问了什么时候用马氏距离比较好：<br>举例很有名的曲线分布图，如下：</p>
<p><img src="/2017/12/01/常见算法工程师面试题目整理-一/19.png" alt=""><br><strong>4.pca的基于特征值压缩的方法</strong><br><strong>5.基于isolation forest识别的方法</strong><br>这边被追问了一次原理：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">method:</span><br><span class="line">1.从原始数据中随机选择一个属性feature；</span><br><span class="line">2.从原始数据中随机选择该属性的下的一个样本值value；</span><br><span class="line">3.根据feature下的value对每条记录进行分类，把小于value的记录放在左子集，把大于等于value的记录放在右子集；</span><br><span class="line">4.repeat 1-3 until：</span><br><span class="line">　　　　4.1.传入的数据集只有一条记录或者多条一样的记录；</span><br><span class="line">　　　　4.2.树的高度达到了限定高度；</span><br></pre></td></tr></table></figure></p>
<p>　　　　以s(x,n)为判断数据是否异常的衡量指标。<br>　　　　<img src="/2017/12/01/常见算法工程师面试题目整理-一/20.png" alt=""><br>　　　　<img src="/2017/12/01/常见算法工程师面试题目整理-一/21.png" alt=""><br>　　　　其中，h(x)为x对应的节点深度，c(n)为样本可信度，s(x,n)~[0,1]，正常数据来讲s(x,n)小于0.8，s(x,n)越靠近1，数据异常的可能性越大。<br>　　　　<img src="/2017/12/01/常见算法工程师面试题目整理-一/22.png" alt=""><br>　　　　详细的可以参见我的另一篇博客：<a href="http://www.jianshu.com/p/ac6418ee8e3f" target="_blank" rel="noopener">http://www.jianshu.com/p/ac6418ee8e3f</a><br>　　　　<br>　　　　本来准备一次写完的，后来写着写着发现真的挺多，准备写个系列，最后谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SVM理论解析及python实现]]></title>
      <url>/2017/12/01/SVM%E7%90%86%E8%AE%BA%E8%A7%A3%E6%9E%90%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p><strong>关于常见的分类算法在不同数据集上的分类效果，在<a href="http://xueshu.baidu.com/s?wd=paperuri%3A%28491eb32b0997a8dde16c12fe69bf3eac%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2697065%26amp%3Bdl%3Dacm%26amp%3Bcoll%3Ddl%26amp%3Bpreflayout%3Dflat&amp;ie=utf-8&amp;sc_us=5730949819175219868" target="_blank" rel="noopener">《Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?》</a>这个篇论文上有比较完善的总结，因为文章内容比较长，这边我总结了下我认为比较关键的一些结论：</strong></p>
<p><strong>仅仅参考论文评价我们常用的：</strong></p>
<ul>
<li>神经网络的效果最好，13.2%的数据集中取得第一</li>
<li>SVM的效果其次，10.7%的数据集中取得第一</li>
<li>Bagging和Boost紧随其后，9%~10%左右的数据集取得第一</li>
<li>Elastic Net等的线性算法效果普通，5%-7%的数据集上取得第一</li>
</ul>
<p><strong>附加一些我个人平时调参的经验及感悟：</strong></p>
<ul>
<li>神经网络拟合的效果好是基于大量的数据量上，如果数据集较小，训练结果通常不如上述其他算法；除此之外，神经网络的训练成本高，关于控制非线性变换的隐藏层层数，控制线性力度的节点个数的设置需要大量的历史经验，相对成本非常高。</li>
<li>SVM对数据集合量以及维度没有很高的要求，而且可以解决线性问题（kernel=linear），非线性问题（kernel=RBF等），而且相对来说效果优秀。但是SVM的核心是计算最大分隔的间隔，如果全都是分类变量，效果会受一定的影响，而且需要额外的操作才能获取概率结果。</li>
<li>Bagging和Boost，能够解决非线性问题，Bagging基于抽样抽特征，控制Var的情况下降低Bias；Boost基于N个弱分类器的强化组合，控制Bias的情况下降低Var，对数据格式的要求也很低，实现上比较友好。缺点可能就是太一般，没有专业领域的亮点。</li>
<li>Elastic Net等线性回归算法，对数据量数据维度没有什么要求，部分算法会自己压缩feature，简单易操作，相比于上述任何一个算法都好实现，除此之外，还可以得到概率结果。缺点就是效果较差，如果在feature和label没有线性关系的时候无法得到理想结果。</li>
</ul>
<p>除了上述的方法，还有比如KNN、线性判别分析、Naive Bayes等方法，每个都有自己适用的场景，也但是通常不做首要考虑的分类算法。</p>
<p>针对其中的SVM，本文接下来和大家解析三个方面：<br>1.感知机、线性感知机、核感知机的理论概览<br>2.如何利用python中的sklearn快速的实现svm分类<br>3.SMO方法的核心功能实现</p>
<p>如果你只是想快速了解分类算法的概览，方便面试或者日常“交流”，到此就可以不用往下看了。<br>如果你是数据分析师或者软件工程师，只是想快速了解如果使用，直接跳到2。<br>如果你是机器学习工程师，需要对整个算法有个了解，贯穿整个SVM过程，直接看1，2。<br>如果你是算法工程师，需要重构算法，或在当前解决核函数计算瓶颈的，请全文阅读，并阅读推荐书籍。</p>
<p>让我们开始正文：</p>
<h1 id="感知机、线性感知机、核感知机的理论概览"><a href="#感知机、线性感知机、核感知机的理论概览" class="headerlink" title="感知机、线性感知机、核感知机的理论概览"></a>感知机、线性感知机、核感知机的理论概览</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a><strong>感知机</strong></h2><p>我们日常说的SVM其实只是一个感知机，也就是没有任何的核函数的情况。<br><img src="/2017/12/01/SVM理论解析及python实现/1.png" alt=""></p>
<p>上面图中，对于二维数据来说，平面π1，π2，π3都可以将红色和蓝色的点给划分开。对于多维数据来说，这边的平面就可以引申为超平面，wx+b=0。</p>
<p>所以，我们可以说，对于数据集:<br><img src="/2017/12/01/SVM理论解析及python实现/2.png" alt=""><br>如果我们可以得到超平面wx+b=0，使得y=1的点集合与y=-1的点的集合分隔在平面两边（如上图所示），那我们就说原始数据集D线性可分，wx+b=0为其超平面。</p>
<p>首先，我们定义损失函数为：<strong>L=max(-y(wx+b),0)，</strong>我们来看下，如果我们预测正确，y=1，我们预测的为wx+b&gt;0或者，y=-1，我们预测的为wx+b<0,y(wx+b)>0,则不贡献梯度；否则，我们可以取-y(wx+b)为梯度，也就得到了上述的梯度公式。<br>接下来的就是常规意义上的对w和b的偏导，然后梯度下降求极值。</0,y(wx+b)></p>
<h2 id="线性感知机"><a href="#线性感知机" class="headerlink" title="线性感知机"></a><strong>线性感知机</strong></h2><p>现在我们思考两个问题：<br>a.上述的π1、π2、π3都是感知机，如何选取最优？<br>b.下面这张图线性不可分，如何解决？</p>
<p><img src="/2017/12/01/SVM理论解析及python实现/3.png" alt="线性不可分"></p>
<p>我们先来看，平面π1外任意点到平面的距离如何计算：<br><img src="/2017/12/01/SVM理论解析及python实现/4.png" alt=""><br>假设任意点为X(x，y)，垂点X1(x1，y1)，垂点在π1平面(wx+b=0)上，所以，我们有X-X1=ρw，w为平面π1的法向量。所以，我们有：<br><code>||X-X1||**2=ρw(X-X1)=ρ(wx+b-(wx1+b))=ρ(wx+b)=ρ(wx+b)</code><br>在计算距离的时候，我们需要去归一化，无量纲化。<br>不难看出，距离的计算方式为：</p>
<p><img src="/2017/12/01/SVM理论解析及python实现/5.png" alt=""><br>所以，我们在超平面选取的时候，需要考虑两点：<br><strong>(1)所以的分类结果要保持正确：</strong></p>
<p><img src="/2017/12/01/SVM理论解析及python实现/6.png" alt=""></p>
<p><strong>(2)保证决策面离正负样本都极可能的远：</strong></p>
<p><img src="/2017/12/01/SVM理论解析及python实现/7.png" alt=""><br>里面的min的作用是计算所有的点到平面π的最小距离，外面的max的作用是尽可能的让最小距离最大，保证决策面离正负样本都尽可能的远。</p>
<p>假设(x1,y1)到决策平面的距离最近，所有y1(wx1+b)&gt;=1,所以目标函数：max(1/||w||)，可以优化为min(||w||^2/2)。<br>但是如果发生1.2节最开始的<strong>线性不可分的问题</strong>的时候，y1(wx1+b)&gt;=1就无法实现，所有我们需要加∆的容忍度，也就是变成了y1(wx1+b)&gt;=1-∆。既然加了∆，我们也需要对∆进行控制：∆=1-y1(wx1+b)，有更新后的目标函数：</p>
<p><img src="/2017/12/01/SVM理论解析及python实现/8.png" alt=""><br>这边的<code>[]+</code>记为神经网络中常用的ReLU函数。<br>有了这个目标函数，接下来就是正常的梯度下降，偏导后求解的过程。</p>
<h2 id="核函数下的感知机"><a href="#核函数下的感知机" class="headerlink" title="核函数下的感知机"></a><strong>核函数下的感知机</strong></h2><p>上面考虑的问题均是线性可分的问题，假设数据分布如下：</p>
<p><img src="/2017/12/01/SVM理论解析及python实现/9.png" alt=""><br>无论通过平面π1、π2、还是π3均无法做到线性的分割。<br>而核函数的目的就是通过内积的形式，将低维度的数据映射到高维度，通常采取的方式是w=∑αx的形式，带回到原来的损失函数：<br>比如普通感知机的：<br><img src="/2017/12/01/SVM理论解析及python实现/10.png" alt=""></p>
<p>比如线性感知机</p>
<p><img src="/2017/12/01/SVM理论解析及python实现/11.png" alt=""></p>
<p>K(xi,xj)常用的有：<br>多项式核函数：<br>(xi+xj+1)^p</p>
<p>径向基核函数：<br>exp(-ρ||xi-xj||^2)</p>
<p>至于之后的计算，还是可以和之前一致，将上述选择的核函数代入损失函数后采取梯度下降的方法，高效计算方式SMO算法在第三模块会简单的梳理一遍。</p>
<p>以上我们就大概的了解了感知机，linear svm，kernel svm的损失函数的来源及构造细节等等，接下来我们来看下如何快速的使用。</p>
<h1 id="如何利用python中的sklearn快速的实现svm分类"><a href="#如何利用python中的sklearn快速的实现svm分类" class="headerlink" title="如何利用python中的sklearn快速的实现svm分类"></a>如何利用python中的sklearn快速的实现svm分类</h1><p>在python的sklearn包中，有SVM的包，其中SVC是分类，SVR是回归，可以快速简单的上手，下面上code，并在注释中解释：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment">#data add,数据读取</span></span><br><span class="line">risk_data=pd.read_table(<span class="string">'/Users/slade/Desktop/Python File/data/data_all.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#data check，删除无用的列</span></span><br><span class="line">risk_data = risk_data.drop(<span class="string">'Iphone'</span>,axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#data scale，数据归一化（必备的操作），上述理论中也体现归一化后的距离计算的原因</span></span><br><span class="line">risk_data_mm = risk_data.max()-risk_data.min()</span><br><span class="line">risk_data_scale = pd.DataFrame([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(risk_data.columns)):</span><br><span class="line">new_columns = (risk_data.iloc[:,i]-risk_data.iloc[:,i].min())/risk_data_mm[i]</span><br><span class="line">risk_data_scale = pd.concat([risk_data_scale,pd.DataFrame(new_columns)],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#split data（将数据分割成训练集和测试集）</span></span><br><span class="line">train_data,test_data = train_test_split(risk_data_scale,test_size = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#update_train,update_test（因为我的数据集是非常不平衡的，这边我采取了欠采样的方法）</span></span><br><span class="line">train_badcase = train_data[train_data[<span class="string">'tag'</span>]==<span class="number">1</span>]</span><br><span class="line">train_goodcase = train_data[train_data[<span class="string">'tag'</span>]!=<span class="number">1</span>]</span><br><span class="line">sample_value=(<span class="number">10</span>*train_badcase.count()[<span class="number">0</span>])</span><br><span class="line">train_goodcase_sample = train_goodcase.sample(n=sample_value)</span><br><span class="line">train_data_update = pd.concat([train_badcase,train_goodcase_sample],axis = <span class="number">0</span>)</span><br><span class="line">y=train_data_update[<span class="string">'tag'</span>]</span><br><span class="line">x=train_data_update.iloc[:,<span class="number">1</span>:len(train_data_update.columns)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#svm（linear、rbf、sigmoid为核的SVM）</span></span><br><span class="line">clf_linear = svm.SVC(kernel=<span class="string">'linear'</span>).fit(x,y)</span><br><span class="line">clf_rbf = svm.SVC(kernel=<span class="string">'rbf'</span>).fit(x,y)</span><br><span class="line">clf_sigmoid = svm.SVC(kernel=<span class="string">'sigmoid'</span>).fit(x,y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#test，训练数据处理</span></span><br><span class="line">y_test = test_data[[<span class="string">'tag'</span>]]</span><br><span class="line">x_test = test_data.iloc[:,<span class="number">1</span>:len(test_data.columns)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型效果对比</span></span><br><span class="line"><span class="comment">#linear</span></span><br><span class="line">test_pred=clf_linear.predict(x_test)</span><br><span class="line">y_test.index = range(y_test.count())</span><br><span class="line">union_actual_pred = pd.concat([y_test,pd.DataFrame(test_pred)],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#show the result</span></span><br><span class="line">recall = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()/union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>].count()</span><br><span class="line">percison = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count() / union_actual_pred[union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()</span><br><span class="line">correction = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==union_actual_pred.iloc[:,<span class="number">1</span>]].count()/union_actual_pred.iloc[:,<span class="number">0</span>].count()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the recall is %s'</span> %recall</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the percison is %s'</span> %percison</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the correction is %s'</span> %correction</span><br><span class="line"></span><br><span class="line"><span class="comment">#rbf</span></span><br><span class="line">test_pred=clf_rbf.predict(x_test)</span><br><span class="line">y_test.index = range(y_test.count())</span><br><span class="line">union_actual_pred = pd.concat([y_test,pd.DataFrame(test_pred)],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#show the result</span></span><br><span class="line">recall = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()/union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>].count()</span><br><span class="line">percison = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count() / union_actual_pred[union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()</span><br><span class="line">correction = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==union_actual_pred.iloc[:,<span class="number">1</span>]].count()/union_actual_pred.iloc[:,<span class="number">0</span>].count()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the recall is %s'</span> %recall</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the percison is %s'</span> %percison</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the correction is %s'</span> %correction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#sigmoid</span></span><br><span class="line">test_pred=clf_sigmoid.predict(x_test)</span><br><span class="line">y_test.index = range(y_test.count())</span><br><span class="line">union_actual_pred = pd.concat([y_test,pd.DataFrame(test_pred)],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#show the result</span></span><br><span class="line">recall = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()/union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>].count()</span><br><span class="line">percison = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==<span class="number">1</span>][union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count() / union_actual_pred[union_actual_pred.iloc[:,<span class="number">1</span>]==<span class="number">1</span>].count()</span><br><span class="line">correction = union_actual_pred[union_actual_pred.iloc[:,<span class="number">0</span>]==union_actual_pred.iloc[:,<span class="number">1</span>]].count()/union_actual_pred.iloc[:,<span class="number">0</span>].count()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the recall is %s'</span> %recall</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the percison is %s'</span> %percison</span><br><span class="line"><span class="keyword">print</span> <span class="string">'about the linear svm , the correction is %s'</span> %correction</span><br></pre></td></tr></table></figure></p>
<p>上述粗略的给出了如何快速的通过svm进行一次训练，现在就svc中的参数进行剖析：<br><code>C</code>:惩罚力度，C越大代表惩罚程度越大，越不能容忍有点集交错的问题<br><code>kernel</code>:核函数，常规的有‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ ，默认的是rbf<br><code>degree</code>:当poly为核函数时启动，默认是3<br><code>gamma</code>:当‘rbf’, ‘poly’ 和 ‘sigmoid’为核函数时的参数设置，默认的是特征个数的倒数<br><code>probability</code>：是否输出概率值<br><code>shrinking</code>:是否自启动<br><code>tol</code>:停止训练的容忍度<br><code>max_iter</code>：最大的训练次数<br><code>class_weight</code>:因变量的权重<br><code>decision_function_shape</code>:因变量的形式：ovo一对一, ovr一对多, 默认’ovr’<br>根据自己的需求，对上述的参数进行grid_search即可完成快速训练任务。</p>
<h1 id="3-SMO方法的核心功能实现"><a href="#3-SMO方法的核心功能实现" class="headerlink" title="3.SMO方法的核心功能实现"></a>3.SMO方法的核心功能实现</h1><p>首先，我们需要明确，SVM学习过程可以转化为以下问题：</p>
<p><img src="/2017/12/01/SVM理论解析及python实现/12.png" alt=""><br>至于什么是KKT条件，请参照<a href="http://www.jianshu.com/p/4f91f0dcba95" target="_blank" rel="noopener">总结：常见算法工程师面试题目整理(二)</a>中的回答。<br>求解的方式也是比较复杂，这主要以梳理流程为主，我们的目的就是找到一组αi满足上述的约束，然后再根据该组的αi求解到w和b即可。</p>
<p>求解αi的过程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.选择两个拉格朗日乘子αi和αj</span><br><span class="line">2.固定其他拉格朗日乘子αk(k不等于i和j)，只对αi和αj优化w(α)</span><br><span class="line">3.根据优化后的αi和αj，更新截距b的值</span><br><span class="line">4.充分1-3直到收敛</span><br></pre></td></tr></table></figure></p>
<p>针对上面的过程存在2个问题<br>a.为什么要选择两个乘子？而不是更加方便计算的一个？<br>在原始的约束条件中，存在：<br><img src="/2017/12/01/SVM理论解析及python实现/13.png" alt=""><br>如果只选择一个为变化乘子的化，根据其他确定的乘子，该变化乘子也无法变化。</p>
<p>b.如何选择两个乘子αi和αj？</p>
<p><img src="/2017/12/01/SVM理论解析及python实现/14.png" alt=""><br>检验样本是否满足KKT条件也就是检验样本是否满足以下条件：</p>
<p><img src="/2017/12/01/SVM理论解析及python实现/15.png" alt=""><br>第一个参数αi优先检验0&lt;αj&lt;C也就是π3和π1平面上的点是否满足条件，如果全部满足条件，再检验全部数据集是否满足条件。</p>
<p>第二个参数αj则可以简单地随机选取，虽然这不是特别好，但效果已然不错。也可以通过最大化αi的误差与αj的误差之差的绝对值去判断，但是计算量会变大，因为又做了一次全量数据循环。</p>
<p>当αi和αj有了之后再去对b进行修正：</p>
<p><img src="/2017/12/01/SVM理论解析及python实现/16.png" alt=""><br>即可。</p>
<p>这边的代码比较复杂，我就不贴了，百度上很多实现了的版本。</p>
<p>总的来说，我们对svm的过程有了个浅尝辄止的了解，部分算法工程师需要要深入的了解其深刻完整的含义，仍需完整完善的学习，<a href="https://www.amazon.cn/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA/dp/B007TSFMTA" target="_blank" rel="noopener">《统计学习方法 》</a>一书讲的深入透彻，建议可以研读一下。</p>
<p>部分软件工程师在运用中可能需要各种版本的svm，这边也贴出地址，供参考<a href="http://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html" target="_blank" rel="noopener">Support Vector Machine for Complex Outputs</a></p>
<p>最后，谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 超平面分割 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[能够快速实现的协同推荐]]></title>
      <url>/2017/12/01/%E8%83%BD%E5%A4%9F%E5%BF%AB%E9%80%9F%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%8D%8F%E5%90%8C%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<p>对于中小型的公司，用户的数据量及公司产品的个数都是较小规模的，需要提供给用户的推荐系统实现的重心也从人性化变成了实现成本，协同推荐就是非常常见、有效且可以快速实现的方法，也是本文想介绍的。</p>
<p>常规的快速简单推荐系统实现方法不排除以下几种：</p>
<ul>
<li><p>热门推荐<br>所有人打开浏览的内容都一致，惊喜性会有所缺失，但是实现特别简单，稍加逻辑带给用户的体验感满足了基本需求。</p>
</li>
<li><p>SVD+推荐<br>之前也讨论过实现方法了，附上链接：<a href="http://www.jianshu.com/p/a3e633e396a0" target="_blank" rel="noopener">SVD及扩展的矩阵分解方法</a></p>
</li>
<li><p>基于模型推荐<br>这个比较偏向业务场景，可以说是经典的场景化模型，之前写过一篇基于用户特征的偏好推荐，可以参考一下：<a href="http://www.jianshu.com/p/fd245999ebfe" target="_blank" rel="noopener">苏宁易购的用户交叉推荐</a></p>
</li>
<li><p>协同推荐<br>这个也是几乎每个公司都会用的，也是非常非常常见有效的算法之一</p>
</li>
</ul>
<hr>
<h1 id="协同推荐介绍"><a href="#协同推荐介绍" class="headerlink" title="协同推荐介绍"></a><strong>协同推荐介绍</strong></h1><p>首先，我们先来了解一下什么叫做协同推荐。<br>基于用户的协同过滤推荐算法是最早诞生的，1992年提出并用于邮件过滤系统，两年后1994年被 GroupLens 用于新闻过滤。一直到2000年左右，该算法都是推荐系统领域最著名的算法。算是非常古董级别的算法之一了，但是古董归古董，它的效果以及实现的成本却奠定了它在每个公司不可取代的地位。</p>
<h2 id="基于用户的协同推荐"><a href="#基于用户的协同推荐" class="headerlink" title="基于用户的协同推荐"></a><strong>基于用户的协同推荐</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">用户u1喜欢的电影是A，B，C</span><br><span class="line">用户u2喜欢的电影是A, C, E, F</span><br><span class="line">用户u3喜欢的电影是B，D</span><br></pre></td></tr></table></figure>
<p>假设u1、u2、u3用户喜欢的电影分布如上，基于用户的协同推荐干了这么一件事情，它根据每个用户看的电影（A、B、C、…）相似程度，来计算用户之间的相似程度，将高相似的用户看过但是目标用户还没有看过的电影推荐给目标用户。</p>
<h2 id="基于商品的协同推荐"><a href="#基于商品的协同推荐" class="headerlink" title="基于商品的协同推荐"></a><strong>基于商品的协同推荐</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">电影A被u1，u2看过</span><br><span class="line">电影B被u1，u3看过</span><br><span class="line">电影C被u1，u2看过</span><br><span class="line">电影D被u3看过</span><br><span class="line">电影E被u2看过</span><br><span class="line">电影F被u2看过</span><br></pre></td></tr></table></figure>
<p>假设A～F电影被用户观影的分布如上，基于商品的协同推荐干了这么一件事情，它根据电影（A、B、C、…）被不同用户观看相似程度，来计算电影之间的相似程度，根据目标用户看过的电影的高相似度的电影推荐给目标用户。</p>
<p>看起来以上的逻辑是非常简单的，其实本来也是非常简单的，我看了下，网上关于以上的代码实现还是比较林散和有问题的，优化了python版本的code，并详细解释了每一步，希望，对初学者有所帮助。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#time  2017-09-17</span></span><br><span class="line"><span class="comment">#author：shataowei</span></span><br><span class="line"><span class="comment">#based-item</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#所要的基础包比较简单</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">startTime = time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据的过程</span></span><br><span class="line"><span class="comment">#/Users/slade/Desktop/machine learning/data/recommender/u1.base</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readdata</span><span class="params">(location)</span>:</span></span><br><span class="line">list2item = &#123;&#125;  <span class="comment">#商品对应的用户列表(1:[[1,2],[2,3]]代表商品1对应用户1的行为程度为2,商品1对应的用户2的行为程度为3)</span></span><br><span class="line">list2user = &#123;&#125;  <span class="comment">#用户对应的商品列表(1:[[1,2],[2,3]]代表用户1对应商品1的行为程度为2,用户1对应的商品2的行为程度为3)</span></span><br><span class="line">f = open(location,<span class="string">'r'</span>)</span><br><span class="line">data = f.readlines()</span><br><span class="line">data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line"><span class="keyword">if</span> int(i[<span class="number">1</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2item.keys():</span><br><span class="line">list2item[int(i[<span class="number">1</span>])] = [[int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">list2item[int(i[<span class="number">1</span>])].append([int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> int(i[<span class="number">0</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2user.keys():</span><br><span class="line">list2user[int(i[<span class="number">0</span>])] = [[int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">list2user[int(i[<span class="number">0</span>])].append([int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"><span class="keyword">return</span> list2item,list2user</span><br><span class="line"><span class="comment">#list2item,list2user=readdata('/Users/slade/Desktop/machine learning/data/recommender/u1.base')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 基于item的协同推荐</span></span><br><span class="line"><span class="comment">#0.将用户行为程度离散化：浏览：1，搜索：2，收藏：3，加车：4，下单未支付5</span></span><br><span class="line"><span class="comment">#1.计算item之间的相似度：item共同观看次数/单item次数连乘</span></span><br><span class="line"><span class="comment">#2.寻找目标用户观看过的item相关的其他item列表</span></span><br><span class="line"><span class="comment">#3.计算其他item的得分：相似度*用户行为程度，求和</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#0 hive操作</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.1统计各商品出现次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">itemcf_itemall</span><span class="params">(userlist = list2user)</span>:</span></span><br><span class="line">I=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> userlist:</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> userlist[key]:</span><br><span class="line"><span class="keyword">if</span> item[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> I.keys():</span><br><span class="line">I[item[<span class="number">0</span>]] = <span class="number">0</span></span><br><span class="line">I[item[<span class="number">0</span>]] = I[item[<span class="number">0</span>]] + <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> I</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.2计算相似矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">itemcf_matrix</span><span class="params">(userlist = list2user)</span>:</span></span><br><span class="line">C=defaultdict(defaultdict)</span><br><span class="line">W=defaultdict(defaultdict)</span><br><span class="line"><span class="comment">#根据用户的已购商品来形成对应相似度矩阵</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> userlist:</span><br><span class="line"><span class="keyword">for</span> item1 <span class="keyword">in</span> userlist[key]:</span><br><span class="line"><span class="keyword">for</span> item2 <span class="keyword">in</span> userlist[key]:</span><br><span class="line"><span class="keyword">if</span> item1[<span class="number">0</span>] == item2[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"><span class="keyword">if</span> item2 <span class="keyword">not</span> <span class="keyword">in</span> C[item1[<span class="number">0</span>]].keys():</span><br><span class="line">C[item1[<span class="number">0</span>]][item2[<span class="number">0</span>]] = <span class="number">0</span></span><br><span class="line">C[item1[<span class="number">0</span>]][item2[<span class="number">0</span>]] = C[item1[<span class="number">0</span>]][item2[<span class="number">0</span>]] + <span class="number">1</span></span><br><span class="line"><span class="comment">#计算相似度，并填充上面对应的相似度矩阵</span></span><br><span class="line"><span class="keyword">for</span> i , j <span class="keyword">in</span> C.items():</span><br><span class="line"><span class="keyword">for</span> z , k <span class="keyword">in</span> j.items():</span><br><span class="line">W[i][z] = k/math.sqrt(I[i]*I[z])</span><br><span class="line"><span class="comment">#k/math.sqrt(I[i]*I[z])计算相似度，其中k为不同商品交集，sqrt(I[i]*I[z])用来压缩那些热门商品必然有高交集的问题</span></span><br><span class="line"><span class="keyword">return</span> W</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.寻找用户观看的其他item</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommendation</span><span class="params">(userid,k)</span>:</span></span><br><span class="line">score_final = defaultdict(int)</span><br><span class="line">useriditem = []</span><br><span class="line"><span class="keyword">for</span> item,score <span class="keyword">in</span> list2user[userid]:</span><br><span class="line"><span class="comment">#3.计算用户的item得分，k来控制用多少个相似商品来计算最后的推荐商品</span></span><br><span class="line"><span class="keyword">for</span> i , smimilarity <span class="keyword">in</span> sorted(W[item].items() , key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>] ,reverse =<span class="keyword">True</span>)[<span class="number">0</span>:k]:</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> list2user[userid]:</span><br><span class="line">useriditem.append(j[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> useriditem:</span><br><span class="line">score_final[i] = score_final[i] + smimilarity * score</span><br><span class="line"><span class="comment">#累加每一个商品用户的评分与其它商品的相似度积的和作为衡量</span></span><br><span class="line"><span class="comment">#最后的10控制输出多少个推荐商品</span></span><br><span class="line">l = sorted(score_final.items() , key = <span class="keyword">lambda</span> x : x[<span class="number">1</span>] , reverse = <span class="keyword">True</span>)[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line"><span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"><span class="comment">#I = itemcf_itemall()</span></span><br><span class="line"><span class="comment">#W = itemcf_matrix()</span></span><br><span class="line"><span class="comment">#result_userid = recommendation(2,k=20)</span></span><br><span class="line"></span><br><span class="line">endTime = time.time()</span><br><span class="line"><span class="keyword">print</span> endTime-startTime</span><br></pre></td></tr></table></figure>
<p>python来实现基于item的协同推荐就完成了，核心的相似度计算可以根据实际问题进行修改，整体流程同上即可，当然数据量大的时候分布式去写也是可以的。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#time  2017-09-17</span></span><br><span class="line"><span class="comment">#author：shataowei</span></span><br><span class="line"><span class="comment">#based-user</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">startTime = time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line"><span class="comment">#/Users/slade/Desktop/machine learning/data/recommender/u1.base</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readdata</span><span class="params">(location)</span>:</span></span><br><span class="line">list2item = &#123;&#125;  <span class="comment">#商品对应的用户列表</span></span><br><span class="line">list2user = &#123;&#125;  <span class="comment">#用户对应的商品列表</span></span><br><span class="line">f = open(location,<span class="string">'r'</span>)</span><br><span class="line">data = f.readlines()</span><br><span class="line">data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line"><span class="keyword">if</span> int(i[<span class="number">1</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2item.keys():</span><br><span class="line">list2item[int(i[<span class="number">1</span>])] = [[int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">list2item[int(i[<span class="number">1</span>])].append([int(i[<span class="number">0</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> int(i[<span class="number">0</span>]) <span class="keyword">not</span> <span class="keyword">in</span> list2user.keys():</span><br><span class="line">list2user[int(i[<span class="number">0</span>])] = [[int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])]]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">list2user[int(i[<span class="number">0</span>])].append([int(i[<span class="number">1</span>]),int(i[<span class="number">2</span>])])</span><br><span class="line"><span class="keyword">return</span> list2item,list2user</span><br><span class="line"><span class="comment">#list2item,list2user=readdata('/Users/slade/Desktop/machine learning/data/recommender/u1.base')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#基于用户的协同推荐</span></span><br><span class="line"><span class="comment">#0.先通过hive求出近一段时间（根据业务频率定义），用户商品的对应表</span></span><br><span class="line"><span class="comment">#1.求出目标用户的邻居，并计算目标用户与邻居之间的相似度</span></span><br><span class="line"><span class="comment">#2.列出邻居所以购买的商品列表</span></span><br><span class="line"><span class="comment">#3.针对第二步求出了商品列表，累加所对应的用户相似度，并排序求top</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#0.hive操作</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.1求出目标用户的邻居，及对应的相关程度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neighbour</span><span class="params">(userid,user_group = list2user,item_group = list2item)</span>:</span></span><br><span class="line">neighbours = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list2user[userid]:</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> list2item[item[<span class="number">0</span>]]:</span><br><span class="line"><span class="keyword">if</span> user[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> neighbours.keys():</span><br><span class="line">neighbours[user[<span class="number">0</span>]] = <span class="number">0</span></span><br><span class="line">neighbours[user[<span class="number">0</span>]] = neighbours[user[<span class="number">0</span>]] + <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> neighbors</span><br><span class="line"><span class="comment">#通常来说，基于item的推荐对于商品量较大的业务会构成一个巨大的商品矩阵，这时候如果用户人均购买量较低的时候，可以考虑使用基于user的推荐，它在每次计算的时候会只考虑相关用户，也就是这边的neighbours(有点支持向量基的意思)，大大的降低了计算量。</span></span><br><span class="line"><span class="comment">#neighbours = neighbour(userid=2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.2就算用户直接的相似程度,这边用的余弦相似度：点积/模的连乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similarity</span><span class="params">(user1,user2)</span>:</span></span><br><span class="line">x=<span class="number">0</span></span><br><span class="line">y=<span class="number">0</span></span><br><span class="line">z=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> item1 <span class="keyword">in</span> list2user[user1]:</span><br><span class="line"><span class="keyword">for</span> item2 <span class="keyword">in</span> list2user[user2]:</span><br><span class="line"><span class="keyword">if</span> item1[<span class="number">0</span>]==item2[<span class="number">0</span>]:</span><br><span class="line">x1 = item1[<span class="number">1</span>]*item1[<span class="number">1</span>]</span><br><span class="line">y1 = item2[<span class="number">1</span>]*item2[<span class="number">1</span>]</span><br><span class="line">z1 = item1[<span class="number">1</span>]*item2[<span class="number">1</span>]</span><br><span class="line">x = x + x1</span><br><span class="line">y = y + y1</span><br><span class="line">z = z + z1</span><br><span class="line"><span class="comment">#避免分母为0</span></span><br><span class="line"><span class="keyword">if</span> x * y == <span class="number">0</span> :</span><br><span class="line">simi = <span class="number">0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">simi = z / math.sqrt(x * y)</span><br><span class="line"><span class="keyword">return</span> simi</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.3计算目标用户与邻居之间的相似度：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">N_neighbour</span><span class="params">(userid,neighbours,k)</span>:</span></span><br><span class="line">neighbour = neighbours.keys()</span><br><span class="line">M = []</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> neighbour:</span><br><span class="line">simi = similarity(userid,user)</span><br><span class="line">M.append((user,simi))</span><br><span class="line">M = sorted(M,key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>] ,reverse = <span class="keyword">True</span>)[<span class="number">0</span>:k]</span><br><span class="line"><span class="keyword">return</span> M</span><br><span class="line"></span><br><span class="line"><span class="comment">#M = N_neighbour(userid,neighbours,k=200)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.列出邻居所购买过的商品并计算商品对应的推荐指数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neighbour_item</span><span class="params">(M=M)</span>:</span></span><br><span class="line">R = &#123;&#125;</span><br><span class="line">M1 = dict(M)</span><br><span class="line"><span class="keyword">for</span> neighbour <span class="keyword">in</span> M1:</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list2user[neighbour]:</span><br><span class="line"><span class="keyword">if</span> item[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> R.keys():</span><br><span class="line">R[item[<span class="number">0</span>]] = M1[neighbour] * item[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">R[item[<span class="number">0</span>]] = R[item[<span class="number">0</span>]] + M1[neighbour] * item[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#根据邻居买过什么及与邻居的相似度，计算邻居买过商品的推荐度</span></span><br><span class="line"><span class="keyword">return</span> R</span><br><span class="line"><span class="comment"># R = neighbour_item(M)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.排序得到推荐商品</span></span><br><span class="line">Rank = sorted(R.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>],reverse = <span class="keyword">True</span>)[<span class="number">0</span>:<span class="number">50</span>]</span><br><span class="line"></span><br><span class="line">endTime = time.time()</span><br><span class="line"><span class="keyword">print</span> endTime-startTime</span><br></pre></td></tr></table></figure>
<p>python来实现基于user的协同推荐就完成了，核心的相似度计算可以根据实际问题进行修改，基于user的实现过程中，用了邻居这个概念，大大降低了计算量，我用了大概20万用户，2千的商品数，基于user的推荐实现速度大概为基于商品的10分之一，效果差异却相差不大。</p>
<p>协同推荐是非常简单的推荐入门算法之一，也是必须要手动快速代码实现的算法之一，希望能给大家一些帮助。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Tensorflow的神经网络解决用户流失概率问题]]></title>
      <url>/2017/12/01/%E5%9F%BA%E4%BA%8ETensorflow%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3%E7%94%A8%E6%88%B7%E6%B5%81%E5%A4%B1%E6%A6%82%E7%8E%87%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>用户流失一直都是公司非常重视的一个问题，也是AAARR中的Retention的核心问题，所以各大算法竞赛都很关注。比如最近的：<a href="https://www.kaggle.com/c/kkbox-churn-prediction-challenge" target="_blank" rel="noopener">KKBOX的会员流失预测算法竞赛</a>，如何能够搭建一个精准的模型成了大家探索的重要问题。<br>本文主要讲解神经网络、TensorFlow的概述、如何利用python基于TensorFlow神经网络对流失用户进行分类预测，及可能存在的一些常见问题，作为深度学习的入门阅读比较适合。</p>
<hr>
<h1 id="行业做法："><a href="#行业做法：" class="headerlink" title="行业做法："></a>行业做法：</h1><p>通常的行业预测用户流失大概分以下几种思路：</p>
<ul>
<li>利用线性模型(比如Logistic)＋非线性模型Xgboost判断用户是否回流逝</li>
</ul>
<p>这种方法有关是行业里面用的最多的，效果也被得意验证足够优秀且稳定的。核心点在于特征的预处理，Xgboost的参数挑优，拟合程度的控制，这个方法值得读者去仔细研究一边。问题也是很明显的，会有一个行业baseline，基本上达到上限之后，想有有提升会非常困难，对要求精准预测的需求会显得非常乏力。</p>
<ul>
<li>规则触发</li>
</ul>
<p>这种方法比较古老，但是任然有很多公司选择使用，实现成本较低而且非常快速。核心在于，先确定几条核心的流失指标(比如近7日登录时长)，然后动态的选择一个移动的窗口，不停根据已经流失的用户去更新流失指标的阈值。当新用户达到阈值的时候，触发流失预警。效果不如第一个方法，但是实现简单，老板也很容易懂。</p>
<ul>
<li>场景模型的预测</li>
</ul>
<p>这个方法比较依赖于公司业务的特征，如果公司业务有部分依赖于评论，可以做文本分析，比如我上次写的<a href="http://www.jianshu.com/p/413cff5b9f3a" target="_blank" rel="noopener">基于word2vec下的用户流失概率分析</a>。如果业务有部分依赖于登录打卡，可以做时间线上的频次预估。这些都是比较偏奇门易巧，不属于通用类别的，不过当第一种方法达到上线的时候，这种方法补充收益会非常的大。</p>
<p>其实还有很多其它方法，我这边也不一一列出了，这个领域的方法论还是很多的。</p>
<hr>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><ul>
<li><strong>核心</strong><br><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/1.png" alt="神经网络流程"><br>上面这张图片诠释了神经网络正向传播的流程，先通过线性变换(上图左侧)Σxw+b将线性可分的数据分离，再通过非线性变换(上图右侧)Sigmoid函数将非线性可分的数据分离，最后将输入空间投向另一个输出空间。</li>
</ul>
<p>根据上面所说，我们可以知道，通过增加左侧线性节点的个数，我们可以强化线性变换的力度；而通过增加层数，多做N次激活函数(比如上面提到的Sigmoid)可以增强非线性变换的能力。</p>
<p>通过矩阵的线性变换+激活矩阵的非线性变换，将原始不可分的数据，先映射到高纬度，再进行分离。但是这边左侧节点的个数，网络的层数选择是非常困难的课题，需要反复尝试。</p>
<ul>
<li><strong>参数训练</strong><br>刚才我们了解了整个训练的流程，但是如何训练好包括线性变换的矩阵系数是一个还没有解决的问题。</li>
</ul>
<p>我们来看下面的过程：<br>input ==&gt; Σxw+b(线性变换) ==&gt; f(Σxw+b)(激活函数) ==&gt; …(多层的话重复前面过程) ==&gt; output(到此为止，正向传播结束，反向修正矩阵weights开始) ==&gt; <strong>error=actual_output-output(计算预测值与正式值误差)==&gt;output处的梯度==&gt;调整后矩阵weight=当前矩阵weight+error<code>x</code>学习速率<code>x</code>output处的负梯度</strong><br>核心目的在于通过比较预测值和实际值来调整权重矩阵，将预测值与实际值的差值缩小。<br>比如：梯度下降的方法，通过计算当前的损失值的方向的负方向，控制学习速率来降低预测值与实际值间的误差。</p>
<p>利用一行代码来解释就是  <code>synaptic_weights += dot(inputs, (real_outputs - output) * output * (1 - output))*η</code><br>这边output * (1 - output))是在output处的Sigmoid的倒数形式，η是学习速率。</p>
<p><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/2.png" alt="weight的循环流程"></p>
<ul>
<li><strong>神经网络流程小结</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1数据集获取（有监督数据整理）</span><br><span class="line">2神经网络参数确定，有多少层，多少个节点，激活函数是什么，损失函数是什么</span><br><span class="line">3数据预处理，pca，标准化，中心化，特征压缩，异常值处理</span><br><span class="line">4初始化网络权重</span><br><span class="line">5网络训练</span><br><span class="line">5.1正向传播</span><br><span class="line">5.2计算loss</span><br><span class="line">5.3计算反向梯度</span><br><span class="line">5.4更新梯度</span><br><span class="line">5.5重新正向传播</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这边只是简单介绍了神经网络的基础知识，针对有一定基础的朋友唤醒记忆，如果纯小白用户，建议从头开始认真阅读理解一遍过程，避免我讲的有偏颇的地方对你进行误导。</p>
<hr>
<h1 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h1><p>理论上讲，TensorFlow工具可以单独写一本书，用法很多而且技巧性的东西也非常的复杂，这边我们主要作为工具进行使用，遇到新技巧会在code中解释，但不做全书的梳理，建议去买一本《TensorFlow实战Google深度学习框架》，简单易懂。</p>
<p>TensorFlow是谷歌于2015年11月9日正式开源的计算框架，由Jeff Dean领导的谷歌大脑团队改编的DistBelief得到的，在ImageNet2014、YouTube视频学习，语言识别错误率优化，街景识别，广告，电商等等都有了非常优秀的产出，是我个人非常喜欢的工具。</p>
<p>除此之外，我在列出一些其他的框架工具供读者使用：</p>
<p><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/3.png" alt=""></p>
<p><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/4.png" alt=""></p>
<p>接下来看一下最基本的语法，方便之后我们直接贴代码的时候可以轻松阅读。</p>
<ul>
<li><p>张量：可以理解为多维array 或者 list，time决定张量是什么<br>tf.placeholder(time,shape,name)</p>
</li>
<li><p>变量：同一时刻下的不变的数据<br>tf.Variable(value,name)</p>
</li>
<li><p>常量：永远不变的常值<br>tf.constant(value)</p>
</li>
<li><p>执行环境开启与关闭，在环境中才能运行TensorFlow语法<br>sess=tf.Session()<br>sess.close()<br>sess.run(op)</p>
</li>
<li><p>初始化所有权重：类似于变量申明<br>tf.initialize_all_variables()</p>
</li>
<li><p>更新权重：<br>tf.assign(variable_to_be_updated,new_value)</p>
</li>
<li><p>加值行为，利用feed_dict里面的值来训练[output]函数<br>sess.run([output],feed_dict={input1:value1,input2:value2})<br>利用input1，input2，来跑output的值</p>
</li>
<li><p>矩阵乘法，类似于dot<br>tf.matmul(input,layer1)</p>
</li>
<li><p>激活函数，relu<br>tf.nn.relu()，除此之外，还有tf.nn.sigmoid，tf.nn.tanh等等<br><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/5.png" alt=""></p>
</li>
</ul>
<hr>
<h1 id="用户流失分析"><a href="#用户流失分析" class="headerlink" title="用户流失分析"></a>用户流失分析</h1><p>说了那么多前置的铺垫，让我们来真实的面对我们需要解决的问题：</p>
<p>首先，我们拿到了用户是否流失的历史数据集20724条，流失与飞流失用户占比在1:4，这部分数据需要进行一下预处理，这边就不细讲预处理过程了，包含缺失值填充(分层填充)，异常值处理(isolation foest)，数据平衡(tomek link)，特征选择(xgboost importance)，特征变形(normalizing)，特征分布优化等等，工程技巧我之前的文章都有讲解过，不做本文重点。</p>
<p><strong>taiking is cheap,show me the code.</strong><br><img src="/2017/12/01/基于Tensorflow的神经网络解决用户流失概率问题/6.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#author:shataowei</span><br><span class="line">#time:20170924</span><br><span class="line">#基础包加载</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import math</span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#数据处理</span><br><span class="line">data = pd.read_table(&apos;/Users/slade/Desktop/machine learning/data/data_all.txt&apos;)</span><br><span class="line">data = data.iloc[:,1:len(data.columns)]</span><br><span class="line">data1 = (data - data.mean())/data.std()</span><br><span class="line">labels = data[&apos;tag&apos;]</span><br><span class="line">items = data1.iloc[:,1:len(data1.columns)]</span><br><span class="line">all_data = pd.concat([pd.DataFrame(labels),items],axis = 1)</span><br><span class="line"></span><br><span class="line">#数据集切分成训练集和测试集，占比为0.8：0.2</span><br><span class="line">train_X,test_X,train_y,test_y = train_test_split(items,labels,test_size = 0.2,random_state = 0)</span><br><span class="line"></span><br><span class="line">#pandas读取进来是dataframe，转换为ndarray的形式</span><br><span class="line">train_X = np.array(train_X)</span><br><span class="line">test_X = np.array(test_X)</span><br><span class="line"></span><br><span class="line">#我将0或者1的预测结果转换成了[0,1]或者[1,0]的对应形式，读者可以不转</span><br><span class="line">train_Y = []</span><br><span class="line">for i in train_y:</span><br><span class="line">if i ==0:</span><br><span class="line">train_Y.append([0,1])</span><br><span class="line">else:</span><br><span class="line">train_Y.append([1,0])</span><br><span class="line">test_Y = []</span><br><span class="line">for i in test_y:</span><br><span class="line">if i ==0:</span><br><span class="line">test_Y.append([0,1])</span><br><span class="line">else:</span><br><span class="line">test_Y.append([1,0])</span><br></pre></td></tr></table></figure>
<p>下面我们就要开始正式开始训练神经网络了，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">input_node = 9 #输入的feature的个数，也就是input的维度</span><br><span class="line">output_node = 2 #输出的[0,1]或者[1,0]的维度</span><br><span class="line">layer1_node = 500 #隐藏层的节点个数，一般在255-1000之间，读者可以自行调整</span><br><span class="line">batch_size = 200 #批量训练的数据，batch_size越小训练时间越长，训练效果越准确（但会存在过拟合）</span><br><span class="line">learning_rate_base = 0.8 #训练weights的速率η</span><br><span class="line">regularzation_rate = 0.0001 #正则力度</span><br><span class="line">training_steps = 10000 #训练次数，这个指标需要类似grid_search进行搜索优化</span><br><span class="line"></span><br><span class="line">#设定之后想要被训练的x及对应的正式结果y_</span><br><span class="line">x = tf.placeholder(tf.float32,[None,input_node])</span><br><span class="line">y_ = tf.placeholder(tf.float32,[None,output_node])</span><br><span class="line"></span><br><span class="line">#input到layer1之间的线性矩阵weight</span><br><span class="line">weight1 = tf.Variable(tf.truncated_normal([input_node,layer1_node],stddev=0.1))</span><br><span class="line">#layer1到output之间的线性矩阵weight</span><br><span class="line">weight2 = tf.Variable(tf.truncated_normal([layer1_node,output_node],stddev=0.1))</span><br><span class="line">#input到layer1之间的线性矩阵的偏置</span><br><span class="line">biases1 = tf.Variable(tf.constant(0.1,shape = [layer1_node]))</span><br><span class="line">#layer1到output之间的线性矩阵的偏置</span><br><span class="line">biases2 = tf.Variable(tf.constant(0.1,shape=[output_node]))</span><br><span class="line"></span><br><span class="line">#正向传播的流程，线性计算及激活函数relu的非线性计算得到result</span><br><span class="line">def interence(input_tensor,weight1,weight2,biases1,biases2):</span><br><span class="line">layer1 = tf.nn.relu(tf.matmul(input_tensor,weight1)+biases1)</span><br><span class="line">result = tf.matmul(layer1,weight2)+biases2</span><br><span class="line">return result</span><br><span class="line">y = interence(x,weight1,weight2,biases1,biases2)</span><br></pre></td></tr></table></figure></p>
<p>正向传播完成后，我们要反向传播来修正weight<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0,trainable = False)</span><br><span class="line">#交叉熵，用来衡量两个分布之间的相似程度</span><br><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = y_,logits=y)</span><br><span class="line">cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line"></span><br><span class="line">#l2正则化，这部分的理论分析可以参考我之前写的：http://www.jianshu.com/p/4f91f0dcba95</span><br><span class="line">regularzer = tf.contrib.layers.l2_regularizer(regularzation_rate)</span><br><span class="line">regularzation = regularzer(weight1) + regularzer(weight2)</span><br><span class="line"></span><br><span class="line">#损失函数为交叉熵+正则化</span><br><span class="line">loss = cross_entropy_mean + regularzation</span><br><span class="line"></span><br><span class="line">#我们用learning_rate_base作为速率η，来训练梯度下降的loss函数解</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(learning_rate_base).minimize(loss,global_step = global_step)</span><br><span class="line"></span><br><span class="line">#y是我们的预测值，y_是真实值，我们来找到y_及y(比如[0.1，0.2])中最大值对应的index位置，判断y与y_是否一致</span><br><span class="line">correction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))</span><br><span class="line"></span><br><span class="line">#如果y与y_一致则为1，否则为0，mean正好为其准确率</span><br><span class="line">accurary = tf.reduce_mean(tf.cast(correction,tf.float32))</span><br></pre></td></tr></table></figure></p>
<p>模型训练结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#初始化环境，设置输入值，检验值</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line">validate_feed = &#123;x:train_X,y_:train_Y&#125;</span><br><span class="line">test_feed = &#123;x:test_X,y_:test_Y&#125;</span><br><span class="line"></span><br><span class="line">#模型训练，每到1000次汇报一次训练效果</span><br><span class="line">for i in range(training_steps):</span><br><span class="line">start = (i*batch_size)%len(train_X)</span><br><span class="line">end = min(start+batch_size,16579)</span><br><span class="line">xs = train_X[start:end]</span><br><span class="line">ys = train_Y[start:end]</span><br><span class="line">if i%1000 ==0:</span><br><span class="line">validate_accuary = sess.run(accurary,feed_dict = validate_feed)</span><br><span class="line">print &apos;the times of training is %d, and the accurary is %s&apos; %(i,validate_accuary)</span><br><span class="line">sess.run(train_op,feed_dict = &#123;x:xs,y_:ys&#125;)</span><br></pre></td></tr></table></figure></p>
<p>训练的结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2017-09-24 12:11:28.409585: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">2017-09-24 12:11:28.409620: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">2017-09-24 12:11:28.409628: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">2017-09-24 12:11:28.409635: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span><br><span class="line">the times of training is 0, and the accurary is 0.736775</span><br><span class="line">the times of training is 1000, and the accurary is 0.99246</span><br><span class="line">the times of training is 2000, and the accurary is 0.993003</span><br><span class="line">the times of training is 3000, and the accurary is 0.992943</span><br><span class="line">the times of training is 4000, and the accurary is 0.992943</span><br><span class="line">the times of training is 5000, and the accurary is 0.99234</span><br><span class="line">the times of training is 6000, and the accurary is 0.993124</span><br><span class="line">the times of training is 7000, and the accurary is 0.992943</span><br><span class="line">the times of training is 8000, and the accurary is 0.993124</span><br><span class="line">the times of training is 9000, and the accurary is 0.992943</span><br></pre></td></tr></table></figure></p>
<p>初步看出，在训练集合上，准确率在能够99%以上，让我们在看看测试集效果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_accuary = sess.run(accurary,feed_dict = test_feed)</span><br></pre></td></tr></table></figure></p>
<p><code>Out[5]: 0.99034983</code>,也是我们的测试数据集效果也是在99%附近，可以看出这个分类的效果还是比较高的。</p>
<p>初次之外，我们还可以得到每个值被预测出来的结果，也可以通过工程技巧转换为0-1的概率：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">result_y = sess.run(y,feed_dict=&#123;x:train_X&#125;)</span><br><span class="line">result_y_update=[]</span><br><span class="line">for i in result_y:</span><br><span class="line">if i[0]&gt;=i[1]:</span><br><span class="line">result_y_update.append([1,0])</span><br><span class="line">else:</span><br><span class="line">result_y_update.append([0,1])</span><br></pre></td></tr></table></figure></p>
<p>==&gt;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Out[7]:</span><br><span class="line">array([[-1.01412344,  1.21461654],</span><br><span class="line">[-3.66026735,  3.81834102],</span><br><span class="line">[-3.78952932,  3.79097509],</span><br><span class="line">...,</span><br><span class="line">[-3.71239662,  3.65721083],</span><br><span class="line">[-1.59250259,  1.89412308],</span><br><span class="line">[-3.35591984,  3.24001145]], dtype=float32)</span><br></pre></td></tr></table></figure></p>
<p>以上就实现了如果用TensorFlow里面的神经网络技巧去做一个分类问题，其实这并不TensorFlow的全部，传统的Bp神经网络，SVM也可以到达近似的效果，在接下来的文章中，我们将继续看到比如CNN图像识别，LSTM进行文本分类，RNN训练不均衡数据等复杂问题上面的优势。</p>
<h2 id="可能存在的问题"><a href="#可能存在的问题" class="headerlink" title="可能存在的问题"></a>可能存在的问题</h2><p>在刚做神经网络的训练前，要注意一下是否会犯以下的错误。</p>
<ul>
<li><p>数据是否规范化<br>模型计算的过程时间长度及模型最后的效果，均依赖于input的形式。大部分的神经网络训练过程都是以input为1的标准差，0的均值为前提的；除此之外，在算梯度算反向传播的时候，过大的值有可能会导致梯度消失等意想不到的情况，非常值得大家注意</p>
</li>
<li><p>batch的选择<br>在上面我也提了，过小的batch会增加模型过拟合的风险，且计算的时间大大增加。过大的batch会造成模型的拟合能力不足，可能会被局部最小值卡住等等，所以需要多次选择并计算尝试。</p>
</li>
<li><p>过拟合的问题<br>是否在计算过程中只考虑了损失函数比如交叉熵，有没有考虑l2正则、l2正则，或者有没有进行dropout行为，是否有必要加入噪声，在什么地方加入噪声（weight？input？），需不需要结合Bagging或者bayes方法等</p>
</li>
<li><p>激活函数的选择是否正确<br>比如relu只能产出&gt;=0的结果，是否符合最后的产出结果要求。比如Sigmoid的函数在数据离散且均大于+3的数据集合上会产生梯度消失的问题，等等</p>
</li>
</ul>
<hr>
<p>到这里，我觉得一篇用TensorFlow来训练分类模型来解决用户流失这个问题就基本上算是梳理完了。很多简单的知识点我没有提，上面这些算是比较重要的模块，希望对大家有所帮助，最后谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CRM预估 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于SSD下的图像内容识别（二）]]></title>
      <url>/2017/12/01/%E5%9F%BA%E4%BA%8ESSD%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>上一节粗略的描述了如何关于图像识别，抠图，分类的理论相关，本节主要用代码，来和大家一起分析每一步骤。<br>看完本节，希望你也能独立完成自己的图片、视频的内容实时定位。</p>
<p>首先，我们需要安装TensorFlow环境，建议利用<a href="https://www.anaconda.com/downloads" target="_blank" rel="noopener">conda</a>进行安装，配置，90%尝试单独安装的人最后都挂了。</p>
<p>其次，我们需要安装从git上下载训练好的模型，git clone <a href="https://github.com/balancap/SSD-Tensorflow" target="_blank" rel="noopener">https://github.com/balancap/SSD-Tensorflow</a><br>如果没有安装git的朋友，请自行百度安装。</p>
<p>最后找到你下载的位置进行解压，unzip ./SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt.zip<br><strong>这边务必注意，网上90%的教程这边就结束了，其实你这样是最后跑不通代码的，你需要把解压的文件进行移动到checkpoint的文件夹下面</strong>，这个问题git上这个同学解释了，详细的去看下<a href="https://github.com/balancap/SSD-Tensorflow/issues/150" target="_blank" rel="noopener">https://github.com/balancap/SSD-Tensorflow/issues/150</a></p>
<p>最后的最后，下载你需要检测的网路图片，就ok了</p>
<p>预处理步骤完成了，下面让我们看代码。<br><strong>加载相关的包：</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> mpcm</span><br><span class="line">sys.path.append(<span class="string">'./SSD-Tensorflow/'</span>)</span><br><span class="line"><span class="keyword">from</span> nets <span class="keyword">import</span> ssd_vgg_300, ssd_common, np_methods</span><br><span class="line"><span class="keyword">from</span> preprocessing <span class="keyword">import</span> ssd_vgg_preprocessing</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>配置相关TensorFlow环境</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpu_options = tf.GPUOptions(allow_growth=<span class="keyword">True</span>)</span><br><span class="line">config = tf.ConfigProto(log_device_placement=<span class="keyword">False</span>, gpu_options=gpu_options)</span><br><span class="line">isess = tf.InteractiveSession(config=config)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>做图片的格式的处理，使他满足input的条件</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们用的TensorFlow下的一个集成包slim，比tensor要更加轻便</span></span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line"><span class="comment">#训练数据中包含了一下已知的类别，也就是我们可以识别出以下的东西，不过后续我们将自己自己训练自己的模型，来识别自己想识别的东西</span></span><br><span class="line">l_VOC_CLASS = [</span><br><span class="line"><span class="string">'aeroplane'</span>,   <span class="string">'bicycle'</span>, <span class="string">'bird'</span>,  <span class="string">'boat'</span>,      <span class="string">'bottle'</span>,</span><br><span class="line"><span class="string">'bus'</span>,         <span class="string">'car'</span>,     <span class="string">'cat'</span>,   <span class="string">'chair'</span>,     <span class="string">'cow'</span>,</span><br><span class="line"><span class="string">'diningTable'</span>, <span class="string">'dog'</span>,     <span class="string">'horse'</span>, <span class="string">'motorbike'</span>, <span class="string">'person'</span>,</span><br><span class="line"><span class="string">'pottedPlant'</span>, <span class="string">'sheep'</span>,   <span class="string">'sofa'</span>,  <span class="string">'train'</span>,     <span class="string">'TV'</span></span><br><span class="line">]</span><br><span class="line"><span class="comment"># 定义数据格式</span></span><br><span class="line">net_shape = (<span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">data_format = <span class="string">'NHWC'</span>  <span class="comment"># [Number, height, width, color]，Tensorflow backend 的格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理将输入图片大小改成 300x300，作为下一步输入</span></span><br><span class="line">img_input = tf.placeholder(tf.uint8, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">3</span>))</span><br><span class="line">image_pre, labels_pre, bboxes_pre, bbox_img = ssd_vgg_preprocessing.preprocess_for_eval(</span><br><span class="line">img_input,</span><br><span class="line"><span class="keyword">None</span>,</span><br><span class="line"><span class="keyword">None</span>,</span><br><span class="line">net_shape,</span><br><span class="line">data_format,</span><br><span class="line">resize=ssd_vgg_preprocessing.Resize.WARP_RESIZE</span><br><span class="line">)</span><br><span class="line">image_4d = tf.expand_dims(image_pre, <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>下面我们来载入SSD作者已经搞定的模型</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 SSD 模型结构</span></span><br><span class="line">reuse = <span class="keyword">True</span> <span class="keyword">if</span> <span class="string">'ssd_net'</span> <span class="keyword">in</span> locals() <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line">ssd_net = ssd_vgg_300.SSDNet()</span><br><span class="line"><span class="keyword">with</span> slim.arg_scope(ssd_net.arg_scope(data_format=data_format)):</span><br><span class="line">predictions, localisations, _, _ = ssd_net.net(image_4d, is_training=<span class="keyword">False</span>, reuse=reuse)</span><br><span class="line"><span class="comment"># 导入官方给出的 SSD 模型参数</span></span><br><span class="line"><span class="comment">#这边修改成你自己的路径</span></span><br><span class="line">ckpt_filename = <span class="string">'/Users/slade/SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt'</span></span><br><span class="line">isess.run(tf.global_variables_initializer())</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.restore(isess, ckpt_filename)</span><br><span class="line">ssd_anchors = ssd_net.anchors(net_shape)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>下面让我们把SSD识别出来的结果在图片中表示出来</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不同类别，我们以不同的颜色表示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colors_subselect</span><span class="params">(colors, num_classes=<span class="number">21</span>)</span>:</span></span><br><span class="line">dt = len(colors) // num_classes</span><br><span class="line">sub_colors = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_classes):</span><br><span class="line">color = colors[i*dt]</span><br><span class="line"><span class="keyword">if</span> isinstance(color[<span class="number">0</span>], float):</span><br><span class="line">sub_colors.append([int(c * <span class="number">255</span>) <span class="keyword">for</span> c <span class="keyword">in</span> color])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">sub_colors.append([c <span class="keyword">for</span> c <span class="keyword">in</span> color])</span><br><span class="line"><span class="keyword">return</span> sub_colors</span><br><span class="line"><span class="comment">#画出在图中的位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bboxes_draw_on_img</span><span class="params">(img, classes, scores, bboxes, colors, thickness=<span class="number">5</span>)</span>:</span></span><br><span class="line">shape = img.shape</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(bboxes.shape[<span class="number">0</span>]):</span><br><span class="line">bbox = bboxes[i]</span><br><span class="line">color = colors[classes[i]]</span><br><span class="line"><span class="comment"># Draw bounding box...</span></span><br><span class="line">p1 = (int(bbox[<span class="number">0</span>] * shape[<span class="number">0</span>]), int(bbox[<span class="number">1</span>] * shape[<span class="number">1</span>]))</span><br><span class="line">p2 = (int(bbox[<span class="number">2</span>] * shape[<span class="number">0</span>]), int(bbox[<span class="number">3</span>] * shape[<span class="number">1</span>]))</span><br><span class="line">cv2.rectangle(img, p1[::<span class="number">-1</span>], p2[::<span class="number">-1</span>], color, thickness)</span><br><span class="line"><span class="comment"># Draw text...</span></span><br><span class="line">s = <span class="string">'%s:%.3f'</span> % ( l_VOC_CLASS[int(classes[i])<span class="number">-1</span>], scores[i])</span><br><span class="line">p1 = (p1[<span class="number">0</span>]<span class="number">-5</span>, p1[<span class="number">1</span>])</span><br><span class="line">cv2.putText(img, s, p1[::<span class="number">-1</span>], cv2.FONT_HERSHEY_SIMPLEX, <span class="number">1</span>, color, <span class="number">2</span>)</span><br><span class="line">colors_plasma = colors_subselect(mpcm.plasma.colors, num_classes=<span class="number">21</span>)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>让我们开始训练吧</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_image</span><span class="params">(img, select_threshold=<span class="number">0.3</span>, nms_threshold=<span class="number">.8</span>, net_shape=<span class="params">(<span class="number">300</span>, <span class="number">300</span>)</span>)</span>:</span></span><br><span class="line"><span class="comment">#先获取SSD网络的层相关的参数</span></span><br><span class="line">rimg, rpredictions, rlocalisations, rbbox_img = isess.run([image_4d, predictions, localisations, bbox_img],</span><br><span class="line">feed_dict=&#123;img_input: img&#125;)</span><br><span class="line"><span class="comment">#获取分类结果，位置</span></span><br><span class="line">rclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(</span><br><span class="line">rpredictions, rlocalisations, ssd_anchors,</span><br><span class="line">select_threshold=select_threshold, img_shape=net_shape, num_classes=<span class="number">21</span>, decode=<span class="keyword">True</span>)</span><br><span class="line">rbboxes = np_methods.bboxes_clip(rbbox_img, rbboxes)</span><br><span class="line">rclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=<span class="number">400</span>)</span><br><span class="line">rclasses, rscores, rbboxes = np_methods.bboxes_nms(rclasses, rscores, rbboxes, nms_threshold=nms_threshold)</span><br><span class="line"><span class="comment"># 让我们在图中画出来就行了</span></span><br><span class="line">rbboxes = np_methods.bboxes_resize(rbbox_img, rbboxes)</span><br><span class="line">bboxes_draw_on_img(img, rclasses, rscores, rbboxes, colors_plasma, thickness=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">return</span> img</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>预处理的函数都写完了，我们就可以执行了。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">img = cv2.imread(<span class="string">"/Users/slade/Documents/Yoho/picture_recognize/test7.jpg"</span>)</span><br><span class="line">img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">plt.imshow(process_image(img))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>img的数据形式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">In [8]: img</span><br><span class="line">Out[8]:</span><br><span class="line">array([[[ 35,  59,  43],</span><br><span class="line">[ 37,  60,  44],</span><br><span class="line">[ 38,  61,  45],</span><br><span class="line">...,</span><br><span class="line">[ 73,  99,  62],</span><br><span class="line">[ 74,  99,  60],</span><br><span class="line">[ 72,  97,  57]],</span><br><span class="line"></span><br><span class="line">[[ 37,  60,  44],</span><br><span class="line">[ 37,  60,  44],</span><br><span class="line">[ 37,  60,  44],</span><br><span class="line">...,</span><br><span class="line">[ 66,  92,  57],</span><br><span class="line">[ 67,  93,  56],</span><br><span class="line">[ 67,  92,  53]],</span><br><span class="line"></span><br><span class="line">[[ 37,  60,  44],</span><br><span class="line">[ 36,  59,  43],</span><br><span class="line">[ 37,  58,  43],</span><br><span class="line">...,</span><br><span class="line">[ 56,  83,  48],</span><br><span class="line">[ 60,  86,  51],</span><br><span class="line">[ 61,  87,  50]],</span><br><span class="line"></span><br><span class="line">...,</span><br><span class="line">[[ 96, 101,  95],</span><br><span class="line">[107, 109, 104],</span><br><span class="line">[ 98,  97,  95],</span><br><span class="line">...,</span><br><span class="line">[ 84, 126,  76],</span><br><span class="line">[ 72, 118,  72],</span><br><span class="line">[ 78, 126,  86]],</span><br><span class="line"></span><br><span class="line">[[ 98, 103,  96],</span><br><span class="line">[114, 116, 111],</span><br><span class="line">[112, 113, 108],</span><br><span class="line">...,</span><br><span class="line">[ 94, 137,  84],</span><br><span class="line">[ 87, 133,  86],</span><br><span class="line">[105, 153, 111]],</span><br><span class="line"></span><br><span class="line">[[ 99, 105,  95],</span><br><span class="line">[110, 113, 106],</span><br><span class="line">[134, 135, 129],</span><br><span class="line">...,</span><br><span class="line">[127, 170, 116],</span><br><span class="line">[121, 167, 118],</span><br><span class="line">[131, 180, 135]]], dtype=uint8)</span><br></pre></td></tr></table></figure></p>
<p>处理后的结果如下：<img src="/2017/12/01/基于SSD下的图像内容识别（二）/1.png" alt=""></p>
<p>是不是非常无脑，上面的代码直接复制就可以完成。</p>
<p>下面在拓展一下视频的处理方式，其实相关的内容是一致的。<br>利用moviepy.editor包里面的VideoFileClip的切片的功能，然后对每一次切片的结果进行process_image过程就可以了，这边就不贴代码了，需要的朋友私密我。</p>
<p>最后感谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 图像识别 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于SSD下的图像内容识别（一）]]></title>
      <url>/2017/12/01/%E5%9F%BA%E4%BA%8ESSD%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      <content type="html"><![CDATA[<p><strong>鸽了将近有一个月的时间没有更新东西，真的不是因为我懒，主要在忙一些工作上的事情，然后就是被安装caffe环境折磨的死去活来。我本来用的上mba来搭caffe环境的，一直在报一个框架问题，索性一怒之下换了mbp，下面就将我在SSD学习过程中遇到的问题和大家一起分享一下。</strong></p>
<h1 id="首先，先看一下我们能达到什么样的效果："><a href="#首先，先看一下我们能达到什么样的效果：" class="headerlink" title="首先，先看一下我们能达到什么样的效果："></a>首先，先看一下我们能达到什么样的效果：</h1><p>比如,这样的：<br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/1.png" alt=""></p>
<p>再比如这样的：<br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/2.png" alt=""></p>
<p>甚至还可以这样：<br><a href="https://v.qq.com/x/page/a0567wd27jz.html" target="_blank" rel="noopener">https://v.qq.com/x/page/a0567wd27jz.html</a><br><a href="https://v.qq.com/x/page/j05679xhryx.html" target="_blank" rel="noopener">https://v.qq.com/x/page/j05679xhryx.html</a><br>这边吐槽一下，简书makedown不支持上传视频，简直差评！</p>
<p>那问题来了，在真实的业务场景中，我们有哪些应用呢？<br><strong>比如天猫的拍照购：</strong><br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/3.png" alt=""></p>
<p><strong>有货的相似推荐：</strong><br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/4.png" alt=""><br>这些都是非常优秀的应用场景。</p>
<h1 id="我们需要做哪些基本的步骤："><a href="#我们需要做哪些基本的步骤：" class="headerlink" title="我们需要做哪些基本的步骤："></a>我们需要做哪些基本的步骤：</h1><h2 id="抠出图片中关键的人或者物"><a href="#抠出图片中关键的人或者物" class="headerlink" title="抠出图片中关键的人或者物"></a>抠出图片中关键的人或者物</h2><p>如果只需要抠出图片中的核心信息的话，其实只需要加载python里面的selectivesearch包就可以（这里多说一句，建议都是使用conda安装所有包和库，不然你会后悔的）。<br>我们先来看下效果：<br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/5.png" alt=""></p>
<p>这个是怎么实现的呢？<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  cv2</span><br><span class="line"><span class="keyword">import</span> selectivesearch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span>  plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span>  mpatches</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span>  np</span><br><span class="line">img = cv2.imread(<span class="string">'/Users/slade/Documents/Yoho/picture_recognize/heshen.jpg'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#图片识别分割</span></span><br><span class="line">img_lbl, regions =selectivesearch.selective_search(</span><br><span class="line">img, scale=<span class="number">500</span>, sigma=<span class="number">0.9</span>, min_size=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#这边的regions里面就有一个个划分出来的box</span></span><br><span class="line">regions[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#Out[3]: &#123;'labels': [0.0], 'rect': (0, 0, 619, 620), 'size': 177325&#125;,其中‘rect’定位了box的位置，‘size’确定了box的大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来我们把窗口和图像打印出来，对它有个直观认识</span></span><br><span class="line">fig, ax = plt.subplots(ncols=<span class="number">1</span>, nrows=<span class="number">1</span>, figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line">ax.imshow(img)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> reg <span class="keyword">in</span> regions:</span><br><span class="line">x, y, w, h = reg[<span class="string">'rect'</span>]</span><br><span class="line">rect = mpatches.Rectangle(</span><br><span class="line">(x, y), w, h, fill=<span class="keyword">False</span>, edgecolor=<span class="string">'red'</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">ax.add_patch(rect)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>搜索完成后展示图：<br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/6.png" alt=""><br>很明显，这里面的方框太多了，所以我们需要把一些过小的，过大的，不规则的全部去掉：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">candidates = []</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> regions:</span><br><span class="line"><span class="comment"># 重复的不要</span></span><br><span class="line"><span class="keyword">if</span> r[<span class="string">'rect'</span>] <span class="keyword">in</span> candidates:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"><span class="comment"># 太小和太大的不要</span></span><br><span class="line"><span class="keyword">if</span> r[<span class="string">'size'</span>] &lt; <span class="number">200</span> <span class="keyword">or</span> r[<span class="string">'size'</span>]&gt;<span class="number">20000</span>:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">x, y, w, h = r[<span class="string">'rect'</span>]</span><br><span class="line"><span class="comment"># 太不方的不要</span></span><br><span class="line"><span class="keyword">if</span> w / h &gt; <span class="number">1.8</span> <span class="keyword">or</span> h / w &gt; <span class="number">1.8</span>:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">candidates.append((x,y,w,h))</span><br><span class="line"></span><br><span class="line"><span class="comment">#剔除大box内的小box</span></span><br><span class="line">candidates_sec = []</span><br><span class="line"><span class="keyword">for</span>  i  <span class="keyword">in</span>  candidates:</span><br><span class="line"><span class="keyword">if</span> len(candidates_sec)==<span class="number">0</span>:</span><br><span class="line">candidates_sec.append(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">Flag=<span class="keyword">False</span></span><br><span class="line">replace=<span class="number">-1</span></span><br><span class="line">index=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j  <span class="keyword">in</span> candidates_sec:</span><br><span class="line"><span class="comment">##新box在小圈 则删除</span></span><br><span class="line"><span class="keyword">if</span> i[<span class="number">0</span>]&gt;=j[<span class="number">0</span>] <span class="keyword">and</span> i[<span class="number">0</span>]+i[<span class="number">2</span>]&lt;=j[<span class="number">0</span>]+j[<span class="number">2</span>]  <span class="keyword">and</span> i[<span class="number">1</span>]&gt;=j[<span class="number">1</span>] <span class="keyword">and</span> i[<span class="number">1</span>]+i[<span class="number">3</span>]&lt;=j[<span class="number">1</span>]+j[<span class="number">3</span>]:</span><br><span class="line">Flag=<span class="keyword">True</span></span><br><span class="line"><span class="keyword">break</span></span><br><span class="line"><span class="comment">##新box不在小圈 而在老box外部 替换老box</span></span><br><span class="line"><span class="keyword">elif</span> i[<span class="number">0</span>]&lt;=j[<span class="number">0</span>] <span class="keyword">and</span> i[<span class="number">0</span>]+i[<span class="number">2</span>]&gt;=j[<span class="number">0</span>]+j[<span class="number">2</span>] <span class="keyword">and</span> i[<span class="number">1</span>]&lt;=j[<span class="number">1</span>] <span class="keyword">and</span> i[<span class="number">1</span>]+i[<span class="number">3</span>]&gt;=j[<span class="number">1</span>]+j[<span class="number">3</span>]:</span><br><span class="line">replace=index</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">index+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> Flag:</span><br><span class="line"><span class="keyword">if</span> replace&gt;=<span class="number">0</span>:</span><br><span class="line">candidates_sec[replace]=i</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">candidates_sec.append(i)</span><br></pre></td></tr></table></figure></p>
<p>然后我们看看更新完后的图片效果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))</span><br><span class="line">ax.imshow(img)</span><br><span class="line">for x, y, w, h in candidates_sec:</span><br><span class="line">rect = mpatches.Rectangle(</span><br><span class="line">(x, y), w, h, fill=False, edgecolor=&apos;red&apos;, linewidth=1)</span><br><span class="line">ax.add_patch(rect)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/2017/12/01/基于SSD下的图像内容识别（一）/7.png" alt=""></p>
<p>在根据重复优化一下，就可以得到最初的那张图片，基本上来说，就可以完成抠图这个事情了。</p>
<h2 id="相关理论概述："><a href="#相关理论概述：" class="headerlink" title="相关理论概述："></a>相关理论概述：</h2><p>上面这样的识别从数学角度上是怎么样实现的呢？<br>这边先引入一篇文章：2014年CVPR上的经典paper：《Rich feature hierarchies for Accurate Object Detection and Segmentation》，这篇文章的算法思想又被称之为：R-CNN（Regions with Convolutional Neural Network Features），是物体检测领域曾经获得state-of-art精度的经典文献。<br>论文较为复杂冗长，我这边主要先看一下我们关系的抠图模块：</p>
<h3 id="抠若干个box过程："><a href="#抠若干个box过程：" class="headerlink" title="抠若干个box过程："></a>抠若干个box过程：</h3><p>先切分图片到若干子区域的集合S<br>1.在计算集合S中找出相似性最大的区域max_similarity{ri,rj}<br>2.合并S_new=ri∪rj<br>3.从S集合中，移走所有与ri,rj的子集<br>4.将新集合S_new与相邻区域的相似度<br>5.repeat step2<br>直到S集合为空<br>这边相似度的计算考虑了三个方面：颜色相似，纹理相似，空间交错相似，分别解释如下：<br><strong>颜色相似：</strong><br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/8.jpg" alt=""><br>其中对每个区域，我们都可以得到一个一维的颜色分布直方图：<br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/9.jpg" alt=""><br>假设两个直方图波峰和波谷高度重合，那么计算下来的值比较大；反之如果波峰和波谷错开的，那么累加的值一定比较小。</p>
<p><strong>纹理相似：</strong><br>这边会用到<a href="http://www.cnblogs.com/saintbird/archive/2008/08/20/1271943.html" target="_blank" rel="noopener">SIFI算法</a>，也是一个比较经典的算法。<br>selectivesearch论文采用方差为1的高斯分布在8个方向做梯度统计，然后将统计结果（尺寸与区域大小一致）以bins=10计算直方图。（这个我也没有仔细去看，只是skip learn了一下）</p>
<p><strong>空间相似：</strong><br>这个最简单，代码呈现了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sim_fill</span><span class="params">(r1, r2, imsize)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">calculate the fill_similarity over the image</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">bbsize = (</span><br><span class="line">(max(r1[<span class="string">"max_x"</span>], r2[<span class="string">"max_x"</span>]) - min(r1[<span class="string">"min_x"</span>], r2[<span class="string">"min_x"</span>]))</span><br><span class="line">* (max(r1[<span class="string">"max_y"</span>], r2[<span class="string">"max_y"</span>]) - min(r1[<span class="string">"min_y"</span>], r2[<span class="string">"min_y"</span>]))</span><br><span class="line">)</span><br><span class="line"><span class="keyword">return</span> <span class="number">1.0</span> - (bbsize - r1[<span class="string">"size"</span>] - r2[<span class="string">"size"</span>]) / imsize</span><br></pre></td></tr></table></figure></p>
<h3 id="若干个box筛选的过程："><a href="#若干个box筛选的过程：" class="headerlink" title="若干个box筛选的过程："></a>若干个box筛选的过程：</h3><p>首先，我们定义：IOU为两个bounding box的重叠度，如下图所示：<br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/10.png" alt=""><br>矩形框A、B的一个重合度IOU计算公式为：<br>IOU=(A∩B)/(A∪B)<br>就是矩形框A、B的重叠面积占A、B并集的面积比例:<br>IOU=SI/(SA+SB-SI)</p>
<p>再引入非极大值抑制（NMS）概念：抑制不是极大值的元素，搜索局部的极大值。<br>翻译一下就是：比如之前看的这张图片：<br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/11.png" alt=""><br>很明显还是很多box是相互inner的，虽然没有被相互包含进去，我们可以先选择最大的box，看其他box与这个最大的box的IOU值，删除IOU值大于预先设定的阈值的box，重复这个过程就是一个方框删除的过程。真实的NMS还会涉及到canny detection等等的细节问题，这边只是让大家快速入门使用起来，如需详细了解，请自行百度。</p>
<h3 id="若干个box内容对应："><a href="#若干个box内容对应：" class="headerlink" title="若干个box内容对应："></a>若干个box内容对应：</h3><p>我们虽然识别出了方框内存在物体，但是我们仍需要将物体与标签对应起来，这边的方法就是很多了，RCNN里面的方法：SVM，还有现在非常热门的CNN都可以对识别出来的子图片进行识别分类：<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/index.html" target="_blank" rel="noopener">VOC物体检测任务</a>是一个非常入门的分类问题。你可以通过任何一种你觉得可以的分类方法进行识别训练。</p>
<p>简单的流程化的识别拆分讲解这边就结束了，主要讲了candidates_boxs的产生，candidates_boxs通过基本属性的初筛，candidates_boxs根据IOU原则下的NMS进行复选，再将复选出来的box根据你已经训练好的分类模型确定到底是啥？<br>可以用下面这个图概述一下：<br><img src="/2017/12/01/基于SSD下的图像内容识别（一）/12.jpg" alt=""></p>
<h1 id="To-do"><a href="#To-do" class="headerlink" title="To-do:"></a>To-do:</h1><p>我们还有很多没讲完的，后面会持续更新：<br>主要包括：<br>1.如何配置一个快速训练的环境？<br>2.如何实现（输入图片，产出结果）整套识别流程？<br>3.如何自己训练一个图片分类器？<br>4.如何做快速迁移一个自己需要的及时图片识别流？</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 图像识别 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[动态最优化经典面试题]]></title>
      <url>/2017/12/01/%E5%8A%A8%E6%80%81%E6%9C%80%E4%BC%98%E5%8C%96%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>最近看到了一条史前的算法面试题，觉得挺有意思的，虽然网上已经有了很多完善的答案，但是我还是想自己整理一遍，强化印象，同时也和大家分享一下这道12年的Google题目：</p>
<p><strong>一幢 200 层的大楼，给你两个鸡蛋。如果在第 n 层扔下鸡蛋，鸡蛋不碎，那么从第 n-1 层扔鸡蛋，都不碎。这两只鸡蛋一模一样，不碎的话可以扔无数次。最高从哪层楼扔下时鸡蛋不会碎？</strong></p>
<p>先形象的理解一下这道题目，假设第一个蛋我们放在了i层，有两种case，碎或者不碎。<br>先看简单的结果，<br>case1:如果碎了，为了求出层数，那么我接下来的那颗蛋需要从第1层开始尝试i-1次，因为我们不允许冒第二次碎的风险了，这很好理解。<br>case2:如果没碎，我们得知一条新的信息，那就是我们要求的目标层在i层之上，但是我们依旧不知道是哪一层，假设是m层（m&gt;i）,那么同样的，和第i层一样，面临2个case，碎或者不碎。</p>
<p>这时候，我们的前提是在最恶劣的情况下，保证我们的每次的风险都尽可能的小，至少要少于上一次的风险。</p>
<p>我们可以让新的m的高度为i+i-1,其中，i是第一次我们放的层数，i-1是我们选择的风险若于第一次风险的层数高度，类推下去：i+i-1+i-2+i-3+…+1=200，得到i=20，就是我们第一次应该放的位置，同理第二次如果没有碎应该放的就是39…</p>
<p><strong>我个人对这道题目的理解中，其实就为了平分风险，让每次碎的高度都相等，也就是i-1 = m-i-1+1==&gt;m=2i-1</strong></p>
<p>这边的python代码网上也有很多，这边我罗列一个我写的，可能和别人的不一样，实现效率也可能较慢，建议大家在网上搜完善版本的，仅供大家熟悉上述的描述：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#n为层数，m为蛋数，f函数为求最优层数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(n, m)</span>:</span></span><br><span class="line"><span class="comment">#如果是0层的，返回最优层数为0</span></span><br><span class="line"><span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment">#如果只有一个鸡蛋，必须要从最低层开始试，所以为当前最安全层n</span></span><br><span class="line"><span class="keyword">if</span> m == <span class="number">1</span>:</span><br><span class="line"><span class="keyword">return</span> n</span><br><span class="line"><span class="comment">#这边我们来看，f(i - 1, m - 1)是如果i层碎了，我们需要计算i-1层下的情况，同时减少一颗蛋；</span></span><br><span class="line"><span class="comment">#f(n - i, m)是i层没碎，那相当于安全层从0变成了n，要计算的就是相当于有 f(n, m)变成了 f(n - i, m)</span></span><br><span class="line"><span class="comment">#最后在最大化风险下找出其中风险最小的层数即可</span></span><br><span class="line">best_floor = min([max([f(i - <span class="number">1</span>, m - <span class="number">1</span>), f(n - i, m)]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>)] + <span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> best_floor</span><br></pre></td></tr></table></figure></p>
<p>结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]:print(f(<span class="number">100</span>, <span class="number">2</span>))</span><br><span class="line"><span class="number">14</span></span><br></pre></td></tr></table></figure></p>
<p>这边f(200,2)实在没跑出来，时间太久了，所以跑了100，2的结果，迭代次数超多，具体我没有算过，建议优化一下计算的代码再执行。</p>
<p>最后谢谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[用户生命周期]]></title>
      <url>/2017/11/02/%E7%94%A8%E6%88%B7%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</url>
      <content type="html"><![CDATA[<blockquote>
<p>摘要：设计一套完整的用户生命周期策略，极大程度上会提高用户活跃，降低用户流失，反应用户留存，为平台运营的不可或缺的一环</p>
</blockquote>
<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>用户生命周期是指用户从加入平台开始，熟悉平台，参与平台，最终流失的整个过程。用户的生命周期相对于自身而言，是一种参与度的变化，参与度也可以称之为活跃度。</p>
<hr>
<h3 id="如何定义参与度？"><a href="#如何定义参与度？" class="headerlink" title="如何定义参与度？"></a>如何定义参与度？</h3><p>以电商平台而言，冒泡（打开app），浏览，点击，搜索，收藏，加购物车，下单，评论等都是用户参与平台的主要行为，综合考虑（但不限于此）这些因素，</p>
<p><strong>活跃度：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">θ = α1* pv + α2 * click + α3* search + α4 * clc + α5* cart + α6* order + α7* comment + bubble</span><br></pre></td></tr></table></figure></p>
<p><em>其中，θ是活跃度，pv是用户浏览量，click是用户点击量，search是用户搜索量，clc是用户收藏量，cart是用户加购物车次，order为用户订单量，comment为用户评论量<br>α1 为全部用户冒泡次数 与 全部用户浏览量之比；<br>α2 为全部用户冒泡次数 与 全部用户点击量之比；<br>…
</em><br><strong>这样保证了，所有平台参与行为与用户活跃情况成正相关，同时动态变化的降低了操作成本低的变量的权重，也满足奥卡姆剃刀原理</strong><br>后续再利用活跃度来直接衡量生命周期状态。</p>
<h1 id="如何定义生命周期？"><a href="#如何定义生命周期？" class="headerlink" title="如何定义生命周期？"></a>如何定义生命周期？</h1><ul>
<li>以电商平台为例，考虑用户的行为，先来定义生命周期状态划分逻辑：<br>1.计算用户连续N(N&gt;3)个周期内的参与度组成特征向量<br>2.形成不同生命周期下的模式特征向量<br>3.分类用户的特征向量如下：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">生命周期状态</th>
<th style="text-align:center">生命周期类型</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">初生期</td>
<td style="text-align:center">新增用户</td>
<td style="text-align:center">处于新生用户没有用户记录</td>
</tr>
<tr>
<td style="text-align:center">成长期</td>
<td style="text-align:center">显性成长</td>
<td style="text-align:center">最近三次生命周期状态都是成长期</td>
</tr>
<tr>
<td style="text-align:center">成长期</td>
<td style="text-align:center">隐性成长</td>
<td style="text-align:center">最近三次生命周期状态不全是成长期</td>
</tr>
<tr>
<td style="text-align:center">稳定期</td>
<td style="text-align:center">低稳定</td>
<td style="text-align:center">处于平稳期阶段，参与度低于1/4分位数</td>
</tr>
<tr>
<td style="text-align:center">稳定期</td>
<td style="text-align:center">中稳定</td>
<td style="text-align:center">处于平稳期阶段，参与度介于1/4-3/4分位数</td>
</tr>
<tr>
<td style="text-align:center">稳定期</td>
<td style="text-align:center">高稳定</td>
<td style="text-align:center">处于平稳期阶段，参与度高于3/4分位数</td>
</tr>
<tr>
<td style="text-align:center">衰退期</td>
<td style="text-align:center">轻微衰退</td>
<td style="text-align:center">连续x个周期进入衰退期或流失期</td>
</tr>
<tr>
<td style="text-align:center">衰退期</td>
<td style="text-align:center">重度微衰退</td>
<td style="text-align:center">连续x个周期进入衰退期或流失期</td>
</tr>
<tr>
<td style="text-align:center">流失期</td>
<td style="text-align:center">流失期</td>
<td style="text-align:center">刚进入流失期</td>
</tr>
<tr>
<td style="text-align:center">沉默期</td>
<td style="text-align:center">沉默期</td>
<td style="text-align:center">长期处于流失期</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>定义完整的用户生命周期状态后，再对用户的生命周期做session切分，根据聚类算法，将样本用户进行聚类，形成聚类中心，判断用户距离聚类中心距离，匹配用户所处的生命周期详细位置，反过来输出分位数，判断用户生命周期类型。</li>
</ul>
<h1 id="下面思考如何优化kmeans解决这个问题："><a href="#下面思考如何优化kmeans解决这个问题：" class="headerlink" title="下面思考如何优化kmeans解决这个问题："></a>下面思考如何优化kmeans解决这个问题：</h1><p>考虑到业务开发的效率等原因，常规的聚类算法中，kmeans常常为优先考虑的算法，但实际运用过程中，需要根据不同的问题有差异化的优化。</p>
<p>1.考虑用户的特征偏移<br>可能存在用户的活跃属性间断，比如用户外出出差一周，导致某个单位统计时间内平台参与度下降，用户的活跃属性下降，而实际用户为真实高活跃用户，只是出现异常间断点，影响用户活跃的最终判断，利用语义分析中的最佳路径计算方式解决这个问题。<br><img src="/2017/11/02/用户生命周期/1.png" alt=""><br>这三条线中，蓝色和青色线的分布走势类似，而红色线条的差异较大；计算蓝色—&gt;红色的欧式距离，蓝色—&gt;青色的欧式距离，发现蓝色—&gt;青色的欧式距离反而大于蓝色—&gt;红色的欧式距离，时间波动的情况下，欧式距离偏差较大。</p>
<p>所以，常规意义上的kmeans等基于欧式距离的算法这种情况下，使用较为局限。所以在整体思路不变的情况下，就距离计算，我们可以参考语音分析里面的DP（最佳路径规划算法），构造邻接矩阵，寻找最小最小路径和</p>
<p><img src="/2017/11/02/用户生命周期/2.png" alt=""><br>实际在计算蓝色曲线到青色曲线的距离的时候，同时计算AB（蓝色曲线当前位置A点到前一个时间段青色曲线位置B）、AC（蓝色曲线当前位置A点到当前时间段青色曲线位置C），AD（蓝色曲线当前位置A点到后一个时间段青色曲线位置D）的距离，综合判断一个点最短路径；再根据曲线上的每一个点，会形成一个矩阵，判断矩阵的每个点的最佳路径即可</p>
<p>可以用如下的公式表述：<br><img src="/2017/11/02/用户生命周期/3.png" alt=""><br>其中，<img src="/2017/11/02/用户生命周期/4.png" alt="">就是路径选择的过程</p>
<p>以上述的计算方式替换掉常规的kmeans中的欧式距离，提高了相似度的计算精度。</p>
<p>2.常规等距划分session不适用于生命周期</p>
<p>就用户平台活跃而言，不同用户可采用的用户时间窗口不同，新加入的用户可能可获取的时间长度较短；用户判断过程中的session与平台确定已知的生命周期session固定判断长度也是不相同的。同时，kmeans中的距离判断方法不能同时考虑到不同session下的距离计算问题</p>
<p><strong>最简单常规的计算方式：</strong><br>是补全较短的session的时间窗口，在相同的时间窗口之下，再去计算较短的时间窗口与较长的时间窗口下的生命周期的均值，这样会人为干涉过多，数据质量较低，图b即为数据补齐</p>
<p><strong>“STS距离”计算方式：</strong><br>在长时间窗口{r}集合中，寻找时间窗口长度子集，使得子集中的元长度与s曲线缺失的长度一致，在以s断点处开始向后寻找{r}子集合中的所有满足的元，再以均值时间序列替换原来的子集中的元作为r和s的拟合曲线，循环往复计算中心曲线2，如图c</p>
<p><img src="/2017/11/02/用户生命周期/5.png" alt=""></p>
<p>有了补齐长度下的中心曲线，再便可采用kmeans的常规方式，计算各时间长度窗口下的生命周期的距离</p>
<p>3.附加限制属性<br>再最后落地生命周期的长度的时候，考虑到商品平台的特殊属性，比如：</p>
<ul>
<li>商品周期性（奶粉用户周期购买等）</li>
<li>用户偏好属性(酒店用户品质偏好等）</li>
<li>平台的时间依赖情况(夏季冬季季节偏好等）</li>
<li>……<br>以上即为如何通过kmeans来确定一个用户所属的生命周期阶段</li>
</ul>
<p><em>本文参考文献如下：<br>1.<a href="http://www.doc88.com/p-0092455469704.html" target="_blank" rel="noopener">不等长时间序列下的滑窗相似度</a><br>2.<a href="http://www.jianshu.com/p/1417fcb06797" target="_blank" rel="noopener">kmeans距离计算方式剖析</a>
</em></p>
]]></content>
      
        <categories>
            
            <category> 特征刻画 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 生命周期 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[交叉销售算法]]></title>
      <url>/2017/11/01/%E4%BA%A4%E5%8F%89%E9%94%80%E5%94%AE%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p><img src="/2017/11/01/交叉销售算法/1.png" alt=""></p>
<p>最近做了一个交叉销售的项目，梳理了一些关键点，分享如下，希望对大家有所启发<br>核心目标：在有限资源下，尽可能的提供高转化率的用户群，辅助业务增长<br>初步效果：商家ROI值为50以上，用户日转化率提升10倍以上，用户日最低转化效果5pp以上<br>以下为正文：<br>数据准备：<br>1.商品相关性<br>存在商品A,B,C…，商品之间用户会存在行为信息的关联度，这边可以参考协调过滤算法中的Item-based，这边拓展为用户在不同商品之间的操作行为的差异性。</p>
<p><img src="/2017/11/01/交叉销售算法/2.png" alt=""></p>
<p>可以形成如下的特征矩阵：<br><img src="/2017/11/01/交叉销售算法/3.png" alt=""></p>
<p>这边相关的常见度量方式有以下几种：<br>a.距离衡量<br>包括浏览、点击、搜索等等各种行为的欧式、马氏、闵式、切比雪夫距离、汉明距离计算<br>b.相似度衡量<br>包括余弦相似度、杰卡德相似度衡量<br>c.复杂衡量<br>包括相关性衡量，熵值衡量，互信息量衡量，相关距离衡量<br>2.商品行为信息<br>探求商品及其对应行为信息的笛卡尔积的映射关系，得到一个商品+用户的行为魔方<br>商品集合：{商品A、商品B、…}<br>商品属性集合：{价格、是否打折、相比其他电商平台的比价、是否缺货…}<br>用户行为集合：{浏览次数、浏览时长、末次浏览间隔、搜索次数、末次搜索间隔…}<br>通过商品集合<em>商品属性集合</em>用户行为集合,形成高维的商品信息魔方，再通过探查算法，筛选优秀表现的特征，这里推荐的有pca，randomforest的importance，lasso变量压缩，相关性压缩，逐步回归压缩等方法，根据数据的属性特点可适当选取方法<br>最后，我们会得到如下一个待选特征组：<br><img src="/2017/11/01/交叉销售算法/4.png" alt=""></p>
<p>3.商品购买周期<br>针对每一件商品，都是有它自身的生命周期的，比如，在三个月内买过冰箱的用户，95%以上的用户是不会选择二次购买的；而在1个月的节点上，会有20%的用户会选择二次购买生活用纸。所以我们需要做的一件事情就是不断更新，平台上面每个类目下面的商品的自身生命周期。除此之外，考虑在过渡时间点，用户的需求变化情况，是否可以提前触发需求；这边利用，艾宾浩斯遗忘曲线和因子衰减规律拟合：</p>
<p><img src="/2017/11/01/交叉销售算法/5.png" alt="艾宾浩斯曲线"></p>
<p><img src="/2017/11/01/交叉销售算法/6.png" alt="衰减因子"></p>
<p>确定lamda和b，计算每个用户对应的每个类目，当前时间下的剩余价值：f(最高价值)<em>lamda</em>b</p>
<p><img src="/2017/11/01/交叉销售算法/7.png" alt="艾宾浩斯"></p>
<p><img src="/2017/11/01/交叉销售算法/8.png" alt="衰减因子公式"><br>4.商品挖掘特征，用户挖掘特征<br>业务运营过程中，通过数据常规可以得到1.基础结论，2.挖掘结论。基础结论就是统计结论，比如昨日订单量，昨日销售量 ，昨日用户量；挖掘结论就是深层结论，比如昨日活跃用户数，每日预估销售量，用户生命周期等<br>存在如下的探索形式，这是一个漫长而又非常有价值的过程：</p>
<p><img src="/2017/11/01/交叉销售算法/9.png" alt="特征分析"></p>
<p>模型整合<br>再确定以上四大类的数据特征之后，我们通过组合模型的方法，判断用户的交叉销售结果</p>
<hr>
<p>1.cart regression<br>确保非线性密度均匀数据拟合效果，针对存在非线性关系且数据可被网格切分的产业用户有高的预测能力<br>2.ridge regression<br>确保可线性拟合及特征繁多数据的效果，针对存在线性关系的产业用户有高的预测能力<br>3.Svm-liner<br>确保线性且存在不可忽视的异常点的数据拟合效果，针对存在异常用户较多的部分产业用户有高的预测能力<br>4.xgboost<br>确保数据复杂高维且无明显关系的数据拟合效果，针对存在维度高、数据杂乱、无模型规律的部分产业用户有高的预测能力<br>以上的组合模型并非固定，也并非一定全部使用，在确定自身产业的特点后，择优选择，然后采取投票、加权、分组等组合方式产出结果即可。</p>
<hr>
<p>附上推荐Rcode简述，<br><strong>cart regression：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fit &lt;- rpart(y~x, data=database, method=&quot;class&quot;,control=ct, parms = list(prior = c(0.7,0.3), split = &quot;information&quot;));</span><br><span class="line"># xval是n折交叉验证</span><br><span class="line"># minsplit是最小分支节点数，设置后达不到最小分支节点的话会继续分划下去</span><br><span class="line"># minbucket：叶子节点最小样本数</span><br><span class="line"># maxdepth：树的深度</span><br><span class="line"># cp全称为complexity parameter，指某个点的复杂度，对每一步拆分,模型的拟合优度必须提高的程度</span><br><span class="line"># kyphosis是rpart这个包自带的数据集</span><br><span class="line"># na.action：缺失数据的处理办法，默认为删除因变量缺失的观测而保留自变量缺失的观测。</span><br><span class="line"># method：树的末端数据类型选择相应的变量分割方法:</span><br><span class="line"># 连续性method=“anova”,离散型method=“class”,计数型method=“poisson”,生存分析型method=“exp”</span><br><span class="line">#parms用来设置三个参数:先验概率、损失矩阵、分类纯度的度量方法（gini和information）</span><br><span class="line"># cost我觉得是损失矩阵，在剪枝的时候，叶子节点的加权误差与父节点的误差进行比较，考虑损失矩阵的时候，从将“减少-误差”调整为“减少-损失”</span><br></pre></td></tr></table></figure></p>
<p><strong>ridge regression：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">library（glmnet）</span><br><span class="line">glmmod&lt;-glmnet(x,y,family = &apos;guassian&apos;,alpha = 0)</span><br><span class="line">最小惩罚：</span><br><span class="line">glmmod.min&lt;-glmnet(x,y,family = &apos;gaussian&apos;,alpha = 0,lambda = glmmod.cv$lambda.min)</span><br><span class="line">1个标准差下的最小惩罚：</span><br><span class="line">glmmod.1se&lt;-glmnet(x,y,family = &apos;gaussian&apos;,alpha = 0,lambda = glmmod.cv$lambda.1se)</span><br></pre></td></tr></table></figure></p>
<p><strong>Svm-liner ：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">svm(x, y, scale = TRUE, type = NULL, kernel = &quot;&quot;,degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),coef0 = 0, cost = 1, nu = 0.5, subset, na.action = na.omit)</span><br><span class="line">#type用于指定建立模型的类别:C-classification、nu-classification、one-classification、eps-regression和nu-regression</span><br><span class="line">#kernel是指在模型建立过程中使用的核函数</span><br><span class="line">#degree参数是指核函数多项式内积函数中的参数，其默认值为3</span><br><span class="line">#gamma参数给出了核函数中除线性内积函数以外的所有函数的参数，默认值为l</span><br><span class="line">#coef0参数是指核函数中多项式内积函数与sigmoid内积函数中的参数，默认值为0</span><br><span class="line">#参数cost就是软间隔模型中的离群点权重</span><br><span class="line">#参数nu是用于nu-regression、nu-classification和one-classification类型中的参数</span><br></pre></td></tr></table></figure></p>
<p><strong>xgboost:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(xgboost)</span><br><span class="line">xgb &lt;- xgboost(data = data.matrix(x[,-1]), label = y, eta = 0.1,max_depth = 15, nround=25, subsample = 0.5,colsample_bytree = 0.5,seed = 1,eval_metric = &quot;merror&quot;,objective = &quot;multi:softprob&quot;,num_class = 12, nthread = 3)</span><br><span class="line">#eta：默认值设置为0.3。步长，控制速度及拟合程度</span><br><span class="line">#gamma:默认值设置为0。子树叶节点个数</span><br><span class="line">#max_depth:默认值设置为6。树的最大深度</span><br><span class="line">#min_child_weight:默认值设置为1。控制子树的权重和</span><br><span class="line">#max_delta_step：默认值设置为0。控制每棵树的权重</span><br><span class="line">#subsample： 默认值设置为1。抽样训练占比</span><br><span class="line">#lambda and alpha：正则化</span><br></pre></td></tr></table></figure></p>
<p>最后通过组合算法的形式产出最终值：</p>
<p><img src="/2017/11/01/交叉销售算法/10.png" alt=""><br><strong>典型算法代表：randomforest,adaboost,gbdt</strong></p>
<p>之前写的没有用markdown，所以看起来很费力，还丢图，这次优化了一下视图，谢谢。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[随机森林-枝剪问题]]></title>
      <url>/2017/10/02/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%9E%9D%E5%89%AA%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>通常情况下， 随机森林不需要后剪枝。</p>
<p>剪枝的意义是：防止决策树生成过于庞大的子叶，避免实验预测结果过拟合，在实际生产中效果很差</p>
<p>剪枝通常有两种：</p>
<p>PrePrune：预剪枝，及早的停止树增长，在每个父节点分支的时候计算是否达到了限制值</p>
<p>PostPrune：后剪枝，基于完全生长（过拟合）的树上进行剪枝，砍掉一些对衡量函数影响不大的枝叶</p>
<p>剪枝的依据：</p>
<p>常见的有错误率校验（判断枝剪是降低了模型预测的正确率），统计学检验，熵值，代价复杂度等等</p>
<p>总结看来，枝剪的目的是担心全量数据在某棵树上的拟合过程中，过度判断了每个点及其对应类别的关系，有如以下这张图（以rule1&amp;rule2代替了rule3）：</p>
<hr>
<p>随机森林：</p>
<p>定义：它是一种模型组合（常见的Boosting，Bagging等，衍生的有gbdt），这些算法最终的结果是生成N(可能会有几百棵以上）棵树，组合判断最终结果。</p>
<p>如何组合判断？</p>
<p>1.通常我们会规定随机森林里面的每棵树的选参个数，常见的有log，sqrt等等，这样的选取是随机选则的，这样有一个好处，让每一棵树上都有了尽可能多的变量组合，降低过拟合程度</p>
<p>2.树的个数及树的节点的变量个数，通常的来说，最快捷的方式是先确定节点的变量个数为sqrt（变量的个数），然后在根据oob的准确率反过来看多个棵树时最优，确定了树的个数的时候再反过来确定mtry的个数，虽然有局限，但是也并不存在盲目性</p>
<p>3.我个人理解，随机森林中的每一棵树我们需要它在某一片的数据中有非常好的拟合性，它并不是一个全数据拟合，只需要在它负责那块上有最佳的拟合效果。每次遇到这些数据(特征)的时候，我们在最后汇总N棵树的结果的时候，给这些数据对应的那块模型以最高权重即可</p>
<p>最后总结一下，就是随机森林里面的每棵树的产生通过选特征参数选数据结构，都已经考虑了避免共线性避免过拟合，剩下的每棵树需要做的就是尽可能的在自己所对应的数据(特征)集情况下尽可能的做到最好的预测结果；如同，公司已经拆分好部门，你不需要考虑这样拆分是不是公司运营最好的一个组合方式，你需要做的就是当公司需要你的时候，尽可能的做好自己的事情，就酱。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 树枝剪问题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[面试之常见决策树异同]]></title>
      <url>/2017/09/01/%E9%9D%A2%E8%AF%95%E4%B9%8B%E5%B8%B8%E8%A7%81%E5%86%B3%E7%AD%96%E6%A0%91%E5%BC%82%E5%90%8C/</url>
      <content type="html"><![CDATA[<p>历史回顾：1984年提出的cart，1986年提出的ID3，1993年提出的c4.5</p>
<p><strong>理论上</strong>总的来说，<br>C4.5是基于ID3优化后产出的算法，主要优化了关于节点分支的计算方式，优化后解决了ID3分支过程中总喜欢偏向取值较多的属性<br>ID3是信息增益分支：</p>
<p><img src="/2017/09/01/面试之常见决策树异同/1.png" alt="">而CART一般是GINI系数分支：</p>
<p><img src="/2017/09/01/面试之常见决策树异同/2.jpg" alt="">C4.5一般是信息增益率分支：</p>
<p><img src="/2017/09/01/面试之常见决策树异同/3.png" alt=""><br><strong>工程上</strong>总的来说：<br>CART和C4.5之间主要差异在于分类结果上，<strong>CART可以回归分析也可以分类，C4.5只能做分类；C4.5子节点是可以多分的，而CART是无数个二叉子节点</strong>；<br>以此拓展出以CART为基础的“树群”random forest ， 以回归树为基础的“树群”GBDT</p>
<p>样本数据的差异：<br>ID3只能对分类变量进行处理，C4.5和CART可以处理连续和分类两种自变量<br>ID3对缺失值敏感，而C4.5和CART对缺失值可以进行多种方式的处理<br>只从样本量考虑，<em>小样本建议考虑c4.5、大样本建议考虑cart</em>。c4.5处理过程中需对数据集进行多次排序，处理成本耗时较高，而cart本身是一种大样本的统计方法，小样本处理下泛化误差较大</p>
<p>目标因变量的差异：<br>ID3和C4.5只能做分类，CART（分类回归树）不仅可以做分类（0/1）还可以做回归（0-1）<br>ID3和C4.5节点上可以产出多叉（低、中、高），而CART节点上永远是二叉（低、非低）</p>
<p>样本特征上的差异：<br>特征变量的使用中，多分的分类变量ID3和C4.5层级之间只单次使用，CART可多次重复使用</p>
<p>决策树产生过程中的优化差异：<br>C4.5是通过枝剪来修正树的准确性，而CART是直接利用全部数据发现所有树的结构进行对比</p>
]]></content>
      
        <categories>
            
            <category> 基础 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SVD及扩展的矩阵分解方法]]></title>
      <url>/2017/08/27/SVD%E5%8F%8A%E6%89%A9%E5%B1%95%E7%9A%84%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>svd是现在比较常见的算法之一，也是数据挖掘工程师、算法工程师必备的技能之一，这边就来看一下svd的思想，svd的重写，svd的应用。<br>这边着重的看一下推荐算法中的使用，其实在图片压缩，特征压缩的工程中，svd也有着非常不凡的作用。</p>
<h1 id="svd的思想"><a href="#svd的思想" class="headerlink" title="svd的思想"></a>svd的思想</h1><h2 id="矩阵因子模型（潜在因子模型）"><a href="#矩阵因子模型（潜在因子模型）" class="headerlink" title="矩阵因子模型（潜在因子模型）"></a>矩阵因子模型（潜在因子模型）</h2><p>假设，我们现在有一个用户u对商品i的程度矩阵，浏览是1，搜索是2，加车是3，下单是4，付款是5:<br><img src="/2017/08/27/SVD及扩展的矩阵分解方法/1.png" alt="用户商品矩阵"><br>实际情况下，用户不可能什么商品都买，所以，该矩阵必然是一个稀疏矩阵，任意一个矩阵必然可以被分解成2个矩阵的乘积：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/2.png" alt=""></p>
<p>k就是潜在因子的个数，举个例子，你去买衣服，你可能买了裙子，露背装，我去买衣服，我买了牛仔裤，潮牌T恤，影响我们购买商品的差异的原因可能有很多点，但是必然有些原因占比重要些，比如性别，收入，有一些可能不那么重要比如天气，心情。<strong>而拆分成的Pu矩阵表示了这些潜在因子对我或者你的影响程度，Qi矩阵表示了各种商品对这些潜在因子的影响程度。</strong><br>当我们尽可能的通过拆分矩阵的形式，目标使得拆分后的两个矩阵的乘积最匹配最上方的用户商品矩阵的已知的数据值，从而可以通过这两个矩阵的乘积填补掉空缺的值。</p>
<h2 id="Baseline-Predictors"><a href="#Baseline-Predictors" class="headerlink" title="Baseline Predictors"></a>Baseline Predictors</h2><p>这个是08年的，Koren在NetFlix大赛的一个思路，后续也延伸了svd多种变种，比如Asvd，有偏的Rsvd，对偶算法下的Svd++，这些算法的核心在于解决了Svd上面我们提到的那个矩阵庞大稀疏的问题，后续我们再看。</p>
<p>Baseline Predictors使用向量bi表示电影i的评分相对于平均评分的偏差，向量bu表示用户u做出的评分相对于平均评分的偏差，将平均评分记做μ。</p>
<p><strong>新的得分计算方式如下：Rui＝μ+bi+bu</strong><br>准备引入了商品及用户的实际分布的情况，有效的降低在测试数据上面的效果。</p>
<h2 id="svd数学原理"><a href="#svd数学原理" class="headerlink" title="svd数学原理"></a>svd数学原理</h2><p>首先，线代或者高等代数里面告诉我们：一个向量可以通过左乘一个矩阵的方式来进行拉伸，旋转，或者同时拉伸旋转。<br>所以，无论什么矩阵M，我们都可以找到一组正交基v1、v2，使得Mv1、Mv2也是正交的，不妨记其方向为为μ1、μ2。<br><strong>Mv1=δ1u1；<br>Mv2=δ2u2；</strong><br>存在向量x，在v1、v2空间里的表示为：x=（x·v1）v1+（x·v2）v2，<br>所以有，Mx有：<br>Mx=（x·v1）Mv1+（x·v2）Mv2<br>Mx=（x·v1）δ1u1+（x·v2）δ2u2<br>Mx=δ1u1(v1.T)x+δ2u2(v2.T)x<br>M=δ1u1(v1.T)+δ2u2(v2.T)<br>所以就有了那个非常有名的公式：<br>M=UΣV.T<br>U是有一组正交基构成的，V也是有一组正交基构成的，Σ是由δ1、δ2构成的，<strong>几何意义上来说，M的作用就是把一个向量由V的正交空间变换到U的正交空间上，而通过Σ的大小来控制缩放的力度。</strong></p>
<p>我们还需要知道一些简单的推论，</p>
<ul>
<li>通过MM.T，我们知道，δ的平方是MM.T的特征值</li>
<li>奇异值δ的数量决定了M=UΣV.T的复杂度，而奇异值的大小变化差异程度很大，通常前几个奇异值的平方就能占到全部奇异值的平方的90%，所以，我们可以通过控制奇异值的数量来优化原始矩阵乘积，去除掉一下噪声数据</li>
</ul>
<h1 id="svd重写"><a href="#svd重写" class="headerlink" title="svd重写"></a>svd重写</h1><h2 id="基础的svd"><a href="#基础的svd" class="headerlink" title="基础的svd"></a>基础的svd</h2><p>首先，我们在刚开始就知道，评分矩阵R可以用两个矩阵P和Q的乘积来表示：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/3.png" alt=""></p>
<p>其中，U表示用户数，I表示商品数，K=就是潜在因子个数。<br>首先通过那些已知的数据比如下方红色区域内的数据去训练这两个乘积矩阵：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/4.png" alt=""></p>
<p>那么未知的评分也就可以用P的某一行乘上Q的某一列得到了：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/5.png" alt=""></p>
<p>这是预测用户u对商品i的评分，它等于P矩阵的第u行乘上Q矩阵的第i列。这个是最基本的SVD算法，下面我们们来看如何确定Pu、Qi：</p>
<p>假设已知的评分为：rui则真实值与预测值的误差为：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/6.png" alt=""></p>
<p>继而可以计算出总的误差平方和：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/7.png" alt=""></p>
<p>只要通过训练把SSE降到最小那么P、Q就能最好地拟合R了。常规的来讲，梯度下降是非常好的求解方式，常见的包括随机梯度下降，批量梯度下降。<br>随机梯度下降一定程度会避免局部最小但是计算量大，批量梯度计算量小但是会存在鞍点计算误区的问题。</p>
<p>先求得SSE在Puk变量（也就是P矩阵的第u行第k列的值）处的梯度：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/8.png" alt=""></p>
<p>现在得到了目标函数在Puk处的梯度了，那么按照梯度下降法，将Puk往负梯度方向变化。令更新的步长（也就是学习速率）为</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/9.png" alt=""></p>
<p>则Puk的更新式为</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/10.png" alt=""></p>
<p>同样的方式可得到Qik的更新式为</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/11.png" alt=""></p>
<h2 id="Rsvd"><a href="#Rsvd" class="headerlink" title="Rsvd"></a>Rsvd</h2><p>很明显，上述这样去求的矩阵QP必然会存在过度拟合的问题，导致对实际数据预测的时候，效果远差于训练数据，仿造elastic net的思维：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/12.png" alt=""><br>对所有的变量就加入正则惩罚项，重新计算上面的梯度如下：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/13.png" alt=""><br>这就是正则svd，也叫做Rsvd，也是我们用的比较多的svd的方法。</p>
<h2 id="偏移Rsvd"><a href="#偏移Rsvd" class="headerlink" title="偏移Rsvd"></a>偏移Rsvd</h2><p>在最开始讲了，Koren在NetFlix大赛里面除了考虑了对原始数据的拟合情况，也考虑了用户的评分、商品的平均得分相对于整体数据的偏移情况，有了新的得分公式：Rui＝μ+bi+bu，影响的只有eui，后面的Pu、qi的正则不受影响，但是新增了bi、bu的正则项，重新计算每一项的偏导数：<br>bu、bi的更新式子：<br><img src="/2017/08/27/SVD及扩展的矩阵分解方法/14.png" alt=""><br>其余的都不发生改变，这就叫做有偏移下的Rsvd</p>
<p>无论是Rsvd还是有偏移的Rsvd，当原始的用户对商品的评分矩阵过大，比如有3亿用户，3亿商品，形成9亿商品集合的时候，这就是一个比较不可能完成的存储任务，而且里面绝大多数都是0的稀疏矩阵。</p>
<h2 id="Asvd及Svd"><a href="#Asvd及Svd" class="headerlink" title="Asvd及Svd++"></a>Asvd及Svd++</h2><p>这边，我们引入两个集合：R(u)表示用户u评过分的商品集合，N(u)表示用户u浏览过但没有评过分的商品集合，Xj和Yj是商品的属性。<br>Asvd的rui的评分方式：<br><img src="/2017/08/27/SVD及扩展的矩阵分解方法/15.png" alt=""></p>
<p>Svd++的rui的评分方式：</p>
<p><img src="/2017/08/27/SVD及扩展的矩阵分解方法/16.png" alt=""><br>无论是Asvd还是Svd++，都干掉了原来庞大的P矩阵，取而代之的是两个用户浏览评分矩阵大大缩小了存储的空间，但是随着而来的是一大把更多的未知参数及迭代的复杂程度，所有在训练时间上而言，会大大的增加。</p>
<p>我这边只重写了一下Rsvd的python版本，网上挺多版本的迭代条件有一定问题，稍作处理了一下，并写成了函数，大家可以参考一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svd</span><span class="params">(mat, feature, steps=<span class="number">2000</span>, gama=<span class="number">0.02</span>, lamda=<span class="number">0.3</span>)</span>:</span></span><br><span class="line"><span class="comment">#feature是潜在因子的数量，mat为评分矩阵</span></span><br><span class="line">slowRate = <span class="number">0.99</span></span><br><span class="line">preRmse = <span class="number">0.0000000000001</span></span><br><span class="line">nowRmse = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">user_feature = matrix(numpy.random.rand(mat.shape[<span class="number">0</span>], feature))</span><br><span class="line">item_feature = matrix(numpy.random.rand(mat.shape[<span class="number">1</span>], feature))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(steps):</span><br><span class="line">rmse = <span class="number">0.0</span></span><br><span class="line">n = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> u <span class="keyword">in</span> range(mat.shape[<span class="number">0</span>]):</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(mat.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> numpy.isnan(mat[u,i]):</span><br><span class="line"><span class="comment">#这边是判断是否为空，也可以改为是否为0:if mat[u,i]&gt;0:</span></span><br><span class="line">pui = float(numpy.dot(user_feature[u,:], item_feature[i,:].T))</span><br><span class="line">eui = mat[u,i] - pui</span><br><span class="line">rmse += pow(eui, <span class="number">2</span>)</span><br><span class="line">n += <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(feature):</span><br><span class="line"><span class="comment">#Rsvd的更新迭代公式</span></span><br><span class="line">user_feature[u,k] += gama*(eui*item_feature[i,k] - lamda*user_feature[u,k])</span><br><span class="line">item_feature[i,k] += gama*(eui*user_feature[u,k] - lamda*item_feature[i,k])</span><br><span class="line"><span class="comment">#n次迭代平均误差程度</span></span><br><span class="line">nowRmse = sqrt(rmse * <span class="number">1.0</span> / n)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'step: %d      Rmse: %s'</span> % ((step+<span class="number">1</span>), nowRmse)</span><br><span class="line"><span class="keyword">if</span> (nowRmse &gt; preRmse):</span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line"><span class="comment">#降低迭代的步长</span></span><br><span class="line">gama *= slowRate</span><br><span class="line">step += <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> user_feature, item_feature</span><br></pre></td></tr></table></figure></p>
<p>直接调用的结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">step: 1956      Rmse: 0.782449675844</span><br><span class="line">step: 1957      Rmse: 0.782449675843</span><br><span class="line">step: 1958      Rmse: 0.782449675843</span><br><span class="line">step: 1959      Rmse: 0.782449675843</span><br><span class="line">step: 1960      Rmse: 0.782449675842</span><br><span class="line">step: 1961      Rmse: 0.782449675842</span><br><span class="line">step: 1962      Rmse: 0.782449675842</span><br><span class="line">step: 1963      Rmse: 0.782449675841</span><br><span class="line">step: 1964      Rmse: 0.782449675841</span><br><span class="line">step: 1965      Rmse: 0.78244967584</span><br><span class="line">step: 1966      Rmse: 0.78244967584</span><br><span class="line">step: 1967      Rmse: 0.78244967584</span><br><span class="line">step: 1968      Rmse: 0.782449675839</span><br><span class="line">step: 1969      Rmse: 0.782449675839</span><br><span class="line">step: 1970      Rmse: 0.782449675839</span><br><span class="line">step: 1971      Rmse: 0.782449675838</span><br><span class="line">step: 1972      Rmse: 0.782449675838</span><br><span class="line">step: 1973      Rmse: 0.782449675838</span><br><span class="line">step: 1974      Rmse: 0.782449675837</span><br><span class="line">step: 1975      Rmse: 0.782449675837</span><br><span class="line">step: 1976      Rmse: 0.782449675837</span><br><span class="line">step: 1977      Rmse: 0.782449675836</span><br><span class="line">step: 1978      Rmse: 0.782449675836</span><br><span class="line">step: 1979      Rmse: 0.782449675836</span><br><span class="line">step: 1980      Rmse: 0.782449675835</span><br><span class="line">step: 1981      Rmse: 0.782449675835</span><br><span class="line">step: 1982      Rmse: 0.782449675835</span><br><span class="line">step: 1983      Rmse: 0.782449675835</span><br><span class="line">step: 1984      Rmse: 0.782449675834</span><br><span class="line">step: 1985      Rmse: 0.782449675834</span><br><span class="line">step: 1986      Rmse: 0.782449675834</span><br><span class="line">step: 1987      Rmse: 0.782449675833</span><br><span class="line">step: 1988      Rmse: 0.782449675833</span><br><span class="line">step: 1989      Rmse: 0.782449675833</span><br><span class="line">step: 1990      Rmse: 0.782449675833</span><br><span class="line">step: 1991      Rmse: 0.782449675832</span><br><span class="line">step: 1992      Rmse: 0.782449675832</span><br><span class="line">step: 1993      Rmse: 0.782449675832</span><br><span class="line">step: 1994      Rmse: 0.782449675831</span><br><span class="line">step: 1995      Rmse: 0.782449675831</span><br><span class="line">step: 1996      Rmse: 0.782449675831</span><br><span class="line">step: 1997      Rmse: 0.782449675831</span><br><span class="line">step: 1998      Rmse: 0.78244967583</span><br><span class="line">step: 1999      Rmse: 0.78244967583</span><br><span class="line">step: 2000      Rmse: 0.78244967583</span><br><span class="line">Out[15]:</span><br><span class="line">(matrix([[-0.72426432,  0.40007415,  1.16887518],</span><br><span class="line">[-0.73130968,  0.40240702,  1.14708432],</span><br><span class="line">[ 0.34759923,  1.35065656, -0.29573789],</span><br><span class="line">[ 1.17462156, -0.04964694,  0.73881335],</span><br><span class="line">[ 2.04035441, -0.06798676,  1.28078727],</span><br><span class="line">[ 0.30446306,  1.71648612, -0.4109819 ],</span><br><span class="line">[ 1.71963828, -0.00833196,  1.25983483],</span><br><span class="line">[-0.86341514,  0.47750529,  1.36135332],</span><br><span class="line">[-0.48881234,  0.57942923,  0.77110915],</span><br><span class="line">[ 0.29282908,  1.5164249 , -0.39811768],</span><br><span class="line">[ 0.35369432, -0.00964055,  0.22328158]]),</span><br><span class="line">matrix([[ 1.3381854 , -0.05425608,  0.87036818],</span><br><span class="line">[ 1.07547853, -0.04515239,  0.69607952],</span><br><span class="line">[ 1.39038494, -0.05799558,  0.90186332],</span><br><span class="line">[-0.60218508,  0.36233309,  0.92136536],</span><br><span class="line">[ 0.39117217,  1.9272062 , -0.50710187],</span><br><span class="line">[-0.93202395,  0.52982297,  1.43309143],</span><br><span class="line">[-0.19393338,  0.40429152,  0.29994372],</span><br><span class="line">[ 1.40108031,  0.06915628,  0.87647945],</span><br><span class="line">[ 1.39446638, -0.05197355,  0.920216  ],</span><br><span class="line">[ 0.31981929,  1.84114505, -0.40108819],</span><br><span class="line">[-0.82317832,  0.52798144,  1.51619052]]))</span><br></pre></td></tr></table></figure>
<h1 id="svd在推荐算法中的使用"><a href="#svd在推荐算法中的使用" class="headerlink" title="svd在推荐算法中的使用"></a>svd在推荐算法中的使用</h1><p>数据集中行代表用户user，列代表物品item，其中的值代表用户对物品的打分。<br>基于SVD的优势在于：用户的评分数据是稀疏矩阵，可以用SVD将原始数据映射到低维空间中，然后计算物品item之间的相似度，更加高效快速。</p>
<p>整体思路：<br>用户未知得分的商品评分计算方式：<br>1.用户的评分矩阵==》用户已经评分过得商品的得分<br>2.商品的用户评分==》用户已经评分过得商品和其他每个商品的相关性<br>3.“用户已经评分过得商品的得分”*“用户已经评分过得商品和某个未知评分商品的相关系数”=某个未知商品该用户的评分</p>
<p>计算上面三个步骤，我们需要：<br>1.考虑采取上面相关性的计算方式<br>2.考虑潜在因子的个数</p>
<p>依旧python代码，我会在代码中注释讲解：<br>1.首先相关性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 欧拉距离相似度，评分可用，程度不建议</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">oulasim</span><span class="params">(A, B)</span>:</span></span><br><span class="line">distince = la.norm(A - B)  <span class="comment"># 第二范式：平方的和后求根号</span></span><br><span class="line">similarity = <span class="number">1</span> / (<span class="number">1</span> + distince)</span><br><span class="line"><span class="keyword">return</span> similarity</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 余弦相似度，评分、1/0、程度都可以用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cossim</span><span class="params">(A, B)</span>:</span></span><br><span class="line">ABDOT = float(dot(A, B))</span><br><span class="line">ABlen = la.norm(A) * la.norm(B)</span><br><span class="line"><span class="keyword">if</span> ABlen == <span class="number">0</span>:</span><br><span class="line">similarity = <span class="string">'异常'</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">similarity = ABDOT / float(ABlen)</span><br><span class="line"><span class="keyword">return</span> similarity</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 皮尔逊相关系数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearsonsim</span><span class="params">(A, B)</span>:</span></span><br><span class="line">A = A - mean(A)</span><br><span class="line">B = B - mean(B)</span><br><span class="line">ABDOT = float(dot(A, B))</span><br><span class="line">ABlen = la.norm(A) * la.norm(B)</span><br><span class="line"><span class="keyword">if</span> ABlen == <span class="number">0</span>:</span><br><span class="line">similarity = <span class="string">'异常'</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">similarity = ABDOT / float(ABlen)</span><br><span class="line"><span class="keyword">return</span> similarity</span><br></pre></td></tr></table></figure></p>
<p>2.指定用户及对应商品的相似度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># svd</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommender</span><span class="params">(datamat,user,index,function)</span>:</span></span><br><span class="line">n = shape(datamat)[<span class="number">1</span>]  <span class="comment"># 商品数目</span></span><br><span class="line">U, sigma, VT = la.svd(datamat)</span><br><span class="line"><span class="comment"># 规约最小维数</span></span><br><span class="line">sigma2 = sigma ** <span class="number">2</span></span><br><span class="line">k = len(sigma2)</span><br><span class="line">n_sum2 = sum(sigma2)</span><br><span class="line">nsum = <span class="number">0</span></span><br><span class="line">max_sigma_index = <span class="number">0</span></span><br><span class="line"><span class="comment">#奇异值的平方占比总数的90%，确定潜在因子数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sigma:</span><br><span class="line">nsum = nsum + i ** <span class="number">2</span></span><br><span class="line">max_sigma_index = max_sigma_index + <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> nsum &gt;= n_sum2 * <span class="number">0.9</span>:</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line"><span class="comment"># item new matrix</span></span><br><span class="line">item = datamat.T * U[:, <span class="number">0</span>:max_sigma_index] * matrix(diag(sigma[<span class="number">0</span>:max_sigma_index])).I</span><br><span class="line">key=item[index,:]</span><br><span class="line">total_similarity=<span class="number">0</span></span><br><span class="line">rank_similarity=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line"><span class="comment">#如果用户没有评分或者与用户选择想知道的商品一致则跳过，不跳过计算出来前者是得分0，后者直接是用户已评分的结果，没有意义</span></span><br><span class="line"><span class="keyword">if</span> datamat[user,i]==<span class="number">0</span> <span class="keyword">or</span> i==index:<span class="keyword">continue</span></span><br><span class="line">similarity=function(key,item[i,:].T)</span><br><span class="line">total_similarity=total_similarity+similarity</span><br><span class="line"><span class="comment">#用户的评分*相关系数</span></span><br><span class="line">rank_similarity=rank_similarity+similarity*datamat[user,i]*similarity</span><br><span class="line">score = rank_similarity/total_similarity</span><br><span class="line"><span class="keyword">return</span> score</span><br></pre></td></tr></table></figure></p>
<p>比如：用户1在商品1的得分为3.99分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [18]: recommender(data,1,1,cossim)</span><br><span class="line">Out[18]: 3.98921610058786</span><br></pre></td></tr></table></figure></p>
<p>3.指定商品，与所有其它商品的相似度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommender</span><span class="params">(datamat, item_set, method)</span>:</span></span><br><span class="line">col = shape(datamat)[<span class="number">1</span>]  <span class="comment"># 物品数量</span></span><br><span class="line">item = datamat[:, item_set]</span><br><span class="line">similarity_matrix = zeros([col, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(col):</span><br><span class="line">index = nonzero(logical_and(item &gt; <span class="number">0</span>, datamat[:, i] &gt; <span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">if</span> sum(index) &gt; <span class="number">0</span>:</span><br><span class="line">similarity = method(datamat[index, item_set].T, datamat[index, i])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">similarity = <span class="string">'-1'</span></span><br><span class="line">similarity_matrix[i] = similarity</span><br><span class="line"><span class="keyword">return</span> similarity_matrix</span><br></pre></td></tr></table></figure></p>
<p>比如：商品0，对其它所有商品的相似度：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [24]: recommender(data,0,cossim)</span><br><span class="line">Out[24]:</span><br><span class="line">array([[ 1.        ],</span><br><span class="line">[ 0.99439606],</span><br><span class="line">[ 0.99278096],</span><br><span class="line">[-1.        ],</span><br><span class="line">[-1.        ],</span><br><span class="line">[-1.        ],</span><br><span class="line">[-1.        ],</span><br><span class="line">[ 0.98183139],</span><br><span class="line">[ 0.97448865],</span><br><span class="line">[-1.        ],</span><br><span class="line">[ 1.        ]])</span><br></pre></td></tr></table></figure></p>
<p>4.直接算出所有商品间的相似度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similarity</span><span class="params">(datamat, method)</span>:</span></span><br><span class="line">item_sum = shape(datamat)[<span class="number">1</span>]</span><br><span class="line">similarity = pd.DataFrame([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(item_sum):</span><br><span class="line">res = recommender(datamat, i, method)</span><br><span class="line">similarity = pd.concat([similarity, pd.DataFrame(res)], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> similarity</span><br></pre></td></tr></table></figure></p>
<p>比如商品的相似矩阵：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">In [25]: similarity(data,cossim)</span><br><span class="line">Out[25]:</span><br><span class="line">0         0         0         0         0         0         0  \</span><br><span class="line">0   1.000000  0.994396  0.992781 -1.000000 -1.000000 -1.000000 -1.000000</span><br><span class="line">1   0.994396  1.000000  0.999484 -1.000000 -1.000000 -1.000000 -1.000000</span><br><span class="line">2   0.992781  0.999484  1.000000 -1.000000 -1.000000 -1.000000 -1.000000</span><br><span class="line">3  -1.000000 -1.000000 -1.000000  1.000000 -1.000000  0.990375  1.000000</span><br><span class="line">4  -1.000000 -1.000000 -1.000000 -1.000000  1.000000 -1.000000  1.000000</span><br><span class="line">5  -1.000000 -1.000000 -1.000000  0.990375 -1.000000  1.000000  1.000000</span><br><span class="line">6  -1.000000 -1.000000 -1.000000  1.000000  1.000000  1.000000  1.000000</span><br><span class="line">7   0.981831  0.956858  0.955304 -1.000000  1.000000 -1.000000 -1.000000</span><br><span class="line">8   0.974489  0.956858  0.955304 -1.000000 -1.000000 -1.000000 -1.000000</span><br><span class="line">9  -1.000000 -1.000000 -1.000000  1.000000  0.994536  1.000000  0.384615</span><br><span class="line">10  1.000000  1.000000  1.000000  1.000000 -1.000000  0.981307  1.000000</span><br><span class="line"></span><br><span class="line">0         0         0         0</span><br><span class="line">0   0.981831  0.974489 -1.000000  1.000000</span><br><span class="line">1   0.956858  0.956858 -1.000000  1.000000</span><br><span class="line">2   0.955304  0.955304 -1.000000  1.000000</span><br><span class="line">3  -1.000000 -1.000000  1.000000  1.000000</span><br><span class="line">4   1.000000 -1.000000  0.994536 -1.000000</span><br><span class="line">5  -1.000000 -1.000000  1.000000  0.981307</span><br><span class="line">6  -1.000000 -1.000000  0.384615  1.000000</span><br><span class="line">7   1.000000  0.991500  1.000000  1.000000</span><br><span class="line">8   0.991500  1.000000 -1.000000  1.000000</span><br><span class="line">9   1.000000 -1.000000  1.000000  1.000000</span><br><span class="line">10  1.000000  1.000000  1.000000  1.000000</span><br></pre></td></tr></table></figure></p>
<p>5.指定用户下的top5商品推荐<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def fianl_recommender(datamat,user,function):</span><br><span class="line">unratedItems=nonzero(datamat[user,:].A==0)[1]</span><br><span class="line">if len(unratedItems)==0: print &apos;ok&apos;</span><br><span class="line">score=[]</span><br><span class="line">for i in unratedItems:</span><br><span class="line">i_score=recommender(datamat,user,i,function)</span><br><span class="line">score.append((i,i_score))</span><br><span class="line">score=sorted(score,key=lambda x:x[1],reverse=True)</span><br><span class="line">return score[:5]</span><br></pre></td></tr></table></figure></p>
<p>比如指定用户1，最适合推荐的商品如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">31</span>]: fianl_recommender(data,<span class="number">1</span>,pearsonsim)</span><br><span class="line">Out[<span class="number">31</span>]:</span><br><span class="line">[(<span class="number">7</span>, <span class="number">3.3356853252871588</span>),</span><br><span class="line">(<span class="number">8</span>, <span class="number">3.3349455396520296</span>),</span><br><span class="line">(<span class="number">0</span>, <span class="number">3.33492840157654</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="number">3.334920725716121</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="number">3.334919898261294</span>)]</span><br></pre></td></tr></table></figure></p>
<p>最后，谢谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 矩阵分解 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习下的电商商品推荐]]></title>
      <url>/2017/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E7%9A%84%E7%94%B5%E5%95%86%E5%95%86%E5%93%81%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<h1 id="常见算法套路"><a href="#常见算法套路" class="headerlink" title="常见算法套路"></a>常见算法套路</h1><p>电商行业中，对于用户的商品推荐一直是一个非常热门而且重要的话题，有很多比较成熟的方法，但是也各有利弊，大致如下：</p>
<ul>
<li><p>基于商品内容：比如食物A和食物B，对于它们价格、味道、保质期、品牌等维度，可以计算它们的相似程度，可以想象，我买了包子，很有可能顺路带一盒水饺回家。<br>优点：冷启动，其实只要你有商品的数据，在业务初期用户数据不多的情况下，也可以做推荐<br>缺点：预处理复杂，任何一件商品，维度可以说至少可以上百，如何选取合适的维度进行计算，设计到工程经验，这些也是花钱买不到的<br>典型：亚马逊早期的推荐系统</p>
</li>
<li><p>基于关联规则：最常见的就是通过用户购买的习惯，经典的就是“啤酒尿布”的案例，但是实际运营中这种方法运用的也是最少的，首先要做关联规则，数据量一定要充足，否则置信度太低，当数据量上升了，我们有更多优秀的方法，可以说没有什么亮点，业内的算法有apriori、ftgrowth之类的<br>优点：简单易操作，上手速度快，部署起来也非常方便<br>缺点：需要有较多的数据，精度效果一般<br>典型：早期运营商的套餐推荐</p>
</li>
<li><p>基于物品的协同推荐：假设物品A被小张、小明、小董买过，物品B被小红、小丽、小晨买过，物品C被小张、小明、小李买过；直观的看来，物品A和物品C的购买人群相似度更高（相对于物品B），现在我们可以对小董推荐物品C，小李推荐物品A，这个推荐算法比较成熟，运用的公司也比较多<br>优点：相对精准，结果可解释性强，副产物可以得出商品热门排序<br>缺点：计算复杂，数据存储瓶颈，冷门物品推荐效果差<br>典型：早期一号店商品推荐</p>
</li>
<li><p>基于用户的协同推荐：假设用户A买过可乐、雪碧、火锅底料，用户B买过卫生纸、衣服、鞋，用户C买过火锅、果汁、七喜；直观上来看，用户A和用户C相似度更高（相对于用户B），现在我们可以对用户A推荐用户C买过的其他东西，对用户C推荐用户A买过买过的其他东西，优缺点与<strong>基于物品的协同推荐</strong>类似，不重复了。</p>
</li>
<li><p>基于模型的推荐：svd++、特征值分解、概率图、聚分类等等。比如潜在因子分解模型，将用户的购买行为的矩阵拆分成两组权重矩阵的乘积，一组矩阵代表用户的行为特征，一组矩阵代表商品的重要性，在用户推荐过程中，计算该用户在历史训练矩阵下的各商品的可能性进行推荐。<br>优点：精准，对于冷门的商品也有很不错的推荐效果<br>缺点：计算量非常大，矩阵拆分的效能及能力瓶颈一直是受约束的<br>典型：惠普的电脑推荐</p>
</li>
<li><p>基于时序的推荐：这个比较特别，在电商运用的少，在Twitter，Facebook，豆瓣运用的比较多，就是只有赞同和反对的情况下，怎么进行评论排序，详细的可以参见我之前写的一篇文章：<a href="http://www.jianshu.com/p/b3e9b300a100" target="_blank" rel="noopener">应用：推荐系统-威尔逊区间法</a></p>
</li>
</ul>
<ul>
<li>基于深度学习的推荐：现在比较火的CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)都有运用在推荐上面的例子，但是都还是试验阶段，但是有个基于word2vec的方法已经相对比较成熟，也是我们今天介绍的重点。<br>优点：推荐效果非常精准，所需要的基础存储资源较少<br>缺点：工程运用不成熟，模型训练调参技巧难<br>典型：当前电商的会员商品推荐</li>
</ul>
<hr>
<h1 id="item2vec的工程引入"><a href="#item2vec的工程引入" class="headerlink" title="item2vec的工程引入"></a>item2vec的工程引入</h1><p>现在某电商的商品有约3亿个，商品的类目有10000多组，大的品类也有近40个，如果通过传统的协同推荐，实时计算的话，服务器成本，计算能力都是非常大的局限，之前已经有过几篇应用介绍：<a href="http://www.jianshu.com/p/fd245999ebfe" target="_blank" rel="noopener">基于推荐的交叉销售</a>、<a href="http://www.jianshu.com/p/e932b9744da6" target="_blank" rel="noopener">基于用户行为的推荐预估</a>。会员研发部门因为不是主要推荐的应用部门，所以在选择上，我们期望的是更加<strong>高效高速且相对准确的简约版</strong>模型方式，所以我们这边基于了word2vec的原始算法，仿造了itemNvec的方式。</p>
<p>首先，让我们对itemNvec进行理论拆分：</p>
<h2 id="part-one：n-gram"><a href="#part-one：n-gram" class="headerlink" title="part one：n-gram"></a>part one：n-gram</h2><p><strong>目标商品的前后商品对目标商品的影响程度</strong><br><img src="/2017/08/19/深度学习下的电商商品推荐/1.png" alt=""><br>这是两个用户userA，userB在易购上面的消费time line，灰色方框内为我们观察对象，试问一下，如果换一下灰色方框内的userA、userB的购买物品，直观的可能性有多大？</p>
<p><img src="/2017/08/19/深度学习下的电商商品推荐/2.png" alt=""></p>
<p>直观的体验告诉我们，这是不可能出现，或者绝对不是常出现的，所以，我们就有一个初始的假设，<strong>对于某些用户在特定的类目下，用户的消费行为是连续影响的</strong>，换句话说，就是我买了什么东西是依赖我之前买过什么东西。如何通过算法语言解释上面说的这件事呢？<br>大家回想一下，naive bayes做垃圾邮件分类的时候是怎么做的？<br>假设“我公司可以提供发票、军火出售、航母维修”这句话是不是垃圾邮件？</p>
<p>P1(“垃圾邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“垃圾邮件”)p(“我公司可以提供发票、军火出售、航母维修”/“垃圾邮件”)/p(“我公司可以提供发票、军火出售、航母维修”)<br>=p(“垃圾邮件”)p(“发票”，“军火”，“航母”/“垃圾邮件”)/p(“发票”，“军火”，“航母”)</p>
<p>同理<br>P2(“正常邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“正常邮件”)p(“发票”，“军火”，“航母”/“正常邮件”)/p(“发票”，“军火”，“航母”)</p>
<p>我们只需要比较p1和p2的大小即可，在<strong>条件独立的情况下</strong>可以直接写成：<br>P1(“垃圾邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“垃圾邮件”)p(“发票”/“垃圾邮件”)p(“军火”/“垃圾邮件”)p(“航母”/“垃圾邮件”)<br>P2(“正常邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“正常邮件”)p(“发票”/“正常邮件”)p(“军火”/“正常邮件”)p(“航母”/“正常邮件”)</p>
<p>但是，我们看到，无论“我公司可以提供发票、军火出售、航母维修”词语的顺序怎么变化，不影响它最后的结果判定，但是我们这边的需求里面前面买的东西对后项的影响会更大。<br>冰箱=&gt;洗衣机=&gt;衣柜=&gt;电视=&gt;汽水，这样的下单流程合理<br>冰箱=&gt;洗衣机=&gt;汽水=&gt;电视=&gt;衣柜，这样的下单流程相对来讲可能性会更低<br>但是对于naive bayes，它们是一致的。<br>所以，我们这边考虑顺序，还是上面那个垃圾邮件的问题。<br>P1(“垃圾邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“垃圾邮件”)p(“发票”)p(“军火”/“发票”)p(“军火”/“航母”)<br>P1(“正常邮件”|“我公司可以提供发票、军火出售、航母维修”)<br>=p(“正常邮件”)p(“发票”)p(“军火”/“发票”)p(“军火”/“航母”)<br>这边我们每个词只依赖前一个词，理论上讲依赖1-3个词通常都是可接受的。以上的考虑顺序的bayes就是基于著名的马尔科夫假设（Markov Assumption）：下一个词的出现仅依赖于它前面的一个或几个词下的联合概率问题，相关详细的理论数学公式就不给出了，这边这涉及一个思想。</p>
<h2 id="part-two：Huffman-Coding"><a href="#part-two：Huffman-Coding" class="headerlink" title="part two：Huffman Coding"></a>part two：Huffman Coding</h2><p><strong>更大的数据存储形式</strong><br>我们常用的user到item的映射是通过one hot encoding的形式去实现的，这有一个非常大的弊端就是数据存储系数且维度灾难可能性极大。<br>回到最初的那组数据：<strong>现在商品有约4亿个，商品的类目有10000多组，大的品类也有近40个</strong>，同时现在会员数目达到5亿，要是需要建造一个用户商品对应的购买关系矩阵做<strong>基于用户的协同推荐</strong>的话，我们需要做一个4亿X6亿的1/0矩阵，这个是几乎不可能的，Huffman采取了一个近似二叉树的形式进行存储：<br>我们以商品购买量为例，讲解一下如何以二叉树的形式替换one hot encoding存储方式：<br>假设，促销期间，经过统计，有冰箱=&gt;洗衣机=&gt;烘干机=&gt;电视=&gt;衣柜=&gt;钻石的用户下单链条（及购买物品顺序如上），其中冰箱总售出15万台，洗衣机总售出8万台，烘干机总售出6万台，电视总售出5万台，衣柜总售出3万台，钻石总售出1万颗</p>
<p><img src="/2017/08/19/深度学习下的电商商品推荐/3.png" alt="Huffman树构造过程"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.给定&#123;15,8,6,5,3,1&#125;为二叉树的节点，每个树仅有一个节点，那就存在6颗单独的树</span><br><span class="line">2.选择节点权重值最小的两颗树进行合并也就是&#123;3&#125;、&#123;1&#125;，合并后计算新权重3+1=4</span><br><span class="line">3.将&#123;3&#125;，&#123;1&#125;树从节点列表删除，将3+1=4的新组合树放回原节点列表</span><br><span class="line">4.重新进行2-3，直到只剩一棵树为止</span><br></pre></td></tr></table></figure></p>
<p><img src="/2017/08/19/深度学习下的电商商品推荐/4.png" alt=""><br>针对每层每次分支过程，我们可以将所有权重大的节点看做是1，权重小的节点看做是0，相反亦可。现在，我们比如需要知道钻石的code，就是1000，也就是灰色方框的位置，洗衣机的code就是111；这样的存储利用了0/1的存储方式，也同时考虑了组合位置的排列长度，节省了数据的存储空间。</p>
<h2 id="part-three：node-probility"><a href="#part-three：node-probility" class="headerlink" title="part three：node probility"></a>part three：node probility</h2><p><strong>最大化当前数据出现可能的概率密度函数</strong><br>对于钻石的位置而言，它的Huffman code是1000，那就意味着在每一次二叉选择的时候，它需要一次被分到1，三次被分到0，而且每次分的过程中，只有1/0可以选择，这是不是和logistic regression里面的0/1分类相似，所以这边我们也直接使用了lr里面的交叉熵来作为loss function。</p>
<p><strong>其实对于很多机器学习的算法而言，都是按照先假定一个模型，再构造一个损失函数，通过数据来训练损失函数求argmin(损失函数)的参数，放回到原模型。</strong></p>
<p>让我们详细的看这个钻石这个例子：</p>
<p><img src="/2017/08/19/深度学习下的电商商品推荐/5.png" alt="第一步"><br>p(1|No.1层未知参数)=sigmoid(No.1层未知参数)</p>
<hr>
<p><img src="/2017/08/19/深度学习下的电商商品推荐/6.png" alt="第二步"><br>p(0|No.2层未知参数)=sigmoid(No.2层未知参数)<br>同理，第三第四层：<br>p(0|No.3层未知参数)=sigmoid(No.3层未知参数)<br>p(0|No.4层未知参数)=sigmoid(No.4层未知参数)<br>然后求p(1|No.1层未知参数)xp(0|No.2层未知参数)xp(0|No.3层未知参数)xp(0|No.4层未知参数)最大下对应的每层的未知参数即可，求解方式与logistic求解方式近似，未知参数分布偏导，后续采用梯度下降的方式（极大、批量、牛顿按需使用）</p>
<h2 id="part-four：approximate-nerual-network"><a href="#part-four：approximate-nerual-network" class="headerlink" title="part four：approximate nerual network"></a>part four：approximate nerual network</h2><p><strong>商品的相似度</strong><br>刚才在part three里面有个p(1|No.1层未知参数)这个逻辑，这个NO.1层未知参数里面有一个就是商品向量。<br>举个例子：<br>存在1000万个用户有过：“啤酒=&gt;西瓜=&gt;剃须刀=&gt;百事可乐”的商品购买顺序<br>10万个用户有过：“啤酒=&gt;苹果=&gt;剃须刀=&gt;百事可乐”的商品购买顺序，如果按照传统的概率模型比如navie bayes 或者n-gram来看，P（啤酒=&gt;西瓜=&gt;剃须刀=&gt;百事可乐）&gt;&gt;p（啤酒=&gt;苹果=&gt;剃须刀=&gt;百事可乐），但是实际上这两者的人群应该是同一波人，他们的属性特征一定会是一样的才对。</p>
<p>我们这边通过了随机初始化每个商品的特征向量，然后通过part three的概率模型去训练，最后确定了词向量的大小。除此之外，还可以通过神经网络算法去做这样的事情。</p>
<p><img src="/2017/08/19/深度学习下的电商商品推荐/7.png" alt=""><br>Bengio 等人在 2001 年发表在 NIPS 上的文章《A Neural Probabilistic Language Model》介绍了详细的方法。<br>我们这边需要知道的就是，对于最小维度商品，我们以商品向量（0.8213，0.8232，0.6613，0.1234，…）的形式替代了0-1点（0，0，0，0，0，1，0，0，0，0…），单个的商品向量无意义，但是成对的商品向量我们就可以比较他们间的余弦相似度，就可以比较类目的相似度，甚至品类的相似度。</p>
<h1 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h1><p>1.数据读取<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mt</span><br><span class="line">from gensim.models import word2vec</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">order_data = pd.read_table(&apos;C:/Users/17031877/Desktop/SuNing/cross_sell_data_tmp1.txt&apos;)</span><br><span class="line">dealed_data = order_data.drop(&apos;member_id&apos;, axis=1)</span><br><span class="line">dealed_data = pd.DataFrame(dealed_data).fillna(value=&apos;&apos;)</span><br></pre></td></tr></table></figure></p>
<p>2.简单的数据合并整理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 数据合并</span><br><span class="line">dealed_data = dealed_data[&apos;top10&apos;] + [&quot; &quot;] + dealed_data[&apos;top9&apos;] + [&quot; &quot;] + dealed_data[&apos;top8&apos;] + [&quot; &quot;] + \</span><br><span class="line">dealed_data[&apos;top7&apos;] + [&quot; &quot;] + dealed_data[&apos;top6&apos;] + [&quot; &quot;] + dealed_data[&apos;top5&apos;] + [&quot; &quot;] + dealed_data[</span><br><span class="line">&apos;top4&apos;] + [&quot; &quot;] + dealed_data[&apos;top3&apos;] + [&quot; &quot;] + dealed_data[&apos;top2&apos;] + [&quot; &quot;] + dealed_data[&apos;top1&apos;]</span><br><span class="line"></span><br><span class="line"># 数据分列</span><br><span class="line">dealed_data = [s.encode(&apos;utf-8&apos;).split() for s in dealed_data]</span><br><span class="line"></span><br><span class="line"># 数据拆分</span><br><span class="line">train_data, test_data = train_test_split(dealed_data, test_size=0.3, random_state=42)</span><br></pre></td></tr></table></figure></p>
<p>3.模型训练<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据训练</span><br><span class="line"># sg=1,skipgram;sg=0,SBOW</span><br><span class="line"># hs=1:hierarchical softmax,huffmantree</span><br><span class="line"># nagative = 0 非负采样</span><br><span class="line">model = word2vec.Word2Vec(train_data, sg=1, min_count=10, window=2, hs=1, negative=0)</span><br></pre></td></tr></table></figure></p>
<p>接下来就是用model来训练得到我们的推荐商品，这边有三个思路，可以根据具体的业务需求和实际数据量来选择：<br>3.1 相似商品映射表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 最后一次浏览商品最相似的商品组top3</span><br><span class="line">x = 1000</span><br><span class="line">result = []</span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line">for i in range(x):</span><br><span class="line">test_data_split = [s.encode(&apos;utf-8&apos;).split() for s in test_data[i]]</span><br><span class="line">k = len(test_data_split)</span><br><span class="line">last_one = test_data_split[k - 1]</span><br><span class="line">last_one_recommended = model.most_similar(last_one, topn=3)</span><br><span class="line">tmp = last_one_recommended[0] + last_one_recommended[1] + last_one_recommended[2]</span><br><span class="line">last_one_recommended = pd.concat([pd.DataFrame(last_one), pd.DataFrame(np.array(tmp))], axis=0)</span><br><span class="line">last_one_recommended = last_one_recommended.T</span><br><span class="line">result = pd.concat([pd.DataFrame(last_one_recommended), result], axis=0)</span><br></pre></td></tr></table></figure></p>
<p>考虑用户最后一次操作的关注物品x，干掉那些已经被用户购买的商品，剩下的商品表示用户依旧有兴趣但是因为没找到合适的或者便宜的商品，通过商品向量之间的相似度，可以直接计算出，与其高度相似的商品推荐给用户。</p>
<p>3.2 最大可能购买商品<br>根据历史上用户依旧购买的商品顺序，判断根据当前这个目标用户近期买的商品，接下来他最有可能买什么？<br>比如历史数据告诉我们，购买了手机+电脑的用户，后一周内最大可能会购买背包，那我们就针对那些近期购买了电脑+手机的用户去推送电脑包的商品给他，刺激他的潜在规律需求。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 向量库</span><br><span class="line">rbind_data = pd.concat(</span><br><span class="line">[order_data[&apos;top1&apos;], order_data[&apos;top2&apos;], order_data[&apos;top3&apos;], order_data[&apos;top4&apos;], order_data[&apos;top5&apos;],</span><br><span class="line">order_data[&apos;top6&apos;], order_data[&apos;top7&apos;], order_data[&apos;top8&apos;], order_data[&apos;top9&apos;], order_data[&apos;top10&apos;]], axis=0)</span><br><span class="line">x = 50</span><br><span class="line">start = []</span><br><span class="line">output = []</span><br><span class="line">score_final = []</span><br><span class="line">for i in range(x):</span><br><span class="line">score = np.array(-100000000000000)</span><br><span class="line">name = np.array(-100000000000000)</span><br><span class="line">newscore = np.array(-100000000000000)</span><br><span class="line">tmp = test_data[i]</span><br><span class="line">k = len(tmp)</span><br><span class="line">last_one = tmp[k - 2]</span><br><span class="line">tmp = tmp[0:(k - 1)]</span><br><span class="line">for j in range(number):</span><br><span class="line">tmp1 = tmp[:]</span><br><span class="line">target = rbind_data_level[j]</span><br><span class="line">tmp1.append(target)</span><br><span class="line">test_data_split = [tmp1]</span><br><span class="line">newscore = model.score(test_data_split)</span><br><span class="line">if newscore &gt; score:</span><br><span class="line">score = newscore</span><br><span class="line">name = tmp1[len(tmp1) - 1]</span><br><span class="line">else:</span><br><span class="line">pass</span><br><span class="line">start.append(last_one)</span><br><span class="line">output.append(name)</span><br><span class="line">score_final.append(score)</span><br></pre></td></tr></table></figure></p>
<p>3.3 联想记忆推荐<br>在3.2中，我们根据了这个用户近期购买行为，从历史已购用户的购买行为数据发现规律，提供推荐的商品。还有一个近似的逻辑，就是通过目标用户最近一次的购买商品进行推测，参考的是历史用户的单次购买附近的数据，详细如下：</p>
<p><img src="/2017/08/19/深度学习下的电商商品推荐/8.png" alt=""><br>这个实现也非常的简单，这边代码我自己也没有写，就不贴了，采用的还是word2vec里面的<code>predict_output_word(context_words_list, topn=10)</code>，Report the probability distribution of the center word given the context words as input to the trained model</p>
<p>其实，这边详细做起来还是比较复杂的，我这边也是简单的贴了一些思路，如果有不明白的可以私信我，就这样，最后，谢谢阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于自然语言识别下的流失用户预警]]></title>
      <url>/2017/08/15/%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%AF%86%E5%88%AB%E4%B8%8B%E7%9A%84%E6%B5%81%E5%A4%B1%E7%94%A8%E6%88%B7%E9%A2%84%E8%AD%A6/</url>
      <content type="html"><![CDATA[<p><strong>update:</strong><br><strong>17.12.20 : 关于IDF处描述，经@余海跃同学提醒，细化了解释内容，感谢！</strong></p>
<hr>
<blockquote>
<p>在电商运营过程中，会有大量的用户反馈留言，包括吐槽的差评，商品不满的地方等等，在用户运营生态中，这部分用户是最有可能流失也是最影响nps的人群，<strong>通过对其评价的语义分析，每日找出潜在的流失人群进行包括”电话回访”,”补券安慰”，”特权享受”等行为，有效的降低了用户的流失。</strong>根据实际的业务营销效果，在模型上线后，<strong>abtest检验下模型识别用户人群进行营销后的流失率比随意营销下降9.2%，效果显著。</strong></p>
</blockquote>
<p>当前文本文义识别存在一些问题：<br>（1）准确率而言，很多线上数据对特征分解的过程比较粗糙，<strong>很多直接基于df或者idf结果进行排序</strong>，在算法设计过程中，<strong>也是直接套用模型</strong>，只是工程上的实现，缺乏统计意义上的分析；</p>
<p>（2）<strong>文本越多，特征矩阵越稀疏，计算过程越复杂</strong>。常规的文本处理过程中只会对文本对应的特征值进行排序，其实在文本选择中，可以先剔除相似度较高的文本，这个课题比较大，后续会单独开一章进行研究；</p>
<p>（3）<strong>扩展性较差</strong>。比如我们这次做的流失用户预警是基于电商数据，你拿去做通信商的用户流失衡量的话，其质量会大大下降，所以重复开发的成本较高，这个属于非增强学习的硬伤，目前也在攻克这方面的问题。</p>
<p>首先，我们来看下，整个算法设计的思路：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.通过hive将近期的用户评价hadoop文件下载为若干个text文件</span><br><span class="line">2.通过R语言将若干个text整合读取为一个R内的dataframe</span><br><span class="line">3.利用R里面的正则函数将文本中的异常符号‘#！@￥%%’，英文，标点等去除</span><br><span class="line">（这边可以在hive里面提前处理好，也可以在后续的分词过程中利用停顿词去除）</span><br><span class="line">4.文本分词，这边可以利用R中的Rwordseg，jiebaR等，我写这篇文章之前看到很多现有的语义分析的文章中，Rwordseg用的挺多，所以这边我采用了jiebaR</span><br><span class="line">5.文本分词特征值提取,常见的包括互信息熵，信息增益，tf-idf，本文采取了tf-idf，剩余方法会在后续文章中更新</span><br><span class="line">6.模型训练</span><br><span class="line">这边我采取的方式是利用概率模型naive bayes+非线性模型random forest先做标签训练，最后用nerual network对结果进行重估</span><br><span class="line">（原本我以为这样去做会导致很严重的过拟合，但是在实际操作之后发现，过拟合并不是很严重，至于原因我也不算很清楚，后续抽空可以研究一下）</span><br></pre></td></tr></table></figure></p>
<p>下面，我们来剖析文本分类识别的每一步</p>
<h1 id="定义用户属性"><a href="#定义用户属性" class="headerlink" title="定义用户属性"></a>定义用户属性</h1><p>首先，我们定义了已经存在的流失用户及非流失用户，易购的用户某品类下的购买周期为27天，针对前60天-前30天下单购物的用户，观察近30天是否有下单行为，如果有则为非流失用户，如果没有则为流失用户。提取每一个用户最近一次商品评价作为msg。</p>
<h1 id="文本合成"><a href="#文本合成" class="headerlink" title="文本合成"></a>文本合成</h1><p>通过hive -e的方式下载到本地，会形成text01，text02…等若干个文本，通过R进行<strong>文本整合</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#先设置文本路径</span><br><span class="line">path &lt;- &quot;C:/Users/17031877/Desktop/Nlp/answer/Cmsg&quot;</span><br><span class="line">completepath &lt;- list.files(path, pattern = &quot;*.txt$&quot;, full.names = TRUE)</span><br><span class="line"></span><br><span class="line">#批量读入文本</span><br><span class="line">readtxt &lt;- function(x) &#123;</span><br><span class="line">ret &lt;- readLines(x)                   #每行读取</span><br><span class="line">return(paste(ret, collapse = &quot;&quot;))     #通过paste将每一行连接起来</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#lappy批量操作，形成list，个人感觉对非关系数据，list处理更加便捷</span><br><span class="line">msg &lt;- lapply(completepath, readtxt)</span><br><span class="line"></span><br><span class="line">#用户属性</span><br><span class="line">user_status &lt;- list.files(path, pattern = &quot;*.txt$&quot;)</span><br><span class="line"></span><br><span class="line">#stringsAsFactors=F，避免很多文本被读成因子类型</span><br><span class="line">comment &lt;- as.data.frame(cbind(user_status, unlist(msg)),stringsAsFactors = F)</span><br><span class="line">colnames(comment) &lt;- c(&quot;user_status&quot;, &quot;msg&quot;)</span><br></pre></td></tr></table></figure></p>
<p>基础的数据整合就完成了。</p>
<p><img src="/2017/08/15/基于自然语言识别下的流失用户预警/1.png" alt=""></p>
<h1 id="数据整理"><a href="#数据整理" class="headerlink" title="数据整理"></a>数据整理</h1><p>也可以看到，基础数据读取完成后，还是很多评论会有一些<strong>不规则的数据</strong>，包括‘#￥%……&amp;’，英文，数字，下面通过正则、停顿词的方式进行处理：</p>
<h2 id="正则化处理"><a href="#正则化处理" class="headerlink" title="正则化处理"></a>正则化处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#直接处理</span><br><span class="line">comment$msg &lt;- gsub(pattern = &quot; &quot;, replacement =&quot;&quot;, comment$msg)  #gsub是字符替换函数，去空格</span><br><span class="line">comment$msg&lt;- gsub(&quot;[[:digit:]]*&quot;, &quot;&quot;, comment$msg) #清除数字[a-zA-Z]</span><br><span class="line">comment$msg&lt;- gsub(&quot;[a-zA-Z]&quot;, &quot;&quot;, comment$msg)   #清除英文字符</span><br><span class="line">comment$msg&lt;- gsub(&quot;\\.&quot;, &quot;&quot;, comment$msg)      #清除全英文的dot符号</span><br><span class="line">--------------------------------------------------------------------------------------------------</span><br><span class="line">#如果是常做nlp处理，可以写成函数打包，后期直接library就可以了</span><br><span class="line">#数值删除</span><br><span class="line">removeNumbers =</span><br><span class="line">function(x)&#123;</span><br><span class="line">ret = gsub(&quot;[0-9]&quot;,&quot;&quot;,x)</span><br><span class="line">return(ret)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#字符删除</span><br><span class="line">removeLiters =</span><br><span class="line">function(x)&#123;</span><br><span class="line">ret = gsub(&quot;[a-z|A-Z]&quot;,&quot;&quot;,x)</span><br><span class="line">return(ret)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#各种操作符处理,\s表示空格,\r表示回车,\n表示换行</span><br><span class="line">removeActions =</span><br><span class="line">function(x)&#123;</span><br><span class="line">ret = gsub(&quot;\\s|\\r|\\n&quot;, &quot;&quot;, x)</span><br><span class="line">return(ret)</span><br><span class="line">&#125;</span><br><span class="line">comment$msg=removeNumbers(comment$msg)</span><br><span class="line">comment$msg=removeLiters(comment$msg)</span><br><span class="line">comment$msg=removeActions (comment$msg)</span><br></pre></td></tr></table></figure>
<p>这边需要对正则化里面的一些表示有所了解，详细可以百度，一般我都是具体需求具体去看，因为太多，自己又懒，所以没记</p>
<h2 id="停顿词"><a href="#停顿词" class="headerlink" title="停顿词"></a>停顿词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#加载jiebaR包</span><br><span class="line">library(jiebaR)</span><br><span class="line"></span><br><span class="line">#找jiebaR存停顿词的地方，自行将需要处理掉的符号存进去，我这边是C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8</span><br><span class="line">tagger&lt;-worker(stop_word=&quot;C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8&quot;)</span><br></pre></td></tr></table></figure>
<p>至于位置可以通过直接输入<code>worker()</code>查看，</p>
<p><img src="/2017/08/15/基于自然语言识别下的流失用户预警/2.png" alt=""><br>当前的是没有stop_word的，所有词存储的位置在：C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/下</p>
<h1 id="文本分词"><a href="#文本分词" class="headerlink" title="文本分词"></a>文本分词</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#jieba 分词,去除停顿词</span><br><span class="line">library(jiebaR)</span><br><span class="line">tagger&lt;-worker(stop_word=&quot;C:/Program Files/R/R-3.3.3/library/jiebaRD/dict/stop_words.utf8&quot;)</span><br><span class="line">words=list()</span><br><span class="line">for (i in 1:nrow(comment))&#123;</span><br><span class="line">tmp=tagger[comment[i,2]]</span><br><span class="line">words=c(words,list(tmp))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>直接先分词，但是分词结果会存在很多只有一个字比如‘的’、‘你’、‘我’等或者很多无意义的长句‘中华人民共和国’、‘长使英雄泪满襟’等，需要把这些<strong>词长异常</strong>明显无意义的词句去掉。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#词长统计</span><br><span class="line">whole_words_set=unlist(words)</span><br><span class="line">whole_words_set_rank=data.frame(table(whole_words_set))</span><br><span class="line"></span><br><span class="line">whole_words_set_dealed=c()</span><br><span class="line">for (i in 1:nrow(whole_words_set_rank))&#123;</span><br><span class="line">tmp=nchar(as.character(whole_words_set_rank[i,1]))</span><br><span class="line">whole_words_set_dealed=c(whole_words_set_dealed,tmp)</span><br><span class="line">&#125;</span><br><span class="line">whole_words_set_dealed=cbind(whole_words_set_rank,whole_words_set_dealed)</span><br><span class="line">whole_words_set_dealed=whole_words_set_dealed[whole_words_set_dealed$whole_words_set_dealed&gt;1&amp;whole_words_set_dealed$whole_words_set_dealed&lt;5,]</span><br><span class="line">whole_words_set_dealed=whole_words_set_dealed[order(whole_words_set_dealed$Freq,decreasing=T),]</span><br><span class="line"></span><br><span class="line">#words的删除异常值,排序</span><br><span class="line">whole_words_set_sequence=words</span><br><span class="line">key_word=nrow(words)</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">for (j in 1:length(words[[i]]))&#123;</span><br><span class="line">tmp=ifelse(nchar(words[[i]][j])&gt;1 &amp; nchar(words[[i]][j])&lt;5,words[[i]][j],&apos;&apos;)</span><br><span class="line">whole_words_set_sequence[[i]][j]=tmp</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">whole_words_set_sequence[[i]]=whole_words_set_sequence[[i]][whole_words_set_sequence[[i]]!=&apos;&apos;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="tf-idf词特征值重要性排序"><a href="#tf-idf词特征值重要性排序" class="headerlink" title="tf-idf词特征值重要性排序"></a>tf-idf词特征值重要性排序</h1><p>首先，我们大致看一下排序的数据依旧：</p>
<p>TF = 某词在文章中出现的次数/文章包含的总词数（或者文章有价值词次数）<br>DF = （包含某词的文档数）/（语料库的文档总数）<br>IDF = log（（语料库的文档总数）/（包含某词的文档数+1））<br>这边的+1是为了避免（语料库的文档总数）/（包含某词的文档数）=1，log(1)=0，使得最后的重要性中出现0的情况，与有意义的前提相互驳斥。<br>TF-IDF = TF*IDF</p>
<p>分别看下，里面的每一项的意义：<br>TF，我们可以看出，<strong>在同一个评论中，词数出现的越多，代表这个词越能成为这篇文章的代表</strong>，当然前提是非无意义的助词等。</p>
<p>IDF，我们可以看出，<strong>所以评论中，包含目标词的评论的占比，占比数越高，目标词的意义越大</strong>，假设1000条评论中，“丧心病狂”在一条评论里面重复了10次，但是其他999条里面一次也没有出现，那就算“丧心病狂”非常能代表这条评论，但是在做文本集特征考虑的情况下，它的价值也是不大的。</p>
<p><strong>注意，经@余海跃同学提醒，这边的IDF解释不清晰，详细剖析如下：</strong><br>首先idf的定义是如下这样的：<br><img src="/2017/08/15/基于自然语言识别下的流失用户预警/3.png" alt=""><br>（D为所有文章，d为单篇文章）<br>通常，会考虑类似近拉普拉斯平滑（+1）这样的方法修正idf值，在NLP领域，真正的意思如你所理解的：随着single word出现在的doc数量的增加，idf值应该是下降的，我们认为，一个词在越多文档中出现，该词代表文章的概述的能力越弱。其实，我在《应用：基于自然语言识别下的流失用户预警》实际R代码编写过程中也是这么去做的，但是当时我考虑了另一个方面：电商的评论与传统的文学文本差异还是很大的，单条评论中独特的词（只出现过一次的词或者短句）非常之多。这意味着：如果原封不动的按照idf去计算的话，最后识别出来的判别标签，也就是’文本分词特征值’会变得非常多，而且对泛化情况的识别能力非常的差。体现在对做后续的有监督分类的时候，如果不做处理会造成异常过拟合的问题。所以，我这边表述的想法是将idf值过大的一些词，也就是single word出现的doc过少的一些词剔除，再根据剩余的其他特征词计算idf提取关键特征词，我这边设定的阈值范围是：特征词至少在3.5%以上的评论中出现过。当然，你完全可以选择另外一种方法，完全按照idf计算，在最后做特分类之前，做特征筛选，去除掉一些冗余特征词变量。</p>
<p>下面，我们来看代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#tfidf_partone 为对应的tf</span><br><span class="line">tdidf_partone=whole_words_set_sequence</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp1=as.data.frame(prop.table(table(whole_words_set_sequence[[i]])))</span><br><span class="line">tdidf_partone[[i]]=tmp1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#tdidf_partfour 为对应的idf</span><br><span class="line">tdidf_parttwo=unique(unlist(whole_words_set_sequence))</span><br><span class="line">tdidf_max=length(tdidf_parttwo)</span><br><span class="line">tdidf_partthree=tdidf_parttwo</span><br><span class="line">for (i in 1:tdidf_max)&#123;</span><br><span class="line">tmp=0</span><br><span class="line">aimed_word=tdidf_parttwo[i]</span><br><span class="line">for (j in 1:key_word)&#123;</span><br><span class="line">tmp=tmp+sum(tdidf_parttwo[i] %in% whole_words_set_sequence[[j]])</span><br><span class="line">&#125;</span><br><span class="line">tdidf_partthree[i]=log(as.numeric(key_word)/(tmp+1))</span><br><span class="line">&#125;</span><br><span class="line">tdidf_partfour=cbind(tdidf_parttwo,tdidf_partthree)</span><br><span class="line">tdidf_partfive=tdidf_partone</span><br><span class="line">colnames(tdidf_partfour)&lt;-c(&apos;Var1&apos;,&apos;Freq1&apos;)</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tdidf_partfive[[i]]=merge(tdidf_partone[[i]],tdidf_partfour,by=c(&quot;Var1&quot;))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#计算tf-idf结果，并排序key_word</span><br><span class="line">tdidf_partsix=tdidf_partfive</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp=tdidf_partfive[[i]][,2:3]</span><br><span class="line">tdidf_partsix[[i]][,2]=as.numeric(tmp[,1])*as.numeric(tmp[,2])</span><br><span class="line">tdidf_partsix[[i]]=tdidf_partsix[[i]][order(tdidf_partsix[[i]][,2],decreasing=T),][]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">key_word=c()</span><br><span class="line">for (i in 1:key_word)&#123;</span><br><span class="line">tmp=tdidf_partsix[[i]][1:5,1]</span><br><span class="line">key_word=rbind(key_word,as.character(tmp))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>理论上讲，如果这边数据存储方式用的是data.frame的话，可以利用spply、apply等批量处理函数，这边用得是list的方式，对lpply不是很熟悉的我，选择了for的循环，后续这边会优化一下，这样太消耗资源了。</p>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><p>这边，我最后采取的是概率模型naive bayes+非线性模型random forest先做标签训练，最后用nerual network对结果进行重估方式，但是在训练过程中，我还有几种模型的尝试，这边也一并贴出来给大家做参考。</p>
<h2 id="数据因子化的预处理"><a href="#数据因子化的预处理" class="headerlink" title="数据因子化的预处理"></a>数据因子化的预处理</h2><p>这边得到了近400维度的有效词，现在将每一维度的词遍做一维的feature，同时，此处的feature的意义为要么评论存在该词，要么评论中不存在该词的0-1问题，<strong>需要因子化一下</strong>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#整合数据</span><br><span class="line">well_dealed_data=cbind(as.character(comment[,1]),key_word)</span><br><span class="line">names=as.data.frame(table(key_word))[,1]</span><br><span class="line">names_count=length(names)</span><br><span class="line">names=as.matrix(names,names_count,1)</span><br><span class="line">feature_matrix=matrix(rep(0,names_count*key_word),key_word,names_count)</span><br><span class="line">for (i in 1:names_count)&#123;</span><br><span class="line">for(j in 1:key_word)&#123;</span><br><span class="line">feature_matrix[j,i]=ifelse(names[i] %in% key_word[j,],1,0)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#art=1,literature=-1,标签0-1化</span><br><span class="line">feature_matrix=cbind(well_dealed_data[,1],feature_matrix)</span><br><span class="line">feature_matrix[feature_matrix[,1]==&apos;aimed&apos;,1]=&apos;1&apos;</span><br><span class="line">feature_matrix[feature_matrix[,1]==&apos;unaimed&apos;,1]=&apos;-1&apos;</span><br><span class="line"></span><br><span class="line">feature_matrix=as.data.frame(feature_matrix)</span><br><span class="line"></span><br><span class="line">num=1:(ncol(feature_matrix)-1)</span><br><span class="line">value_name=paste(&quot;feature&quot;,num)</span><br><span class="line">value_name=c(&apos;label&apos;,value_name)</span><br><span class="line">colnames(feature_matrix)=value_name</span><br><span class="line"></span><br><span class="line">#feature0-1化</span><br><span class="line">for (i in 1:ncol(feature_matrix))&#123;</span><br><span class="line">feature_matrix[,i]=as.factor(as.numeric(as.character(feature_matrix[,i])))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="数据切分训练测试"><a href="#数据切分训练测试" class="headerlink" title="数据切分训练测试"></a>数据切分训练测试</h2><p>这边就不适用切分函数了，自己写了一个更加快速。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n_index=sample(1:nrow(feature_matrix),round(0.7*nrow(feature_matrix)))</span><br><span class="line">train_feature_matrix=feature_matrix[n_index,]</span><br><span class="line">test_feature_matrix=feature_matrix[-n_index,]</span><br></pre></td></tr></table></figure></p>
<h2 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="backpropagation-neural-network"><a href="#backpropagation-neural-network" class="headerlink" title="backpropagation neural network"></a>backpropagation neural network</h3><p><strong>这边需要用网格算法对size和decay进行交叉检验，这边不贴细节，可以百度搜索详细过程。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(nnet)</span><br><span class="line">nn &lt;- nnet(label~., data=train_feature_matrix, size=2, decay=0.01, maxit=1000, linout=F, trace=F)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">nn.predict_train = predict(nn,train_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),nn.predict_train)</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">nn.predict_test = predict(nn,test_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),nn.predict_test)</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="Linear-Support-Vector-Machine"><a href="#Linear-Support-Vector-Machine" class="headerlink" title="Linear Support Vector Machine"></a>Linear Support Vector Machine</h3><p><strong>这边需要用网格算法对cost进行交叉检验，这边不贴细节，可以百度搜索详细过程。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">svmfit &lt;- svm(label~., data=train_feature_matrix, kernel = &quot;linear&quot;, cost = 10, scale = FALSE) # linear svm, scaling turned OFF</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">svmfit.predict_train=predict(svmfit, train_feature_matrix, type = &quot;probabilities&quot;)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(svmfit.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">svmfit.predict_test = predict(svmfit,test_feature_matrix,type = &quot;class&quot;)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(svmfit.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h3><p><strong>这边我没调参，我觉得这边做的好坏在于数据预处理中剩下来的特征词</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(e1071)</span><br><span class="line">sms_classifier &lt;- naiveBayes(train_feature_matrix[,-1], train_feature_matrix$label)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">sms.predict_train=predict(sms_classifier, train_feature_matrix)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(sms.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">sms.predict_test = predict(sms_classifier,test_feature_matrix)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(sms.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p><strong>这边因为是最后的整合模型，需要调参的地方比较多，首先根据oob确定在mtry=log（feature）下的最优trees数量，在根据确定的trees的数量，反过来去确定mtry的确定值。除此之外，还需要对树的最大深度，子节点的停止条件做交叉模拟，是整体模型训练过程中最耗时的地方</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(randomForest)</span><br><span class="line">randomForest=randomForest(train_feature_matrix[,-1], train_feature_matrix$label)</span><br><span class="line"></span><br><span class="line">#train数据集效果</span><br><span class="line">rf.predict_train=predict(randomForest, train_feature_matrix)</span><br><span class="line">result_combind_train=cbind(as.numeric(as.character(train_feature_matrix$label)),as.numeric(as.character(rf.predict_train)))</span><br><span class="line">correction_train=nrow(result_combind_train[result_combind_train[,1]==result_combind_train[,2],])/nrow(result_combind_train)</span><br><span class="line"></span><br><span class="line">#test数据集效果</span><br><span class="line">rf.predict_test = predict(randomForest,test_feature_matrix)</span><br><span class="line">result_combind_test=cbind(as.numeric(as.character(test_feature_matrix$label)),as.numeric(as.character(rf.predict_test)))</span><br><span class="line">correction_test=nrow(result_combind_test[result_combind_test[,1]==result_combind_test[,2],])/nrow(result_combind_test)</span><br></pre></td></tr></table></figure></p>
<p>就单模型下的test集合的准确率如下：</p>
<p><img src="/2017/08/15/基于自然语言识别下的流失用户预警/4.png" alt=""><br>整体上看，nnet是过拟合的，所以在测试集上的效果折扣程度最大；naive bayes模型的拟合效果应该是最弱的，但是好在它的开发成本低，逻辑简单，有统计意义；svm和randomforest这边的效果不相上下。本次训练的数据量在20w条左右，理论上讲再扩大数据集的话，randomforest的效果应该会稳定，svm会下降，nnet会上升。</p>
<h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p><img src="/2017/08/15/基于自然语言识别下的流失用户预警/5.png" alt=""></p>
<p>这边的train_data的准确率在92.1%，test_data的准确率在84.3%，与理想的test_data90%以上的准确率还是有差距，所以后续准备：<br>1.细化流失用户的定义方式，当前定义过于笼统粗糙<br>2.以RNN的模型去替代BpNN去做整合训练，探索特征到特征本身的激活会对结果的影响<br>3.重新定义词重要性，考虑互信息熵及isolation forest的判别方式</p>
<p>最后谢谢大家的阅读。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据预处理-异常值识别]]></title>
      <url>/2017/08/09/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%BC%82%E5%B8%B8%E5%80%BC%E8%AF%86%E5%88%AB/</url>
      <content type="html"><![CDATA[<p>系统总结了常用的异常值识别思路，整理如下：</p>
<h1 id="空间识别"><a href="#空间识别" class="headerlink" title="空间识别"></a>空间识别</h1><h2 id="分位数识别"><a href="#分位数识别" class="headerlink" title="分位数识别"></a>分位数识别</h2><p>代表的执行方法为<strong>箱式图</strong>：</p>
<p><img src="/2017/08/09/数据预处理-异常值识别/1.png" alt="箱式图（来自百度百科）"></p>
<p>上四分位数Q3，又叫做升序数列的75%位点<br>下四分位数Q1，又叫做升序数列的25%位点<br>箱式图检验就是摘除大于<code>Q3+3/2*（Q3-Q1）</code>，小于<code>Q1-3/2*（Q3-Q1）</code>外的数据，并认定其为异常值；针对全量样本已知的问题比较好，缺点在于数据量庞大的时候的排序消耗<br>R语言中的<code>quantile</code>函数，python中的<code>percentile</code>函数可以直接实现。</p>
<h2 id="距离识别"><a href="#距离识别" class="headerlink" title="距离识别"></a>距离识别</h2><p>最常用的就是<strong>欧式距离</strong>，<br>比如：两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：<br> <img src="/2017/08/09/数据预处理-异常值识别/2.jpeg" alt="欧式距离"></p>
<p> 可以直观感受的到，图中，距离蓝色B点距离为基准衡量的话，红色A1，红色A2，红色A3为距离较近点，A4为距离较远的异常点。</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/3.png" alt=""><br> 但是这样看问题会有一个隐患，我们犯了“就点论点”的错误没有考虑到全局的问题，让我在看下面这张图：</p>
<p> ! <img src="/2017/08/09/数据预处理-异常值识别/4.png" alt=""><br> 还是刚才那张图，橙色背景为原始数据集分布，这样看来A4的位置反而比A1、A3相对更靠近基准点B，所以在存在纲量不一致且数据分布异常的情况下，可以使用<strong>马氏距离</strong>代替<strong>欧式距离</strong>判断数据是否离群。</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/5.png" alt="马氏距离"><br> 其中，<code>μ</code>为feature的均值，<code>X</code>为观察值，<code>Σ</code>为feature的协方差矩阵<br> 马氏距离除了用来判断点是否异常，也可以用来判断两个数据集相识度，在图像识别，反欺诈识别中应用的也是非常普遍；问题在于太过于依赖<code>Σ</code>，不同的base case对应的<code>Σ</code>都是不一致的，不是很稳定</p>
<h2 id="密度识别"><a href="#密度识别" class="headerlink" title="密度识别"></a>密度识别</h2><p> 密度识别的方式方法比较多，这边就提供其中比较经典的，首先我们可以通过密度聚类中大名鼎鼎的<code>dbscan</code>入手，这边只讲思路，详细的算法过程另行介绍。<br> 简单的来讲，下面这张图可以协助理解</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/6.png" alt=""></p>
<p> 我们可以通过随机选择联通点，人为设置联通点附近最小半径a，半径内最小容忍点个数b，再考虑密度可达，形成蓝色方框内的正常数据区域，剩下的黄色区域内的点即为异常点。</p>
<p> 除此之外，密度识别里面还有一种方式，是参考单点附近的<strong>点密度</strong>判断，伪代码如下：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> 1.从特征集合中任选历史上没有被选择过的两维</span><br><span class="line"> 2.将原始点集映射到该两维平面上，刻画点集中心a</span><br><span class="line"> 3.以点集中心a，x为半径画圆，不断扩大x的值，直到被覆盖的点集数/原始点集数的最低阈值</span><br><span class="line"> 4.没有被覆盖到的点集打上outlier_label</span><br><span class="line"> 5.repeat 1-4</span><br><span class="line"> 6.统计点对应的outlier_label个数</span><br><span class="line"> 7.排序高则优先为异常点</span><br></pre></td></tr></table></figure></p>
<p> 这边参考了<code>Clique</code>的映射+<code>Denclue</code>的密度分布函数的思路，注意问题就是计算量大，所以对小样本的适用程度更高一些，针对大样本多特征的数据可以考虑对样本进行子集抽样，再根据子集进行1-5，汇总后整体进行6-7步骤，实际检验效果仍然可以达到不抽样的85%以上</p>
<h2 id="拉依达准则"><a href="#拉依达准则" class="headerlink" title="拉依达准则"></a>拉依达准则</h2><p> 这个方法更加偏统计一些，设计到一些距离的计算，勉强放在空间识别里面</p>
<p> 这种判别处理原理及方法仅局限于对<strong>正态或近似正态分布的样本</strong>数据处理，它是以<strong>数据充分大</strong>为前提的，当数据较少的情况下，最好不要选用该准则。</p>
<p> 正态分布（高斯分布）是最常用的一种概率分布，通常正态分布有两个参数μ和σ为标准差。N(0,1)即为标准正态分布。</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/7.png" alt=""><br> 从上面这张图可以看出，当数据对象值偏离均值3倍标准差的时候，该数据合理的出现可能性小于3‰，所以可以直接认定，数据偏离均值±3倍标准差时，为异常点。但是最后再次强调一遍：<strong>它是以数据充分大为前提的，当数据较少的情况下，最好不要选用该准则。</strong></p>
<h1 id="计量识别"><a href="#计量识别" class="headerlink" title="计量识别"></a>计量识别</h1><h2 id="G-test或者说是likelihood-Ratio方法"><a href="#G-test或者说是likelihood-Ratio方法" class="headerlink" title="G-test或者说是likelihood Ratio方法"></a>G-test或者说是likelihood Ratio方法</h2><p> G-test这类方法运用在医学方面较多，常用于检验观测变量值是否符合理论期望的比值。现在也用在电商、出行、搜索领域检验一些无监督模型的质量、数据质量。</p>
<p> 当我们新上一个模型，部分用户的反馈特别异常，我们不知道是不是异常数据，在接下来的分析中需不需要剔除，我们可以用统计学方法予以取舍。<br> <img src="/2017/08/09/数据预处理-异常值识别/8.png" alt=""><br> 其中O为观测值，E为期望值，假如我们的网站每天24个小时的订单量分布稳定，分时段计算出一个均值，E1，E2，..E24，新模型产出后，我们问题用户群对应的24个小时的订单分布值O1,O2,…..O24,套用上面的公式，我们就可以计算出一个G值出来。</p>
<p> 然后在根据G-test的base值，观察目标用户可信的最大置信度，判断置信度是否符合我们的最低要求；likelihood Ratio方法类似，相关论文可以直接搜索。</p>
<h2 id="模型拟合"><a href="#模型拟合" class="headerlink" title="模型拟合"></a>模型拟合</h2><p> 这类方法属于简单有监督识别，常见的包括贝叶斯识别，决策树识别，线性回归识别等等。</p>
<p> 需要提前知道两组数据：正常数据及非正常类数据，再根据它们所对应的特征，去拟合一条尽可能符合的曲线，后续直接用该条曲线去判断新增的数据是否正常。<br> 举个例子：<br> 金融借贷中，我们事先活动一批正常借贷用户，和逾期不还用户，我们通过<strong>打分卡模型</strong>去识别已知用户的特征，假设得到芝麻分、手机使用时长、是否为男性为关键特征。接下来判断未知标签的新增数据是否为正常用户的话，直接根据之前判断出来的拟合打分卡曲线去做0-1概率预估就行了。</p>
<p> 但是模型拟合的方式使用情况较为局限，绝大多数异常识别问题是无法拿到前置的历史区分数据，或者已分好的数据不能够覆盖全量可能，导致时间判断误差较大，顾一般只做emsemble model的其中一种组合模块，不建议做主要依赖标注。</p>
<p> 明尼苏达州大学有过一篇异常论文识别的总结，里面关于有监督模型、半监督模型、无监督模型等模型拟合讲的非常细致，如果感兴趣可以研究一下，附上论文Survey：<a href="http://cucis.ece.northwestern.edu/projects/DMS/publications/AnomalyDetection.pdf" target="_blank" rel="noopener">http://cucis.ece.northwestern.edu/projects/DMS/publications/AnomalyDetection.pdf</a></p>
<h2 id="变维识别"><a href="#变维识别" class="headerlink" title="变维识别"></a>变维识别</h2><p> 首先，我们来看一下PCA的伪代码<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> 1.去除平均值,方便后续协方差，方差矩阵的计算</span><br><span class="line"> 2.计算协方差矩阵及其特征值和特征向量</span><br><span class="line"> 3.将特征值从大到小排序，特征值可以反映方差贡献度，特征值越大，方差贡献度越大</span><br><span class="line"> 4.保留最大的N个特征值以及它们的所对应的特征向量</span><br><span class="line"> 5.将数据映射到上述 N 个特征向量构造的新空间中</span><br></pre></td></tr></table></figure></p>
<p> pca的核心思路在于<strong>尽可能通过feature的组合代替原始feature，使得原始数据的方差最大化</strong>。<br> 通过pca可以得到第一主成分、第二主成分…。<br> 对于正常数据集来说，正常数据量远远大于异常数据，所以正常数据所贡献的方差远远大于异常数据；通过pca得到的排名靠前的主成分解释了原始数据较大的方差占比，所以理论上讲，<strong>第一主成分反映了正常值的方差，最后一个主成分反映了异常点的方差。</strong>通过第一主成分对原始数据进行映射后，原始数据中的正常样本和异常样本的属性不会随之改变。</p>
<p> 存在一个p个维度的数据集orgin_data，X为其协方差矩阵，通过<strong>奇异值分解</strong>可以得到：</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/9.png" alt=""><br> 其中，D为对角阵，其每一个值为X所对应的特征值；P的每一列为X的所对应的特征向量，并将D中的特征值从大到小排列，相应的改变P所对应的列向量。<br> 我们选取top(j)个D中的特征值，及其P所对应的特征向量构成(p,j) 维的矩阵 pj ，再将目标数据集orgin_data进行映射：<code>new_data = orgin_data*pj</code>；new_data是一个 (N,j) 维的矩阵。如果考虑拉回映射的话（也就是从主成分空间映射到原始空间），重构之后的数据集合是:<code>back_data=transpose(pj*transpose(new_data))=new_data*transpose(pj)</code>,是使用 top-j 的主成分进行重构之后形成的数据集，是一个 (N,p) 维的矩阵。</p>
<p> 所以，我们有如下的outlier socres的定义：</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/10.png" alt=""><br> ev(j) subject to.</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/11.png" alt=""></p>
<p> 解释一下上面两个公式，先计算score中orgindata的列减去前j个主成分映射回原空间的newdata下的欧式范数值；再考虑不同主成分所需乘以的权重，这边，我们认为，第一主成分所代表的数据中正常数据更多，所以权重越小；当j取到最后一维的主成分下，我们认为权重最高，达到1。</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/12.png" alt=""><br> 对于outlier socres过高的点，即为异常点。</p>
<h2 id="神经网络识别"><a href="#神经网络识别" class="headerlink" title="神经网络识别"></a>神经网络识别</h2><p> 之前比较火的神经网络分析，同样可以用来做有监督的异常点识别，这边介绍一下Replicator Neural Networks (RNNs)。</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/13.png" alt=""><br> 这边我们通过图像可以看出：<br> 1.输入层中，输入变量个数与输出层中，输出变量一致<br> 2.中间层的节点数小于输入输出层节点<br> 3.整个训练过程是一个先压缩后解压的过程</p>
<p> 常规的，我们通过mse来看模型的误差<br> <img src="/2017/08/09/数据预处理-异常值识别/14.png" alt=""><br> 我们来大致了解一下RNN的运行逻辑，首先，最左边的为输入层即为原始数据，最右层的为输出层即为输出数据。中间各层的激活函数不同，入参经过激活函数所得到的出参的值也不一致，但是在同一层激活函数都是一致的。<br> 对于我们异常识别而言，第二层和第四层 (k=2,4)，激活函数选择为<br> <img src="/2017/08/09/数据预处理-异常值识别/15.png" alt=""></p>
<p> tanh图像如下，可以将原始数据压缩在-1到1之间，使得原始数据有界。<br> <img src="/2017/08/09/数据预处理-异常值识别/16.png" alt=""></p>
<hr>
<p> 对于中间层 (k=3) 而言，激活函数是一个类阶梯 (step-like) 函数。<br> <img src="/2017/08/09/数据预处理-异常值识别/17.png" alt=""><br> 其中，N为阶梯分层数，a3为提升的效率。N的个数越多，层次分的更多。</p>
<p> 比如N=5的形式下：<br> <img src="/2017/08/09/数据预处理-异常值识别/18.png" alt="N=5"></p>
<p> 比如N=3的形式下：</p>
<p> <img src="/2017/08/09/数据预处理-异常值识别/19.png" alt="N=3"><br> 这样做的好处就是，随着N的增加可以将异常点或者异常点群集中在某一个离散阶梯范围内。<br> 通过对RNN的有监督训练，构造异常样本分类器，进行异常值识别。</p>
<h2 id="isolation-forest"><a href="#isolation-forest" class="headerlink" title="isolation forest"></a>isolation forest</h2><p> 2010年南大的周志华教授提出了一个基于二叉树的异常值识别算法，在工业界来说，效果是非常不错的，最近我也做了一个流失用户模型，实测效果优秀。</p>
<p> 和random forest一样，isolation forest是由isolation tree构成，先看一下isolation tree的逻辑：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> method:</span><br><span class="line"> 1.从原始数据中随机选择一个属性feature；</span><br><span class="line"> 2.从原始数据中随机选择该属性的下的一个样本值value；</span><br><span class="line"> 3.根据feature下的value对每条记录进行分类，把小于value的记录放在左子集，把大于等于value的记录放在右子集；</span><br><span class="line"> 4.repeat 1-3 until：</span><br><span class="line"> 　　　　4.1.传入的数据集只有一条记录或者多条一样的记录；</span><br><span class="line"> 　　　　　　　　4.2.树的高度达到了限定高度；</span><br></pre></td></tr></table></figure></p>
<p> 　　　　　　　　大致的思路如下图：<br> 　　　　　　　　<br> 　　　　　　　　<img src="/2017/08/09/数据预处理-异常值识别/20.png" alt=""><br> 　　　　　　　　<br> 　　　　　　　　<br> 　　　　　　　　理论上，异常数据一般都是离群数据，非常容易在早期就被划分到最终子节点。所以，通过计算每个子节点的深度h(x)，来判断数据为异常数据的可能性。论文中，以s(x,n)为判断数据是否异常的衡量指标。<br> 　　　　　　　　<img src="/2017/08/09/数据预处理-异常值识别/21.png" alt=""><br> 　　　　　　　　<br> 　　　　　　　　<img src="/2017/08/09/数据预处理-异常值识别/22.png" alt=""><br> 　　　　　　　　<br> 　　　　　　　　其中，h(x)为x对应的节点深度，c(n)为样本可信度，s(x,n)~[0,1]，正常数据来讲s(x,n)小于0.8，s(x,n)越靠近1，数据异常的可能性越大。<br> 　　　　　　　　<br> 　　　　　　　　单棵树的可信性不足，所以我们通过用emsemble model的思路，去构造一个forest的树群来提高准确性。<br> 　　　　　　　　但是作为isolation forest的时候，需要对原s(x,n)的公式有所更改，通过E(h(x))来替代h(x),其中E(h(x))为数据x在各棵树上的h(x)的平均。<br> 　　　　　　　　同时，<br> 　　　　　　　　1.因为树的个数大大增加，所以需要控制计算的开销，所以每个棵树我们可以采取数据抽样的方式，使得抽样数据集远远小于原始数据集，且根据周志华老师的论文，采样大小超过256效果就提升不大了。<br> 　　　　　　　　2.我们可以控制深度，使得没棵树的最大深度limit length=ceiling(log2(样本大小))，当树深度大于最大深度时，其产生的子节点绝大多数均为正常数据节点，失去异常检验的意义。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据处理 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何校验用户画像的准确性？]]></title>
      <url>/2017/07/20/%E5%A6%82%E4%BD%95%E6%A0%A1%E9%AA%8C%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E7%9A%84%E5%87%86%E7%A1%AE%E6%80%A7%EF%BC%9F/</url>
      <content type="html"><![CDATA[<blockquote>
<p>在用户研究的课题中，用户画像是几乎每个公司都会去做的，浅层的包括统计类的：上月购买量，上周活跃天数等；深层的包括洞察类的：潜在需求偏好，生命周期阶段等；前者的校验简单，后者的校验需要通过一些特别的方式。本文就洞察类画像校验做一系列的梳理。</p>
</blockquote>
<p><img src="/2017/07/20/如何校验用户画像的准确性？/1.png" alt=""></p>
<p>省略掉预处理设计的过程，画像校验的步骤主要集中在画像开发，画像上线，画像更新中，并且三个阶段中，每个阶段的校验方式完全不同</p>
<p><img src="/2017/07/20/如何校验用户画像的准确性？/2.gif" alt=""></p>
<h1 id="用户画像开发中"><a href="#用户画像开发中" class="headerlink" title="用户画像开发中"></a>用户画像开发中</h1><p>当我们所开发的用户画像是类似于用户的下单需求、用户的购车意愿、用户是否有注册意愿这一类存在<strong>历史的正负样本</strong>的有监督的问题，我们可以利用历史确定的数据来校验我们的画像准确性。比如，银行在设计用户征信的画像前，会有一批外部购买的坏样本和好样本，其实画像问题就转化为分类问题去解决评估了。<br><strong>1.1 Recall、Pecision、K-S、F1曲线、Roc曲线、Confusion Matrix、AUC</strong><br>针对这类问题，已经有较为成熟的理论基础，直接利用测试样本判断的准确程度判断画像是否准确</p>
<p><img src="/2017/07/20/如何校验用户画像的准确性？/3.png" alt=""></p>
<p>这张图是一张非常常见也是有效的来总结Recall、Pecision、Lift曲线、Roc曲线、Confusion Matrix的图。<br>FPR = FP/(FP + TN)<br>Recall=TPR=TP/(TP+FN)<br>Precision=TP/(TP+FP)<br>F1曲线:2<code>*</code>Precision<code>*</code>Recall/(Precision+Recall)<br>Roc曲线：TPR vs FPR，也就是Precision vs Recall<br>Auc：area under the roc curve ，也就是roc曲线下面的面积，积分或者投点法均可求解。<br>这边不详细讲细节，需要的可以参考<a href="https://www.zhihu.com/question/30643044" target="_blank" rel="noopener">精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？</a></p>
<p><strong>1.2 交叉验证</strong><br>并不是所有画像都是有监督训练的画像，举个例子，用户的性别画像，是一个无监督的刻画，当你无法通过app端资料填写直接获取到的时候，你只能够通过其他数据特征的对用户进行分群。</p>
<p><img src="/2017/07/20/如何校验用户画像的准确性？/4.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">input :</span><br><span class="line">Data Set：测试数据集</span><br><span class="line"></span><br><span class="line">output :</span><br><span class="line">model：画像模型</span><br><span class="line">label：0(无效)，1（有效）</span><br><span class="line"></span><br><span class="line">methods：</span><br><span class="line">1.从原始数据集中确定画像模型关键features</span><br><span class="line">2.关键features分层，分为train features、test features</span><br><span class="line">3.train featrues训练画像</span><br><span class="line">4.test freatrues校验画像</span><br><span class="line">5.输出值对（model，label）</span><br><span class="line">6.重复2~5</span><br></pre></td></tr></table></figure>
<p>首先，我们在总的数据集中筛选出所有关键影响特征，每次将筛选出的特征分为两块，测试特征训练特征，利用训练特征建立模型，再利用测试特征去判断模型是否合理（比如女鞋用户群的女鞋购买次数小于男性用户群，则次模型异常，删除），最后集成所有合理模型。<br>这样的逻辑中，我们将所有异常不合理的模型全部剔除，训练过程中就校验了用户画像的准确性<br><img src="/2017/07/20/如何校验用户画像的准确性？/5.gif" alt=""></p>
<h1 id="用户画像上线后"><a href="#用户画像上线后" class="headerlink" title="用户画像上线后"></a>用户画像上线后</h1><h2 id="ABTest"><a href="#ABTest" class="headerlink" title="ABTest"></a>ABTest</h2><p><strong>不得不说，abtest是用户画像校验最为直观有效的校验方式。</strong></p>
<p>用户分流模块：<br><img src="/2017/07/20/如何校验用户画像的准确性？/6.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">methods:</span><br><span class="line">1. 全量用户流量为Users，切分流量为三块Users：A1、Users：A2、Users：B，且满足Users：A1+Users：A2=Users：B</span><br><span class="line"></span><br><span class="line">2. 对流量Users：A1、Users：A2不做任何动作</span><br><span class="line">3. 对流量Users：B做相应的模型预测，保存结果</span><br><span class="line">4. 以用户活跃度为例子，选取观察日期周下平均登陆次数y为代价函数，</span><br><span class="line">if y（Users：A1）=y（Users：A2）then</span><br><span class="line">if  y（Users：B）&gt; y（Users：A1+Users：A2）</span><br><span class="line">then 模型有效（差值越大代表准确越高）</span><br><span class="line">else 模型无效（差值越小代表准确越差）</span><br><span class="line">else    模型无效</span><br><span class="line">5.准确程度量化：K=(p*exp(-(dist(y(Users:B),y(Users:A1+Users:A2)))^2/(2*最小容忍度^2)))^(-1)</span><br></pre></td></tr></table></figure>
<p>一句话解释，就是A1=A2保证分配随机，A3好于A1+A2的效果检验画像是否准确？多准确？<br><img src="/2017/07/20/如何校验用户画像的准确性？/7.gif" alt=""></p>
<h1 id="用户画像更新"><a href="#用户画像更新" class="headerlink" title="用户画像更新"></a>用户画像更新</h1><h2 id="用户回访"><a href="#用户回访" class="headerlink" title="用户回访"></a>用户回访</h2><p>在画像刻画完成后，必然会存在画像优化迭代的过程，客服回访是非常常见且有效的方式。<br>比如，我们定义了一波潜在流失用户10万人，随机抽取1000人，进行回访，根据回访结果做文本挖掘，提取关键词，看消极词用户的占比；<br><img src="/2017/07/20/如何校验用户画像的准确性？/8.png" alt="这个图随机找的，别在意"></p>
<h2 id="机制检测"><a href="#机制检测" class="headerlink" title="机制检测"></a>机制检测</h2><p>再比如，我们定义了一波忠诚用户10万人，随机抽取100人，后台随机获取用户安装app的列表，看用户同类app的下载量数目的分布；</p>
<p><img src="/2017/07/20/如何校验用户画像的准确性？/9.png" alt=""><br><em>横轴为用户手机中同类竞品安装量的个数，纵轴为对应的随机抽样的100人中的个数</em><br>人群1分布为忠诚用户画像最准确的，同类app下载量集中在1附近，定义的用户极为准确<br>人群2分布杂乱，人群3分布在下降量异常高的数值附近，定义人群不准确<br><img src="/2017/07/20/如何校验用户画像的准确性？/10.gif" alt=""><br><strong>用户画像是数据运营运营的基础，也是做深度挖掘的一个不可或缺的模块，只有先打好画像基础，确保画像质量，后续的深挖行为才有突破的可能</strong>，最后，谢谢大家阅读。</p>
]]></content>
      
        <categories>
            
            <category> 特征刻画 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 用户画像 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[大数据量下的划分聚类方法]]></title>
      <url>/2017/07/19/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%8F%E4%B8%8B%E7%9A%84%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>在常规聚类案例中，数据一般都是以iris集或者不足GB级的数据作为测试案例，实际商业运用中，数据量级要远远大于这些。比如滴滴出行15年日均单量就达到1000万单，出行轨迹的数据存储达到上百TB，常规的k均值聚类，二分聚类等无法完成如此量级的数据聚类，这边就提供一个以CLARANS为基础的算法思路。</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/1.png" alt=""></p>
<p>什么是聚类?<br>定义是这样的，把一个数据对象，划分成子集的过程，使得子集内相似度大，子集外相似度小。这样的一个过程叫做聚类。</p>
<p>大学课程老师以一个公式概括过这样的过程：<code>max(子集内相似度/子集间相似度)</code>，我觉得也很形象便于理解。</p>
<p>什么是划分聚类？<br>聚类方法有很多种，包括基于划分、基于密度、基于网格、基于层次、基于模型等等，这边主要介绍基于划分的聚类方法，剩余的方法会在后续的文章中持续更新[如果不鸽的话]。划分聚类一般是：<strong>采取互斥族（子集）划分</strong>，说的更直白一点就是<strong>每个点属于且仅属于一个族（子集）</strong>。</p>
<p>常见的划分聚类有哪些？<br>k均值划分：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">input：</span><br><span class="line">- k：族的个数</span><br><span class="line">- D：输入数据集合</span><br><span class="line">output：</span><br><span class="line">k个族（子集）的数据集合</span><br><span class="line">methods：</span><br><span class="line">1.在D中任选（常用的包库中都是这样做，但是建议自己写的同学以密度先分块，在密度块中任选）k个对象作为初始中心</span><br><span class="line">2.计算剩余对象到k对象的聚类，聚类远近分配到对应的族</span><br><span class="line">3.更新族均值作为新的族中心</span><br><span class="line">4.重复2-4直到中心不变化</span><br></pre></td></tr></table></figure></p>
<p>如图过程：</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/2.png" alt="kmeans"></p>
<p>以上为最简单的k均值，很容易看出，它存在几个问题，首先计算量非常的大，假设有m条数据，k个中心点，那距离计算的次数就是<code>o(mkt)=k*(m-k)*迭代次数t</code>，重复t次直到收敛的过程是非常大的计算过程；再而，如果数据均为‘男、女’，‘高、中、低’等，那距离定义就是非常不合理的，此外，初始k难确定，非凸数据，离群点等等都存在问题</p>
<p>围绕中心划分（PAM）：<br>刚才说到了异常点会影响k均值，那么我们看看为什么？<br>假设与点1、2、3、8、9、10、25，一眼就知道(1、2、3)，(8，9，10)为族，但如果由k均值的话，以k=2为例，((1，2，3)，(8，9，10，25))，mse=196；<br>((1，2，3，8)，(9，10，25))，mse=189.7；它重复以mse为损失函数，而不去考虑数据是否合理，所以针对的，我们有<strong>绝对误差标准</strong>：<br><img src="/2017/07/19/大数据量下的划分聚类方法/3.png" alt="绝对误差标准"></p>
<p>这样做，通过全局距离最小化，可以一定程度上避免异常点的问题，但是，思考一下计算量是什么？是o(n**2)，这意味着对数据量大的问题，这就是一个典型的NP问题（一定有解，但是不一定在有限时间资源内可以被解出来）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">input:</span><br><span class="line">- k：族的个数</span><br><span class="line">- D：输入数据集合</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">k个族（子集）的数据集合</span><br><span class="line"></span><br><span class="line">methods:</span><br><span class="line">1.D中任选k个对象最为初始种子</span><br><span class="line">2.仿照k均值分配剩余对象</span><br><span class="line">3.随机选取非种子对象O</span><br><span class="line">4.计算若是以O为中心下的总损失函数代价S=原始种子下的绝对误差E-新的对象O下的绝对误差E</span><br><span class="line">5.如果S&gt;0,则以新对象O替换旧的种子对象，否则不变化</span><br><span class="line">6.重复2-5，直到收敛</span><br></pre></td></tr></table></figure></p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/4.png" alt="绝对误差标准"><br>我们看这个图好理解一点，就是存在族（集合）中任一点p，当前的初始种子为Q1，随机选取剩余其他对象为族中心Qrandom1，计算PQ1的距离与PQrandom1的距离，图中dist(PQ1)<dist(pqrandom1)的距离，所以p仍属于q1为中心的族，若存在族中心qrandom2，使得dist(pq1)>dist(PQrandom2)，则更新族中心为Qrandom2,此时绝对误差E会变化，计算是否降低了绝对误差E以确定是否更好族中心。<br>如何解决大数据量下的聚类问题？<br>其实看了以上两个算法，大同小异，但是都不可避免有一个弱点，就是计算量上都是随着初始数据量的增大而几何增长的，所以这边需要对数据量进行控制。</dist(pqrandom1)的距离，所以p仍属于q1为中心的族，若存在族中心qrandom2，使得dist(pq1)></p>
<p>大家回想一下，同样的对数据量进行控制的算法有哪些给我们有启发？<br><a href="http://www.jianshu.com/writer#/notebooks/14156301/notes/14303651" target="_blank" rel="noopener">数据平衡算法</a><br>这种方法好像可以减少数据量，哪有没有历史成功案例支持呢？<br><a href="http://www.jianshu.com/writer#/notebooks/13640327/notes/13715869" target="_blank" rel="noopener">基于决策树引申出的集成算法</a><br>貌似存在一个叫做adaboost、randomforest这类的算法，好像就用了<strong>数据平衡的算法</strong>。</p>
<p>那么，我们是否可以用在聚类里面呢？答案是可以的，我们现在看一个由上述思路得到的CLARANS算法，实际开发中，我们team对其进行了优化，内部称之为’CLARANS+’<br>在理解CLARANS+之前，我们先理解CLARA:</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/5.png" alt=""></p>
<p>从这张图上，我们可以很清晰的看出，CLARA首先通过类似randomforest里面的随机抽样的方法，将原始数据集随机抽样成若干个子数据集sample data，理论上采样的子集分布应该与原分布近似，所以样本中心点必然与原分布中心近似。</p>
<hr>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/6.png" alt="确定中心"></p>
<p>在数据量较少的子集上，我们可以重复确定每个子集的中心Medoid，<strong>这边计算中心的方法有很多，包括上述讲到的K均值，PAM，也可以参考相似度比如常见的余弦相似，likelihood rate，高斯核相似等等</strong></p>
<p>最后采取随机抽取，或者投票加权等方法确定原始样本的中心即可。</p>
<p>CLARA的有效性依赖于样本的大小，分布及质量，所以该算法一定程度上会依赖于初始抽样的质量。除此之外，每一个随机样本的计算负责度为O（ks*s+k（n-k）），s为样本的大小，k为族数，n为总对象数，若抽取样本子集过少，其简化计算的程度也越低。</p>
<p>说到这里，CLARA的算法是确定了中心后不在改变，这就有一定的运气成分，假设确定的k个钟均离最佳中心很远的情况下，CLARA最后无论如何去选已知中心，都得不到最优秀的聚类中心。</p>
<p>所以，<strong>我们来看看可以提高CLARA的聚类质量及可伸缩性的CLARANS算法</strong></p>
<p>上述思路不变，但在CLARA确定中心之后，我们新增了一步，就是按照PAM中的方法一样，我们在子集上选取一个与当前中心x(Medoid)不一样的对象y(New Medoid)，计算用y(New Medoid)替换x(Medoid)后绝对误差是否下降，下降则替换否则不变，重复l次之后，我们可以认为此时的中心点为局部中心最优解；整体数据集所有子集均重复m次后，得出的中心点为全局局部最优解。如下图：</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/7.png" alt="新增New Medoid"></p>
<p>实际上，我们可以做的还很多<br>理论上讲，以上的算法结果已经尽可能的保证了数据的合理压缩，压缩后的数据集内的中心点足够鲁棒，但是实际运用过程中，我们没有尽可能的考虑到开头说的那句：<br><strong>什么是聚类?</strong><br>定义是这样的，把一个数据对象，划分成子集的过程，使得子集内相似度大，子集外相似度小。这样的一个过程叫做聚类。</p>
<p>所以，我们尝试性的做了CLARANS+，我们把CLARANS里面确定出来的每个sample data子集里面最优秀的top k个New Medoids映射回同一个空间：<br>以绿色和天蓝色数据集为例子：</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/8.png" alt="CLARANS"><br>橘色方框内为CLARANS最后确定中心后做的随机或者加权投票后采纳的被橘黄色框框住的天蓝色数据与绿色数据的中心点，很明显可以看出，这样导致的结果违背了“子集外相似度最小的原则”。</p>
<p>我们，仿照Lasso对应lambda.1se的方式，考虑除了最优点外，在其可接受的范围附近，认为他们同样属于最优点，也就是top k个New Medoids重新选择距离最远的点作为最优中心，也就是如下图中的紫色方框中的点：</p>
<p><img src="/2017/07/19/大数据量下的划分聚类方法/9.png" alt="最远距离点"></p>
<p>通过实际的业务测试，我们建议top k个点中的个默认为2-3比较好（数据分布差异大选择2，否则选择3）,如果不能确定，就默认为3。</p>
<p>以上理论方法就解释了如何在大量数据量下，简单快速的寻找到最优中心点的过程，谢谢大家。</p>
<p>参考文献：<br>[1] Jiawei Han.[数据挖掘概念与技术]2001，8<br>[2] 毛国君等.数据挖掘原理与算法[M].北京:清华大学出版社,2005.<br>[3] <a href="http://www.jianshu.com/writer#/notebooks/14156301/notes/14303651" target="_blank" rel="noopener">数据平衡算法</a><br>[4] <a href="http://www.jianshu.com/writer#/notebooks/13640327/notes/13715869" target="_blank" rel="noopener">基于决策树引申出的集成算法</a><br>[5] <a href="http://scikit-learn.org/stable/modules/clustering.html#clustering" target="_blank" rel="noopener">http://scikit-learn.org/stable/modules/clustering.html#clustering</a><br>[6] <a href="https://en.wikipedia.org/wiki/Clustering" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Clustering</a></p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 理论解析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据抽样平衡方法重写]]></title>
      <url>/2017/07/13/%E6%95%B0%E6%8D%AE%E6%8A%BD%E6%A0%B7%E5%B9%B3%E8%A1%A1%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99/</url>
      <content type="html"><![CDATA[<p>之前在R里面可以通过调用Rose这个package调用数据平衡函数，这边用python改写了一下，也算是自我学习了。</p>
<p>R：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定工作目录</span></span><br><span class="line">setwd(path)</span><br><span class="line"><span class="comment"># 安装包</span></span><br><span class="line">install.packages(<span class="string">"ROSE"</span>)</span><br><span class="line"><span class="keyword">library</span>(ROSE)</span><br><span class="line"><span class="comment"># 检查数据</span></span><br><span class="line">data(hacide)</span><br><span class="line">table(hacide.train$cls)</span><br><span class="line"><span class="number">0</span>     <span class="number">1</span></span><br><span class="line"><span class="number">980</span>    <span class="number">20</span></span><br></pre></td></tr></table></figure></p>
<hr>
<p>过抽样实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_balanced_over &lt;- ovun.sample(cls ~ ., data = hacide.train, method = &quot;over&quot;,N = 1960)$data</span><br><span class="line">table(data_balanced_over$cls)</span><br><span class="line">0    1</span><br><span class="line">980 980</span><br></pre></td></tr></table></figure></p>
<p><strong>这边需要注意是<code>ovun</code>不是<code>over</code></strong></p>
<hr>
<p>欠采样实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_balanced_under &lt;- ovun.sample(cls ~ ., data = hacide.train, method = &quot;under&quot;, N = 40, seed = 1)$data</span><br><span class="line">table(data_balanced_under$cls)</span><br><span class="line">0    1</span><br><span class="line">20  20</span><br></pre></td></tr></table></figure></p>
<p>这边需要注意的是欠采样是不放回采样，同时对数据信息的损失也是极大的</p>
<hr>
<p>组合采样实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_balanced_both &lt;- ovun.sample(cls ~ ., data = hacide.train, method = &quot;both&quot;, p=0.5, N=1000, seed = 1)$data</span><br><span class="line">table(data_balanced_both$cls)</span><br><span class="line">0    1</span><br><span class="line">520 480</span><br></pre></td></tr></table></figure></p>
<p><code>method</code>的不同值代表着不同的采样方法，p这边是控制正类的占比，seed保证抽取样本的固定，也就是种子值。</p>
<hr>
<hr>
<p>在python上，我也没有发现有现成的package可以import，所以就参考了R的实现逻辑重写了一遍，新增了一个分层抽样<code>group_sample</code>,删除了过采样，重写了组合抽样<code>combine_sample</code>,欠抽样<code>under_sample</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math <span class="keyword">as</span> ma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sample_s</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">''''this is my pleasure'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_sample</span><span class="params">(self, data_set, label, percent=<span class="number">0.1</span>)</span>:</span></span><br><span class="line"><span class="comment"># 分层抽样</span></span><br><span class="line"><span class="comment"># data_set:数据集</span></span><br><span class="line"><span class="comment"># label:分层变量</span></span><br><span class="line"><span class="comment"># percent:抽样占比</span></span><br><span class="line"><span class="comment"># q:每次抽取是否随机,null为随机</span></span><br><span class="line"><span class="comment"># 抽样根据目标列分层，自动将样本数较多的样本分层按percent抽样，得到目标列样本较多的特征欠抽样数据</span></span><br><span class="line">x = data_set</span><br><span class="line">y = label</span><br><span class="line">z = percent</span><br><span class="line">diff_case = pd.DataFrame(x[y]).drop_duplicates([y])</span><br><span class="line">result = []</span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(diff_case)):</span><br><span class="line">k = np.array(diff_case)[i]</span><br><span class="line">data_set = x[x[y] == k[<span class="number">0</span>]]</span><br><span class="line">nrow_nb = data_set.iloc[:, <span class="number">0</span>].count()</span><br><span class="line">data_set.index = range(nrow_nb)</span><br><span class="line">index_id = rd.sample(range(nrow_nb), int(nrow_nb * z))</span><br><span class="line">result = pd.concat([result, data_set.iloc[index_id, :]], axis=<span class="number">0</span>)</span><br><span class="line">new_data = pd.Series(result[<span class="string">'label'</span>]).value_counts()</span><br><span class="line">new_data = pd.DataFrame(new_data)</span><br><span class="line">new_data.columns = [<span class="string">'cnt'</span>]</span><br><span class="line">k1 = pd.DataFrame(new_data.index)</span><br><span class="line">k2 = new_data[<span class="string">'cnt'</span>]</span><br><span class="line">new_data = pd.concat([k1, k2], axis=<span class="number">1</span>)</span><br><span class="line">new_data.columns = [<span class="string">'id'</span>, <span class="string">'cnt'</span>]</span><br><span class="line">max_cnt = max(new_data[<span class="string">'cnt'</span>])</span><br><span class="line">k3 = new_data[new_data[<span class="string">'cnt'</span>] == max_cnt][<span class="string">'id'</span>]</span><br><span class="line">result = result[result[y] == k3[<span class="number">0</span>]]</span><br><span class="line"><span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">under_sample</span><span class="params">(self, data_set, label, percent=<span class="number">0.1</span>, q=<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="comment"># 欠抽样</span></span><br><span class="line"><span class="comment"># data_set:数据集</span></span><br><span class="line"><span class="comment"># label:抽样标签</span></span><br><span class="line"><span class="comment"># percent:抽样占比</span></span><br><span class="line"><span class="comment"># q:每次抽取是否随机</span></span><br><span class="line"><span class="comment"># 抽样根据目标列分层，自动将样本数较多的样本按percent抽样，得到目标列样本较多特征的欠抽样数据</span></span><br><span class="line">x = data_set</span><br><span class="line">y = label</span><br><span class="line">z = percent</span><br><span class="line">diff_case = pd.DataFrame(pd.Series(x[y]).value_counts())</span><br><span class="line">diff_case.columns = [<span class="string">'cnt'</span>]</span><br><span class="line">k1 = pd.DataFrame(diff_case.index)</span><br><span class="line">k2 = diff_case[<span class="string">'cnt'</span>]</span><br><span class="line">diff_case = pd.concat([k1, k2], axis=<span class="number">1</span>)</span><br><span class="line">diff_case.columns = [<span class="string">'id'</span>, <span class="string">'cnt'</span>]</span><br><span class="line">max_cnt = max(diff_case[<span class="string">'cnt'</span>])</span><br><span class="line">k3 = diff_case[diff_case[<span class="string">'cnt'</span>] == max_cnt][<span class="string">'id'</span>]</span><br><span class="line">new_data = x[x[y] == k3[<span class="number">0</span>]].sample(frac=z, random_state=q, axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> new_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_sample</span><span class="params">(self, data_set, label, number, percent=<span class="number">0.35</span>, q=<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="comment"># 组合抽样</span></span><br><span class="line"><span class="comment"># data_set:数据集</span></span><br><span class="line"><span class="comment"># label:目标列</span></span><br><span class="line"><span class="comment"># number:计划抽取多类及少类样本和</span></span><br><span class="line"><span class="comment"># percent：少类样本占比</span></span><br><span class="line"><span class="comment"># q:每次抽取是否随机</span></span><br><span class="line"><span class="comment"># 设定总的期待样本数量，及少类样本占比，采取多类样本欠抽样，少类样本过抽样的组合形式</span></span><br><span class="line">x = data_set</span><br><span class="line">y = label</span><br><span class="line">n = number</span><br><span class="line">p = percent</span><br><span class="line">diff_case = pd.DataFrame(pd.Series(x[y]).value_counts())</span><br><span class="line">diff_case.columns = [<span class="string">'cnt'</span>]</span><br><span class="line">k1 = pd.DataFrame(diff_case.index)</span><br><span class="line">k2 = diff_case[<span class="string">'cnt'</span>]</span><br><span class="line">diff_case = pd.concat([k1, k2], axis=<span class="number">1</span>)</span><br><span class="line">diff_case.columns = [<span class="string">'id'</span>, <span class="string">'cnt'</span>]</span><br><span class="line">max_cnt = max(diff_case[<span class="string">'cnt'</span>])</span><br><span class="line">k3 = diff_case[diff_case[<span class="string">'cnt'</span>] == max_cnt][<span class="string">'id'</span>]</span><br><span class="line">k4 = diff_case[diff_case[<span class="string">'cnt'</span>] != max_cnt][<span class="string">'id'</span>]</span><br><span class="line">n1 = p * n</span><br><span class="line">n2 = n - n1</span><br><span class="line">fre1 = n2 / float(x[x[y] == k3[<span class="number">0</span>]][<span class="string">'label'</span>].count())</span><br><span class="line">fre2 = n1 / float(x[x[y] == k4[<span class="number">1</span>]][<span class="string">'label'</span>].count())</span><br><span class="line">fre3 = ma.modf(fre2)</span><br><span class="line">new_data1 = x[x[y] == k3[<span class="number">0</span>]].sample(frac=fre1, random_state=q, axis=<span class="number">0</span>)</span><br><span class="line">new_data2 = x[x[y] == k4[<span class="number">1</span>]].sample(frac=fre3[<span class="number">0</span>], random_state=q, axis=<span class="number">0</span>)</span><br><span class="line">test_data = pd.DataFrame([])</span><br><span class="line"><span class="keyword">if</span> int(fre3[<span class="number">1</span>]) &gt; <span class="number">0</span>:</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; (int(fre3[<span class="number">1</span>])):</span><br><span class="line">data = x[x[y] == k4[<span class="number">1</span>]]</span><br><span class="line">test_data = pd.concat([test_data, data], axis=<span class="number">0</span>)</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">result = pd.concat([new_data1, new_data2, test_data], axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p>
<p>后续使用，只需要复制上述code，存成<code>.py</code>的文件，后续使用的时候：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载函数</span></span><br><span class="line"><span class="keyword">import</span> sample_s <span class="keyword">as</span> sa</span><br><span class="line"><span class="comment">#这边可以选择你需要的分层抽样、欠抽样、组合抽样的函数</span></span><br><span class="line">sample = sa.group_sample()</span><br><span class="line"><span class="comment">#直接调用函数即可</span></span><br><span class="line">new_data3 = sample.combine_sample(data_train, <span class="string">'label'</span>, <span class="number">60000</span>, <span class="number">0.4</span>)</span><br><span class="line"><span class="comment">#将data_train里面的label保持正样本（少类样本）达到0.4的占比下，总数抽取到60000个样本</span></span><br></pre></td></tr></table></figure></p>
<p>其实不是很难的一个过程，只是强化自己对python及R语言的书写方式的记忆，谢谢。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据平衡 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[订单需求预估]]></title>
      <url>/2017/07/12/%E8%AE%A2%E5%8D%95%E9%9C%80%E6%B1%82%E9%A2%84%E4%BC%B0/</url>
      <content type="html"><![CDATA[<p>之前写了一篇以基于elastic的需求预估的文章，只不过用的是R语言开发的，最近在学python，就仿照逻辑写了一篇python的，主要修改点如下：</p>
<ul>
<li>用决策树替换了elastic算法</li>
<li>用分层抽样替换了组合抽样</li>
</ul>
<p><strong>需要看详细理论及思考过程参考链接：<a href="http://www.jianshu.com/p/e932b9744da6" target="_blank" rel="noopener">商品需求预估</a></strong></p>
<p>python code如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import random as rd</span><br><span class="line">from sklearn import tree</span><br><span class="line"></span><br><span class="line"># 读取数据</span><br><span class="line">data_orgin = pd.read_table(&quot;C:/Users/17031877/Desktop/supermarket_second_hair_washing_train.txt&quot;)</span><br><span class="line">data_deal_1 = data_orgin.drop([&apos;aimed_date&apos;, &apos;member_id&apos;, &apos;age&apos;, &apos;gender&apos;, &apos;diff_rgst&apos;], axis=1)</span><br></pre></td></tr></table></figure></p>
<p>这边是常规的数据读取，删除了不必要的列</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#因变量单列</span><br><span class="line">label = data_deal_1[&apos;label&apos;]</span><br><span class="line"></span><br><span class="line"># 用户分量级</span><br><span class="line">value00 = [&apos;max_date_diff&apos;, &apos;aimed_max_date_diff&apos;]</span><br><span class="line">data00 = data_deal_1[value00]</span><br><span class="line"></span><br><span class="line">value01 = [&apos;max_pay&apos;, &apos;per_pay&apos;, &apos;six_month_max_pay&apos;, &apos;six_month_per_pay&apos;, &apos;three_month_max_pay&apos;, &apos;three_month_per_pay&apos;,</span><br><span class="line">&apos;one_month_max_pay&apos;, &apos;one_month_per_pay&apos;, &apos;fifteen_day_max_pay&apos;, &apos;fifteen_day_per_pay&apos;, &apos;aimed_max_pay&apos;,</span><br><span class="line">&apos;aimed_per_pay&apos;, &apos;aimed_six_month_max_pay&apos;, &apos;aimed_six_month_per_pay&apos;, &apos;aimed_three_month_max_pay&apos;,</span><br><span class="line">&apos;aimed_three_month_per_pay&apos;, &apos;aimed_one_month_max_pay&apos;, &apos;aimed_one_month_per_pay&apos;,</span><br><span class="line">&apos;aimed_fifteen_day_max_pay&apos;, &apos;aimed_fifteen_day_per_pay&apos;, &apos;qty_drtn_seven&apos;, &apos;qty_drtn_fourteen&apos;]</span><br><span class="line">data01 = data_deal_1[value01]</span><br><span class="line"></span><br><span class="line">value02 = [&apos;cnt_time&apos;, &apos;six_month_cnt_time&apos;, &apos;three_month_cnt_time&apos;, &apos;one_month_cnt_time&apos;, &apos;fifteen_day_cnt_time&apos;,</span><br><span class="line">&apos;aimed_cnt_time&apos;, &apos;aimed_six_month_cnt_time&apos;, &apos;aimed_three_month_cnt_time&apos;, &apos;aimed_one_month_cnt_time&apos;,</span><br><span class="line">&apos;aimed_fifteen_day_cnt_time&apos;, &apos;pv_times_seven&apos;, &apos;pv_times_fourteen&apos;, &apos;search_times_seven&apos;,</span><br><span class="line">&apos;search_times_fourteen&apos;, &apos;clc_times_seven&apos;, &apos;clc_times_fourteen&apos;, &apos;cart2_times_seven&apos;,</span><br><span class="line">&apos;cart2_times_fourteen&apos;, &apos;cart1_times_seven&apos;, &apos;cart1_times_fourteen&apos;, &apos;unpay_times_seven&apos;,</span><br><span class="line">&apos;unpay_times_fourteen&apos;]</span><br><span class="line">data02 = data_deal_1[value02]</span><br><span class="line"></span><br><span class="line">value03 = [&apos;pv_visit_last_period&apos;, &apos;search_last_period&apos;, &apos;clc_last_period&apos;, &apos;cart2_last_period&apos;, &apos;cart1_last_period&apos;,</span><br><span class="line">&apos;unpay_last_period&apos;]</span><br><span class="line">data03 = data_deal_1[value03]</span><br></pre></td></tr></table></figure>
<p>因为不同量级的数据之后做异常点处理的时候截断位置不同，所有需要分割数据处理</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def test_function_one(x, l):</span><br><span class="line">k = x.dropna(how=&apos;any&apos;)</span><br><span class="line">y = k.quantile(l)</span><br><span class="line">z = k.max()</span><br><span class="line">x[x &gt; y] = y</span><br><span class="line">x = x.fillna(value=z)</span><br><span class="line">return x</span><br><span class="line">for i in range(len(data00.columns)):</span><br><span class="line">data00.iloc[:, i] = test_function_one(data00.iloc[:, i], 0.98)</span><br><span class="line"></span><br><span class="line">def test_function_two(x, l):</span><br><span class="line">k = x.dropna(how=&apos;any&apos;)</span><br><span class="line">y = k.quantile(l)</span><br><span class="line">z = 0</span><br><span class="line">x[x &gt; y] = y</span><br><span class="line">x = x.fillna(value=z)</span><br><span class="line">return x</span><br><span class="line">for i in range(len(data01.columns)):</span><br><span class="line">data01.iloc[:, i] = test_function_two(data01.iloc[:, i], 0.95)</span><br><span class="line">for i in range(len(data02.columns)):</span><br><span class="line">data02.iloc[:, i] = test_function_two(data02.iloc[:, i], 0.99)</span><br><span class="line"></span><br><span class="line">def test_function_three(x):</span><br><span class="line">z = 14</span><br><span class="line">x[x &gt; z] = z</span><br><span class="line">x = x.fillna(value=z)</span><br><span class="line">return x</span><br><span class="line">for i in range(len(data03.columns)):</span><br><span class="line">data03.iloc[:, i] = test_function_three(data03.iloc[:, i])</span><br><span class="line"># 数据合并</span><br><span class="line">data_train = pd.concat([label, data00, data01, data02, data03], axis=1)</span><br></pre></td></tr></table></figure>
<p>根据数据量的不同做数据分割，跑上面写完的code函数就可以</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#数量级对比</span><br><span class="line">zero_case = data_train[data_train[&apos;label&apos;] == 0][&apos;label&apos;].count()</span><br><span class="line">print &apos;负样本数：%d&apos; % zero_case</span><br><span class="line">one_case = data_train[data_train[&apos;label&apos;] == 1][&apos;label&apos;].count()</span><br><span class="line">print &apos;正样本数: %d&apos; % (one_case)</span><br><span class="line"></span><br><span class="line">负样本数：292936</span><br><span class="line">正样本数: 3973</span><br><span class="line">Backend TkAgg is interactive backend. Turning interactive mode on.</span><br></pre></td></tr></table></figure>
<p>实际看下来，正负样本的差异的确还是很大，这个其实做多了就有经验，常规的来看，潜在的浏览、搜索到最后的成单，普遍自然转化不到1%，也正是这么低的转化，才需要一些算法来做信息抓去。</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def case_sample(x, y, z):</span><br><span class="line">diff_case = pd.DataFrame(x[y]).drop_duplicates([y])</span><br><span class="line">result = []</span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line">for i in range(len(diff_case)):</span><br><span class="line">k = np.array(diff_case)[i]</span><br><span class="line">data_set = x[x[y] == k[0]]</span><br><span class="line">nrow_nb = data_set.iloc[:, 0].count()</span><br><span class="line">data_set.index = range(nrow_nb)</span><br><span class="line">index_id = rd.sample(range(nrow_nb), int(nrow_nb * z))</span><br><span class="line">result = pd.concat([result, data_set.iloc[index_id, :]], axis=0)</span><br><span class="line">return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">zero_case = data_train[data_train[&apos;label&apos;] == 0]</span><br><span class="line">one_case = data_train[data_train[&apos;label&apos;] == 1]</span><br><span class="line"># 开始分层抽样</span><br><span class="line">new_zero_case = case_sample(zero_case, &apos;unpay_last_period&apos;, 0.1)</span><br><span class="line"># 新数量级对比</span><br><span class="line">new_zero_case_count = new_zero_case[new_zero_case[&apos;label&apos;] == 0][&apos;label&apos;].count()</span><br><span class="line"># 数据集合并</span><br><span class="line">new_data_train = pd.concat([new_zero_case, one_case], axis=0)</span><br></pre></td></tr></table></figure>
<p><code>case_sample</code>是一个简单的分层抽样的小函数，<code>x</code>是数据集，<code>y</code>是分层变量，<code>z</code>是抽样占比；新的样本<code>new_data_train</code>中正负样本比例在1:10左右，这边的样本比是我自己设置的，不一定是最合理的；且此处也不一定要求一定用分层抽样，只是我用来练练手的；推荐还是遵从奥卡姆原理，在未知的情况下，尽可能简单的解决问题，比如组合抽样就是很不错的方法。</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#函数设置</span><br><span class="line">clf = tree.DecisionTreeRegressor(criterion=&apos;mse&apos;, max_features=&apos;log2&apos;, random_state=1)</span><br><span class="line"></span><br><span class="line">#函数拟合</span><br><span class="line">y = new_data_train[&apos;label&apos;]</span><br><span class="line">x = new_data_train.drop(&apos;label&apos;, 1)</span><br><span class="line">clf.fit(x, y)</span><br><span class="line"></span><br><span class="line">#数据预测</span><br><span class="line">y_predict = clf.predict(x)</span><br><span class="line"></span><br><span class="line"># 结果对比</span><br><span class="line">y.index = range(len(y))</span><br><span class="line">combined_date = pd.concat([y, pd.DataFrame(y_predict)], axis=1)</span><br><span class="line">combined_date.columns = [&apos;actual&apos;, &apos;predict&apos;]</span><br></pre></td></tr></table></figure>
<p>这边稍微讲解一下，我认为的<code>sklearn</code>中<code>DecisionTreeRegressor</code>中比较终于的参数设置，<code>criterion</code>这边为模型优化的标准，常规的有<code>mse</code>和<code>mae</code>，建议在数据量差异不大的时候多考虑<code>mse</code>；<code>max_features</code>是每次训练用的特征个数，综合特征量级考虑，一般有<code>log2</code>，<code>sqrt</code>，尽可能是抽取比例在70%；<code>max_depth</code>刚开始可以默认，第一类模型出来后，可在结果附近迭代，寻找<code>out of bag</code>最小的error下的值；<strong>另外，我没有发现有weight设置，可能是我不熟悉，但是如果sklearn这边不提供weight的化，我们在做数据预处理的时候一定要平衡数据，不然当数据集过偏的时候最后的结果会以“牺牲”少类的判断正确率去完善整体正确率。</strong></p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># case 1</span><br><span class="line">x = []</span><br><span class="line">y = []</span><br><span class="line">for i in range(1, 10):</span><br><span class="line">test_data = combined_date</span><br><span class="line">i = i / float(10)</span><br><span class="line">for j in range(combined_date[&apos;actual&apos;].count()):</span><br><span class="line">if test_data.iloc[j, 1] &gt; i:</span><br><span class="line">test_data.iloc[j, 1] = 1</span><br><span class="line">else:</span><br><span class="line">continue</span><br><span class="line">z = test_data[test_data[&apos;actual&apos;] == test_data[&apos;predict&apos;]][&apos;actual&apos;].count() / float(test_data[&apos;actual&apos;].count())</span><br><span class="line">x.append(i)</span><br><span class="line">y.append(z)</span><br></pre></td></tr></table></figure>
<p>这边写了检查函数，检查了分别0.1~1，以0.1为间隔的情况下的分割点，每个分割点下<strong>预测正确的数量/所有统计的样本数</strong>，也就是下面的<strong>accuracy</strong>.<br><img src="/2017/07/12/订单需求预估/1.png" alt=""></p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># case 2</span><br><span class="line">test_data = combined_date</span><br><span class="line">aimed_data = test_data[test_data[&apos;predict&apos;]&gt;0]</span><br><span class="line">k1=aimed_data[aimed_data[&apos;actual&apos;]==1][&apos;predict&apos;].count()</span><br><span class="line">k2=float(aimed_data[&apos;predict&apos;].count())</span><br><span class="line"></span><br><span class="line">print &apos;所有预测可能下单用户中真实下单用户数：%d&apos; %(k1)</span><br><span class="line">print &apos;所有预测可能下单用户数：%d&apos; %(k2)</span><br></pre></td></tr></table></figure>
<p>因为这边需要对用户营销，所以更关系topN的转化率，需要看一下实际正样本被覆盖了多数，以上即为code，这边的效果值为98.7%，还是比较高的，但是应该是过拟合了，所有一般不建议单纯使用决策树模型</p>
<hr>
<p>所有的python code到这里就结束了，后续我做项目的同时会同时更新R及python两种code的思考，和大家讨论分享学习，谢谢。</p>
<p><em>参考文献：</em></p>
<ul>
<li><em><a href="http://scikitlearn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" target="_blank" rel="noopener">sklearn.tree.DecisionTreeRegressor</a></em></li>
<li><em><a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">维基百科中对ROC的介绍</a></em></li>
<li><em><a href="https://www.zhihu.com/question/27205203/answer/148900663" target="_blank" rel="noopener">决策树常见问题及面试关键点介绍</a></em></li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 预测 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[推荐系统-威尔逊区间法]]></title>
      <url>/2017/06/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%A8%81%E5%B0%94%E9%80%8A%E5%8C%BA%E9%97%B4%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>我推荐一种之前在惠普做过一种排序方法：威尔逊区间法</p>
<p>我们先做如下设定：</p>
<p>（1）每个用户的打分都是独立事件。</p>
<p>（2）用户只有两个选择，要么投喜欢’1’，要么投不喜欢’0’。</p>
<p>（3）如果总人数为n，其中喜欢的为k，那么喜欢的比例p就等于k/n。</p>
<p>这是一种统计分布，叫做”二项分布”（binomial distribution）</p>
<p>理论上讲，p越大应该越好，但是n的不同，导致p的可信性有差异。100个人投票，50个人投喜欢；10个人投票，6个人喜欢，我们不能说后者比前者要好。</p>
<p>所以这边同时要考虑（p，n）</p>
<p>刚才说满足二项分布，这里p可以看作”二项分布”中某个事件的发生概率，因此我们可以计算出p的置信区间。</p>
<p>所谓”置信区间”，就是说，以某个概率而言，p会落在的那个区间。</p>
<p>置信区间展现的是这个参数的真实值有一定概率落在测量结果的周围的程度。置信区间给出的是被测量参数的测量值的可信程度，即前面所要求的“一个概率”，也就是结论的可信程度。</p>
<p>二项分布的置信区间有多种计算公式，最常见的是”正态区间”（Normal approximation interval）。但是，它只适用于样本较多的情况（np &gt; 5 且 n(1 − p) &gt; 5），对于小样本，它的准确性很差。</p>
<p>这边，我推荐用t检验来衡量小样本的数据，可以解决数据过少准确率不高的问题。</p>
<p>这样一来，排名算法就比较清晰了：</p>
<p>第一步，计算每个case的p（好评率）。</p>
<p>第二步，计算每个”好评率”的置信区间（参考z Test或者t Test，以95%的概率来处理）。</p>
<p>第三步，根据置信区间的下限值，进行排名。这个值越大，排名就越高。</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/1.png" alt=""></p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/2.png" alt=""></p>
<p>解释一下，n为评价数，p为好评率，z为对应检验对应概率区间下的统计量</p>
<p>比如t-分布：</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/3.png" alt=""></p>
<p>可以看到，当n的值足够大时，这个下限值会趋向p，如果n非常小，这个下限值会大大小于p，更加符合实际。</p>
<p>Reddit的评论排名，目前就使用这个算法。国内的化，滴滴也有部分业务涉及，效果也不错。</p>
<hr>
<p>更新一下，没想到这个话题还是有高达9个人关注，所以这边我再说一些更细化的过程吧</p>
<p>在计算排名的时候，我们通常会考虑三个事情</p>
<p>1.上文讲到的，次数+好评率的分布，次数越多好评率越可靠，好评率越高该项越值得推荐</p>
<p>2.时间因素，如果一个项目是10天前推送的，一个项目是昨天推送的，很明显前者的次数远大于后者</p>
<p>3.影响权重，你这边只考虑了喜欢和不喜欢，其实所有的排序不可能只以1个维度考虑，通常会考虑多个维度，比如浏览次数，搜索次数等，你需要考虑每个的重要性或者说权重大小</p>
<p>1这里就不讲了，其他方法也有很多，比如贝叶斯平均的优化版本、再比如经典的Hacker公式：</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/4.png" alt=""></p>
<p>2.时间因素：</p>
<p>时间越久，代表之前的投票结果对当前的影响越小，这边有很多不同的影响方式，举几个例子：</p>
<p>比如艾宾浩斯遗忘规律：</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/5.png" alt=""></p>
<p>这里的c、k决定下降速度，业务运用过程中，c值一般在[1,2],k值一般在[1.5,2.5]</p>
<p>比如时效衰减：</p>
<p><img src="/2017/06/21/推荐系统威尔逊区间法/6.png" alt=""></p>
<p>这里就是比较常见的移动窗口式的，永远只看近期某一段时间，而且时间内呈线性下降，不过可以改变变化方式</p>
<p>3.不同种的属性对于结果的影响自然不同</p>
<p>举个例子，用户主动搜索和用户浏览相比，用户主动搜索的情况下，用户的需求更为强烈</p>
<p>通常需要判断这些强烈程度都是通过：</p>
<p>相关性：看因变量与自变量之间的相关系数，如：cor函数</p>
<p>importance：看删除或者修改自变量，对应变量的判断影响大小，如：randomForest的重要性</p>
<p>离散程度：看自变量的数据分布是否足够分散，是否具有判断依据，如：变异系数或者pca</p>
<p>等等</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[协同过滤推荐]]></title>
      <url>/2017/06/21/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<p>set.seed ( 1234 )</p>
<h1 id="加载数据包"><a href="#加载数据包" class="headerlink" title="加载数据包"></a>加载数据包</h1><p>library ( “recommenderlab” )</p>
<h1 id="构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as-函数转为raringMatrix类型。"><a href="#构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as-函数转为raringMatrix类型。" class="headerlink" title="构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as()函数转为raringMatrix类型。"></a>构造数据运用recommenderlab包实现协同过滤推荐，其数据类型采用S4类构造，需通过as()函数转为raringMatrix类型。</h1><p>val1&lt;- matrix ( sample ( c ( as.numeric ( 0 : 5 ) ,NA ) ,50 ,replace = TRUE ,prob = c ( rep ( .4 / 6 , 6 ) , .6 ) ) ,ncol = 10 , dimnames = list ( user = paste ( “u” ,1 : 5 ,sep = ‘’ ) ,item = paste ( “i” ,1 : 10 ,sep = ‘’ ) ) )</p>
<p>val2 &lt;- as ( val1, “realRatingMatrix” )</p>
<p>数据转换</p>
<p>val3&lt;- normalize ( val2 )</p>
<h1 id="二元分类转换，normalize-函数进行标准化处理，标准化的目的是为了去除用户评分的偏差"><a href="#二元分类转换，normalize-函数进行标准化处理，标准化的目的是为了去除用户评分的偏差" class="headerlink" title="二元分类转换，normalize()函数进行标准化处理，标准化的目的是为了去除用户评分的偏差"></a>二元分类转换，normalize()函数进行标准化处理，标准化的目的是为了去除用户评分的偏差</h1><p>val4 &lt;- binarize ( val3 , minRating = 4 )</p>
<p>val5 &lt;- as ( val4 , “matrix” )</p>
<p>数据可视化</p>
<p>接下来，我们采用MovieLense数据集，</p>
<p>data ( MovieLense )</p>
<p>key1 &lt;- sample ( MovieLense , 943 , replace = F )<br>image ( MovieLense )</p>
<p>hist ( getRatings ( normalize ( MovieLense ) ) , breaks = 100 )</p>
<p>hist ( rowCounts ( key1 ) , breaks = 50 )</p>
<p>建立模型</p>
<p>对于realRatingMatrix有六种方法：IBCF(基于物品的推荐)、UBCF（基于用户的推荐）、PCA（主成分分析）、RANDOM（随机推荐）、SVD（矩阵因子化）、POPULAR（基于流行度的推荐）</p>
<p>建立协同过滤推荐算法模型，主要运用recommender(data=ratingMatrix,method,parameter=NULL)函数，getModel()可查看模型参数</p>
<p>key1_recom &lt;- Recommender (key1 , method = “IBCF” )</p>
<p>key1_popul &lt;- Recommender ( key1, method = “POPULAR” )</p>
<h1 id="查看模型方法"><a href="#查看模型方法" class="headerlink" title="查看模型方法"></a>查看模型方法</h1><p>names ( getModel ( key1_recom ) )</p>
<p>模型预测</p>
<p>TOP-N预测</p>
<p>对模型预测可运用predict()函数，在此分别以TOP-N预测及评分预测为例，预测第940-943位观影者的评分情况。n表示最终为TOP-N的列表推荐，参数type = “ratings”表示运用评分预测观影者对电影评分，模型结果均需转为list或矩阵表示</p>
<p>pred &lt;- predict ( key1_popul ,key1 [ 940 : 943,] , n = 5 )</p>
<p>as ( pred , “list” )</p>
<h1 id="top-N为有序列表，抽取最优推荐子集"><a href="#top-N为有序列表，抽取最优推荐子集" class="headerlink" title="top-N为有序列表，抽取最优推荐子集"></a>top-N为有序列表，抽取最优推荐子集</h1><p>pred3 &lt;- bestN ( pred , n = 3 )</p>
<p>as ( pred3 , “list” )</p>
<h1 id="评分预测"><a href="#评分预测" class="headerlink" title="评分预测"></a>评分预测</h1><p>rate &lt;- predict ( key1_popul , key1 [ 940 : 943 ] , type = “ratings” )</p>
<p>as ( rate , “matrix” ) [ , 1 : 5 ]</p>
<p>预测模型评价</p>
<p>评分预测模型评价</p>
<p>eva &lt;- evaluationScheme (key1 [ 1 : 800 ] , method = “split” , train = 0.9,given = 15)<br>method=”split”&amp;train=0.9为按90%划分训练测试集合,given为评价的类目数</p>
<p>r_eva1&lt;- Recommender ( getData ( eva , “train” ) , “UBCF” )</p>
<p>p_eva1&lt;- predict ( r_eva1 , getData ( eva, “known” ) , type = “ratings” )</p>
<p>r_eva2 &lt;- Recommender ( getData ( eva, “train” ) , “IBCF” )</p>
<p>p_eva2 &lt;- predict ( r_eva2 , getData ( eva, “known” ) , type = “ratings” )<br>c_eva1 &lt;- calcPredictionAccuracy ( p_eva1 , getData ( eva , “unknown” ) )</p>
<p>c_eva2 &lt;- calcPredictionAccuracy ( p_eva2 , getData ( eva , “unknown” ) )</p>
<p>error &lt;- rbind ( c_eva1 , c_eva2 )</p>
<p>rownames ( error ) &lt;- c ( “UBCF” , “IBCF” )<br>计算预测模型的准确度</p>
<p>TOP-N预测模型评价</p>
<p>通过4-fold交叉验证方法分割数据集，运用evaluate()进行TOP-N预测模型评价,评价结果可通过ROC曲线及准确率-召回率曲线展示:</p>
<h1 id="4-fold交叉验证"><a href="#4-fold交叉验证" class="headerlink" title="4-fold交叉验证"></a>4-fold交叉验证</h1><p>tops &lt;- evaluationScheme ( key1 [ 1 : 800 ] , method = “cross” , k = 4 , given = 3 ,goodRating = 5 )</p>
<p>results &lt;- evaluate ( tops , method = “POPULAR” , type = “topNList” ,n = c ( 1 , 3 , 5 , 10 ) )</p>
<h1 id="获得混淆矩阵"><a href="#获得混淆矩阵" class="headerlink" title="获得混淆矩阵"></a>获得混淆矩阵</h1><p>getConfusionMatrix ( results ) [ [ 1 ] ]</p>
<p>avg ( results )</p>
<p>推荐算法的比较</p>
<p>除了对预测模型进行评价，还可以对不同推荐算法进行比较。可首先构建一个推荐算法列表，通过ROC曲线、、准确率-召回率曲线或RMSE直方图进行比较</p>
<p>TOP-N算法比较</p>
<p>set.seed ( 2016 )</p>
<p>scheme &lt;- evaluationScheme ( key1 , method = “split” , train = 0.9 , k = 1 , given = 10 , goodRating = 5 )</p>
<h1 id="构建不同算法模型"><a href="#构建不同算法模型" class="headerlink" title="构建不同算法模型"></a>构建不同算法模型</h1><p>results &lt;- evaluate ( scheme ,test_data ,n = c ( 1 ,3 ,5 ,10 ,15 ,20 ) )</p>
<h1 id="模型比较-ROC曲线"><a href="#模型比较-ROC曲线" class="headerlink" title="模型比较#ROC曲线"></a>模型比较#ROC曲线</h1><p>plot ( results , annotate = c ( 1 , 3 ) , legend = “bottomright” )</p>
<h1 id="准确率-召回率曲线"><a href="#准确率-召回率曲线" class="headerlink" title="准确率-召回率曲线"></a>准确率-召回率曲线</h1><p>plot ( results , “prec/rec” , annotate = c ( 2 , 3 , 4 ) , legend = “topleft” )</p>
<p>预测评分算法比较</p>
<p>results2 &lt;- evaluate ( scheme , algorithms , type = “ratings” )</p>
<p>plot ( results2 , ylim = c ( 0 , 20 ) )</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[聚类算法思路总结]]></title>
      <url>/2017/06/20/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>1.cost function</p>
<p>1.1 距离</p>
<p>常见的为欧式距离（L1 norm）&amp;&amp;p=2，拓展的可以有闵可夫斯基距离（L2 norm）&amp;&amp;p=1：</p>
<p>当p趋向于无穷的时候，切比雪夫距离（Chebyshev distance）：</p>
<p>红色的时候为切比雪夫距离，蓝色为闵可夫斯基距离，绿色为欧式距离。</p>
<p>1.2相似系数</p>
<p>夹角余弦及相关系数，相关系数不受线性变换的影响，但是计算速度远慢于距离计算。</p>
<p>1.3dynamic time warping动态时间规整</p>
<p>举例子：</p>
<p>序列A：1,1,1,10,2,3，序列B：1,1,1,2,10,3</p>
<p>欧式距离：distance[i][j]=(b[j]-a[i])*(b[j]-a[i])来计算的话，总的距离和应该是128</p>
<p>应该说这个距离是非常大的，而实际上这个序列的图像是十分相似的。因为序列A中的10对应得是B中的2，A中的2对应的B中的10，导致计算膨胀，现在将A中的10对应B中的10，A中的1对应B中的2再计算，膨胀因素会小很多（时间前推一步）。</p>
<p>2.聚类算法</p>
<p>2.1分层聚类：</p>
<p>自上而下：所有点先聚为一类，然后分层次的一步一步筛出与当前类别差异最大的点</p>
<p>自下而上：所有点先各自为一类，组合成n个类的集合，然后寻找出最靠近的两者聚为新的一类，循环往复</p>
<p>数值类分类：（适用于计算量巨大或者数据量巨大的时候）</p>
<p>BIRCH算法，层次平衡迭代规约和聚类，</p>
<p>主要参数包含：聚类特征和聚类特征树：</p>
<p>聚类特征：</p>
<p>给定N个d维的数据点{x1,x2,….,xn}，CF定义如下：CF=（N，LS，SS）,其中，N为子类中的节点的个数，LS是子类中的N个节点的线性和，SS是N个节点的平方和</p>
<p>存在计算定义：CF1+CF2=（n1+n2, LS1+LS2, SS1+SS2）</p>
<p>假设簇C1中有三个数据点：（2,3），（4,5），（5,6），则CF1={3，（2+4+5,3+5+6），（2^2+4^2+5^2,3^2+5^2+6^2）}={3，（11,14），（45,70）}</p>
<p>假设一个簇中，存在质心C和半径R，若有xi，i=1…n个点属于该簇，质心为：C=(X1+X2+…+Xn)/n，R=(|X1-C|^2+|X2-C|^2+…+|Xn-C|^2)/n</p>
<p>其中，簇半径表示簇中所有点到簇质心的平均距离。当有一个新点加入的时候，属性会变成CF=（N，LS，SS）的统计值，会压缩数据。</p>
<p>聚类特征树：</p>
<p>内节点的平衡因子B，子节点的平衡因子L，簇半径T。</p>
<p>B=6，深度为3，T为每个子节点中簇的范围最大不能超过的值，T越大簇越少，T越小簇越多。</p>
<p>名义分类：</p>
<p>ROCK算法：凝聚型的层次聚类算法</p>
<p>1.如果两个样本点的相似度达到了阈值（θ），这两个样本点就是邻居。阈值（θ）有用户指定，相似度也是通过用户指定的相似度函数计算。常用的分类属性的相似度计算方法有：Jaccard系数，余弦相似度</p>
<p>Jaccard系数：J=|A∩B|/|A∪B|，一般用于分类变量之间的相似度</p>
<p>余弦相似度：【-1，1】之间，越趋近于0的时候，方向越一致，越趋向同一。</p>
<p>2.目标函数（criterion function）：最终簇之间的链接总数最小，而簇内的链接总数最大</p>
<p>3.相似度合并：遵循最终簇之间的链接总数最小，而簇内的链接总数最大的规则计算所有对象的两两相似度，将相似性最高的两个对象合并。通过该相似性度量不断的凝聚对象至k个簇，最终计算上面目标函数值必然是最大的。</p>
<p>load(‘country.RData’)</p>
<p>d&lt;-dist(countries[,-1])</p>
<p>x&lt;-as.matrix(d)</p>
<p>library(cba)</p>
<p>rc &lt;-rockCluster(x, n=4, theta=0.2, debug=TRUE)</p>
<p>KNN算法：</p>
<p>先确定K的大小，计算出每个点之外的所有点到这个目标点的距离，选出K个最近的作为一类。一般类别之间的归类的话，投票和加权为常用的，投票及少数服从多数，投票的及越靠近的点赋予越大的权重值。</p>
<p>2.2分隔聚类：</p>
<p>需要先确定分成的类数，在根据类内的点都足够近，类间的点都足够远的目标去做迭代。</p>
<p>常用的有K-means，K-medoids，K-modes等，只能针对数值类的分类，且只能对中等量级数据划分，只能对凸函数进行聚类，凹函数效果很差。</p>
<p>2.3密度聚类：</p>
<p>有效的避免了对分隔聚类下对凹函数聚类效果不好的情况，有效的判别入参主要有1:单点外的半径2：单点外半径内包含的点的个数</p>
<p>DBSCAN为主要常见的算法，可优化的角度是现在密度较高的地方进行聚类，再往密度较低的地方衍生，优化算法：OPTICS。</p>
<p>2.4网格聚类：</p>
<p>将n个点映射到n维上，在不同的网格中，计算点的密度，将点更加密集的网格归为一类。</p>
<p>优点是：超快，超级快，不论多少数据，计算速度只和维度相关。</p>
<p>缺点：n维的n难取，受分布影响较大（部分行业数据分布及其不规则）</p>
<p>2.5模型聚类：</p>
<p>基于概率和神经网络聚类，常见的为GMM，高斯混合模型。缺点为，计算量较大，效率较低。</p>
<p>GMM：每个点出现的概率：将k个高斯模型混合在一起，每个点出现的概率是几个高斯混合的结果</p>
<p>假设有K个高斯分布，每个高斯对data points的影响因子为πk，数据点为x，高斯参数为theta，则：</p>
<p>利用极大似然的方法去求解均值Uk，协方差矩阵（Σk），影响因子πk，但是普通的梯度下降的方法在这里求解会很麻烦，这边就以EM算法代替估计求解。</p>
<p>3.优化数据结构：</p>
<p>1.数据变换：</p>
<p>logit处理，对所有数据进行log变换</p>
<p>傅里叶变换</p>
<p>小波变换</p>
<p>2.降维：</p>
<p>PCA：</p>
<p>利用降维（线性变换)的思想，整体方差最大的情况下（在损失很少信息的前提下），把多个指标转化为几个不相关的综合指标（主成分),将变量线性组合代替原变量，保持代替后的数据信息量最大（方差最大）。</p>
<p>LLE：</p>
<p>(1) 寻找每个样本点的k个近邻点；</p>
<p>(2)由每个样本点的近邻点计算出该样本点的局部重建权值矩阵；</p>
<p>(3)由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值。</p>
<p>(换句话说，就是由周围N个点构成改点的一个向量矩阵表示）</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 聚类 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[常用R语言包介绍]]></title>
      <url>/2017/06/19/%E5%B8%B8%E7%94%A8R%E8%AF%AD%E8%A8%80%E5%8C%85%E4%BB%8B%E7%BB%8D/</url>
      <content type="html"><![CDATA[<p>r与python差异比较大的一个地方就是，python的机器学习算法集中程度比较高，比如sklearn，就集成了很多的算法，而R语言更多时候需要一个包一个包去了解，比较费时费力，对于python转过来的朋友非常不友好，抽空整理了工作中常用的R包如下：</p>
<p>常用检验函数：</p>
<p>基本上分布中常见的都罗列了：</p>
<p>常用作图函数包：</p>
<p>ggplot2：万能，基本上excel能画的图它都能画</p>
<p>rattle：fancyRpartPlot函数，决策树画图函数</p>
<p>基础包函数：barplot、pie、dotchart、hist、densityplot、boxplot、contour等等</p>
<p>正态检验：qqplot、qqline、qqnorm</p>
<p>连续分类回归模型：</p>
<p>stats包 lm函数，实现多元线性回归；glm函数，实现广义线性回归；nls函数，实现非线性最小二乘回归；knn函数，k最近邻算法</p>
<p>rpart包 rpart函数，基于CART算法的分类回归树模型</p>
<p>randomForest包 randomForest函数，基于rpart算法的集成算法</p>
<p>e1071包 svm函数，支持向量机算法</p>
<p>kernlab包 ksvm函数，基于核函数的支持向量机</p>
<p>nnet包 nnet函数，单隐藏层的神经网络算法</p>
<p>neuralnet包 neuralnet函数，多隐藏层多节点的神经网络算法</p>
<p>RSNNS包 mlp函数，多层感知器神经网络；rbf函数，基于径向基函数的神经网络</p>
<p>离散分类回归模型：</p>
<p>stats包 glm函数，实现Logistic回归，选择logit连接函数</p>
<p>kknn包 kknn函数，加权的k最近邻算法</p>
<p>rpart包 rpart函数，基于CART算法的分类回归树模型</p>
<p>adabag包bagging函数，基于rpart算法的集成算法；boosting函数，基于rpart算法的集成算法</p>
<p>party包ctree函数，条件分类树算法</p>
<p>RWeka包OneR函数，一维的学习规则算法；JPip函数，多维的学习规则算法；J48函数，基于C4.5算法的决策树</p>
<p>C50包C5.0函数，基于C5.0算法的决策树</p>
<p>e1071包naiveBayes函数，贝叶斯分类器算法</p>
<p>klaR包NaiveBayes函数，贝叶斯分类器算分</p>
<p>MASS包lda函数，线性判别分析；qda函数，二次判别分析</p>
<p>聚类：Nbclust包Nbclust函数可以确定应该聚为几类</p>
<p>stats包kmeans函数，k均值聚类算法；hclust函数，层次聚类算法</p>
<p>cluster包pam函数，k中心点聚类算法</p>
<p>fpc包dbscan函数，密度聚类算法；kmeansruns函数，相比于kmeans函数更加稳定，而且还可以估计聚为几类；pamk函数，相比于pam函数，可以给出参考的聚类个数</p>
<p>mclust包Mclust函数，期望最大（EM）算法</p>
<p>关联规则：arules包apriori函数</p>
<p>Apriori关联规则算法</p>
<p>recommenderlab协调过滤</p>
<p>DRM：重复关联</p>
<p>ECLAT算法： 采用等价类，RST深度搜索和集合的交集： eclat</p>
<p>降维算法：</p>
<p>psych包prcomp函数、factanal函数</p>
<p>时序分析：</p>
<p>ts时序构建函数</p>
<p>timsac包时序分析</p>
<p>holtwinter包时序分析</p>
<p>decomp、tsr、stl成分分解</p>
<p>zoo 时间序列数据的预处理</p>
<p>统计及预处理：</p>
<p>常用的包 Base R, nlme</p>
<p>aov, anova 方差分析</p>
<p>density 密度分析</p>
<p>t.test, prop.test, anova, aov:假设检验</p>
<p>rootSolve非线性求根</p>
<p>reshape2数据预处理</p>
<p>plyr及dplyr数据预处理大杀器</p>
<p>最后剩下常用的就是读入和写出了：</p>
<p>RODBC 连接ODBC数据库接口</p>
<p>jsonlite 读写json文件</p>
<p>yaml 读写yaml文件</p>
<p>rmakdown写文档</p>
<p>knitr自动文档生成</p>
<p>一般业务中使用比较多的就是上面这些了，当然R里面有很多冷门的包，也很好用滴~</p>
]]></content>
      
        <categories>
            
            <category> 工具 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> R语言工具 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[决策树及衍射指标]]></title>
      <url>/2017/06/02/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%8A%E8%A1%8D%E5%B0%84%E6%8C%87%E6%A0%87/</url>
      <content type="html"><![CDATA[<p>一、常用的决策树节点枝剪的衡量指标：</p>
<p>熵：</p>
<p>如果一件事有k种可的结果，每种结果的概率为 pi（i＝1…k）</p>
<p>该事情的信息量：</p>
<p>熵越大，随机变量的不确定性越大。</p>
<p>信息增益：</p>
<p>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下的经验条件熵H(D|A)之差</p>
<p>换句话说，就是原信息集下的信息量－在A特征条件下的信息集的信息量</p>
<p>信息增益越大，信息增多，不确定性减小</p>
<p>信息增益率：</p>
<p>信息增益率定义:特征A对训练数据集D的信息增益比定义为其信息增益与训练数据D关于特征A的值的熵HA(D)之比</p>
<p>注：p：每个唯独上，每个变量的个数／总变量个数</p>
<p>二、常用的决策树介绍：</p>
<p>ID3算法：</p>
<p>ID3算法的核心是在决策树各个子节点上应用信息增益准则选择特征，递归的构建决策树，具体方法是:从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。</p>
<p>解释：在做每次选择差分枝的时候，以不确定性最小点作为loss fuction，直到无法细分</p>
<p>缺点：</p>
<p>1.ID3算法只有树的生成，所以该算法生成的树容易产生过拟合，分得太细，考虑条件太多。</p>
<p>2.不能处理连续属性</p>
<p>3.选择具有较多分枝的属性，而分枝多的属性不一定是最优的选择。</p>
<p>4.局部最优化，整体熵值最小，贪心算法算子节点的分支</p>
<p>C4.5算法：</p>
<p>基于ID3算法，用信息增益比来选择属性，对非离散数据也能处理，能够对不完整数据进行处理。</p>
<p>采用增益率（GainRate）来选择分裂属性。计算方式如下：</p>
<p>CART算法：</p>
<p>CART算法选择分裂属性的方式是比较有意思的，首先计算不纯度，然后利用不纯度计算Gini指标。</p>
<p>计算每个子集最小的Gini指标作为分裂指标。</p>
<p>不纯度的计算方式为：</p>
<p>pi表示按某个变量划分中，目标变量不同类别的概率。</p>
<p>某个自变量的Gini指标的计算方式如下：</p>
<p>计算出每个每个子集的Gini指标，选取其中最小的Gini指标作为树的分支（Gini（D）越小，则数据集D的纯度越高）。连续型变量的离散方式与信息增益中的离散方式相同。</p>
<p>三、基于决策树的一些集成算法：</p>
<p>随机森林：</p>
<p>随机生成n颗树，树之间不存在关联，取结果的时候，以众数衡量分类结果；除了分类，变量分析，无监督学习，离群点分析也可以。</p>
<p>生成过程：</p>
<p>1.n个样本，随机选择n个样本（有放回），训练一颗树</p>
<p>从原始训练数据集中,应用bootstrap方法有放回地随机抽取 K个新的自助样本集,并由此构建 K棵分类回归树,每次未被抽到的样本组成了 K个袋外数据(Out-of-bag,OOB)</p>
<p>2.每个样本有M个属性，随机选m个，采取校验函数（比如信息增益、熵啊之类的），选择最佳分类点</p>
<p>3.注意，每个树不存在枝剪</p>
<p>4.将生成的多棵树组成随机森林,用随机森林对新的数据进行分类,分类结果按树分类器的投票多少而定</p>
<p>树的个数随机选取，一般500，看三个误差函数是否收敛；变量的个数一般取均方作为mtry</p>
<p>GBDT：</p>
<p>DT步骤：</p>
<p>GBDT里面的树是回归树！</p>
<p>GBDT做每个节点上的分支的时候，都会以最小均方误差作为衡量（真实值－预测值）的平方和／N，换句话说，就是存在真实线l1，预测线l2，两条线之间的间距越小越好。</p>
<p>BT步骤：</p>
<p>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。</p>
<p>换句话说，就是第一次预测的差值记为下一次预测的初始值，一直到某一次计算出的差值为0，把前n次的结果相加，就是一个真实预测。</p>
<p>Adaboost：</p>
<p>步骤：</p>
<p>1.初始化所有训练样例的权重为1 / N,其中N是样本数</p>
<p>2.对其中第1~m个样本:</p>
<p>a.训练m个弱分类器，使其最小化bias：</p>
<p>b.接下来计算该弱分类器的权重α，降低错判的分类器的权重，：</p>
<p>c.更新权重：</p>
<p>3.最后得到组合分类器：</p>
<p>核心的思想如下图：</p>
<p>全量数据集在若干次训练后，降低训练正确的样本的权重，提高训练错误样本的权重，得到若干个Y对应的分类器，在组合投票得到最终的分类器</p>
<p>四、惠普实验室-集成并行化的随机森林：</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 树划分问题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[因子分析原理剖析]]></title>
      <url>/2017/06/02/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>因子分析概述：</p>
<p>因子分析分为Q型和R型，我们对R型进行如下研究：</p>
<p>一.因子分析步骤：</p>
<p>1.确认是是否适合做因子分析</p>
<p>2.构造因子变量</p>
<p>3.旋转方法解释</p>
<p>4.计算因子变量得分</p>
<p>二.因子分析的计算过程：</p>
<p>1.将原始数据标准化</p>
<p>目的：消除数量级量纲不同</p>
<p>2.求标准化数据的相关矩阵</p>
<p>3.求相关矩阵的特征值和特征向量</p>
<p>4.计算方差贡献率和累计方差贡献率</p>
<p>5.确定因子</p>
<p>F1,F2,F3…为前m个因子包含数据总量（累计贡献率）不低于80%。可取前m各因子来反映原评价</p>
<p>6.因子旋转</p>
<p>当所得因子不足以明显确定或不易理解时选择此方法</p>
<p>7.原指标的线性组合求各因子的得分</p>
<p>两种方法：回归估计和barlett估计法</p>
<p>8.综合得分：以各因子的方差贡献率为权，各因子的线性组合得到各综合评价指标函数</p>
<p>F=（λ1F1+…λmFm）/(λ1+…λm)</p>
<p>=W1F1+…WmFm</p>
<p>9.得分排序</p>
<p>因子分析详解：</p>
<p>因子分析模型，又名正交因子模型</p>
<p>X=AF+ɛ</p>
<p>其中：</p>
<p>X=[X1,X2,X3…XP]‘</p>
<p>A=</p>
<p>F=[F1,F2…Fm]’</p>
<p>ɛ=[ɛ1,ɛ2…ɛp]’</p>
<p>以上满足：</p>
<p>（1）m小于等于p</p>
<p>（2）cov(F,ɛ)=0</p>
<p>(3)Var(F)=Im</p>
<p>D(ɛ)=Var(ɛ)=</p>
<p>ɛ1,ɛ2…ɛp不相关，且方差不同</p>
<p>我们把F成为X公共因子，A为荷载矩阵，ɛ为X特殊因子</p>
<p>A=(aij)</p>
<p>数学上证明：aij就是i个变量与第j个因子的相关系数，参见层次分析法aij定义。</p>
<p><1>荷载矩阵</1></p>
<p>就荷载矩阵的估计和解释方法有主因子和极大似然估计，我们就主因子分析而言：（是主因子不是主成份）</p>
<p>设随机向量X的协方差阵为Ʃ</p>
<p>λ1,λ2,λ3..&gt;0为Ʃ的特征根</p>
<p>μ1，μ2，μ3…为对应的标准正交向量</p>
<p>我们大一学过线代或者高代，里面有个东西叫谱分析：</p>
<p>Ʃ=λ1μ1μ1’+……+λpμpμp’</p>
<p>=</p>
<p>当因子个数和变量个数一样多，特殊因子方差为0.</p>
<p>此时，模型为X=AF,其中Var(F)=Ip</p>
<p>于是，Var(X)=Var(AF)=AVar(F)A’=AA’</p>
<p>对照Ʃ分解式,A第j列应该是</p>
<p>也就是说，除了uj前面部分，第j列因子签好为第j个主成份的系数，所以为主成份法。</p>
<p>如果非要作死考虑ɛ</p>
<p>原来的协方差阵可以分解为：</p>
<p>Ʃ=AA’+D=</p>
<p>以上分析的目的；</p>
<p>1.因子分析模型是描述原变量X的协方差阵Ʃ的一种模型</p>
<p>2.主成份分析中每个主成份相应系数是唯一确定的，然而因子分析中的每个因子的相应系数不是唯一的，因而我们的因子荷载矩阵不是唯一的</p>
<p>(主成分分析是因子分析的特例，非常类似，有兴趣的可以去看看，这两者非常容易混淆)</p>
<p><2>共同度和方差贡献</2></p>
<p>无论是在spss或者R的因子分析中都围绕着贡献度，我们来看下，它到底是什么意思。</p>
<p>由因子分析模型，当仅有一个公因子F时，</p>
<p>Var(Xi)=Var(aiF)+Var(ɛi)</p>
<p>由于数据标准化，左端为1，右端分别为共性方差和个性方差</p>
<p>共性方差越大，说明共性因子作用越大。</p>
<p>因子载荷矩阵A中的第i行元素之平方和记为hi2</p>
<p>成为变量(Xi)共同度</p>
<p>它是公共因子对(Xi)的方差锁做出的贡献，反映了全部公共因子对变量(Xi)的影响。</p>
<p>hi2大表明第i个分量对F的每一个分量F1,F2,…Fm的共同依赖程度大</p>
<p>将因子载荷矩阵A的第j列的各元素的平方和记为gj2</p>
<p>成为公共因子Fj对x的方差贡献。</p>
<p>gj2表示第j个公共因子Fj对x的每一个分量Xi所提供的方差的总和，他就是衡量公共因子的相对重要行的指标。gj2越大，表明公共因子Fj对x的贡献越大，或者说对x的影响和作用就越大。</p>
<p>如果将载荷矩阵A的所有gj2都计算出来，按大小排列，就可以提炼最有影响力的公共因子。</p>
<p><3>因子旋转</3></p>
<p>这方面涉及较为简单，我就简单提一下</p>
<p>目的：建立因子分析模型不是只要找主因子，更加重要的是意义，以便对实际进行分析，因子旋转就是使所得结论更加清晰的表示。</p>
<p>方法：正交旋转，斜交旋转两大类，常用正交。</p>
<p>便于理解，我解释下旋转的意义，以平面直角坐标系为例，我们想得到的数据正好为：y=x和y=-x上的点，我们能解释的却在x=0和y=0上，这时候我们就可以旋转坐标系，却不影响结果。</p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 因子分析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[正则化-Lasso规约]]></title>
      <url>/2017/06/02/%E6%AD%A3%E5%88%99%E5%8C%96-Lasso%E8%A7%84%E7%BA%A6/</url>
      <content type="html"><![CDATA[<blockquote>
<p>摘要：lasso的目的主要是避免数据拟合过渡，导致训练数据效果优秀，测试数据效果较差</p>
</blockquote>
<hr>
<p>先看一波过拟合：<br><img src="/2017/06/02/正则化-Lasso规约/1.png" alt="拟合曲线"></p>
<p>图中，红色的线存在明显的过拟合，绿色的线才是合理的拟合曲线，为了避免过拟合，我们可以引入正则化。</p>
<p>下面可以利用正则化来解决曲线拟合过程中的过拟合发生，存在均方根误差也叫标准误差，即为√[∑di^2/n]=Re，n为测量次数；di为一组测量值与真值的偏差。<br><img src="/2017/06/02/正则化-Lasso规约/2.png" alt=""></p>
<p>实际考虑回归的过程中，我们需要考虑到误差项，<br><img src="/2017/06/02/正则化-Lasso规约/3.png" alt=""></p>
<p><img src="/2017/06/02/正则化-Lasso规约/4.png" alt=""></p>
<p>这个和简单的线性回归的公式相似，而在正则化下来优化过拟合这件事情的时候，会加入一个约束条件，也就是惩罚函数：</p>
<p><img src="/2017/06/02/正则化-Lasso规约/5.png" alt=""></p>
<p>这边这个惩罚函数有多种形式，比较常用的有l1,l2，大概有如下几种：<br><img src="/2017/06/02/正则化-Lasso规约/6.png" alt=""></p>
<p>讲一下比较常用的两种情况，q＝1和q＝2的情况：<br>q＝1，也就是今天想讲的lasso回归，为什么lasso可以控制过拟合呢，因为在数据训练的过程中，可能有几百个，或者几千个变量，再过多的变量衡量目标函数的因变量的时候，可能造成结果的过度解释，而通过q＝1下的惩罚函数来限制变量个数的情况，可以优先筛选掉一些不是特别重要的变量，见下图：</p>
<p><img src="/2017/06/02/正则化-Lasso规约/7.png" alt=""></p>
<p>作图只要不是特殊情况下与正方形的边相切，一定是与某个顶点优先相交，那必然存在横纵坐标轴中的一个系数为0，起到对变量的筛选的作用。<br>q＝2的时候，其实就可以看作是上面这个蓝色的圆，在这个圆的限制下，点可以是圆上的任意一点，所以q＝2的时候也叫做岭回归，岭回归是起不到压缩变量的作用的，在这个图里也是可以看出来的。</p>
<hr>
<p>lasso回归：<br>lasso回归的特色就是在建立广义线型模型的时候，这里广义线型模型包含一维连续因变量、多维连续因变量、非负次数因变量、二元离散因变量、多元离散因变，除此之外，无论因变量是连续的还是离散的，lasso都能处理，总的来说，lasso对于数据的要求是极其低的，所以应用程度较广；除此之外，lasso还能够对变量进行筛选和对模型的复杂程度进行降低。这里的变量筛选是指不把所有的变量都放入模型中进行拟合，而是有选择的把变量放入模型从而得到更好的性能参数。 复杂度调整是指通过一系列参数控制模型的复杂度，从而避免过度拟合(Overfitting)。 对于线性模型来说，复杂度与模型的变量数有直接关系，变量数越多，模型复杂度就越高。 更多的变量在拟合时往往可以给出一个看似更好的模型，但是同时也面临过度拟合的危险。<br>lasso的复杂程度由<code>λ</code>来控制，<code>λ</code>越大对变量较多的线性模型的惩罚力度就越大，从而最终获得一个变量较少的模型。除此之外，另一个参数α来控制应对高相关性(highly correlated)数据时模型的性状。 lasso回归<code>α=1</code>，Ridge回归<code>α=0</code>，这就对应了惩罚函数的形式和目的。我们可以通过尝试若干次不同值下的λ，来选取最优λ下的参数，还可以结合CV选择最优秀的模型。</p>
<h6 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">setwd(&quot;~/Desktop&quot;)</span><br><span class="line">library(glmnet)</span><br><span class="line">train_origin&lt;-read.table(&apos;trian.txt&apos;,header = T,fill = T)</span><br><span class="line">test_origin&lt;-read.table(&apos;test.txt&apos;,header = T,fill = T)</span><br><span class="line">train_test1&lt;-train_origin</span><br><span class="line">train_test1&lt;-train_test1[,-9]</span><br><span class="line">train_test1$tag&lt;-as.factor(train_test1$tag)</span><br><span class="line">train_test1$risk_level&lt;-as.factor(train_test1$risk_level)</span><br><span class="line">x&lt;-train_test1[,3:11]</span><br><span class="line">y&lt;-train_test1[,2]</span><br><span class="line">## one hot encoding</span><br><span class="line">x1&lt;-model.matrix(~., x)</span><br></pre></td></tr></table></figure>
<p>通常数据中会存在离散点，而lasso在R里面是通过数值矩阵来做输入的，所以需要对原数据做一步预处理，不然这边会抛错误；除此之外，如果数据之间差别的数量级较大，还需要进行标准化，R里面也是可以进行处理的，这边就不赘述了，<code>glmnet()</code>函数中添加参数<code>standardize = TRUE</code>来实现，<code>scale()</code>函数也可以实现，自行选择即可。</p>
<h6 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = glmnet(x1, y, family=&quot;binomial&quot;, nlambda=50, alpha=1)</span><br></pre></td></tr></table></figure>
<p><code>family</code>里面是指选择函数的类型：<br><code>family explation</code>,<code>gaussian univariate</code>,<code>mgaussian multivariate</code>,<code>poisson count</code>,<code>binomial binary</code>,<code>multinomial category</code><br>lambda是指随机选择λ，做lambda个模型；alpha是上述讲到的α，选择惩罚函数，正常情况下，1是lasso，0是岭回归<br>这边模型拓展可以交叉检验一下，有内置的函数：<br>cvmodel = cv.glmnet(x1, y, family = “binomial”, type.measure = “class”,nfolds=10)<br>这边会多出来一个<code>type.measure</code>，这个<code>type.measure</code>是指期望最小化的目标参量是什么，换句话说，就是衡量这个模型的指标函数是啥：<br><code>type.measure</code>  details<br><code>deviance</code>:  <em>-2倍的Log-likelihood</em><br><code>mse</code>:<em>mean squred error</em><br><code>mae</code>:<em>mean absolute error</em><br><code>class</code>:<em>missclassification error</em><br><code>auc</code>:<em>area under the ROC curve</em><br>比较常用的是<code>auc</code>，这个就是现在比较主流的衡量一个模型好坏的roc所衍生出来的一个值；我们这边用的是<code>class</code>，也就是模型错误分配的概率，结合我这次业务开发的实际业务场景，这个更合适一点；<code>nfolds</code>是指folds数目，也可以通过foldid数来控制每个fold里面的数据数量。<br>对于glmnet，可以通过<code>plot(model)</code>来观察每个自变量的变化轨迹，<code>cv.glmnet</code>可以通过<code>plot(cvmodel)</code><br>举个<code>plot(cvmodel)</code>的例子：</p>
<p><img src="/2017/06/02/正则化-Lasso规约/8.png" alt=""></p>
<p>可以通过<code>c(cvfit$lambda.min, cvfit$lambda.1se)</code>来看在所有的<code>λ</code>值中，得到最小目标函数<code>type.measure</code>均值的<code>cvfit$lambda.min</code>，以及其所对应的<code>λ</code>值可接受的一个标准误差之内对应的<code>cvfit$lambda.1se</code>。<br><img src="/2017/06/02/正则化-Lasso规约/9.png" alt=""><br>我们可以<code>print(model)</code>，在实际的选择模型中<code>λ</code>值的过程里，存在三个指标：<code>df</code>：自由度， <code>%Dev</code>：残差被解释的占比，也就是模型的好坏程度，类似于线性模型中的R平方，Lambda也就是<code>λ</code>值所对应的值，然后我们可以通过<code>coef(fit, s=c(fit$lambda[35],0.002))</code>得出当时模型所对应的系数。</p>
<hr>
<p>最后，讲一下elastic net<br>elastic net融合了l1范数和l2范数两种正则化的方法，上面的岭回归和lasso回归都可以看做它的特例：</p>
<p><img src="/2017/06/02/正则化-Lasso规约/10.png" alt=""></p>
<p>elastic net对于p远大于n,或者严重的多重共线性情况有明显的效果，很好理解，当alpha接近1时，elastic net表现很接近lasso，一般来说，elastic net是岭回归和lasso的很好的折中，当alpha从0变化到1，目标函数的稀疏解（部分变量的系数为0）也从0单调增加到lasso的稀疏解。</p>
<p><strong>特征规约初步总结如下：</strong><br>1）子集选择 这是传统的方法，包括逐步回归和最优子集法等，对可能的部分子集拟合线性模型，利用判别准则 （如AIC,BIC,Cp,调整R2 等）决定最优的模型<br>2）收缩方法（shrinkage method） 收缩方法又称为正则化（regularization）。主要是岭回归（ridge regression）和lasso回归。通过对最小二乘估计加入罚约束，使某些系数的估计为0。（岭回归：消除共线性；模的平方处理；Lasso回归：压缩变量，起降维作用；模处理）<br>(3)维数缩减 主成分回归（PCR）和偏最小二乘回归（PLS）的方法。把p个预测变量投影到m维空间</p>
<p><em>部分图片转载于：<a href="http://bbs.pinggu.org/thread-3848519-1-1.html" target="_blank" rel="noopener">http://bbs.pinggu.org/thread-3848519-1-1.html</a></em></p>
]]></content>
      
        <categories>
            
            <category> 算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 正则化问题 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
