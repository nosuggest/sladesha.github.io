<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[博客导读公告(置顶)]]></title>
    <url>%2F2029%2F01%2F20%2F%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%AF%BB%E5%85%AC%E5%91%8A%2F</url>
    <content type="text"><![CDATA[首先，我很荣幸您出现在我的个人博客，下面请允许我花费您3分钟左右的时间，简单的为您介绍一下博客中的相关功能，这将极大的提高您在后续阅读中的体验： 打个广告，欢迎各位老爷关注我的微信公众号：ml_trip，期待与大家交流！ 我的个人介绍大家可以在首页的标题下面找到me这个图标，点击即可，里面有我的个人介绍： 我的个人简历下载地址在me跳转页面中的位置如下： 快速阅读您可在任何一篇文章的右侧看到红色方框： 如果您对其中部分内容感兴趣，可直接点击绿色方框内的文章，会自动跳转到您关心的模块； 如果您想要看到我的更多联系方式，可以点击蓝色模块中的站点概览； 如果您不想红色方框影响您的阅读，在黑色条块的最下方的小叉点击即可。 赞助激励如果您觉得我写的东西对您有一些帮助，在您宽裕的情况下可以在文章下面的打赏中给我发一个小红包，或者直接扫描下面的二维码，感谢您对我的认可： 如果您还是一个学生或者您正处于人生的低谷，感谢您对我的认可，打赏就不需要了，我会一如既往的给大家整理工作中的一些想法和心得 自定义搜索为了方便大家找到自己关心的内容，建议直接点击搜索图标： 比如搜索svm，有模糊匹配结果如下： 标签检索大家可以在首页的标题下面找到标签这个图标，点击即可： 该类别下生成了标签云，为较为仔细的文章内容概括： 其他 因为本博客部署在GitHub，如果您有时候遇到打不开网页的问题，建议您重复刷新或者收藏slade_sal简书地址，两者内容是一致的 如果您有任何疑惑或者疑问都可以通过站点概览的邮箱联系我，非常愿意解答您的问题 如果您遇到算法学习过程的困难，需要内推或者就业方向建议，也可以通过站点概览的邮箱联系我，非常愿意和您进行交流 绝大多数代码都可以在我的Github上找到，所有的数据都经过脱敏处理，您可以放心使用，希望对您有所帮助 最后，感谢大家一路以来对我的认可。]]></content>
      <categories>
        <category>基础</category>
      </categories>
      <tags>
        <tag>公告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PKUseg在货运领域的评测]]></title>
    <url>%2F2019%2F01%2F14%2FPKUseg%E5%9C%A8%E8%B4%A7%E8%BF%90%E9%A2%86%E5%9F%9F%E7%9A%84%E8%AF%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[先说结论，再和大家闲聊，对比jieba与PKUseg在公路货运切词能力上： 默认模型下，jieba效果优于PKUseg PKUseg提供场景精细化的预训练（还没有提供入口），长远来讲适合专业领域使用 PKUseg在特定的场景下有令人惊喜的效果（地址切分） 给大家的建议就是，如果大家赶时间求稳定适应范围需要非常广的时候，目前来说jieba是非常好的选择，如果说在面临一些精细化领域的特殊需求的时候，可以用PKUseg进行一波尝试，有意外惊喜。 那是一个风和日丽的早上，突然群里老大发出一条消息：我感觉我的心脏有一丝隐隐作痛的感觉，人在办公室坐，活从天上来，虽然身后站着一堆催上线的产品，我还是屈服于老大的正义（淫威），简单测评了新出来的PKUseg与Jieba在公路货运/运输行业上的效果对比。 在我们的热词数据库中已经有人工切词完成的2万多条货运的词条：1234567891011121314151617181920description standard高博集团装货卸宝华 高博 集团 装货 卸 宝华北安到吉林农安饲料90吨每吨105 北安 到 吉林 农安 饲料 90吨 每吨 105需要4个车 需要 4个 车叶张公路装香闵路曲吴路两卸 叶张公路 装 香闵路 曲吴路 两卸从福通物流到吴滩镇 从 福通 物流 到 吴滩镇霞浦宏霞路到中通物流 霞浦宏霞路 到 中通物流石大路3场到德兴西门山 石大路 3场 到 德兴 西门山公园西路装 公园 西路 装不押车每吨150 不 押车 每吨 150速订价钱好商量 速订 价钱 好商量慈溪胜山装 慈溪 胜山装好装好卸高价急走 好装好卸 高价急走九顶山路与东方大道位置装货可以配货 九顶 山路 与 东方 大道 位置 装货 可以 配货要二部 要 二部青浦工业园区久远路提货到奉贤新杨公路进仓 青浦 工业园区 久远路 提货 到 奉贤 新杨公路 进仓园光路装博学南路卸 园光路 装 博学南路 卸公兴装卸荣昌广顺 公兴 装卸 荣昌 广顺打备注电话18458331112 打 备注 电话 18458331112... 首先看，不加任何词库，预训练下的，最后的效果对比：可以看到，在默认的分词模型下，jieBa分词还是拥有绝对优势的，但是在pkuSeg的git里面 所以我想看看能不能进行一下预训练下后再对比一下，可惜的是我在git（git地址传送门）上找了半天也没有找到预训练的入口，只有已经被官方预训练好的词库等有时间了，可以邮件沟通一下再补充这个部分的效果对比，我觉得，应该还是有提升的。 但是，在我们实际去测的过程中，我们发现了一些差异话的东西比较有意思。我们其实现在在做一个语音发货的产品，涉及到把一串地址切分开的需求： 其中涉及到地址切分的时候，jieba的能力会比如PKUseg要弱不少，比如“山西大同”，“上海浦东”，我们需要把一级二级地址切开的时候，PKUseg可以做到，而jieba并不能按照需求切块。所以，我们已经打算在地址模块切换PKUseg的模型来适应了。 最后吐槽一下，虽然我知道PKUseg需要加载模型，但是一加载就是一二十秒也是有点夸张了。酒浆，各位下回见。 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>开源项目</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas中的问题记录]]></title>
    <url>%2F2018%2F10%2F23%2Fpandas%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[最近发现pandas的一个问题，记录一下：有一组数据（test.txt）如下：12345620181016 1483068029890327320181016 1483960347395306920181016 1483960347395307920181016 1483960347395308920181016 1483960347395309920181016 14839603473953019 剖析出来看，数据是按照\t进行分隔的：&#39;20181016\t14830680298903273\n&#39;123with open('test.txt','r') as f:line = f.readline()print(line) 我平时一直在用pandas去读数据，所以我很熟练的写下来如下的代码：pd.read_table(&#39;test.txt&#39;,header=None)然后发现，第一列变成了科学记数法的方式进行存储了： 很明显，科学记数法是可以转换的：12345678def as_number(value):try:return &apos;&#123;:.0f&#125;&apos;.format(value)except:return value# 应用到目标列去即可data.uid.apply(as_number) 诡异的事情发生了，对于14830680298903273在as_number函数转换下变成了14830680298903272，理论上讲14830680298903273没有小数部分不存在四舍五入的原因，网上搜了也没有很明确的解释，初步讨论后猜测应该是pandas在用float64去存这种长度过长的数字的时候有精度丢失的问题。 要解决也是很简单的： 用open的形式打开，在切割逐步去用list进行append，在合并 用read_table的函数的时候，默认是用float64去存在的，改成object去存(dtype=object) 在生产数据的时候，对于这种过长的数据采取str的形式去存 也是给自己提个醒，要规范一下自己的数据存储操作，并养成数据核对的习惯。 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>代码集合</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YoutubeNet的数据答疑]]></title>
    <url>%2F2018%2F10%2F16%2FYoutubeNet%E7%9A%84%E6%95%B0%E6%8D%AE%E7%AD%94%E7%96%91%2F</url>
    <content type="text"><![CDATA[实在是太忙了，抽空给大家解析一下之前写的YoutubeNet的数据是怎么构造的，协助大家可以自行构造一下。 这边和大家说一下，我没有上传数据的原因有两个： 涉及公司的数据财产，不方便上传 懒得做脱敏处理 数据一共有1300多万条，传输实在不方便 主要数据处理的部分在map_id_idx.py脚本下，其中包含all_item_20180624.txt和click_thirty_day_data_20180609.txt两个数据集合。 其中，all_item_20180624.txt是当日所有的商品集合：包含’Prd_Id’, ‘ItemId’, ‘BrandId’, ‘MsortId’和‘GenderId’五列，分别代表着商品id，skuid，低级品牌id，中级品牌id，产品性别，最后形如： 1234567891011125675 50000055 175 1500 32577 50000056 187 66 32002 50000057 63 11 22007 50000058 137 58 32075 50000060 80 50 32348 50000061 138 16 2423 50000062 162 237 3469 50000063 10 1500 31102 50000064 176 11 11896 50000066 37 27 12489 50000067 27 44 1... click_thirty_day_data_20180609.txt为近三十天的用户点击流，包含’UId’, ‘ItemId’, ‘clickTime’三列，分别代表着uid、点击的skuid，点击时间，最后形如： 12345678910111234 51668064 152860240634 51890512 152878838934 51884724 152878839334 51884720 152878839934 51884718 152878841434 51580974 152878844234 51854970 152878848734 51514910 152878849934 51855000 152878853534 51854990 152878856934 51854998 1528788572... 通过map_id_idx.py对所有的商品进行标序号，然后带入用户的点击流中，方便后期做embedding操作，就酱。 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GloVe向量化做文本分类]]></title>
    <url>%2F2018%2F09%2F25%2FGloVe%E5%90%91%E9%87%8F%E5%8C%96%E5%81%9A%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[向量化在之前，我对向量化的方法一直局限在两个点， 第一种是常规方法的one-hot-encoding的方法，常见的比如tf-idf生成的0-1的稀疏矩阵来代表原文本： 这种方法简单暴力，直接根据文本中的单词进行one-hot-encoding，但是数据量一但大了，这个单句话的one-hot-encoding结果会异常的长，而且没办法得到词与词之间的关系。 第二种是基于神经网络的方法，常见的比如word2vec，YouTubeNet： 这种方法（这边以CBOW为例子）都是初始一个固定长度的随机向量作为每个单词的向量，制定一个目标词的向量，以上下文词向量的sum结果作为input进行前向传递，使得传递的结果和目标词向量尽可能一致，以修正初始的随机向量。换句话说，就是刚开始，我随意定义生成一个vector代表一个词，然后通过上下文的联系去修正这个随机的vector。好处就是我们可以得到词与词之间的联系，而且单个词的表示不复杂，坏处就是需要大量的训练样本，毕竟涉及到了神经网络。 最近，我们突然发现了第三种方法，GloVe向量化。它也是开始的时候随机一个vector作为单词的表示，但是它不利用神经网络去修正，而是利用了一个自己构造的损失函数： 通过我们已有的文章内容，去是的这个损失函数最小，这就变成了一个机器学习的方法了，相比较暴力的前馈传递，这也高快速和高效的多。同时，它还兼具了word2vec最后结果里面vector方法的优点，得到词与词之间的联系，而且单个词的表示不复杂。 这边就不展开GloVe算法的细节了，后面有空和大家补充，这个算法的构造非常巧妙，值得大家借鉴一下。 文本分类刚才开门见山的聊了蛮久向量化，看起来和文本分类没什么关系，确实在通常意义上来讲，我们的最简单最常用的方法并不是向量化的方法，比如通过朴素贝叶斯，N-Grams这些方法来做分类识别。 tfidf+N-grams1.其实很简单，首先对语料库进行切词，维护自己的词典，做高频词的人工复审，将无意词进行stop_words归总 可以看到，高频词其实是非常非常少的，而且如果你真的去做了，你就会发现，”了”、“的”、“啊”这种语气词，和一些你公司相关的领域词汇会非常靠前，这些词作为stop_words会有效的降低训练成本、提高模型效果。 2.进行tf-idf，将词进行重赋权，字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降，有效的将向量化中的one hot encoding结果进行了修正。但是依然存在问题：在TFIDF算法中并没有体现出单词的位置信息。 123456# sublinear_tf：replace tf with 1 + log(tf)# max_df：用来剔出高于词频0.5的词# token_pattern：(?u)\b\w+\b是为了匹配出长度为1及以上的词，默认的至少需要词长度为2# ngram_range：这边我做了3-grams处理，如果只想朴素计算的话(1,1)即可# max_features：随着我做了各种宽松的条件，最后生成的词维度会异常大，这边限制了前3万vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5, token_pattern=r"(?u)\b\w+\b",ngram_range=(1, 3), max_features=30000) 不得不说，python处理机器学习，深度学习的便捷程度是异常的高。 3.在经过TfidfVectorizer处理之后的结果是以稀疏矩阵的形式来存的，如果想看内容的话，可以用todense()转化为matrix来看。接下来，用贝叶斯来训练刚才得到的矩阵结果就可以了。 12mnb_tri = MultinomialNB(alpha=0.001)mnb_tri.fit(tri_train_set.tdm, tri_train_set.label) tf-idf + n-grams + naive-bayes + lr这种方法是上面方法的升级版本，我们先看下架构： 其实主要差异在于右侧的算法模型详细部分，我们做了一个由3-grams到3-grams+naive-bayes+lr的扩充，提升精度。 在模型的过程中，上面的第一步，都是一样的，在第二、三步有所差异：2.在第二步中，我们除了要构造出一个3-grams的sparse matrix也需要构造出一个朴素的sparse matrix 12# 朴素结果vectorizerby = TfidfVectorizer(stop_words=stpwrdlst, token_pattern=r"(?u)\b\w+\b", max_df=0.5, sublinear_tf=True,ngram_range=(1, 1), max_features=100000) 3.不仅仅用bayes进行一次分类，而是根据3-grams和朴素情况下的sparse matrix进行预测，再用logistics regression来合并两个的结果做个stack进行0-1压缩。 12345# 构造出一个3-grams的sparse matrix也需要构造出一个朴素的sparse matrixmnb_tri = MultinomialNB(alpha=0.001)mnb_tri.fit(tri_train_set.tdm, tri_train_set.label)mnb_by = MultinomialNB(alpha=0.001)mnb_by.fit(by_train_set.tdm, by_train_set.label) 123# 加bias，cv选择最优正则结果，lbfgs配合l2正则lr = LogisticRegressionCV(multi_class="ovr", fit_intercept=True, Cs=np.logspace(-2, 2, 20), cv=2, penalty="l2",solver="lbfgs", tol=0.01)re = lr.fit(adv_data[['f1', 'f2']], adv_data['rep_label']) 总结一下上面两种方法，我觉得是入门快，效果也不错的小练手，也是完全可以作为我们开始一个项目的时候，用来做baseline的方法，主要是快啊～/斜眼笑 GloVe+lr因为我目前的带标签数据比较少，所以之前一直没有敢用word2vec去向量化作死，但是GloVe不存在这个问题啊，我就美滋滋的进行了一波。首先，先讲下GloVe的使用： https://github.com/stanfordnlp/GloVe 在最大的代码抄袭网站下载(git clone)坦福大佬的代码，友情提醒，不要作死自己看了理论就觉得自己会写，自己搞个GloVe。(别问我是怎么知道的) cd到对应目录下，vim demo.sh这个文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#!/bin/bashset -e# Makes programs, downloads sample data, trains a GloVe model, and then evaluates it.# One optional argument can specify the language used for eval script: matlab, octave or [default] python# 请把make这边注释掉，这个是让你去下个demo，我们直接改成自己的数据# make# if [ ! -e text8 ]; then# if hash wget 2&gt;/dev/null; then# wget http://mattmahoney.net/dc/text8.zip# else# curl -O http://mattmahoney.net/dc/text8.zip# fi# unzip text8.zip# rm text8.zip# fi# CORPUS需要对应自己的欲训练的文档CORPUS=content.txtVOCAB_FILE=vocab.txtCOOCCURRENCE_FILE=cooccurrence.binCOOCCURRENCE_SHUF_FILE=cooccurrence.shuf.binBUILDDIR=buildSAVE_FILE=vectorsVERBOSE=2MEMORY=4.0# 单词至少出现几次VOCAB_MIN_COUNT=3# 向量长度VECTOR_SIZE=128# 迭代次数MAX_ITER=30# 窗口长度WINDOW_SIZE=15BINARY=2NUM_THREADS=8X_MAX=10echoecho "$ $BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE &lt; $CORPUS &gt; $VOCAB_FILE"$BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE &lt; $CORPUS &gt; $VOCAB_FILEecho "$ $BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE &lt; $CORPUS &gt; $COOCCURRENCE_FILE"$BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE &lt; $CORPUS &gt; $COOCCURRENCE_FILEecho "$ $BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE &lt; $COOCCURRENCE_FILE &gt; $COOCCURRENCE_SHUF_FILE"$BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE &lt; $COOCCURRENCE_FILE &gt; $COOCCURRENCE_SHUF_FILEecho "$ $BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE"$BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSEif [ "$CORPUS" = 'text8' ]; thenif [ "$1" = 'matlab' ]; thenmatlab -nodisplay -nodesktop -nojvm -nosplash &lt; ./eval/matlab/read_and_evaluate.m 1&gt;&amp;2 elif [ "$1" = 'octave' ]; thenoctave &lt; ./eval/octave/read_and_evaluate_octave.m 1&gt;&amp;2elseecho "$ python eval/python/evaluate.py"python eval/python/evaluate.pyfifi 这边多说一下，CORPUS=content.txt这边content.txt里面的格式需要按照空格为分隔符进行存储，我之前一直以为是\t。 直接sh demo.sh，你会得到vectors.txt，这个里面就对应每个词的向量表示 1234天气 -0.754142 0.386905 -1.200074 -0.587121 0.758316 0.373824 0.342211 -1.275982 -0.300846 0.374902 -0.548544 0.595310 0.906426 0.029255 0.549932 -0.650563 -0.425185 1.689703 -1.063556 -0.790254 -1.191287 0.841529 1.080641 -0.082830 1.062107 -0.667727 0.573955 -0.604460 -0.601102 0.615299 -0.470923 0.039398 1.110345 1.071094 0.195431 -0.155259 -0.781432 0.457884 1.093532 -0.188207 -0.161646 0.246220 -0.346529 0.525458 0.617904 -0.328059 1.374414 1.020984 -0.959817 0.670894 1.091743 0.941185 0.902730 0.609815 0.752452 1.037880 -1.522382 0.085098 0.152759 -0.562690 -0.405502 0.299390 -1.143145 -0.183861 0.383053 -0.013507 0.421024 0.025664 -0.290757 -1.258696 0.913482 -0.967165 -0.131502 -0.324543 -0.385994 0.711393 1.870067 1.349140 -0.541325 -1.060084 0.078870 0.773146 0.358453 0.610744 0.407547 -0.552853 1.663435 0.120006 0.534927 0.219279 0.682160 -0.631311 1.071941 -0.340337 -0.503272 0.150010 1.347857 -1.024009 -0.181186 0.610240 -0.218312 -1.120266 -0.486539 0.264507 0.266192 0.347005 0.172728 0.613503 -0.131925 -0.727304 -0.504488 1.773406 -0.700505 -0.159963 -0.888025 -1.358476 0.540589 -0.243272 -0.236959 0.391855 -0.133703 -0.071120 1.050547 -1.087613 -0.467604 1.779341 -0.449409 0.949411好了 1.413075 -0.226177 -2.024229 -0.192003 0.628270 -1.227394 -1.054946 -0.900683 -1.958882 -0.133343 -1.014088 -0.434961 0.026207 -0.066139 0.608682 -0.362021 0.314323 0.261955 -0.571414 1.738899 -1.013223 0.503853 -0.536511 -0.212048 0.611990 -0.627851 0.297657 -0.187690 -0.565871 -0.234922 -0.845875 -0.767733 0.032470 1.508012 -0.204894 -0.495031 -0.159262 0.181380 0.050582 -0.333469 0.454832 -2.091174 0.448453 0.940212 0.882077 -0.617093 0.616782 -0.993445 -0.385087 0.251711 0.259918 -0.222614 -0.595131 0.661472 0.194740 0.619222 -1.253610 -0.838179 0.781428 -0.396697 -0.530109 0.022801 -0.558296 -0.656034 0.842634 -0.105293 0.586823 -0.603681 -0.605727 -0.556468 0.924275 -0.299228 -1.121538 0.237787 0.498935 -0.045423 0.171536 -1.026385 -0.262225 0.390662 1.263240 0.352172 0.261121 0.915840 1.522183 -0.498536 2.046169 0.012683 -0.073264 -0.361662 0.759529 -0.713268 0.281747 -0.811104 -0.002061 -0.802508 0.520559 0.092275 -0.623098 0.199694 -0.134896 -1.390617 0.911266 -0.114067 1.274048 1.108440 -0.266002 1.066987 0.514556 0.144796 -0.606461 0.197114 0.340205 -0.400785 -0.957690 -0.327456 1.529557 -1.182615 0.431229 -0.084865 0.513266 -0.022768 -0.092925 -0.553804 -2.269741 -0.078390 1.376199 -1.163337随意 0.410436 0.776917 -0.381131 0.969900 -0.804778 -0.785379 -0.887346 -1.463543 -1.574851 0.313285 0.685253 -0.918359 0.199073 -0.305374 -0.642721 0.098114 -0.723331 0.353159 0.042807 0.369208 -1.534930 -0.084871 0.020417 -0.384782 0.276833 -0.160028 1.107051 0.884343 -0.204381 -0.459738 -0.387128 0.125867 0.093569 1.192471 -0.473752 -0.314541 -1.029249 0.481447 1.358753 -1.688778 -0.113080 -0.401443 -0.958206 0.605638 1.083126 0.131617 0.092507 0.476506 0.801755 1.096883 -0.102036 0.461804 0.820297 -0.104053 -0.126638 0.957708 -0.722038 0.223686 0.583582 0.201246 -1.254708 0.770717 -1.271523 -0.584094 -1.142426 1.066567 0.071951 -0.182649 0.014365 -0.577141 0.037340 -0.166832 -0.247827 0.165994 1.143665 -0.258421 -0.335195 0.170218 -0.212838 0.013709 0.088847 0.663238 -0.597439 0.632847 0.370871 0.652707 0.306935 0.195127 -0.252443 0.588479 0.191633 -1.587564 0.564600 -0.306158 -0.648177 -0.488595 1.532795 -0.462473 -0.643878 1.292369 -0.051494 -1.032738 0.453587 0.411327 -0.469373 0.428398 -0.020839 0.307422 0.518331 -0.860913 -2.170098 -0.277532 -0.966210 0.615336 -0.924783 0.042679 1.289640 1.272992 1.367773 0.426600 -0.187254 -0.781009 1.331301 -0.088357 -1.113550 -0.262879 0.300137 0.437905.. 有了每个词的向量，我们这边采取了借鉴YoutubeNet网络的想法： 举个例子：存在一句话”我爱中国”，“我”的向量是[0.3,0.2,0.3]，”爱”的向量是[0.1,0.2,0.3]，“中国”的向量是[0.6,0.6,0.4]，那么average后就是[0.33,0.33,0.33]，然后这就类似一个特征为三的input。 这种方法的好处就是快捷，预处理的工作代价要小，随着数据量的增多，模型的效果要更加的好。 效果对比最后这边粗略的给出一下业务数据对比： experiment date intercepted_recall 3-grams 20180915 79.3% 3-grams 20180917 78.7% 3-grams+bayes+lr 20180915 83.4% 3-grams+bayes+lr 20180917 88.6% gloVe+lr 20180915 93.1% gloVe+lr 20180917 93.9% 欢迎大家关注我的个人bolg，知乎，相关代码已经上传到我的Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google团队在DNN的实际应用方式的整理]]></title>
    <url>%2F2018%2F08%2F29%2FGoogle%E5%9B%A2%E9%98%9F%E5%9C%A8DNN%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E6%96%B9%E5%BC%8F%E7%9A%84%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[很荣幸有机会和论文作者Emre Sargin关于之前发的Deep Neural Networks for YouTube Recommendations进行交流，梳理如下： 提问对话汇总： 如何进行负采样的？ 构造了千万量级热门视频集合，每个用户的负采样结果来源于这个集合，会有一些筛选的tricks，比如剔除浏览过的商品，负采样的数量Google在200万条。（也就是说，在计算loss的时候，google的label是一个200万长度的向量，瑟瑟发抖.jpg。） 推荐算法应用上，有什么评估方式和评估指标？ 主要基于线上进行小批量的abtest进行对比，在考虑ctr指标的同时也会综合全站的信息加以分析，同时对新颖程度和用户兴趣变换也是我们考察的对象。 冷启动的解决方式？从来没有被点击过的video如何处理？新上的video如何处理？ google的推荐基于多种推荐算法的组合，YouTubeNet主要解决的是热门商品的一个推荐问题，冷启动或者没有被点击的video会有其他算法进行计算。换句话说，解决不了。 example age如何定义？ user+vedio的组合形式，train过程中，是用户点击该vedio的时间距离当前时间的间隔；predict过程中，为0。该部分对模型的鲁棒性非常重要。 是否遇到神经元死亡的问题？ 有，解决方案很常规，都是大家了解的，降低learning_rate，使用batchnormalization。 是否预到过拟合？ 没，youtube的用户上亿，可以构造出上千亿的数据，过拟合的情况不明显。但是会存在未登录用户，我们会通过一些其他CRM类的算法补充构造出他们的基本信息，比如gender、age… vedio vector在哪边进行构造和修正？ history click部分进行vedio embedding，并进行修正。另外，50是我们尝试的历史点击长度，20-30也有不错的效果。 会有工程计算压力么？ 不存在，建议在GPU上计算，后面由于VPN网络信号抖动没听清，大概就说Google在训练模型的时候会有大量GPU支持，每天大概更新2-3模型，没有遇到什么计算瓶颈。 (以上为我个人针对提问结果的理解及总结) 个人感想如下：有钱任性 最后，我觉得算法还是要适应实际情况，大公司的方法可以借鉴但是可能很多时候抄不来，也没条件抄。 原问题如下（实际有删改）：How to do video embedding?Is there any pre-training?How to use example age in the model?How to deal dead ReLU neurons？How to sample negative classes？How does the video embedding generated？How to recommend the video never been clicked and new uploaded videos？How to do ab testing? What’s the metrics?Have you facing overfitting？How to solve it?There is any difficulty in calculating the embedding for millions of videos and users.During input embedding generation, are they simply averaged? 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stanford Word Segmenter问题整理]]></title>
    <url>%2F2018%2F08%2F27%2FSegmenter%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[最近在做一些nlp相关的项目，在涉及到Stanford CoreNLP工具包处理中文分词的时候，发现耗时问题很严重： Item time(s) jieba 0.4 snownlp 7.4 pynlpir 0.8 StanfordCoreNLP 21.5 pyltp 5.3 因为Stanford CoreNLP调用的是这个pipeline，而我们实际用的是切词功能，所以尝试只用它的切词部分功能，但是在做的过程中发现一些问题，整理如下： 官网给出的方法nltk.tokenize.stanford_segmenter module是这么写的： 123from nltk.tokenize.stanford_segmenter import StanfordSegmenterseg = StanfordSegmenter()seg.default_config('zh') 但是这个缺少各种数据路径的，是完全不通的。 然后度娘的top1的答案给出的解决方案是：`1segmenter = StanfordSegmenter(path_to_jar="stanford-segmenter-3.4.1.jar", path_to_sihan_corpora_dict="./data", path_to_model="./data/pku.gz", path_to_dict="./data/dict-chris6.ser.gz") 如果你的nltk的版本比较新，恭喜你，你会遇到下面这个问题：TypeError: expected str, bytes or os.PathLike object, not NoneType 我在stackoverflow上找了半天，发现有如下的解决方案：1234from nltk.parse.corenlp import CoreNLPParser corenlp_parser = CoreNLPParser('http://localhost:9001', encoding='utf8')result = corenlp_parser.api_call(text, &#123;'annotators': 'tokenize,ssplit'&#125;)tokens = [token['originalText'] or token['word'] for sentence in result['sentences'] 可以完美解决，原因之前作者也说了，据称升级版本后不兼容，各位看看就好“TypeError: expected str, bytes or os.PathLike object, not NoneType” about Stanford NLP 。 这个坑花了我两个多小时（主要在下载各种gz包），希望大家能够避免。 欢迎大家关注我的个人bolg，知乎，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码、转行疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>代码集合</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logistic regression一点理解]]></title>
    <url>%2F2018%2F08%2F14%2Fregression%E4%B8%80%E7%82%B9%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Hexo没有办法用Latex，所以采取了截图的方式，更舒适的阅读体验可以参见logistic regression一点理解。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>理论解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Tensorflow实现FFM]]></title>
    <url>%2F2018%2F08%2F06%2F%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0FFM%2F</url>
    <content type="text"><![CDATA[没错，这次登场的是FFM。各大比赛中的“种子”算法，中国台湾大学Yu-Chin Juan荣誉出品，美团技术团队背书，Michael Jahrer的论文的field概念灵魂升华，土豪公司鉴别神器。通过引入field的概念，FFM把相同性质的特征归于同一个field，相当于把FM中已经细分的feature再次拆分，可不可怕，厉不厉害？好，让我们来看看怎么一个厉害法。 FFM理论特征交互网上已经说烂了的美团技术团队给出的那张图：针对Country这个变量，FM的做法是one-hot-encoding，生成country_USA，country_China两个稀疏的变量，再进行embedding向量化。FFM的做法是cross-one-hot-encoding，生成country_USA_Day_26/11/15，country_USA_Ad_type_Movie…M个变量，再进行embedding向量化。就和上图一样，fm做出来的latent factor是二维的，就是给每个特征一个embedding结果；而暴力的ffm做出的latent factor是三维的，出来给特征embedding还考虑不同维度特征给不同的embedding结果，也是FFM中“field-aware”的由来。同时从公式中看，对于xi这个特征为什么embedding的latent factor向量的V是Vifj，其实就是因为xi乘以的是xj，所以latent factor向量的信息提取才是field j，也就是fj。多说一句，网上很多给出的现成的代码，这边都是写错了的，都写着写着变成了vifi，可能是写的顺手。 都说到这里了，我再多说一句，为什么说ffm是土豪公司鉴别神器呢？我们看下仅仅是二次项，ffm需要计算的参数有 nfk 个，远多于FM模型的 nk个，而且由于每次计算都依赖于乘以的xj的field，所以，无法用fm的那个计算技巧(ab = 1/2(a+b)^2-a^2-b^2)，所以计算复杂度是 O(kn^2)。这种情况下，没有GPU就不要想了，有GPU的特征多于50个，而且又很离散的，没有个三位数的GPU也算了。之前我看美团说他们在用，我想再去看看他们的实际用的过程的时候，发现文章被删了，真的是可惜，我其实一直也想知道如何减轻这个变态的计算量的方法。 给个实例给大家看下以上的这些的应用：依旧来自美团技术研发团队中给出的案例，有用户数据如下：这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。红色部分对应的是field，来自于原始特征的个数；蓝色部分对应的是feature，来自于原始特征onehot之后的个数。对于特征Feature:User=YuChin而言，有Movie=3Idiots、Genre=Comedy、Genre=Drama、Price四项要交互：User=YuChin与Movie=3Idiots交互是·1·1，也就是第一项，为什么是V1,2呢？因为User=YuChin是Featureindex=1，而交互的Movie=3Idiots是Fieldindex=2，同理V2,1也是这样的，以此类推，那么，FFM的组合特征有10项，如下图所示：这就是一个案例的实际操作过程。 特征处理为什么要把这个单拎出来说呢？我看了网上不少的对于特征的处理过程，版本实在是太多了，而且差异化也蛮大，这边就和大家一起梳理一下：1.feature index * feature value这个就是上面我这个实际案例的方式，对于分类变量采取onehot，对于连续变量之间进行值的点积，不做处理。优点是快速简单，不需要预处理，但是缺点也很明显，离群点影响，值的波动大等。 2.连续值离散化这个方法借鉴了Cart里面对连续值的处理方式，就是把所有的连续值都当成一个分类变量处理。举例，现在有一个年龄age的连续变量[10,19,20,22,22,34]，这种方法就生成了age_10,age_19,age_20,age_22,age_34这些变量，如果连续值一多，这个方法带来的计算量就直线上升。 3.分箱下的连续值离散化这种方法优化了第二种方法，举例解释，现在有一个年龄age的连续变量[10,19,20,22,22,34]，我们先建立一个map，[0,10):0,[10,20):1,[20,30):2,[30,100):3。原始的数据就变成了[1,1,2,2,2,3]，再进行2的连续值离散化方法，生成了age_1,age_2,age_3这几个变量，优化了计算量，而且使得结果更具有解释性。 损失函数logisitc loss这个是官方指定的方法，是-1/1做二分类的时候常用的loss计算方法：这边需要注意的是，在做的时候，需要把label拆分成-1/1而不是0/1，当我们预测正确的时候，predlabel&gt;0且越大正确的程度越高，相应的log项是越小的，整体loss越小；相反，如果我们预测的越离谱，predlabel&lt;0且越小离谱的程度越高，相应的log项是越大的，整体loss越大。 交互熵我看到很多人的实现依旧用了tf.nn.softmax_cross_entropy_with_logits，其实就是多分类中的损失函数，和大家平时的图像分类、商品推荐召回一模一样：这边需要注意的是，在做的时候，需要把label拆分成[1,0]和[0,1]进行计算。不得不说，大家真的是为了省事很机智(丧心病狂)啊！ 代码实现我这边只给一些关键地方的代码，更多的去GitHub里面看吧。 embedding part1self.v = tf.get_variable('v', shape=[self.p, self.f, self.k], dtype='float32',initializer=tf.truncated_normal_initializer(mean=0, stddev=0.01)) 看到了，这边生成的v就是上面Vffm的形式。 inference part12345678910111213for i in range(self.p):# 寻找没有match过的特征，也就是论文中的j = i+1开始for j in range(i + 1, self.p):print('i:%s,j:%s' % (i, j))# vifjvifj = self.v[i, self.feature2field[j]]# vjfivjfi = self.v[j, self.feature2field[I]]# vi · vjvivj = tf.reduce_sum(tf.multiply(vifj, vjfi))# xi · xjxixj = tf.multiply(self.X[:, i], self.X[:, j])self.field_cross_interaction += tf.multiply(vivj, xixj) 我这边强行拆开了写，这样看起来更清晰一点，注意这边的vifj和vjfi的生成，这边也可以看到找对于的field的方法是用了filed这个字典，这就是为什么不能用fm的点击技巧。 loss part12# -1/1情况下的logistic lossself.loss = tf.reduce_mean(tf.log(1 + tf.exp(-self.y * self.y_out))) 这边记得论文中的负号，如果有batch的情况下记得求个平均再进行bp过程。 论文结论原始的ffm论文中给出了一些结论，我们在实际使用中值得参考： k值不用太大，没啥提升 正则项lambda和学习率alpha需要着重调参 epoch别太大，既会拖慢速度，而且造成过拟合；在原论文中甚至要考虑用early-stopping避免过拟合，所以epoch=1，常规的来讲就可以了，论文中提到的early-stopping操作：12345671. Split the data set into a training set and a validation set.2. At the end of each epoch, use the validation set to calcu-late the loss.3. If the loss goes up, record the number of epochs. Stop orgo to step 4.4. If needed, use the full data set to re-train a model withthe number of epochs obtained in step 3. 总结FFM是一个细化隐向量非常好的方法，虽然很简单，但还是有很多细节之处值得考虑，比如如何线上应用，如何可解释，如何求稀疏解等等。在部署实现FFM之前，我还是建议大家先上线FM，当效果真的走投无路的时候再考虑FFM，FFM在工业界的影响着实不如学术界那么强大，偷偷说一句，太慢了，真的是太慢了，慢死了，我宁可去用deepfm。 最后，给出代码实现的Github地址FFM，这边是我自己写的，理解理解算法可以，但是实际用的时候建议参考FFM的实现比较好的项目比如libffm，最近比较火的xlearn。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征交叉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Tensorflow实现DeepFM]]></title>
    <url>%2F2018%2F07%2F30%2F%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0DeepFM%2F</url>
    <content type="text"><![CDATA[前言DeepFM，Ctr预估中的大杀器，哈工大与华为诺亚方舟实验室荣耀出品，算法工程师面试高频考题，有效的结合了神经网络与因子分解机在特征学习中的优点：同时提取到低阶组合特征与高阶组合特征，这样的称号我可以写几十条出来，这也说明了DeepFM确实是一个非常值得手动撸一边的算法。 当然，早就有一票人写了一车封装好的deepFM的模型，大家随便搜搜肯定也能搜到，既然这样，我就不再搞这些东西了，今天主要和大家过一遍，deepFM的代码是咋写的，手把手入门一下，说一些我觉得比较重要的地方，方便大家按需修改。（只列举了一部分，更多的解释参见GitHub代码中的注释） 本文的数据和部分代码构造参考了nzc大神的deepfm的Pytorch版本的写法，改成tensorflow的形式，需要看原版的自取。 网络结构 DeepFM包含两部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取，两部分权重共享。DeepFM的预测结果可以写为：y = sigmoid(y(fm)+y(DNN)) FM部分 FM公式为： 我很久之前一篇文章细节讲过，这边就不多扯了，更多详见FM理论解析及应用。 DNN部分 这边其实和我上篇文章说的MLPS差距不大，也就是简单的全链接，差就差在input的构造，这边采取了embedding的思想，将每个feature转化成了embedded vector作为input，同时此处的input也是上面计算FM中的V，更多的大家看代码就完全了解了。 代码部分我一共写了两个script，build_data.py和deepfm.py，也很好理解。build_data.py用来预处理数据，deepfm.py用来跑模型。 build_data.py1234567891011121314151617181920212223242526for i in range(1, data.shape[1]):target = data.iloc[:, I]col = target.namel = len(set(target))if l &gt; 10:target = (target - target.mean()) / target.std()co_feature = pd.concat([co_feature, target], axis=1)feat_dict[col] = cntcnt += 1co_col.append(col)else:us = target.unique()print(us)feat_dict[col] = dict(zip(us, range(cnt, len(us) + cnt)))ca_feature = pd.concat([ca_feature, target], axis=1)cnt += len(us)ca_col.append(col)feat_dim = cntfeature_value = pd.concat([co_feature, ca_feature], axis=1)feature_index = feature_value.copy()for i in feature_index.columns:if i in co_col:feature_index[i] = feat_dict[I]else:feature_index[i] = feature_index[i].map(feat_dict[I])feature_value[i] = 1. 核心部分如上，重要的是做了两件事情，生成了feature_index和feature_value。 feature_index是把所有特征进行了标序，feature1，feature2……featurem，分别对应0，1，2，3，…m，但是，请注意分类变量需要拆分！就是说如果有性别：男|女|未知，三个选项。需要构造feature男，feature女，feature未知三个变量，而连续变量就不需要这样。 feature_value就是特征的值，连续变量按真实值填写，分类变量全部填写1。 更加形象的如下： deepfm.py特征向量化123456789# 特征向量化，类似原论文中的vself.weight['feature_weight'] = tf.Variable(tf.random_normal([self.feature_sizes, self.embedding_size], 0.0, 0.01),name='feature_weight')# 一次项中的w系数，类似原论文中的wself.weight['feature_first'] = tf.Variable(tf.random_normal([self.feature_sizes, 1], 0.0, 1.0),name='feature_first') 可以对照下面的公式看，更有感觉。 deep网络部分的weight123456789101112131415161718192021222324252627# deep网络初始input：把向量化后的特征进行拼接后带入模型，n个特征*embedding的长度input_size = self.field_size * self.embedding_sizeinit_method = np.sqrt(2.0 / (input_size + self.deep_layers[0]))self.weight['layer_0'] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(input_size, self.deep_layers[0])), dtype=np.float32)self.weight['bias_0'] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(1, self.deep_layers[0])), dtype=np.float32)# 生成deep network里面每层的weight 和 biasif num_layer != 1:for i in range(1, num_layer):init_method = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[I]))self.weight['layer_' + str(i)] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(self.deep_layers[i - 1], self.deep_layers[i])),dtype=np.float32)self.weight['bias_' + str(i)] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(1, self.deep_layers[i])),dtype=np.float32)# deep部分output_size + 一次项output_size + 二次项output_sizelast_layer_size = self.deep_layers[-1] + self.field_size + self.embedding_sizeinit_method = np.sqrt(np.sqrt(2.0 / (last_layer_size + 1)))# 生成最后一层的结果self.weight['last_layer'] = tf.Variable(np.random.normal(loc=0, scale=init_method, size=(last_layer_size, 1)), dtype=np.float32)self.weight['last_bias'] = tf.Variable(tf.constant(0.01), dtype=np.float32) input的地方需要注意一下，这边用了个技巧，直接把把向量化后的特征进行拉伸拼接后带入模型，原来的v是batch*n个特征*embedding的长度，直接改成了batch*（n个特征*embedding的长度），这样的好处就是全值共享，又快又有效。 网络传递部分都是一些正常的操作，稍微要注意一下的是交互项的计算：12345678910111213141516# second_orderself.sum_second_order = tf.reduce_sum(self.embedding_part, 1)self.sum_second_order_square = tf.square(self.sum_second_order)print('sum_square_second_order:', self.sum_second_order_square)# sum_square_second_order: Tensor("Square:0", shape=(?, 256), dtype=float32)self.square_second_order = tf.square(self.embedding_part)self.square_second_order_sum = tf.reduce_sum(self.square_second_order, 1)print('square_sum_second_order:', self.square_second_order_sum)# square_sum_second_order: Tensor("Sum_2:0", shape=(?, 256), dtype=float32)# 1/2*((a+b)^2 - a^2 - b^2)=abself.second_order = 0.5 * tf.subtract(self.sum_second_order_square, self.square_second_order_sum)self.fm_part = tf.concat([self.first_order, self.second_order], axis=1)print('fm_part:', self.fm_part) 直接实现了下面的计算逻辑： loss部分我个人重写了一下我认为需要正则的地方，和一些loss的计算方式：123456789101112# lossself.out = tf.nn.sigmoid(self.out)# loss = tf.losses.log_loss(label,out) 也行，看大家想不想自己了解一下loss的计算过程self.loss = -tf.reduce_mean(self.label * tf.log(self.out + 1e-24) + (1 - self.label) * tf.log(1 - self.out + 1e-24))# 正则：sum(w^2)/2*l2_reg_rate# 这边只加了weight，有需要的可以加上bias部分self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg_rate)(self.weight["last_layer"])for i in range(len(self.deep_layers)):self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg_rate)(self.weight["layer_%d" % I]) 大家也可以直接按照我注释掉的部分简单操作，看个人的理解了。 梯度正则12345678self.global_step = tf.Variable(0, trainable=False)opt = tf.train.GradientDescentOptimizer(self.learning_rate)trainable_params = tf.trainable_variables()print(trainable_params)gradients = tf.gradients(self.loss, trainable_params)clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)self.train_op = opt.apply_gradients(zip(clip_gradients, trainable_params), global_step=self.global_step) 很多网上的代码跑着跑着就NAN了，建议加一下梯度的正则，反正也没多复杂。 执行结果1234567891011121314151617181920212223242526272829/Users/slade/anaconda3/bin/python /Users/slade/Documents/Personalcode/machine-learning/Python/deepfm/deepfm.py[2 1 0 3 4 6 5 7][0 1 2][6 0 8 2 4 1 7 3 5 9][2 3 1 0]W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.embedding_part: Tensor("Mul:0", shape=(?, 39, 256), dtype=float32)first_order: Tensor("Sum:0", shape=(?, 39), dtype=float32)sum_square_second_order: Tensor("Square:0", shape=(?, 256), dtype=float32)square_sum_second_order: Tensor("Sum_2:0", shape=(?, 256), dtype=float32)fm_part: Tensor("concat:0", shape=(?, 295), dtype=float32)deep_embedding: Tensor("Reshape_2:0", shape=(?, 9984), dtype=float32)output: Tensor("Add_3:0", shape=(?, 1), dtype=float32)[&lt;tensorflow.python.ops.variables.Variable object at 0x10e2a9ba8&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112885ef0&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3c18&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3da0&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3f28&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x1129b3c50&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112a03dd8&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112a03b38&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x16eae5c88&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x112b937b8&gt;]time all:7156epoch 0:the times of training is 0, and the loss is 8.54514the times of training is 100, and the loss is 1.60875the times of training is 200, and the loss is 0.681524the times of training is 300, and the loss is 0.617403the times of training is 400, and the loss is 0.431383the times of training is 500, and the loss is 0.531491the times of training is 600, and the loss is 0.558392the times of training is 800, and the loss is 0.51909... 看了下没啥大问题。 还有一些要说的 build_data.py中我为了省事，只做了标准化，没有进行其他数据预处理的步骤，这个是错误的，大家在实际使用中请按照我在公众号里面给大家进行的数据预处理步骤进行，这个非常重要！ learing_rate是我随便设置的，在实际大家跑模型的时候，请务必按照1.0，1e-3，1e-6，三个节点进行二分调优。 如果你直接搬上面代码，妥妥过拟合，请在真实使用过程中，务必根据数据量调整batch的大小，epoch的大小，建议在每次传递完成后加上tf.nn.dropout进行dropout。 如果数据量连10万量级都不到，我还是建议用机器学习的方法，xgboost+lr，mixed logistics regression等等都是不错的方法。 好了，最后附上全量代码的地址Github，希望对大家有所帮助。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>特征交叉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Tensorflow实现多层感知机网络MLPs]]></title>
    <url>%2F2018%2F07%2F25%2F%E5%9F%BA%E4%BA%8ETensorflow%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%BD%91%E7%BB%9CMLPs%2F</url>
    <content type="text"><![CDATA[之前在基于Tensorflow的神经网络解决用户流失概率问题写了一个MLPs的网络，很多人在问，其实这个网络看起来很清晰，但是却写的比较冗长，这边优化了一个版本更方便大家修改后直接使用。 直接和大家过一遍核心部分： 上次我们计算过程中，通过的是先定义好多层网络中每层的weight，在通过tf.matual进行层与层之间的计算，最后再通过tf.contrib.layers.l2_regularizer进行正则；而这次我们直接通过图像识别中经常使用的全连接（FC）的接口，只需要确定每层的节点数，通过layers_nodes进行声明，自动可以计算出不同层下的weight，更加清晰明了。另外，还增加了dropout的部分，降低过拟合的问题。 123456din_all = tf.layers.batch_normalization(inputs=din_all, name='b1')layer_1 = tf.layers.dense(din_all, self.layers_nodes[0], activation=tf.nn.sigmoid,use_bias=True,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name='f1')layer_1 = tf.nn.dropout(layer_1, keep_prob=self.drop_rate[0])layer_2 = tf.layers.dense(layer_1, self.layers_nodes[1], activation=tf.nn.sigmoid,use_bias=True,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name='f2')layer_2 = tf.nn.dropout(layer_2, keep_prob=self.drop_rate[1])layer_3 = tf.layers.dense(layer_2, self.layers_nodes[2], activation=tf.nn.sigmoid,use_bias=True,kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularzation_rate),name='f3') tf.layers.dense接口信息如下：12345678910111213141516tf.layers.dense(inputs,units,activation=None,use_bias=True,kernel_initializer=None,bias_initializer=tf.zeros_initializer(),kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,trainable=True,name=None,reuse=None) inputs：必需，即需要进行操作的输入数据。 units：必须，即神经元的数量。 activation：可选，默认为 None，如果为 None 则是线性激活。 use_bias：可选，默认为 True，是否使用偏置。 kernel_initializer：可选，默认为 None，即权重的初始化方法，如果为 None，则使用默认的 Xavier 初始化方法。 bias_initializer：可选，默认为零值初始化，即偏置的初始化方法。 kernel_regularizer：可选，默认为 None，施加在权重上的正则项。 bias_regularizer：可选，默认为 None，施加在偏置上的正则项。 activity_regularizer：可选，默认为 None，施加在输出上的正则项。 kernel_constraint，可选，默认为 None，施加在权重上的约束项。 bias_constraint，可选，默认为 None，施加在偏置上的约束项。 trainable：可选，默认为 True，布尔类型，如果为 True，则将变量添加到 GraphKeys.TRAINABLE_VARIABLES 中。 name：可选，默认为 None，卷积层的名称。 reuse：可选，默认为 None，布尔类型，如果为 True，那么如果 name 相同时，会重复利用。 除此之外，之前我们定义y和y_的时候把1转化为[1,0]，转化为了[0,1]，增加了工程量，这次我们通过： 12cross_entropy_mean = -tf.reduce_mean(self.y_ * tf.log(self.output + 1e-24))self.loss = cross_entropy_mean 直接进行计算，避免了一些无用功。 最后，之前对于梯度的值没有进行限制，会导致整体模型的波动过大，这次优化中也做了修改，如果大家需要也可以参考一下：123456# 我们用learning_rate_base作为速率η，来训练梯度下降的loss函数解，对梯度进行限制后计算lossopt = tf.train.GradientDescentOptimizer(self.learning_rate_base)trainable_params = tf.trainable_variables()gradients = tf.gradients(self.loss, trainable_params)clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)self.train_op = opt.apply_gradients(zip(clip_gradients, trainable_params), global_step=self.global_step) MLPs是入门级别的神经网络算法，实际的工业开发中使用的频率也不高，后面我准备和大家过一下常见的FM、FFM、DeepFM、NFM、DIN、MLR等在工业开发中更为常见的网络，欢迎大家持续关注。 完整代码已经上传到Github中。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伪标签半监督学习]]></title>
    <url>%2F2018%2F07%2F24%2F%E4%BC%AA%E6%A0%87%E7%AD%BE%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[之前在训练YoutubeNet和DCN的时候，我都发现平台用户中基础用户的信息数据缺失率特别高，比如性别一栏准确填写的不足60%，所以我一直想调研一下有没有什么更好的填充方法，要保证既不能太复杂太耗时，也要有足够好的效果。 其实这个问题就是一个缺失值填充，之前的文章中也写过很多办法，常规的也总结过： 均值、众数填充最简单的填充，效果也惨不忍睹 根据没有缺失的数据线性回归填充填充的好会造成共线性错误，填充的不好就没价值，很矛盾 剔除丢失信息量 设置哑变量会造成数据分布有偏 smote连续值有效，离散值就无法实施了 我在Google上看imbalance问题的时候，偶然看到了这个视频教程，上面讲了图像的缺失处理，提到了伪标签处理的半监督学习方式。我就在国内的论坛上找了下，阿里云技术论坛也同样注意到了这个问题，但是只给出了如下的粗糙的构思图： 有一份整理了的流程图，具体执行步骤总结，和大家一起看一下： 将有标签部分数据分为两份：train_set&amp;validation_set，并训练出最优的model1 用model1对未知标签数据(test_set)进行预测，给出伪标签结果pseudo-labeled 将train_set中抽取一部分做新的validation_set，把剩余部分与pseudo-labeled部分融合作为新的train_set，训练出最优的model2 再用model2对未知标签数据(test_set)进行预测，得到最终的final result label 我利用了已知标签的数据对这个方法进行测试，用了最简单的mixed logistic regression模型作为Basic Model，得到结果如下：利用伪标签半监督的方式，同样的mixed logistic regression模型AUC值会提高0.1pp左右，效果还不错，而且实施并不复杂，大家可以在缺失值处理或者分类问题中应用尝试一下。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我。]]></content>
      <categories>
        <category>特征刻画</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[热传导-物质扩散算法应用于推荐]]></title>
    <url>%2F2018%2F07%2F19%2F%E7%83%AD%E4%BC%A0%E5%AF%BC-%E7%89%A9%E8%B4%A8%E6%89%A9%E6%95%A3%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E4%BA%8E%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[没有大量的数据，没有大量的人力就不能做好推荐么？当然不是，热传导/物质扩散推荐算法就是作为冷启动及小规模团队非常实用的推荐召回部分的算法。 目标是为a图中标有星号（不妨记为用户1）的用户推荐商品，该用户已经购买过的两件商品是我们可以利用的信息，用来给目标用户进行推荐。 物质扩散算法：初始，我们认为每件被目标用户购买过的商品的信息量为1。商品把自己的信息平均分给所有购买过它的用户，用户的信息值则是从所有商品所得到的信息值得总和，比如上图(b)中的第一个节点的信息就等于第一个商品平均分给三个用户的的平均信息1/3，再加上第四个商品平均分给两个用户的平均信息1/2，即为1/3+1/2=5/6；接下来，每一个用户再把自己的信息平均分给所有购买过的商品，商品的信息则是从所有用户收到的信息值得总和，如对于图(c)中的第一个商品，它的信息值就等于第一个用户信息值的一半，为5/12，加上第二个用户信息值的1/4，为5/24，再加上第三个用户信息值得一半，为1/6，总的能量值即为:5/12+5/24+1/6=19/24。 以上两个步骤加起来为从商品到商品信息扩散一步。针对大规模系统的推荐，为了保持实时性和效率，往往只需扩散三步以内。如果以一步为界，基于图(c)中的结果，则在目标用户没有购买过的所有商品中，第三个商品的信息值最大，因此基于物质扩散算法的推荐系统则会将此商品推荐给目标用户，同时可以得到对于用户1的商品得分排序，自然可以得到用户召回集。值得注意的是物质扩散这种算法得到的所有商品最后的信息值之和就等于初始时所有商品的信息值，即能量是守恒的，图(c)中所有商品的信息之和仍为2。 热传导算法： 初始，我们认为目标用户购买过的每件商品的信息量为1。目标用户的信息等于所有他购买过的商品信息的平均值，如图(d)所示，目标用户购买了商品1和商品4，则该用户的信息值即为(1 + 1) / 2 = 1。再根据目标用户浏览过的商品给所以商品计算信息，第一个商品、第四个商品信息量为1/2，其他商品的信息量为0（因为目标用户没有买过），接下来根据每一个商品的信息计算其他的用户的信息，如图(d)中的第二个用户的信息就为商品1,2,3,4的信息的平均值（1/2 + 1/2）/4 = 1/2；再根据每个用户的信息量平均分配信息到每个商品，如图(e)中的第一个商品来自第一个、第二个、第三个用户的信息的和，即为1/21/2+1/21/3+1/2*/12=2/3。 以上两个步骤加起来为从商品到商品热传导一步。因此基于热传导算法的推荐系统则会将此信息量大的商品推荐给目标用户，同时可以得到对于用户1的商品得分排序，自然可以得到用户召回集。与物质扩散不同的是这种算法得到的所有商品最后的信息值之和就不一定等于初始时所有商品的信息值，即不满足守恒定律，这是因为在信息传到的第二步过程中，有的用户的信息可能会被多次计算，从而导致不守恒。 基于物质扩散和基于热传导的推荐算法的区别在于： 基于物质扩散的方法在进行个性化推荐时，系统的总信息是守恒的；而热传导在推荐过程中，目标用户（即被推荐用户）的收藏品将被视作信息初始点，负责提供能量，所以系统的总信息量随着传递步骤的增加是在不断增加的。 如果对物理比较熟悉的朋友很容易联想到凸透镜和凹透镜，是的，我个人在理解的时候也是这样迁移理解，原理上确实一致。 基于物质扩散的方法相当于凸透镜一样把用户历史点击的信息聚焦到了少量优势的skn上了； 基于热传导的方法相当于是凹透镜一样把用户的历史点击信息发散到了那些较不流行的物品上，从而提高了推荐的新颖多样性。 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[量化评估推荐系统效果]]></title>
    <url>%2F2018%2F07%2F19%2F%E9%87%8F%E5%8C%96%E8%AF%84%E4%BC%B0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%95%88%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[推荐系统最有效的方法就是A/B test进行模型之间的对比，但是由于现实原因的局限，存在现实实时的困难性，所以，梳理了一些可以补充替代的指标如下，但是离线评估也存在相应的问题：1) 数据集的稀疏性限制了适用范围，用户之间的交集稀疏。2) 评价结果的客观性，由于用户的主观性，不管离线评测的结果如何，都不能得出用户是否喜欢某推荐系统的结论，只是一个近似的评估。3) 深度评估指标的缺失。(如点击深度、购买客单价、购买商品类别、购买偏好)之间的关联关系。4）冷启动5）Exploration 和 Exploitation问题 离线模型之间的评估召回集测试 recall命中skn个数/用户真实点击skn个数 precision命中skn个数/所有预测出来的skn总数 F1-Measure2/(1/recall+1/precison) 交互熵 MAE RMSE 相关性常见的比如：Pearson、Spearman和Kendall’s Tau相关，其中Pearson是更具数值之间的相似度，Spearman是根据数值排序之间的相似度，Kendall’s Tau是加权下的数值排序之间的相似度。 基尼系数 信息熵 排序部分测试 NDCG（Normalize DCG） RBP（rank-biased precision） RBP和NDCG指标的唯一不同点在于RBP把推荐列表中商品的浏览概率p按等比数列递减，而ND CG则是按照log调和级数形式。 离线模型与在线模型之间的评估很多时候，我们需要确定离线模型的效果足够的健壮才能允许上线进行线上测试，那如何进行离线模型与线上模型的评估对比就是一个比较复杂的问题。 难点 缺乏公平的测试数据实际处理过程中，我们发现，所有的已知点击都是来自线上模型推荐的结果，所以极端情况下，线上的recall是100% 缺乏公认的衡量指标在线下对比中，我们发现比如recall、precision、F1-Measure等指标都是大家约定俗成的，不存在很大的争议，而离线在线模型对比却没有一个准确公认的衡量指标 指标设计 online_offline_cover_rate&amp;first_click_hit_rate 这一组指标是结合在一起看的，其中online_offline_cover_rate是指针对每一个用户计算理线模型推荐的商品与在线模型推荐的商品的重合个数/在线模型的推荐商品个数，online_offline_cover_rate越低代表离线模型相对在线模型越独立；first_click_hit_rate是指offline模型对用户每天第一次点击的命中率，也就是命中次数/总统计用户数。结合这两个指标，我们可以得到在online_offline_cover_rate越低的情况下，却能覆盖线上用户真实点击的次数越多，代表offline模型的效果优于线上模型。 online_precision_rate/offline_precision_rate 离线模型的准确率和在线模型的准确率。这边在实际计算的时候采取了一个技巧，针对某个推荐位计算在线模型准确率的时候，用的是从来没有浏览过这个推荐位的用户的浏览历史匹配这个用户这个推荐位的推荐结果。这样可以避免用户的点击结果受到推荐位推荐结果影响的问题。 举个例子：用户在推荐位A上没有浏览过，他的点击是不受推荐位A推荐的商品影响的，拿这个用户推荐位A我们给他线上推荐的结果作为线上模型的推荐结果去计算，这样才更加合理。 online_recall_rate/offline_recall_rate 离线模型的召回率和在线模型的召回率。同上解释。 roi_reall/roi_precision 同上解释，只是把未来的点击作为match源更换成了加购物车、购买、收藏这些数据。 其他评估方向覆盖率推荐覆盖率越高， 系统给用户推荐的商品种类就越多 ，推荐多样新颖的可能性就越大。如果一个推荐算法总是推荐给用户流行的商品，那么它的覆盖率往往很低，通常也是多样性和新颖性都很低的推荐。 多样性采用推荐列表间的相似度（hamming distance、Cosine Method），也就是用户的推荐列表间的重叠度来定义整体多样性。 新颖性计算推荐列表中物品的平均流行度。 其他用户满意度、用户问卷、信任度、鲁棒性、实时性、 评测维度最后说一下评测维度分为如下3种，多角度评测： 用户维度主要包括用户的人口统计学信息、活跃度以及是不是新用户等。 物品维度包括物品的属性信息、流行度、平均分以及是不是新加入的物品等。 时间维度包括季节，是工作日还是周末，是白天还是晚上等。 附常规评价指标的整理结果(来自论文Evaluation Metrics for Recommender Systems)： 欢迎大家关注我的个人bolg，更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过公众号发消息给我哦。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐评估方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow在GPU下的Poolallocator Message]]></title>
    <url>%2F2018%2F06%2F28%2FTensorflow%E5%9C%A8GPU%E4%B8%8B%E7%9A%84Poolallocator%20Message%2F</url>
    <content type="text"><![CDATA[我在在用GPU跑我一个深度模型的时候，发生了以下的问题：1234567891011121314151617...2018-06-27 18:09:11.701458: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 63521 get requests, put_count=63521 evicted_count=1000 eviction_rate=0.0157428 and unsatisfied allocation rate=0.01731712018-06-27 18:09:11.701503: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110Global_step 2000 Train_loss: 0.0758Global_step 3000 Train_loss: 0.0618Global_step 4000 Train_loss: 0.0564Global_step 5000 Train_loss: 0.0521Global_step 6000 Train_loss: 0.0492Global_step 7000 Train_loss: 0.0468Global_step 8000 Train_loss: 0.0443Global_step 9000 Train_loss: 0.0422Global_step 10000 Train_loss: 0.0410Global_step 11000 Train_loss: 0.0397Global_step 12000 Train_loss: 0.03832018-06-27 18:13:59.743133: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 71532 get requests, put_count=71532 evicted_count=1000 eviction_rate=0.0139798 and unsatisfied allocation rate=0.01430132018-06-27 18:13:59.743167: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281... 除了常规的loss数据之外，我看到穿插在之间的warming informations ，虽然最后的结果没有任何问题，但是我抱着好奇的心态在stackoverflow找到了原因： TensorFlow has multiple memory allocators, for memory that will be used in different ways. Their behavior has some adaptive aspects.In your particular case, since you’re using a GPU, there is a PoolAllocator for CPU memory that is pre-registered with the GPU for fast DMA. A tensor that is expected to be transferred from CPU to GPU, e.g., will be allocated from this pool.The PoolAllocators attempt to amortize the cost of calling a more expensive underlying allocator by keeping around a pool of allocated then freed chunks that are eligible for immediate reuse. Their default behavior is to grow slowly until the eviction rate drops below some constant. (The eviction rate is the proportion of free calls where we return an unused chunk from the pool to the underlying pool in order not to exceed the size limit.) In the log messages above, you see “Raising pool_sizelimit“ lines that show the pool size growing. Assuming that your program actually has a steady state behavior with a maximum size collection of chunks it needs, the pool will grow to accommodate it, and then grow no more. It behaves this way rather than simply retaining all chunks ever allocated so that sizes needed only rarely, or only during program startup, are less likely to be retained in the pool.These messages should only be a cause for concern if you run out of memory. In such a case the log messages may help diagnose the problem. Note also that peak execution speed may only be attained after the memory pools have grown to the proper size. 加粗部分解释机制、处理方式和原因。总结起来就是，PoolAllocator会有一个内存分配机制，GPU和CPU之间不是独立的可以相互传输，如果你使用的空间太多，他就会提高原有的预设的空间大小，如果够用了，就没有什么影响了，但是，需要注意的是，兄弟你的数据加载量太大了，看看是不是改改batch size，一次性少加载点数据，或者干掉隔壁同事的任务。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow代码解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于'Deep Neural Networks for YouTube Recommendations'的一些思考和实现]]></title>
    <url>%2F2018%2F06%2F26%2F%E5%85%B3%E4%BA%8EDeep-Neural-Networks-for-YouTube-Recommendations%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83%E5%92%8C%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[论文 Deep Neural Networks for YouTube Recommendations 来自google的YouTube团队，发表在16年9月的RecSys会议。我想应该很多人都读过，之前参与了公司的推荐系统优化的项目，本来想从各大搜索引擎中寻找到现成的分析，但是出人意料的一无所获。Github上的代码实现也出奇的少以及不清晰，所以就借着这个机会和大家分享一下自己做的过程中的一些理论心得、工程坑、代码实现等等。 本文基于大家对Deep Neural Networks for YouTube Recommendations已经完成通读的基础上，不会做细致的论文解析，只会涉及到自己实现过程中的一些总结，如果没有论文了解，会非常不易理解。 系统概览上面这张图可以说是比较详细的涵盖了基础框架部分，整体的模型的优点我就不详述了，包括规模容纳的程度大啊、鲁棒性好啊、实时性优秀啊、延展性好啊等等，网上很多水字数的文章很多，我们主要总结几个愿论文上的亮点和实际去做的时候需要注意的地方： DNN网络可以怎么改 负采样的“避坑” example age有没有必要构造 user feature的选择方向 attention 机制的引入 video vectors的深坑 实时化的选择 整体上来说，G厂的这套算法基于的是两个部分：matching+ranking，这个的也给我们带来了更大的工作量，在做的时候，分成两个部分，我们在实际处理的时候，通过recall rate来判断matching部分的好坏，通过NDCG来判断排序部分的好坏。总体如下： candidate generation就是我们matching的模块，目的是把百万级的商品、视频筛选出百级、千级的可排序的量级；再通过ranking模块，选出十位数的展示商品、视频作为最后的推送内容。之所以把推荐系统划分成Matching和Ranking两个阶段，主要是从性能方面考虑的。Matching阶段面临的是百万级，而Ranking阶段的算法则非常消耗资源，不可能对所有目标都算一遍，而且就算算了，其中大部分在Ranking阶段排名也很低，也是浪费计算资源。 Matching &amp; Ranking Problems首先，我们都知道，G厂给出的这个解决方案用的就是基于DNN的超大规模多分类思想，即在时刻t，为用户U（上下文信息C）在视频库V中精准的预测出视频i的类别（每个具体的视频视为一个类别，i即为一个类别），用数学公式表达如下： 很显然上式为一个softmax多分类器的形式。向量u是信息的高纬“embedding”，而向量v则是视频 j 的embedding向量，通过u与v的点积大小来判断用户与对应视频的匹配程度。所以DNN的目标就是在用户信息和上下文信息为输入条件下学习视频的embedding向量v，从而得到用户的向量u。 说完基本思想，让我们看看实际的效果对比： DNN网络可以怎么改：softmax及revise的考虑 如我图中两处红色标记，论文中虽然给出了模型的整体流程，但是却没有指明，1处的video vectors需要单独的embedding还是沿用最下方的embedded video watches里面的已经embedding好的结果，我们称之为softmax问题；2处论文没有提及一个问题，就是在固定好历史watch的长度，比如过去20次浏览的video，可能存在部分用户所有的历史浏览video数量都不足20次，在average的时候，是应该除以固定长度（比如上述例子中的20）还是选择除以用户真实的浏览video数量，我们称之为revise问题。 根据我们的数据实测，效果对比如下： nosoft:沿用最下方的embedded video watches里面的已经embedding好的结果revise:除以用户真实的浏览video数量 我们尝试的去探求原因发现，nosoft比softmax好的原因在于user vector是由最下方的embedded video watches里面的已经embedding好的结果进行多次FC传递得来的，如果新增一个video embedded vector 的话，和FC传递得到的u vector的点积的意义就难以解释；revise比norevise好的原因是，实际在yoho!buy的购物场景下，用户的点击历史比较我们实际选取的window size要短不少，如果所有的用户都除以固定长度的话，大量的用户history click average的vector大小接近于0。 DNN网络可以怎么改：神经元死亡及网络的内部构造这是一个异常恶心还有没什么好方法的问题，在刚开始做的时候，我们遇到了一个常见的问题，神经元批量死亡的问题。在增加了batch normalization、clip_by_global_norm和exponential_decay learning rate 后有所缓解。 网络结构的变化比较常规，对比场景的激活函数，参考了论文中推荐的深度、节点数，效果对比如下： 虽然我们看到增加网络的深度（3–&gt;4）一定程度上会提高模型的命中率，增加leakyrelu的一层网络也可以有些许的提升，但是总的来说，对模型没有啥大的影响，所以在之后的实际模型中，我们选择了原论文中relu+relu+relu，1024+512+256的框架。 负采样的“避坑”我们都知道，算法写起来小半天就可以搞定，但是前期的数据处理要搞个小半个月都不一定能出来。作为爱省事的我，为了快速实现算法，没有重视负采样的部分，采取了列表页点击为label=1，未点击为label=0的方式，详情如下： 看上去没什么问题，省略了从全量样本中抽样作为负样本的复杂过程，实际上，我把代码狂改了n边效果也一直维持在1.57%，可以说是没有任何提升，在此过程期间，我还是了拿用户的尾次点击（last_record）进行训练，拿了有较多行为的用户的尾次点击（change_last_record）进行训练，效果很感人： 在我孤注一掷一致，选择按照原论文中说的，每次label=0的我不拿展现给用户但是用户没有点击的商品，而是随机从全量商品中抽取用户没有点击过的商品作为label=0的商品后，奇迹发生了： 事后我仔细分析了原因：a.在当次展现的情况下，虽然用户只点击了click商品，其他商品没有点击，但是很多用户在后续浏览的时候未click的商品也在其他非列表页的地方进行click，我实际上将用户感兴趣的商品误标记为了负样本b.后来我咨询看了论文，也发现了原论文中也提及到，展现商品极有可能为热门商品，虽然该商品该用户未点击，但是我们不能降低热门商品的权重（通过label=0的方式），实际上最后的数据也证明了这一点c.“偷窥未来的行为”，如下图，原论文中指出input构造时候不能拿还未发生的点击，只能拿label=1产生时之前的所有历史点击作为input；同理，在构造label=0的时候，只能拿在label=0的时候已经上架的商品，由于训练时间的拉长，不能偷窥label=1发生时还未上架的商品作为label=0的负样本 example age有没有必要构造首先，先稍微解释一下我对example age的概念的理解。所有的训练数据其实都是历史数据，距离当前的时刻都过去了一段时间，站在当前来看，距离当前原因的数据，对当前的影响应该是越小的。就比如1年前我买了白色的铅笔，对我现在需要不需要再买一支黑色的钢笔的影响是微乎其微的。而example age其实就是给了每一条数据一个权重，引用一下原论文的描述In (5b), the example age is expressed as tmax − tN where tmax is the maximum observed time in the training data，我这边采取了(tmax − tN)/tmax的赋权方式： 很悲催的是，直观的离线训练数据并没有给出很直观的效果提升，但是由于评估机制的问题（我们后面会说到），我会在实际上线 做abtest的时候重新验证我的这个example age的点，但是可以肯定的是，理论和逻辑上，给样本数据进行权重的更改，是一个可以深挖的点，对线上的鲁棒性的增强是正向的。 user feature的选择方向很不幸的是，在这一块的提升，确实没有论文中说的那么好，对于整个网络的贡献，以我做的实际项目的结果来说，history click embedded item &gt; history click embedded brand &gt; history click embedded sort &gt; user info &gt; example age &gt; others。不过，因为时间、数据质量、数据的真实性的原因，可能作为原始input的数据构造的就没有那么好。这边主要和大家说两个点： 1.topic数据原论文中在第四节的RANKING中指出:We observe that the most important signals are those that describe a user’s previous interaction with the item itself and other similar items, matching others’ experience in ranking ads论文中还举出了比如用户前一天的每个频道（topic）的浏览视频个数，最后一次浏览距今时间，其实说白了就是强调了过去的行为汇总对未来的预测的作用，认为过去的行为贯穿了整体的用户点击轨迹。除此之外，G厂大佬还认为一些用户排序性质的描述特征对后面的ranking部分的提高也是蛮重要的，这边还举出了用户视频评分的例子，更多的内容大家可以自己去看一下原论文的部分，应该都会有自己的体会。 回到我们的项目，因为yoho!buy是电商，我类比着做了用户每个类目（裤子、衣服、鞋子…）的历史浏览点击购买次数、最后一次点击距今时长等等的topic信息，提升不是很明显。但是在大家做G厂这边论文，准确率陷入困境的时候，可以尝试一下这边的思路。 2.query infomation相比于论文中的user information的添加，在实际模型测试中，我们发现，query的information的部分有更多的”遐想”。 原论文中点名指出user language and video language 做为basic info的重要性，这边给出的提升也是相对于user info有明显的增长的： 有提升也自然有该部分的缺点：1.语言模型的处理复杂，耗时久在该部分的处理中，我强行拖着隔壁组的nlp博士和我一起搞了一周，每天都加班的搞去做数据清理，句法分析，语句树解析。如果需要让一个常规做推荐的人去弄，会有各种各样的坑，而且耗时还久2.语言新增问题商品的标题这类的文本处理还好，毕竟每日更新的数据存在一个可控的范围，但是用户搜索内容的变化是巨大的，粗略估测一下，一周时间间隔后，原提纯文本数据和新提纯文本数据的交集覆盖率不到78%，这意味着要重复的做nlp工作 attention 机制的引入attention 机制的引入是我老大的硬性需求，我这边也就做了下，如果不了解attention 机制的朋友，可以阅读以下这边文章：Attention model。 我通俗的解释一下，不准确但是方便理解，Attention model就是让你每一个input与你的output计算一个similarity，再通过这些similarities给出每个input的权重。但是，很明显，我们离线训练还好，既有input也有output，但是线上预测的时候，就没有output了，所以，我们采取了lastclick替代的方式： 不得不说，老祖宗传下来的东西确实有独到之处，但是在提升了近1pp的rate代价之下，会有一个让人头疼的问题耗时。因为每一个input的weight需要和output进行一次相似度计算，而且后续还要对计算出的相似度进行处理，原本只需要6-7小时训练完的模型，在我加了3层Multihead Attention后被拖到了一天。数据量还只采样了一半，确实需要斟酌带来的提升与投入的成本之间的平衡问题。 video vectors的深坑G厂一句话，我们测断腿。这句话不是瞎说的，大家应该还记得一开始我给出的那张图，在最上面有一行不是很明显的小字：video vectors。G厂的大佬们既没有说这些video vectors该怎么构造，也没有说video vectors需不需要变动，留下了一个乐趣点让大家体验。 刚开始我很傻的用了我们最开始的embedded item作为video vectors，与模型FC出来的user vectors进行点击，计算top items。我来来回回测了一个月，老命都快改没了，最后提升rate到4pp。然而RNN随便跑跑就能到达3pp，我说很不服气的，所以拉着同事一起脑洞了一下，我们之前做图片相似度匹配的时候，喜欢把图片的向量拆成颜色+款式+性别，所以我们就借用了一下，改成了embedded item + embedded brand + embedded sort作为video vectors，历史总是给我们惊喜，效果上一下子就能大到5.2pp左右，这个点的提升应该是得来的最意外的，建议大家在用的时候考虑一下。 实时化的选择实时部署上，我们用了tensor flow serving，没什么好说的，给一下关键代码，大家看下自己仿一下就行，一般自己做做demo不需要，企业级上线才需要，企业级上线的那些大佬可能也比我有更多想法，所以就不展开了。 123456789101112部署及用python作为Client进行调用的测试：#1.编译服务bazel build //tensorflow_serving/model_servers:tensorflow_model_server#2.启动服务bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9005 --model_name=test --model_base_path=/Data/sladesha/tmp/test/ #3.编译文件 bazel build //tensorflow_serving/test:test_client#4.注销报错的包注销：/Data/muc/serving/bazel-bin/tensorflow_serving/test/test_client.runfiles/org_tensorflow/tensorflow/contrib/image/__init__.pyc中的from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms参考：https://github.com/tensorflow/serving/issues/421#5.运行bazel-bin/tensorflow_serving/test/test_client --server=localhost:9005 相关的问题，有大佬已经梳理好了，自取其他可选的一些参数设置：tensorflow serving 参数设置。 还有一些评估技巧，模型之间的对比技巧，这边就不细讲了，可借鉴的意义也不大。 总结虽然早就读过这篇文章，但是实现之后，发现新收获仍然不少。我特别赞成清凇的一句话:’对于普通的学术论文，重要的是提供一些新的点子，而对于类似google这种工业界发布的paper，特别是带有practical lessons的paper，很值得精读。’G厂的这个推荐代码和attention model的代码之前是准备放GitHub的，想想还是算了。一是之前也放过很多此代码，也没什么反馈，二是这两个代码自己写也不是很难，可以作为练手项目。 鸣谢以上我个人在Yoho!Buy团队在实践中的一点总结，不代表公司的任何言论，仅仅是我个人的观点。最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！溜了溜了。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.nn.embedding_lookup]]></title>
    <url>%2F2018%2F06%2F11%2FTensorflow%E4%B8%93%E9%A2%98tf-nn-embedding-lookup%2F</url>
    <content type="text"><![CDATA[我觉得这张图就够了，实际上tf.nn.embedding_lookup的作用就是找到要寻找的embedding data中的对应的行下的vector。 1tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None) 官方文档位置，其中，params是我们给出的，可以通过：1.tf.get_variable(&quot;item_emb_w&quot;, [self.item_count, self.embedding_size])等方式生产服从[0,1]的均匀分布或者标准分布2.tf.convert_to_tensor转化我们现有的array然后，ids是我们要找的params中对应位置。 举个例子：1234567import numpy as npimport tensorflow as tfdata = np.array([[[2],[1]],[[3],[4]],[[6],[7]]])data = tf.convert_to_tensor(data)lk = [[0,1],[1,0],[0,0]]lookup_data = tf.nn.embedding_lookup(data,lk)init = tf.global_variables_initializer() 先让我们看下不同数据对应的维度：123456In [76]: data.shapeOut[76]: (3, 2, 1)In [77]: np.array(lk).shapeOut[77]: (3, 2)In [78]: lookup_dataOut[78]: &lt;tf.Tensor 'embedding_lookup_8:0' shape=(3, 2, 2, 1) dtype=int64&gt; 这个是怎么做到的呢？关键的部分来了，看下图：lk中的值，在要寻找的embedding数据中下找对应的index下的vector进行拼接。永远是look(lk)部分的维度+embedding(data)部分的除了第一维后的维度拼接。很明显，我们也可以得到，lk里面值是必须要小于等于embedding(data)的最大维度减一的。 以上的结果就是：12345678910111213141516171819202122232425262728293031323334353637In [79]: dataOut[79]:array([[[2],[1]],[[3],[4]],[[6],[7]]])In [80]: lkOut[80]: [[0, 1], [1, 0], [0, 0]]# lk[0]也就是[0,1]对应着下面sess.run(lookup_data)的结果恰好是把data中的[[2],[1]],[[3],[4]]In [81]: sess.run(lookup_data)Out[81]:array([[[[2],[1]],[[3],[4]]],[[[3],[4]],[[2],[1]]],[[[2],[1]],[[2],[1]]]]) 最后，partition_strategy是用于当len(params) &gt; 1，params的元素分割不能整分的话，则前(max_id + 1) % len(params)多分一个id.当partition_strategy = ‘mod’的时候，13个ids划分为5个分区：[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]，也就是是按照数据列进行映射，然后再进行look_up操作。当partition_strategy = ‘div’的时候，13个ids划分为5个分区：[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]，也就是是按照数据先后进行排序标序，然后再进行look_up操作。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow代码解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.scan]]></title>
    <url>%2F2018%2F06%2F04%2FTensorflow%E4%B8%93%E9%A2%98tf-scan%2F</url>
    <content type="text"><![CDATA[tf.scan(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=None) fn：计算函数elems：以elems的第一维度的变量list作函数计算直到遍历完整个elemsinitializer：fn计算的初始值，替代elems做第一次计算 举个好理解的例子：123456789101112131415x = [1,2,3]z = 10x = tf.convert_to_tensor(x)z = tf.convert_to_tensor(z)def f(x,y):return x+yg = tf.scan(fn=f,elems = x,initializer=z)sess = tf.Session()sess.run(tf.global_variables_initializer)sess.run(g) 会得到：12In [97]: sess.run(g)Out[97]: array([11, 13, 16], dtype=int32) 详细的计算逻辑如下：11 = 10(初始值initializer)+ 1(x[0])13 = 11(上次的计算结果)+2(x[1])16 = 13(上次的计算结果)+3(x[2])]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow代码解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写给想转行机器学习深度学习的同学]]></title>
    <url>%2F2018%2F03%2F18%2F%E5%86%99%E7%BB%99%E6%83%B3%E8%BD%AC%E8%A1%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%8C%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[update 1:很多同学还是私信我，让我推荐或者提供一些电子书给他们，我这边也打包了一些我认为比较重要的，如果有需要的同学可以「邮箱」联系我。申明，我所发送的书个人均已购买正版实体书，建议大家也支持正版，谢谢。 自从我毕业以来，先是火机器学习，然后火大数据，之后火深度学习，现在火人工智能这些算法领域。越来越多的朋友想从工业，金融等等行业转行到算法相关的行业，我一年前在知乎上写了一个答案本科生怎样通过努力拿到较好的机器学习/数据挖掘相关的offer？，当时拿了不少的赞，所以也一直有同学找我咨询相关的问题，确确实实也有相当一批人拿到了不错的offer。 我个人不是很喜欢更新非技术的文章，但是我还是觉得如果能帮助到一些人，其实也是另一种技术输出的展现，所以我就写下了下面这篇短文，希望对迷茫的人有所帮助。 评估转行难度今天一大早，我在刷知乎的时候，刷到这个题目非计算机专业学生如何转行AI，并找到算法offer?，我看到这个叫做BrianRWang的答主的一个“10问检验你的基础水平”，我觉得是至少我看来非常全面考验数学基础的，所以这边就和大家分享一下（答案我会在最后给出，有兴趣的最好自己做一下，括号里面的我个人觉得没有意义所以没有给出解释，有兴趣的却又解不出来的同学可以私信我）： 1.什么是贝叶斯定理？请简述其公式？现分别有 A，B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器 A 的概率是多少? 这题考了概率论的基础，虽然考了贝叶斯，但是后面的容器问题完全可以不用贝叶斯也可以算出来，算是一题数学敏感度的测试题，看看自己适不适合去努力切入这个方向。 2.请简述卡方分布和卡方检验的定义？(给你一个2*2的列表让你算卡方分布，你会怎么做？) 这题考了梳理统计的基础，括号里面的我个人觉得没有意义，有兴趣的可以查表算一下。 3.在概率统计学里，自由度是如何被定义的，又该怎样去应用？ 原作者BrianRWang认为这题比较偏，属于冷门题目。个人看法：其实我觉得如果是任何一个理工科的同学，这题都应该能答出来，大学的课程里，自由度的理解直接决定了统计科目大家的学习质量。 以上的三题考了概率论与数理统计的基础，在机器学习理论中，概率论和数理统计的基础是否扎实直接决定了能否很好的理解各个理论的前置条件，适用场景，提升方向等，着实重要。 4.请简述什么是线性代数里的矩阵特征值和特征向量？(求矩阵:A=np.array([[1,2],[3,4]])的特征值，特征向量，写出其运算公式) 线性代数题目，很简单给出对应的公式即可，我在SVD介绍的时候就完全讲过。如果换成，如何理解特征值及特征向量在空间中的实际意义，这题就会变得非常卡人。 5.如何使用级数分解的方法求解e^x?(并给出在数值计算中可能遇到的问题。) 数学分析的题目，一个公式。 以上的题目都是线性代数，数学分析的题目，都是比较考验大学的基本功，如果不记得也很正常，只要能说出大概的思想就行，比如空间选择啊，点导数展开。 6.数据结构的定义是什么？运用数据结构的意义是什么？ 计算机题，这题应该是几个问答中最简单的了。 7.请说明至少两种用于数据可视化（data visualization）的package。并且说明，在数据分析报告里用数据可视化的意义是什么？ 前一问如果主动接触过计算科学的人这题比较好答，如果是纯新手，这题就是无从下手的。后面一小问也是属于考察你的数据敏感度的，如果能够match到一些点，很加分。 8.假如让你用编程方法，比如python，处理一个你没见过的数学问题，比如求解一个pde或者整快速傅里叶变换，你应该查什么东西，找哪一个package的参考资料？ 同上一条前一部分。 9.请简述面向对象编程和函数式编程分别的定义，并举出其案例。 计算机题，考了基础的编程的一些风格的了解程度，说实话，这题我第一次看到也很懵，还去Google了一下。 原作者还有一个第10题，不涉及技术，我就没放。以上四题更偏向coding的能力，虽然说算法工程师、数据挖掘工程师、NLP工程师，等等，都是挂着科研的title，但是过硬的coding能力是完全不能缺少的，要其他人把很复杂的数学理论用代码帮你实现出来的交流成本巨大，我觉得精通或者熟悉至少一门语言还是非常重要的。 原作者认为： 以上提问如果能闭卷对7个及以上，证明一个学生的基础还是比较好的。只要聪明肯学，一定是有所裨益的。在7个，到3个之间，不妨提高一下自己的数学水平；努努力还是可以学会机器学习的。如果写对不了两个（“这都啥啊？”），郴州勃学院复读班欢迎你过去。 其实我还是比较认同的，答对3个或者2.5个以上的同学，完全可以试一试转一转，我觉得不存在说入不了门的情况。能答对7个或者7.5以上的同学，我觉得可以投简历了，如果我收到你的简历，即便是你没有历史的工作经验，我很愿意让你试一试的。 一些资料很多转行的朋友会问我，到底看什么书会比较好，我刚开始会推荐一堆，后来自己想了想发现，还是太天真，大家工作忙的要死，看一本就很难了，别说一堆。 我最后就浓缩了三本:：周志华老师的西瓜书（《机器学习》周志华 清华大学出版社），李航的带你玩转基础理论（《统计学习方法》李航 清华大学出版社），经典厕所读物（《数学之美》吴军 人民邮电出版社）。 确实是很经典很经典的书，我现在基本上每次必回答以上三本。 除此之外，在coursera上找吴恩达（Andrew Ng）教授的机器学习课程，他把要用到的数学知识也做了简单的讲解，机器学习方面的理论和算法讲的也很详细，而且很基础，肯定可以看懂。Machine Learning | Coursera，应该是最适合看的视频类的资料没有之一。 我不反对也不支持大家去参加几千几万的速成班，几十几百的live课程，但是我觉得你不妨先看完以上的书和视频再做决定，一定不会让你失望。之前我一直在给team做吴恩达（Andrew Ng）在线课程的分享，一直到最近我发现不如整理出来给team以外的大家一起看算了，所以在Gradient Checking(9-5)这节课之后的所有课程，如果有价值的地方，我都做了笔记后面会分享在我的GitHub中，希望给大家一些帮助。 最后，希望我们都不负自己的青春。 附录：1.BrianRWang的十条问题的答案链接2.吴恩达（Andrew Ng）Gradient Checking(9-5)这节课之后的课程整理（持续更新中）]]></content>
      <categories>
        <category>基础</category>
      </categories>
      <tags>
        <tag>公告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yoho!buy注册概率预估]]></title>
    <url>%2F2018%2F03%2F04%2Fyoho!buy%E6%B3%A8%E5%86%8C%E6%A6%82%E7%8E%87%E9%A2%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[GRU 部分的demo代码：model.py，注意，其中上传的lr_classification.py并没有做wide&amp;deep设计 前言本文主要介绍yoho!buy大数据团队在深度学习传统应用方向上的一些实践和思考。传统用户行为预估方向上，如何根据用户的行为数据，对用户行为建模，进而预测用户的购买行为，点击行为，注册行为等等一直以来都受到工业界及学术界的关注。相对而言，就用户注册概率的预测受限数据获取的局限性、传统的计算模型的时效性等原因并没有很多可参考的研究案例。我们想和大家分享的「yoho!buy基于GRU+LR算法下的用户注册概率预估」，基于循环神经网络的框架，充分的利用了用户在app上的行为信息，保证了高效的结果反馈速度，兼备算法框架良好的延拓性能。 注册概率预估定义注册概率预估，即预估用户下载app后，浏览app过程中主动注册的可能性。通过识别出有注册倾向的人群，辅助以人为介入的方式（优惠、折扣，关怀等），可以提高用户实际注册的概率。 基本的注册概率预估算法设计的流程如下： 数据整理节点：在于收集用户行为信息，包括地理位置，当时的时间，用户来源的渠道，用户点击行为等等；模型计算节点：在于根据数据整理节点的结果判断用户的注册概率高低；计算结果推送节点：在于根据不同注册概率用户采取不同的营销策略，个性化引导用户注册。 目前注册概率预估主要有两大难点： 传统模型难以实时预测：因为缺乏平台忠诚度，用户第一次也有可能是最后一次登陆app之前这段时间是相对暂短，如何压缩用户每一次步操作下的模型计算时间，提高反馈频率是需要考虑的重要问题，而传统模型在这方面的表现比较平庸。 传统模型特征加工复杂：因为用户可能是第一次接触app，在没有注册信息，历史行为信息，完成订单信息等等数据下，利用有限的数据进行的特征处理，如果想要有不错的效果相对而言特征加工过程的复杂程度和难度要比普通的项目更加具有挑战性。 我们通过以Recurrent Neural Network 及 Logistic Regression为基模型，通过Stacking方式，针对性的尝试去解决以上两个注册预估中的难点。其中Recurrent Neural Network最本质的能够work的地方在于，实际上在没有过多特征的时候，对于新用户来说，他的浏览路径实际上就是反映了他对这个app的喜好。 算法设计Recurrent Neural Network基模型数据整理部分无论在Kaggle还是天池大赛上，数据特征工程是非常重要而且繁琐的一个过程：关于预处理，通常我们会采取： 数据检查，提出异常字符、乱码等数据 缺失值处理，剔除、填充、拟合构造等 方差衡量，剔除方差低的低贡献特征 共线性检查，提高泛化能力 异常检验，剔除错误异常数据 … 在数据预处理结束之后，我们还会在更新完的数据集上进行特征筛选： 基于自变量与因变量之间的交互熵 基于模型中的特征贡献程度（Xgboost里面的importance/Lasso、Ridge中的参数绝对大小） 基于预训练模型中的特征参数的显著性 基于自变量之间的相关性 … 在数据特征筛选结束之后，我们还需要进行特征组合寻找最大方差下的新的特征，还会通过PCA/LDA/t-SNE/FM等寻找是否可以进行降维或者升维，交叉特征构造等等。通常无论是在离线训练还是线上预测中，对特征的加工处理过程都是非常耗时的，极有可能在用户已经离开app后，用户的注册概率还没有算出来。 我们利用Recurrent Neural Network来解决注册预估的时候，我们需要做的数据整理就非常的轻松，只需根据用户浏览的顺序，将用户浏览的页面编号Item_Page,同时记录用户浏览的先后顺序Time_Rank： 构造如下的数据形式： User_Imme Item_Page Time_Rank 012939003331092 92374573284354 1518422132 012939003331092 82374573771273 1518422142 012939003331092 92374573284354 1518422147 078939002221093 66774573284354 1518422247 078939002221093 66774573284442 1518422249 … … … 数据处理过程，只需要按照用户的浏览先后顺序进行排序即可，大大的降低了耗时，对整体算法的实效性上不会产生任何影响。因此，我们甚至可以在用户每一次产生动作之后就对其的注册概率进行重新判定，得到用户的浏览流对应的注册概率波动情况。 我们的用户可以大致可分为铁粉用户和普通用户两种。从用户的注册概率分布就可以很清晰的看出：铁粉用户浏览目的明确，寻找自己关注的商品，一旦找到立马注册下单，所以铁粉用户的时间流较短，注册概率呈现上升趋势，注册概率均值较高，需要我们运营手段干涉的情况较少；而普通用户属于无明确目的的浏览，所以注册概率的波动较大，注册概率均值较低，但是一旦察觉到用户有高概率注册的行为却未注册且注册概率持续下降时立刻进行营销引导，多次营销尝试后用户成功注册。 模型计算部分整体的模型框架概览如下： 我们试图通过循环网络部分提取出用户的浏览行为的汇总信息，再通过Logistic Regression部分融合用户基础信息，以行为特点+基础特征猜测用户每次浏览是属于随便点点还是认真挑选。 Recurrent Neural Network部分1.循环单元结构 在循环神经网络模块中，我们采取了Gated Recurrent Unit (GRU)代替普通RNN作为最小循环单元进行计算，以此来避免梯度消失等问题： 隐藏状态计算如下： 对比LSTM，GRU只用了两个gates，将LSTM中的输入门和遗忘门合并成了更新门。并且并不把线性自更新建立在额外的memory cell上，而是直接线性累积建立在隐藏状态上，并靠gates来调控，这样就可以大大的加快离线训练的速度，同时在RNN的官方论文中，我们看到了实测的效果如下： 很明显的可以看到:虽然GRU减少了一个门的存在，但是效果与LSTM相当，但是几乎每次测试的test效果都要优秀于传统方法，同时GRU是真的肉眼可见的比LSTM快。综合考虑了计算速度及最后计算结果的准确程度，我们选择了多层GRU模型而并非是以输入门、输出门、遗忘门为信息传递的LSTM模型。 2.循环网络结构 整体上看，网络结构也是非常简单的。如上图，先将用户浏览的所有商品进行embedding操作，然后根据用户每个商品的浏览顺序构建时序序列，进行多层的GRU模型训练，最后再以前馈网络传递，softmax后得到用户下次点击每个商品的概率，再根据预测结果Output Second Items 和 Real Second Items修正多层GRU layer中的参数。 通过已经训练好的循环网络，我们根据新用户浏览商品的顺序，得到用户每次浏览的后一次浏览每个商品的概率（output scores）及用户前N次浏览信息的trend，seasonality的汇总（隐藏状态GRU States）。与此同时，我们可以通过控制GRU layer的层数及优化多次隐藏状态GRU States的拼接方式控制整体模型框架的复杂程度。 3.用户数据构造 用户数据训练过程常常采用如上图这样的最小批处理。每一个Session就可以看作是一个用户，每一个i可以看作是一个商品，商品i的下标代表着商品的浏览顺序。 input中，每一行表示的为多个用户正常的浏览顺序中的起始商品，比如Session1用户浏览顺序：i1.1–&gt;i1.2–&gt;i1.3–&gt;1.4中的i1.1，i1.2，i1.3；对应的，在output中，每一行代表多个用户在input位置的下一次浏览内容，比如Session1用户浏览顺序：i1.1–&gt;i1.2，i1.2–i1.3，i1.3–&gt;i1.4中的i1.2，i1.3，i1.4；当input中的一个的用户或者说是一个的Session的点击信息被全部使用后，追加一个新的用户或者说是Session的点击信息，同时通过控制同时计算的用户或者说Session的个数，直到所有的Session信息都被使用完一遍，这样就构造完成了一个由用户的上次点击结果预测用户的下次点击结果的循环神经网络。 这样设计避免了通常的神经网络构造在用户行为上应用中的两个问题： 固定滑动窗口导致大量用户信息不能获取完整 拆分用户的浏览行为计算导致循环网络在信息理解上的歧义 4.损失函数选取 关于损失函数的选取，我们这边主要推荐两种方案：基于贝叶斯后验优化(BayesianPersonalizedRanking,BPR)和第一准则(TOP1)： 贝叶斯后验优化(BayesianPersonalizedRanking,BPR)是一个近似矩阵分解的方法，我们分别选取当前用户当前商品的下一个真实浏览的商品作为Positive Item，随机抽样的商品作为Negative Item，具体表达形如：，其中Ns即为随机采样个数，中k为i时对应Positive Item的真实计算得分，K为j时对应Negative Item，保持i=Positive Item不变，计算所有抽样出来的Negative Item作为j进行计算即可。 第一准则(TOP1)是我们自己设计的一个近似排序的方法，我们想要真实的Positive Item所计算出来的结果尽可能的接近1，Negative Item计算出来的结果尽可能的接近0，所以我们要保证采样出来的Negative Item比Positive Item在当前计算方式下的得分要低，所以，我们可以设计损失函数如下：，最末项增加了个正则项修正拟合程度。 除此之外，《SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS》中还提到了POP，S-POP，Item-KNN，BPR-MF等方法，可单独了解。 Logistics Regression部分通过Logistics Regression部分提高融合Recurrent Neural Network的潜藏层和传统的用户基础特征，进行一次重排序的操作，个性化的提供用户的注册激励也是非常重要的一环。整个Recurrent Neural Network部分在一定程度上帮助我们获取到了用户的浏览操作行为中的trend和seasonality（隐藏状态GRU States），但是缺乏考虑外部信息，比如热点爆品，用户区域，用户性别，用户需求等等。最典型的一个例子就是，我们不能向一个浏览了多个中性黑色太阳眼镜的正在试图走酷雅风格的东北女性推送潮牌男性短裤优惠券作为注册激励，很多时候会起到相反的作用。 如何兼顾Recurrent Neural Network部分的潜藏数据与为数不多的用户基础特征数据，并加以融合快速反馈出结果，是需要多方面考虑的：数据应用 数据类别 数据详解 基础用户画像 人口属性，地点，性别，消费力等 主动行为数据 品类偏好、品牌偏好、行为性别等 文本偏好数据 浏览商品文字描述特征 反馈数据 停留时长，复停留行为，当前时间段等 … … 基础用户画像&amp;主动行为数据：我们可以在用户原始日志中快速清洗出用户的地址，环境，设备等基础信息，结合用户浏览商品的性别+价格+品牌加权预估出用户的性别，消费力等价值属性，品类偏好、品牌偏好、行为性别等基础汇总属性。 文本偏好数据：根据用户的浏览商品，去匹配是否命中了我们预先提取出的注册用户浏览高频关键词，比如“鬼洗”，“典藏”等等及当前的一些热点词汇“小白鞋”，“华莱士”等。 反馈数据：在整个Recurrent Neural Network部分我们考虑的是用户的浏览顺序，但是忽略了用户的浏览质量，用户进入平台后的15s内，A商品重复浏览了3次，停留了9S，B商品重复浏览了1次，停留了1S，商品A的注册激励价值是远远高于商品B的。 通过以上的方式获取到的“用户画像”好处在于快速，再扩充了用户基本属性的同时还能够在规定的时间内完成所有的重排序计算，但是缺点在于一定程度上降低了用户特征刻画的精度。 数据融合 在实际的应用过程中，我们发现，在一定程度上交叉部分高价值的用户特征有助于提高最后的预测结果的准确性，构造的框架图如下： 这边借鉴但是没有完全采用wide&amp;deep的方法，借鉴了对原始用户特征需要通过embedding layer进行处理，比如通过简单的one hot encoding的形式，然后采取特征交叉的方式获取新的用户特征，最后再进行前向传播或者logistics regression；但是此处，在embedding layer的过程中会采取人为限制分箱逻辑去噪（剔除了比如地点归属000ex00f这样的错误数据），在交叉过程中只选取了部分对最后的用户注册影响较大的因素进行交叉，在提升了模型对用户拟合的能力的同时也保证了模型的实效性。 数据流设计 简单的数据执行流如下： 主要步骤如下： Kafka+Flume解析实时点击、搜索、浏览等用户操作日志流，在线进行用户操作数据的抽取 实时解析基础用户环境信息，获取环境特征：手机型号，网络，地址等，实时写入到线上Hadoop/Spark中的HDFS里 根据离线存储在HDFS中的用户操作数据、用户点击流数据和用户是否真实注册的结果离线更新循环网络GRU及LR模型参数 将新的模型参数应用于线上用户数据的预测 最后可得到个人及全站的注册概率变化可视化如下： 可优化方向 GRU卷积神经网络的层数优化，由多层隐藏层替代单层隐藏层提高对用户行为的汇总效果 Logistics Regression部分可以由多模型bagging替换，降低过拟合的可能 反馈数据清洗，对于有强烈意愿注册的用户进行识别，避免干扰正样本池 推荐内容干扰，那些热门爆款更多的用户看到，而且“被看到”这个行为也加深了它接下去被接连看到的可能性 GRU卷积神经网络的构造中，修改上一次操作预测下一次操作为上一次操作预测目标行为（常停留时长的点击、收藏点击等等高价值的行为节点） 总结传统的机器学习方案给用户行为预估的项目一个基准水平线，而深度学习的出现，一定程度上使得这个上限有所提高，但是以数据为基础，用算法去雕琢，只有将二者有机结合，才会带来更好的效果提升。 以上是yoho!buy团队在实践中的一点总结，当然我们还有还多事情要做，keep learning！最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>理论解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提升有监督学习效果的实战解析]]></title>
    <url>%2F2018%2F01%2F20%2F%E6%8F%90%E5%8D%87%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%95%88%E6%9E%9C%E7%9A%84%E5%AE%9E%E6%88%98%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言最近很长时间没有和大家分享东西了，最近一直在忙公司的项目，先说一声抱歉。 之前写过销售预估算法,但是被诸多大佬吐槽有监督学习部分毫无深度，其实我是想写给一些刚入门的朋友看的，这边我boss最近也想让我总结一些相对”上档次”的一点的东西，我做了一些稍微深入一点的总结，希望能够给新人朋友有稍微深入的方法介绍。 去年年末的那段时间里，看了很多天池大赛里面得高分的选手的算法思路，大概总结了有监督学习中的一些核心流程及重要细节： feature processing tricks 这个是老生常谈的问题，但是我还是看到了一些不错的点，比如根据high importance feature剔除高度缺失的cases这些等等 single feature + crossing feature 交叉特征组合原始特征，可以显著的提升auc，提高命中的准确程度，这边除了FM，我们也可以在常规的算法中去实现这个trick 有监督学习架构思路 下面，我们来看看针对每个点，具体是如何实现的，及我们需要注意哪些相关的东西： feature processing trickscase and feature selection我们在做模型训练之前通常会对模型的feature做一些删减，比如共线性检验，去除掉相似度过高的连续feature；比如变异度检验，去除掉一些数据变化差异过小的feature等等。然而，在常规的样本处理中，我们通常只会根据初始的数据分布去看，比如用户在feature上缺失大于来某个阈值才回去剔除这个用户；其实，在深入的思考一下这个问题，会发现，如果用户在高重要性的feature缺失程度高去剔除才更合理一些，这样想可能不是很清晰，这边看下面这个feature flow： 针对uid来看，如果普通的统计的话，uid3的null的个数5个，uid5的null的个数4个，我们应该优先剔除uid3，再考虑剔除uid5，因为null过多的用户所能提供的信息量会相对的少，会增大泛化误差。 但如果我们提前知道，对于判断label的能力，feature3&gt;feature5&gt;feature6&gt;feature8&gt;其他，那么uid5在高重要性的缺失情况极度严重于uid3，所以我们应该优先剔除uid5，相对于上面一种情况，我们预先知道feature的重要性排序就显得很重要了。关于如何判断提供了几种简单的方法： 方差膨胀系数：我们认为，在数据归一化之后，数据波动的更大的feature能够提供的信息量相对而言也是更大的，举个很明显的例子，如果feature1全都是1的话，它对我们判断用户是否下单这样结果毫无意义。 互信息：我一直认为，互信息是判断feature重要性的非常好的方法。方差膨胀系数只单纯了考虑feature本身的特征，而互信息在考虑feature的同时也考虑了label之间的关系，H(X,Y) = H(X) - H(X/Y)，这个信息量的公式很好的解释了这一点。 xgb’s importance：如果互信息是方差膨胀系数的进阶，那么xgb’s importance则是互信息的进阶，在考虑label与feature之间的关系的时候，同时还考虑了feature与feature之间的关系，这样得出来的重要性排序更加全面了一些。 除此之外： Logistic regression的params的参数 Recursive feature elimination（递归参数选择方法） Pearson Correlation Distance correlation Mean decrease impurity/Mean decrease accuracy … 诸如这样的方法很多，需要根据数据的形式，目标变量的形式，时间成本，效率等等综合考虑，这边只是给大家梳理一下常规的方法，至于实际使用的情况，需要大家累积项目经验。 null-feature treatment method在空值或者异常值的处理上，基本上分为2个派别，要么剔除这个feature或者case，要么填充这个feature或者case，它们的缺点也显而易见，随意剔除会减少判断的信息，如果数据较少的时候，会降低模型的效果；填充的话会造成困惑，到底是众数？平均数？中位数？最大值？最小值？现在很多人的处理方法都是观察数据的分布，如果偏态分布就考虑分位数填充，如果是正态分布就考虑均值或者众数填充，相对而言，这样处理的时间成本会更高，而且很多时候解释的说服力不是很强。 我在看了17年3月份JD的订单预估赛，17年的天池工业赛等等的高分答案中，不得不说，有一个分箱的方法确实能够提高0.5-1.5的auc，我之前思考过，可能存在的原因： 保存了原始的信息，没有以填充或者删除的方式改变真实的数据分布 让feature存在的形式更加合理，比如age这个字段，其实我们在乎的不是27或者28这样的差别，而是90后，80后这样的差别，如果不采取分箱的形式，一定程度上夸大了27与26之前的差异 在数据计算中，不仅仅加快了计算的速度而且消除了实际数据记录中的随机偏差，平滑了存储过程中可能出现的噪音 这边就直接给大家分享一下我的梳理：这边涉及到一个问题，连续数值特征是否一定要切为离散特征，建议综合考虑以下几个问题:a.所使用的算法是否为knn、svm这样的距离计算的算法b.是否在实际业务中依赖于离散判断c.连续数值特征的实际意义是否支持离散化。如果以上问题都没有问题的话，我建议优先考虑离散化连续特征，在一定程度上，离散完的feature有更好的解释意义。 single feature + crossing feature我们在之前的FM理论解析及应用中提到过特征交叉这个概念，当时的文章中紧接着通过矩阵的计算技巧： 构造了全部feature的C(n,2)的形式，后面追加了线性模型，这样一定程度上可以提高分类算法的准确度。这是一个非常好的将低维特征向高维转化的方式，所以在我们其他算法的过程中也可以借鉴这种思路，但是假设我们初始的feature量特别多，比如我在日常的CTR预估或者feature梳理的过程中，很容易就整理500以上的feature集合，如果仅考虑C(n,2)的形式的话，就有250499个feature的新增组合，这个是不可能接受的，所以回到我们上面一节feature processing tricks中提到的case and feature selection就是一个非常好的解决办法，我们可以先通过比如xgboost中的importance：我这边实际的画出了我做下单概率预测时初始筛选完成后的417个feature经过xgboost初步分类后的importance，可以很明显看前37，前53，前94个feature对应了三次importance的拐点，我们可以在这些拐点中选择一个既能够涵盖绝大多数的信息量，又不会造成后续交叉特征个数过多的值，比如我这边选择的是60，那么我接下来会生成的新的的交叉feature就是30`59个，比不做处理下的417*`208要小很多倍，而且相对而言不会减少很多的信息量。 整体的流程我这边也画出来了，希望能够给大家一个比较清晰的认识： 可以看到，样本cases在经过了最初的空值筛选及第一轮高重要性feature后的空值筛选后，就保持不变了，而特征feature的筛选过程则贯穿了整个交叉特征生成流。 bagging及stacking的思路架构我相信在读的各位，不论是机器学习从业者抑或是算法工程师甚至是其他研发工程师，一定看过类似如下的快速拖动的模块流： 它相当于把每个功能封装到一个固定的盒子中，当我们需要使用某个模块的时候，进行模块的操作，不需要的时候直接切断模块的流向即可，我们甚至可以空值每个模块的var及bias的偏向程度，在bagging和stacking的思路框架中，我非常常用的就是类似这样的思想：确定好我要进行的组合模块的组合方式（stacking还是bagging还是blending），再确定这次为想要做的子模块是什么，在根据组合形式及子模块细微调节每个子模块。 首先，子模块可以有哪些？ svm分类/回归 logistic分类/回归 神经网络分类/回归 xgboost分类/回归 gbdt分类/回归 xgboost叶子节点index gbdt叶子节点index randomforest分类/回归 elastic net 除了这些，还有么？当然，如果你愿意的话，每一个你自己构造出来的分类或者回归的single model都可以成为你bagging或者stacking或者blending之前的子模块。 如何训练子模块？这边的方法可谓是多种多样，百花齐放，很大程度上来讲，你在天池也好，kaggle也好，你能前十还是前十开外决定因素是你的feature处理的好坏，但是你能拿第一还是第十很大程度上就是依赖你的子模块构造及子模块组合上。这边给大家分享我最近看到的比较有意思的三个子模块形式： 1.wepon的Large-Scale SVM 读过我之前写的SVM理论解析及python实现这篇文章的朋友应该还记得，我当时说过svm在10.7%的数据集中取得第一，算是传统的机器学习方法中非常值得一学的算法，但是实际应用中，在处理大规模数据问题时存在训练时间过长和内存空间需求过大的问题比较让人头疼，wepon同学采取的方法如下： 这种方法看似增加了计算复杂度，实际上是却是减小的，假设原始训练数据大小是n，则在原始数据上训练的复杂度是o(n^2)，将数据集n分成p份，则每份数据量是(n/p)，每一份训练一个子svm，复杂度是o((n/p)^2)，全加起来o(n^2/p)，复杂度比在原始数据上训练减小了p倍。变向的解决了在量大的数据集合上使用svm，提高速度同时保证质量这个问题。论文支持建议参考Ensemble SVM。 2.爱奇艺 Gbdts’ Node Leafs 我们分别来解释一下左右的Dense features 和 Spare Features。 首先，左侧这块很好理解，在上一次的文章中,我们已经讲了如何利用xgboost或者gbdt获得用户的数据落在的每棵树上面的叶子节点的index值：如果有不清楚的同学，请回顾一下上次讲的内容。 右侧这块分别写了user preference 和video content，当然这是因为它是视频公司的原因，在我实际的使用中，我用的是user preference 和 item content，这里的preference和content其实就是你个人信息及行为的向量化的形式。 最简单的表示就是把你的基本信息和item信息先onehotencoding，再首尾相接成一个超长的vector，这就是一个稀疏的Spare Features。 当然除了这种粗暴的办法，还有比如我们在若干天之前讲过的深度学习下的电商商品推荐中的word2vec的技巧，先将所有的用户随机生成为我们需要的长度N维下一一对应的向量，在通过huffman编码的形式找到每个item对应的Huffman树子的唯一路径，再通过在每个节点上生成一个logsitic分类的办法，使得所有该路径成立的概率最高，以此来修正我们最初随便生成的N维向量，最后这个N维向量就可以看作是一个Spare Features。 还有么？当然，我私下问了我之前在该公司任职的同学，他们还有一种思路就是划分数据集到M个子集，每个子集上面生成一个xgboost，然后每个子集取xgboost的叶子节点，相当于把左侧的Dense features复制了M份Dense features放在了右边的Spare Features，最后会得到一个M+1个Dense features。实际使用起来的效果完全不比word2vec的结果差。 3.基于GRU的潜藏层 Domonkos Tikk和Alexandros Karatzoglou在《Session-based Recommendations with Recurrent Neural Networks》文章中提到了可以用循环神经网络RNN来预估用户的行为，如下图： 我们可以清晰的看到，针对每个用户Session1，他的行为由i1.1变化至i1.4其实是一个有序的过程，我们可以设计一个从i1.1—-&gt;i1.2,i1.2—-&gt;i1.3,i1.3—-&gt;i1.4这样的一个循环流程。同时在他的文章中还解释了这样的设计解决的两个问题： the length of sessions can be very different breaking down into fragments 一来通过了首尾相接，解决不同用户的session不同长度；二来通过了embedding layer，解决了不完整session下预测的可能。具体网络设计如下： 模型的更新流可以参考下面： 我们只需要拿到每个用户的item流下所对应的state即可，这state就包含了这个用前M次的操作潜藏信息，同时我们还可以随意定义这个信息向量的长度，这个就可以看作用户状态向量，作为子模块的输出。 这个思路的缺点就是，要预测的基础数据不存在时序性，效果极差。比如滴滴打车的下单过程，从登陆到打到车的时间最短在20s，最长在1分钟，否则用户就退出了app，这样的情况下，时序性质就显得格外薄弱，强行用这样的RNN获得的用户属性非常不存在代表性。 如何组合子模块？bagging 这个是我们Kaggle&amp;TianChi分类问题相关纯算法理论剖析就强调过的bagging的最简单的形式，在每个子模块的设计选择过程中要尽可能的保证： low biase high var 也就是说子模块可以适当的过拟合，增加子模型拟合准确程度，通过加权平均的时候可以降低泛化误差 stacking 这个是我们Kaggle&amp;TianChi分类问题相关纯算法理论剖析就强调过的stacking的最简单的形式，在每个子模块1、子模块2的设计选择过程中要尽可能的保证： high biase low var 在子模块3的时候，要保证： low biase high var 也就是说，在子模块1，2的选择中，我们需要保证可稍欠拟合，在子模块3的拟合上再保证拟合的准确度及强度 blending我们知道单个组合子模块的结果不够理想，如果想得到更好的结果，需要把很多单个子模块的结果融合在一起：这种方法也可以提高我们最后的预测的效果。 关于有监督学习的方法大概就梳理到这边，最后希望能够给一些新人同学对有监督的理解和实战有一些帮助。 没啥广告要打，就这样吧。 Reference:[1] 周志华。《机器学习》，清华大学出版社，3.7，2016[2] wepon。 《PPD_RiskControlCompetition》[3] 爱奇艺技术产品团队。 《爱奇艺个性化推荐排序实践》[4] slade。 《Kaggle&amp;TianChi分类问题相关纯算法理论剖析》[5] E Cernadas，D Amorim。 《Do we need hundreds of classifiers to solve real world classification problems?》[6] slade。 《深度学习下的电商商品推荐》[7] Domonkos Tikk，Alexandros Karatzoglo。 《Session-based Recommendations with Recurrent Neural Networks》[8] slade. 《FM理论解析及应用》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle&TianChi分类问题相关纯算法理论剖析]]></title>
    <url>%2F2017%2F12%2F28%2FKaggle%26TianChi%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9B%B8%E5%85%B3%E7%BA%AF%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[17/12/30-update ：很多朋友私密我想要代码，甚至利用金钱诱惑我，好吧，我沦陷了。因为原始代码涉及到公司的特征工程及一些利益trick，所以我构造了一个数据集后复现了部分算法流程，需要看详细代码实现朋友可以移步Ensemble_Github 更多代码内容欢迎follow我的个人Github，如果有任何算法、代码疑问都欢迎通过stw386@sina.com联系我，知无不答。 导读在上一次的文章中，我们讲了，如何快速的利用bagging、boosting、stacking、ensemble的形式实现一个分类算法，当时我们直接看了代码以及核心的理论注意点。如果需要有更加优异的结果表现，对整套算法的设计及相关的理论了解是必不可少的。本文将从数学、工程、领域经验的角度去剖析如何用好bagging、boosting、stacking、ensemble去训练一个相对完善的模型。 再次提醒，本文中的数据公式较多，抽象概念较多，需要一定的高等代数、泛函分析、机器学习基础作为前置条件。如果你只需要知道如何运行或者完成分类识别，请参考Kaggle-TianChi分类问题相关算法快速实现。如果需要更详尽的理论解析或者有哪些地方不明白的同学，建议私下联系我stw386@sina.com。如果你想skip read本文，请直接阅读最后一个小节：调参流程梳理。 那么接下来让我们开始正文，虽然本文写的很冗长，我依旧建议阅读完此文，即便是处于懵懂的状态，对后续模型调整的理解也是有一定益处的，而且我会尽可能的用通俗易懂的语言来讲，很多地方会存在解释不严谨的地方但是更易于理解。 Bias-Variance-Tradeof在上次的文章中，我们就提到了一个好的模型应该有着非常好的拟合能力，就是说我的偏差要尽可能的小；同时，也要保证方差尽可能的小，这样我们才能在泛化能力上有很不错的表现。 设样本容量为n的训练集为随机变量的集合(X1, X2, …, Xn)，那么模型是以这些随机变量为输入的随机变量函数（这边的F虽然上函数，但是也是随机变化的）：F(X1, X2, …, Xn)。抽样的随机性带来了模型的随机性。那如何定义一个模型的Bias和Variance呢？这边我们采取的是基模型的加权均值E(F)=E(∑(γ*fi))移动来替代bias、基模型的加权方差Var(F)=Var(∑(γ*fi))替代Variance，更详细的数学定义如下：这边在Variance推导过程中用到了这个性质：Cov(X*Y) = E(X*Y) - E(X)*E(Y)，同时将方差拆分成协方差的形式。我们可以看到，组合后的模型的Bias和基模型的Bias的权重γ相关，组合后的模型的Var和基模型的权重γ、基模型个数m、基模型相关性ρ相关。请务必深刻的记得上述bias和var的数学式子，在后续无论是调参数还是模型设计，我们都是围绕着，降低bias和var的角度去做的。 Bagging Bias and Variance很多同学在没看这篇文章之前就知道，bagging算法和stacking算法是需要基模型保持强基模型（偏差低方差高）的，但是不知道大家有没有想过为什么？阿里15年校招的时候，我有幸回答过一个题目就是这个问题，下面让我们看看为什么是这样的？ 对于bagging算法而言，每次的抽样都是以尽可能使得基模型相互独立为前提的，为了维持这样的假设，我们做了三件事： 样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样 子抽样：整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量 弱抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）下的feature的抽样 同时，由此我们由此也可以得到bias和var公式中的γ=1/m，基模型的权重一定程度是可以看作是相等的，所以原来的E和Var公式就变成：组合模型F的bias和基模型fi的bias一致，这就是我们为什么要求基模型fi的bias要低的原因，因为组合模型F的拟合能力E(F)不随着基模型个数的增加而上升。 组合模型F的var与基模型fi的var、基模型fi的个数m、基模型的相关性ρ相关，很明显可以看出，随着基模型的个数上升Var(F)第二项是在下降的，所以基模型的个数上升会降低组合模型的方差，这就是为什么基模型的方差可以高一些。 除此之外，我们还可以看出，如果基模型相关性ρ越低，整体的方差是越小的，所以我们才去做样本抽样，子抽样，弱抽样等等行为。还有，基模型的个数上升一定程度上会降低组合模型F的方差，但是不是无限递减的，公式中它只能降低第二项的方差值，第一项的方差值不随基模型的个数而增减。 从这个角度看，是不是对Bagging算法的理解又深刻了一些？接下来让我们看看Boosting。 Boosting Bias and Variance依旧的是很多同学在没看这篇文章之前就知道，boosting算法是需要基模型保持弱基模型（偏差高方差低）的，让我们一探究竟。熟悉boosting算法的同学都知道，boosting算法的基模型的相关性几乎≈1的，后续模型强依赖于前模型，所以我们可以认为ρ=1，得到如上的简化式子。组合模型F的bias是基模型的bias的累加，基模型的准确度都不是很高（因为其在训练集上的准确度不高），随着基模型数的增多，整体模型的期望值增加，更接近真实值。而站在方差的角度，组合模型F的方差是随着基模型的fi个数上升而平方上升的，这就要求我们的基模型的方差不能太高，否则组合模型的F就会增长爆炸。这就是为什么我们在boosting模型设计的时候，需要基模型保持弱模型（偏差高方差低）的原因。 多说一句，大家也看到了，boosting的Var(F)是依赖于基模型的权重γ的，所以在后续的gbdt、xgboost，各位数据科学家选择了类似bagging的采样模式，降低模型的γ，控制方差，所以说，了解原理再去重新或者优化还是很重要的。 Bagging、Boosting、Stacking Bias&amp;Variance总结纵观刚才说了的这么多，我们在Bagging、Boosting、Stacking的模型设计中所围绕的就是降低bias的同时，降低Variance。而我们所做的sklearn里面的参数调整就可以说用来： a.构建基模型，变化基模型的Bias和Variance b.组合模型构建，控制基模型后，如何把这个基模型很好的组合成一个优秀的ensemble模型。 GBDT 理论剖析模型过程推导其实random forest和gbdt、xgboost都是非常好的Bagging、Boosting、Stacking算法的优化升级版本，我个人用的gbdt稍多，所以就以gbdt为例子给大家梳理一遍，从理论，到调参数，到trick分享。 我最讨厌很多博主贴个上面的伪代码就跑了，我念书的时候，老师讲题目也是，我靠，我要是看得懂还需要你贴？所以，我们选择一步一步的来看这个伪代码。 让我们先概览一下整个流程，gbdt有递归设计如下： 如果y(i)代表着第i个基模型，第i+1个基模型其实是基于第i个基模型的结果而追加了一个New function去修正前i个基模型的误差。如果以F代替y，h代替f的话，我们可以得到下面这个递归函数： 第i个基模型是由前i-1个基模型中的h(x)累计得到的，我们最后想要得到的分类器即为： 每一轮迭代中，只要集中精力使得Fi(x)每次loss下降的最快即可：每次构建一个残差负梯度作为更新方向的New function(即hi(x))，就可以解决这个复杂的递归问题，从而得到F(x)的解析式。 接下来，我们再看看更加详细的做法： 初始化部分，在这次梳理之前，我也一直认为是随机构造的，这边看完伪代码我才知道，在初始值设置的时候，考虑了直接使得损失函数极小化的常数值，它是只有一个根节点的树，即是一个c常数值。 构建回归树，这边就稍许复杂，让我们拆开一步一步来看： 首先，先求整体损失函数的反向梯度。先举个mse的例子，如果现在我们考虑的是mse(着重注意，只有mse的情况下以下的梯度才是这样的)的形式，我们要做的就是在每一步的时候让我们的预测值Fi(x)与真实值y的损失函数：1/2*(y-Fi(x))^2最小(前面的1/2是为了方便求导计算加上去的)，如果对梯度下降有了解的朋友就知道，此刻要求它的最小就是去求偏导： 按照这个方向去更新Fi(x)，可以保证，组合模型F(x)的每次都是按照最优的方向去优化。 MSE的损失函数确实是残差形式，不代表所有的损失函数下更新的方向都满足这样的残差形式。kaggle master 在blog里面提到Although we can minimize this function directly, gradient descent will let us minimize more complicated loss functions that we can’t minimize directly，我们就需要设计出一种更快速能提升泛化能力且不失一般性的解决方案，所以有大神提出了以梯度下降的值直接代替因变量y，也就是我每次预测不去比预测值与真实值y的差异，我们比较的是预测的梯度方向与真实的梯度方向。 再基于已经预估出来的负梯度去计算最优的更新步长，使得预测值与真实值更加靠近，因为每层的负梯度的方向不固定，所以每层i的步长都是变化的。 最后通过缩减率v（这边就是类似logistics里面的∂）控制速率。 综上，假设test集合第i轮预测中，根据训练集训练出来的负梯度拟合模型不妨记为fi(x)、最优步长γi、缩减率v，可得到最终的递归公式为： 损失函数介绍刚才上面我举了一个mse作为损失函数的例子，其实还有很多其他的，参考如下： 这边说个有意思的东西，就是如果有兴趣的朋友可以把exponential：指数损失函数计算一下，反向梯度为： 则有第i轮损失函数： 这货就是adaboost的第i轮损失函数的非归一化的结果，是不是很有趣，虽然知道了没啥用，但是起码得到了我们在用gbdt的时候，loss=’exponential’即为adaboost的结论啊，哈哈～所以说，我觉得去推推公式，还是很有意思的。 到此为止，gbdt是怎么构造得来的就讲完了，其实这个和bias&amp;variance关系不大，但是为了铺垫后续的GBDT实战剖析，我觉得还是非常有必要梳理一遍的，但就比起Bias-Variance-Tradeof这节的内容，我觉得各位还是着重理解Bias-Variance-Tradeof，这节可以看作是’甜品’缓解下气氛。 GBDT 实战剖析我们以python下的sklearn.ensemble中的GradientBoosting及RandomForest为例子，实战分析一下，如何能够理性的调好参数而并非玄学的gridsearch。 在Bias-Variance-Tradeof我们提到了参数设置分为两块：a.构建基模型,b.构建组合模型，我们分GradientBoosting参数如下： 构建组合模型：1) n_estimators:基模型的个数，对于gbdt来说，因为我们需要通过基模型的个数来提升准确率所以n_estimators一般都会大于random forest的n_estimators的个数，实际上RandomForestClassifier默认为10，GradientBoostingClassifier默认为100也证明了这点。 2）learning_rate：步长，对于gbdt来说，步长依赖于n_estimators，100=2*50=5*20，就是这个道理。而random forest里面不存在步长这个概念。在gbdt里面，关于步长的优化一般是伴随着基模型的变化而变化的。 3）subsample：子采样率，还记得我们上面对子采样的描述么？一般来说，降低“子采样率”（subsample），也会造成子模型间的关联度降低，整体模型的方差减小，但是当子采样率低到一定程度时，子模型的偏差增大，将引起整体模型的准确度降低。 4) init：初始化，更多见GBDT 理论剖析中，我们对初始化的描述。 5）loss：对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。分类模型不说了，刚才在GBDT 理论剖析中讲了，一般用”deviance”比较多；回归模型中，”ls”我们也在GBDT 理论剖析中讲了，异常点多的情况下”huber”,训练集分段预测的话用”quantile”，但是我个人建议异常点或者分段预测还是在数据已处理中完成。 6) alpha：这个参数只有Huber损失”huber”和分位数损失”quantile”下的GradientBoostingRegressor，alpha越小对噪声处理的力度越强，alpha越小分位数的值越小。 构建基模型：1）max_features：每次划分最大特征数，有log2，sqrt，None等等，默认的是sqrt，该值越小，我们每次能获得信息越少，造成偏差时变大的，同时方差是变小的，所以当我们模型拟合能力不足的时候，可以考虑提升该值。 2）max_depth:基模型最大深度，深度越大，模型的拟合能力越强，bias越小。根据Bias-Variance-Tradeof我们对bagging和boosting里面的Var和Bias的描述可知，如果在boost（gbdt）采用了过深的基模型，组合模型的var会很大，在泛化能力会降低，造成训练集效果优秀，测试集差；如果在bagging（random forest）采取了过浅的基模型，组合模型的拟合能力会不足，我们可以考虑增加深度，甚至不控制生长。 3）min_samples_split：内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。随着分裂所需的最小样本数的增加，子模型的结构变得越来越简单，极端情况下，方差减小导致整体模型的拟合能力不足。 4）min_weight_fraction_leaf：叶节点最小权重总值，这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和其他子叶节点一起被剪枝，会使得模型变得简单，降低了方差，提高了偏差，如果正负样本不一致，需要考虑调整这个值。 5）max_leaf_nodes：最大叶子节点数，通过限制最大叶子节点数，可以防止过拟合，会使得模型变得简单，降低了方差，提高了偏差。这边需要注意如果设置了max_leaf_nodes，会忽略max_depth参数值。 梳理完以上每个参数的对模型拟合的能力及对Vaild集合泛化能力的影响，我们可以根据项目训练中，实际模型的训练集拟合效果，检验集的泛化效果进行优化参数。 调参流程梳理ok，问题来了，很多同学看了之后说，你说了这么多参数作用，你还是没告诉我改如何调参数？我就喜欢这种很关注结果的同学，以下干货来自个人及个人朋友及我从知乎等网站”剽窃”来的观点，不负任何理论责任。 我第一任老大，现在在阿里做算法专家，他根据24个数据集合上以不同的调参流程去训练相同的测试集得出的效果对比，总结出以下一个流程： 先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值 再确定合适的subsample 再调优最大树深度（max_depth） 再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）,这边是不一定使用的 再调优最大叶节点数（max_leaf_nodes） 再组合优化分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf） 最后，优化分裂时考虑的最大特征数（max_features） 组合调整n_estimators和learning_rate 但是，我今年在逛知乎的时候偶然看到一个帖子，里面讲的就是调参数的困扰，提到了一个点，就是先确定了max_depth=3后，无论怎么优化min_samples_split和min_samples_leaf对结果都没有任何影响了。当时我想了很久，最后是一位知友解答了这疑惑，其实这样的： 假设原始数据中正负样本比是1:1000，在做max_depth=3的时候，因为样本不均衡，已经可以通过非常简单的少量feature对正负样本进行区分，所以，在之后怎么调节分裂所需要最小样本树和子节点最小样本数都不能够影响到回归树的构造，然而该区分的回归树是没有泛化能力的。 要解决这个问题要么平衡数据，要么就是先确定回归决策树每个叶子结点最小的样本数(min_samples_leaf),再确定分裂所需最小样本数（min_samples_split），才能确定最大深度,这样就能保证不会出现某棵树通过一个feature将数量较少的的正类以较过拟合的简单浅层树拟合出来，而是优先保证了每一次我构造树都尽可能的平衡满足了数据量合理，数据具有样本具有代表性，不会过拟合这样的假设。所以，可以优化为： 先确定快速训练的n_estimators和learning_rate，之后所有的调参基于这个确定的值 再确定合适的subsample 再组合调优最大树深度（max_depth）和叶节点最小样本数（min_samples_leaf） 再调优最大叶节点数（max_leaf_nodes） 再考虑是否有必要修改叶节点最小权重总值（min_weight_fraction_leaf）,这边是不一定使用的 再组合优化分裂所需最小样本数（min_samples_split） 最后，优化分裂时考虑的最大特征数（max_features） 组合调整n_estimators和learning_rate 去年Aarshay Jain大神总结的调参数整理也给出了一种调优思路： 优先，调整最大叶节点数和最大树深度 其次，分裂所需最小样本数（min_samples_split）、叶节点最小样本数（min_samples_leaf）及叶节点最小权重总值（min_weight_fraction_leaf） 然后，分裂时考虑的最大特征数（max_features） 容我多嘴一句，我们思考了这么多，其实如果能在最开始做一个正负样本平衡就会避免很多问题，所以，再次强调数据预处理的重要性。 除此在外，很多人会选择在以上模型调优结束后再以10*learning_rate进行”鞍点逃逸”，以0.1*learning_rate进行”极限探索”。至于random forest及xgboost的更多调参数的细节与gbdt类似，我就不赘述了，有问题可以问我。 终于结束了，这篇文章真的是又繁琐又冗长，希望能够给一些同学对gbdt更深刻的理解。 没啥广告要打，就这样吧。 另求一个比较好的公式编辑器，鬼知道我现在在excel里面写完公式截图过来有多扯淡，而且图片质量超差，谢谢了。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>模型设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[风控用户识别方法]]></title>
    <url>%2F2017%2F12%2F09%2F%E9%A3%8E%E6%8E%A7%E7%94%A8%E6%88%B7%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[update:18.1.1 :Frcwp已如期上线，满足本文中的所有方法，欢迎拍砖 前言因为工作方向相关，之前我也尝试着在Google、arXiv、wikipedia等等地方搜一些风控识别的资料或者思路，但是事与愿违的是，绝大多数的与风控算法都毫无关系，基本上都是推销自己家的产品的，所以，我之前也尝试着写了一些方法的梳理，如: 多算法识别撞库刷券等异常用户 异常值识别与处理 但是在我前几天再回过头去看自己写的这些东西的时候，作为一个老司机来说，我都不想去看一篇又一篇动则上千字的文章，理论交错，文笔粗陋，正巧现在公司内部也有一个风控的项目，所以，我准备做一个开源的项目Frcwp，核心在于： 简单操作，几乎不用多少调参，自动识别异常点 理论清晰，支持的方法多，兼容性好 集成数据预处理的过程，减轻前置工作量 “纠结”了几个朋友的情况下，一期已经完工，主要是搭建了最简单的框架，我相信，这只是一个开始，欢迎大家试用，也欢迎每一个人来批评，更希望有想法的同学一起来做这个事情。 接下来，让我们来讲讲，一期我们做了什么？核心我们一期做的异常点识别中，核心是利用的14年周志华教授提出的isolation forest算法进行识别，详细的理论部分请参见：Isolation Forest，重复说一个事情的意义也不大。这边需要解释几点： 具体是怎么得到当前的算法流程的呢？ 为什么用当前的算法进行识别而不用其他的识别算法？ 当前的设计下存在哪些问题？ 未来的方向会在哪边？ 让我们来一一来回答这些问题。 为了用Isolation Forest而不用其他的识别算法？在设计这套算法之前，我们其实是遇到了一个实际的业务问题，黑产撞库。相信大家毫不陌生这个词，无论是阿里、京东、滴滴还是腾讯，被撞库是一件普通了不能再普通的事情，“黑产”的人从第三方渠道，获取到你历史上的手机号和一些你曾经用的密码，重复的登陆，暴力的尝试，如果你的密码设置的比较简单，比如：“123456”，“qwerty”…非常容易被破解，然后再根据你历史下单的情况，进行假冒“客服”退款，进行诈骗，百度一搜就有一堆这样的新闻： 频发假冒电商客服 当心!近期有人冒充假客服行骗 几分钟骗走市民八九万元… 所以，我们需要阻止“黑产”人员进行这样的暴力破解，获取用户的资料，由此而引发了我们对这个问题的思考。我们在对这个问题分析的时候，巧妙的发现了如下的一些信息：因为涉及公司机密，这边隐去了具体坐标和值，很容易发现以下问题： 正常扇面内数据分布密集，未知扇面内数据分布松散，异常扇面内数据分布稀疏 正常扇面内的数据量占全量数据的绝大多数 不存在明显的分割线，正常扇面和异常扇面存在过度地带 这个给了我们一些启发，我们做了如下的分析： 我们观察了异常扇面内的用户黑白比，如我们预计的黑白比为20:3，也就是说分布远离大量数据点的用户绝大多数存在问题 为止区域的用户黑白比为1:2，这说明在黑白用户之间不存在明显的界限，有交错地带 正常区域内也存在黑名单用户，比例在504:1，也就是说，我们划分有一定识别能力，但是还是不能做到全量识别 综合上述这些预先的处理，我们要用算法完成三件事情：1.切分全量用户，做到识别出正常，未知，异常用户2.识别出异常用户和正常用户之间的差异约束切割3.在异常用户+未知用户里面，找出利用差异约束切割出黑名单 为什么用当前的算法进行识别而不用其他的识别算法？切分数据的时候，我们这边采用的是切比雪夫切割。非理工科的同学可能比较疑惑什么是切比雪夫切割，这边如果数据是正态下，箱式图的Q3+3/2xQI作为上top点进行切割，大家就应该很熟悉了，其实利用的就是数据出现的概率。上面这张图很好的解释了，在数据服从正态分布的情况下，出现数据值比均值+3x标准差要大的概率不足0.1%，所以，我们可以认为这些数据是异常点了。那现在出现了一个问题，日常数据分布都不一定是正态的，所以引出来了类似的切比雪夫理论，它用的是马氏距离距离中心点的程度，详细的马氏距离理论见马氏距离分布。 切分完成数据之后，我们要做寻找差异约束切割逻辑。从最上面的扇面图，我们很容易发现，正常数据与异常数据之间的密度差异很明显，所以如何识别密度差异的算法就是我们需要的，这边我大概找了6、7种常见的切分方法，这边主要讲三种：isolation forest，lof，distance similarity。理论我之前也讲过，贴上地址，不废话了：密度算法。这边主要展示效果差异：通过68个数据集，很明显的可以看出LOF的识别出来的用户的异常用户异常程度是低于Isolation Forest和Distince Similarity的，起码在我们这些数据集样本中，Isolation Forest和Distince Similarity识别效果差异不大，所以，我们再考虑了另一个性能问题：我们用了CV=10的交叉检验，发现，平均下来，Isolation Forest识别速度是Distince Similarity的1/3以下。综合上述，还有一些其他因素，最后我们选择了Isoation Forest的方法。 当前的设计下存在哪些问题？上面说的都是比较正面的问题，让我们看看，有哪些缺点。首先，从头到尾，我们一直在围绕密度差异这个问题，但是就我平时做的一些小爬虫都知道，降低暴力获取的速度，慢慢搞，这时候就以上的方法就无法做到有效的识别。除此之外，因为我们用了切比雪夫不等式，所以对其有概念的同学知道，算马氏距离的时候需要算协方差矩阵，当数据量异常异常大(我测算的是12mx100)的时候计算资源紧张，可能算不出来；数据量异常异常小的时候feature严重共线性，也可能计算不出来。 未来的方向会在哪边？所以，后续我们会新增其他算法，支持过大过小情况下的识别方法。针对数据量过小的识别情况，我在V0.0.3版本下更新了一个简单识别的方法，之后会优化更好的算法替代掉的。只要数据量太大无法计算的问题，我之后会采取矩阵切割分块计算的方法，这个是后话了。 最后，我们以当前算法包的使用来结束整篇介绍：12#安装pip install Frcwp 自动识别过程：123456789101112131415161718from Frcwp import Frcwptraindata = pd.read_table('../路径')#数据可以在https://github.com/sladesha/machine_learning/tree/master/data下的data_all.csv获取frc = Frcwp()traindata = frc.changeformat(traindata, index=0)# You can define your own outlier size , the details of these params can be got from ../Frcwp/Frcwp.py:params = &#123;'na_rate': 0.4,'single_dealed': 1,'is_scale': 0,'distince_method': 'Maha','outlier_rate': 0.05,'strange_rate': 0.15,'nestimators': 150,'contamination': 0.2&#125;# train the frc modelfrc.fit(traindata, **params) 相关的结果显示：123456789# predict outliers with the trained frc modelpredict_params = &#123;'output': 20,'is_whole': 1&#125;frc.predict(frc.potentialdata_set, **predict_params)# if you want get the whole probability of your potential outliersfrc.similarity_label 以上部分内容截取自我的github，希望对大家有一些帮助。 最后，谢谢大家的阅读，欢迎大家关注我的个人博客。 本文拒绝任何形式的转载，若要转载请联系stw386@sina.com]]></content>
      <categories>
        <category>开源项目</category>
      </categories>
      <tags>
        <tag>风控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FM理论解析及应用]]></title>
    <url>%2F2017%2F12%2F04%2FFM%E7%90%86%E8%AE%BA%E8%A7%A3%E6%9E%90%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[FM的产生背景我其实没有做过很多CTR预估的事情，但是我在工作中常常遇到CRM流失预估、订单预估这些依赖于特征工程的事情，其中就涉及到特征的组合问题。 one-hot过程在feature选取过程中，不可避免的会出现，学历这种高中、大学、研究生等多分类的feature，在实际应用中，我们对单个feature需要进行一种one hot过程，就是将原来的学历拆解为 是否为高中，是否为大学，注意，可以不用加是否为研究生一列，因为是否为高中，是否为大学的两列已经可以推导这个用户是否为研究生，加上这一列有时候反而会共线性。但是这样做，看起来没什么问题，想想看要是100个这样的特征，每个特征有100个这样单独的feature value的话，整体数据将是一个非常庞大的稀疏矩阵，无论是计算还是分析都是会存在巨大的问题的，所以看看我们能不能组合一些特征降低维度。 什么叫做组合问题现在有一组数据，其中特征包含性别（男女），学历（高中，大学，研究生），想要判断这两个feature对是否对化妆品感谢兴趣。单独的观察性别这一栏，发现有一定相关性，但是比较弱，并不是所有的女性都对化妆品感兴趣；单独的观察学历这一栏也发现，学历与对化妆品感兴趣的程度并没有显著的相关性。其实，我们可以从自己的感知理解，首先，数据中女生可能比男生对化妆品更感兴趣，但是女生数据中存在大量的高中生，相对于高中生而言，大学生和研究生可能对化妆品更加感兴趣一点，所以原来的两个feature：性别，学历就组合成了是否为性别女+学历大于高中一个feature，这就是特征组合的过程。如果feature总个数少还可以，要是要有上千上万个，光两两组和就有n*(n-1)/2种可能，所以我们需要想一个其他办法。 组合特征后的表达形式首先，我们都知道一般的线性模型为： 为了考虑组合特征的作用，我们采用多项式来代表，形如特征xi与xj的组合用xixj表示，具体的表达式如下：其中，wij为组合特征xixj的权重，n表示样本的feature个数，xi为第i个feature。 方程定义完成了，下面就要开始数学定义对每一个特征xi引入辅助向量Vi=(vi1,vi2,…vik),这边的k就是矩阵拆解的规模值，利用ViVj.T对交叉项的系数wij进行估计,及则这边需要注意一点，k理论上讲，越大越能强化拟合的能力，但是实际在运算过程中，一来受限于计算能力，二来受限于数据量，过大的k只会带来过拟合的问题。我实测了40w左右的数据，观察到k值在6-8左右，valid集合数据拟合效果最优，仅供参考 很明显，上面这么多未知数：1+n是线性未知数个数，nxfeature是组合特征的未知数个数，常规求解的效率可想而知。但是看到xixj这样的形式，我们很容易联想到：2ab = (a+b)^2 -a^2 -b^2，所以在解决这个wij、xi、xj点积的问题上，我们采用了：1/2 * ( (a+b+c)^2 - a^2 - b^2 - c^2)的方式 下面让我们来解这个式子这边需要一点导数功底，我们先来看对w0也就是bias求导，这个毫无意外，梯度为1；再对wi求导，这个也很简单，xi即可，这个也很简单，少许繁琐的就是wij求导，让我来仔细看看：ok，我知道我的字很丑，别说话，看问题，所以我们可以总结为下面这个网上到处都有的式子：这个式子就是上面这么来的。把上面的那个点积形式代入求解及为： 引申一个FFM概念在FM模型中，每一个特征会对应一个隐变量，但在FFM模型中，认为应该将特征分为多个field，每个特征对应每个field分别有一个隐变量。 举个例子，我们的样本有3种类型的字段：qualifications, age, gender，分别可以代表学历，年龄段，性别。其中qualifications有3种数据，age有5种数据，gender有男女2种，经过one-hot编码以后，每个样本有7个特征，其中只有3个特征非空。如果使用FM模型，则7个特征，每个特征对应一个隐变量。如果使用FFM模型，则7个特征，每个特征对应3个隐变量，即每个类型对应一个隐变量，及对应qualifications, age, gender各占一个。 我看了Yu-Chin Juan实现了一个C++版的FFM模型的源码，倒过来想他的表达式应该是这样的：其他模块都与fm差不多，主要看Vj1f2Vj2f1这个东西。我们假设j1特征属于f1这个field，j2特征属于f2这个feild，则Vj1f2表示j1这个特征对应j2所属的field的隐变量。很恶心的解释，通俗的来讲就是，性别为女与学历这个field的组合有个隐变量，性别女与年龄这个field的组合又有一个不一样的隐变量，而却不考虑到底是什么学历是啥，年龄具体到什么细节。Yu-Chin Juan大神在实际写code的过程中，干掉来常数和一次项，可能是为了方便计算，保留的如下：整理的最优化损失函数如下：前面为l2正则，后面为交互熵形式，我们看到了y*Φ(V,x)这个及其类似hinge loss里面的1−t⋅y部分，所以注意这边的y属于{-1，1}这边的求导，我算了一个小时都没搞出来，等哪天有空了，再仔细的去算一下，去翻了原论文，最后的迭代形式如下：η是常规的速率，V是初始均匀分布即可 代码实现我这边完成了FM的代码实现，详细见我的github：fm代码为了方便不想看细节，只想撸代码的同学，我打包上传到了pypi，你只需要pip install Fsfm即可体验至于ffm，我下午实在没写出来，对不起彭老师，丢脸了，后续看什么时候有空再研究一下。 最后，着重提示，本文很多思路很解析都参考的Yu-Chin Juan的源代码，附上github地址，欢迎去关注原作者的内容，感谢大神带路，谢谢大家阅读。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征交叉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python开发：特征工程代码模版(二)]]></title>
    <url>%2F2017%2F12%2F01%2Fpython%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[update:17.12.21 : Mutual Information互信息中mic_entroy函数里的I应该是i，已修正 转载请注明文章来源：python开发：特征工程代码模版（二），你们免费转我文章，不标注来源就算了，现在还开始写“原创”，这就过分了～ 正题开始：这篇文章是入门级的特征处理的打包解决方案的python实现汇总，如果想get一些新鲜血液的朋友可以叉了，只是方便玩数据的人进行数据特征筛选的代码集合，话不多说，让我们开始。 首先，让我们看一张入门级别的数据预处理的基本操作图，网上有很多版本，这个是我自己日常干活的时候必操作的行为罗列，其中数据整理部分已经在上一篇文章中给出了，下面我们讲一起来看看特征筛选这块。此图请尊重一下我，别拿出去传播，纯属个人的方法论，大家看看就行，谢谢。网上有其他版本的，你们去传播那些就ok了～ 方差选择法1234567def var_filter(data, k=None):var_data = data.var().sort_values()if k is not None:new_data = VarianceThreshold(threshold=k).fit_transform(data)return var_data, new_dataelse:return var_data 这个方法的思路很明确，我们筛掉方差过小的feature，也很好理解，一列值完全或者几乎完全一致的feature对于我们去训练最后的模型没有任何好处。熵理论也同样印证了这一点。 线性相关系数衡量123456789101112131415161718192021def pearson_value(data, label, k=None):label = str(label)# k为想删除的feature个数Y = data[label]x = data[[x for x in data.columns if x != label]]res = []for i in range(x.shape[1]):data_res = np.c_[Y, x.iloc[:, i]].Tcor_value = np.abs(np.corrcoef(data_res)[0, 1])res.append([label, x.columns[i], cor_value])res = sorted(np.array(res), key=lambda x: x[2])if k is not None:if k &lt; len(res):new_c = [] # 保留的featurefor i in range(len(res) - k):new_c.append(res[i][1])return res, new_celse:print('feature个数越界～')else:return res 当你明确了自变量与因变量之间存在线性关系的时候，你就需要剔除掉一些关心比较弱的变量，奥卡姆剃刀原理告诉我们，在尽可能压缩feature个数大小的情况下去得到效果最优的模型才是合理模型。 共线性检验123456789101112131415161718192021222324252627def vif_test(data, label, k=None):label = str(label)# k为想删除的feature个数x = data[[x for x in data.columns if x != label]]res = np.abs(np.corrcoef(x.T))vif_value = []for i in range(res.shape[0]):for j in range(res.shape[0]):if j &gt; I:vif_value.append([x.columns[i], x.columns[j], res[i, j]])vif_value = sorted(vif_value, key=lambda x: x[2])if k is not None:if k &lt; len(vif_value):new_c = [] # 保留的featurefor i in range(len(x)):if vif_value[-i][1] not in new_c:new_c.append(vif_value[-i][1])else:new_c.append(vif_value[-i][0])if len(new_c) == k:breakout = [x for x in x.columns if x not in new_c]return vif_value, outelse:print('feature个数越界～')else:return vif_value 2-3年前面试必考题，什么叫做共线性？如何解决共线性？答案之一就是共线性检验啊，判断feature之间的相关性，剔除相关性较高的feature，在R语言里面有个VIF函数可以直接求的。除此之外，采用非线性函数做特征拆解也是很好的方法。共线性严重的情况下，会导致泛化误差异常大，需着重注意～ Mutual Information互信息12345678910111213141516171819202122232425262728293031323334353637383940def MI(X, Y):# len(X) should be equal to len(Y)# X,Y should be the class featuretotal = len(X)X_set = set(X)Y_set = set(Y)if len(X_set) &gt; 10:print('%s非分类变量，请检查后再输入' % X_set)sys.exit()elif len(Y_set) &gt; 10:print('%s非分类变量，请检查后再输入' % Y_set)sys.exit()# Mutual informationMI = 0eps = 1.4e-45for i in X_set:for j in Y_set:indexi = np.where(X == i)indexj = np.where(Y == j)ijinter = np.intersect1d(indexi, indexj)px = 1.0 * len(indexi[0]) / totalpy = 1.0 * len(indexj[0]) / totalpxy = 1.0 * len(ijinter) / totalMI = MI + pxy * np.log2(pxy / (px * py) + eps)return MIdef mic_entroy(data, label):# mic_value值越小，两者相关性越弱label = str(label)# k为想删除的feature个数x = data[[x for x in data.columns if x != label]]Y = data[label]mic_value = []for i in range(x.shape[1]):if len(set(x.iloc[:, i])) &lt;= 10:res = MI(Y, x.iloc[:, i])mic_value.append([x.columns[i], res])mic_value = sorted(mic_value, key=lambda x: x[1])return mic_value 本来我想偷懒，直接import minepy然后就得了，发现真的是特么难装，各种报错，一怒之下自己写了，这边求大佬告知，为什么pip install minepy会有这样的问题：12345678xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrunerror: command '/usr/bin/clang' failed with exit status 1----------------------------------------Command "/Users/slade/anaconda3/bin/python -u -c "import setuptools, tokenize;__file__='/private/var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-build-hr9ej0lw/minepy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))" install --record /var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-30cn7rbs-record/install-record.txt --single-version-externally-managed --compile" failed with error code 1 in /private/var/folders/hv/kfb7n4lj06590hqxjv6f3dd00000gn/T/pip-build-hr9ej0lw/minepy/ 回到正题，互信息其实很简单，我们看个公式I(X;Y)=H(X)-H(X|Y)，看完是不是超级清晰了，其实就是X发生的概率中去掉Y发生后X发生的概率，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。计算公式如下，你们也可以在上面的代码里找到影子。最后还是吐槽下，这个minepy太难装了，为了个互信息，不至于不至于～ 递归特征消除法123456789101112131415def wrapper_way(data, label, k=3):# k 为要保留的数据feature个数label = str(label)label_data = data[label]col = [x for x in data.columns if x != label]train_data = data[col]res = pd.DataFrame(RFE(estimator=LogisticRegression(), n_features_to_select=k).fit_transform(train_data, label_data))res_c = []for i in range(res.shape[1]):for j in range(data.shape[1]):if (res.iloc[:, i] - data.iloc[:, j]).sum() == 0:res_c.append(data.columns[j])res.columns = res_creturn res 这边开始的代码就基本上是方法梳理了，没啥亮点，我就大概和大家聊聊，递归特征消除法，用R语言里面的step()函数是一毛一样的东西，都是循环sample特征，选一个对于当前模型，特征组合最好的结果。如果数据量大，你会有非一般的感觉，这边就有小trick了，以后有空可以和大家分享～ l1/l2正则方法12345678910111213def embedded_way(data, label, way='l2', C_0=0.1):label = str(label)label_data = data[label]col = [x for x in data.columns if x != label]train_data = data[col]res = pd.DataFrame(SelectFromModel(LogisticRegression(penalty=way, C=C_0)).fit_transform(train_data, label_data))res_c = []for i in range(res.shape[1]):for j in range(data.shape[1]):if (res.iloc[:, i] - data.iloc[:, j]).sum() == 0:res_c.append(data.columns[j])res.columns = res_creturn res 正则理论参考：总结：常见算法工程师面试题目整理(二)，这边要提一点，并不是所有情况下都需要正则预处理的，很多算法自带正则，比如logistic啊，比如我们自己去写tensorflow神经网络啊，模型会针对性的解决问题，而这边单纯用的logstic方法来筛选，相对而言内嵌的效果会更好的。 基于树模型特征选择12345678910111213def tree_way(data,label):label = str(label)label_data = data[label]col = [x for x in data.columns if x != label]train_data = data[col]res = pd.DataFrame(SelectFromModel(GradientBoostingClassifier()).fit_transform(train_data, label_data))res_c = []for i in range(res.shape[1]):for j in range(data.shape[1]):if (res.iloc[:, i] - data.iloc[:, j]).sum() == 0:res_c.append(data.columns[j])res.columns = res_creturn res 这边用的是决策树每次分支下，如果改变一列值为随机值，观察对整体数据效果的影响。举个通俗易懂的例子，看看你在公司的重要性，就去和你老板提离职，要是老板疯狂给你加工资做你的思想工作，代表你很重要；如果你的老板让你去财务结账，代表你没啥意义。这里你就是这个feature，你老板就是数据效果的检验指标，常见的就是oob之类的。 这边facebook有个非常好的拓展的思路，但是大家都吹的多实际应用很少，我最近在搞这事情，等下更完这边的特征工程和下面一个nlp的case后，我想专门聊聊这个事情，用的就是决策树的另一角度，以叶子结点代替原feature，做到了非线性的特征融入线性模型，虽然很老套，但是我稍稍做了测试，效果斐然： 最后的最后，感谢大家阅读，希望能够给大家带来收获，谢谢～]]></content>
      <categories>
        <category>代码集合</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python开发：特征工程代码模版(一)]]></title>
    <url>%2F2017%2F12%2F01%2Fpython%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[作为一个算法工程师，我们接的业务需求不会比数据分析挖掘工程师少，作为一个爱偷懒的人，总机械重复的完成一样的预处理工作，我是不能忍的，所以在最近几天，我正在完善一些常规的、通用的预处理的code，方便我们以后在每次分析之前直接import快速搞定，省的每次都要去做一样的事情。 如果大家有什么想实现但是懒得去弄的预处理的步骤也可以私信我，我相对而言闲暇还是有的（毕竟工资少工作也不多，摊手：《），我开发完成后直接贴出来，大家以后一起用就行了 我们需要预加载这些包，而且接下来所有的操作均在dataframe格式下完成，所以我们需要将数据先处理成dataframe格式123456789101112131415from __future__ import divisionimport numpy as npimport pandas as pdfrom sklearn import preprocessingfrom sklearn.cross_validation import train_test_splitfrom sklearn.neighbors import NearestNeighbors__author__ = 'slade_sal'__time__ = '20171128'def change_data_format(data):# 以下预处理都是基于dataframe格式进行的data_new = pd.DataFrame(data)return data_new 空值处理接下来就开始我们的正题了，首先，我们需要判断哪些列是空值过多的，当一列数据的空值占列数的40%以上（经验值），这列能够带给我们的信息就不多了，所以我们需要把某个阀值（rate_base）以上的空值个数的列干掉，如下：123456789101112# 去除空值过多的featuredef nan_remove(data, rate_base=0.4):all_cnt = data.shape[0]avaiable_index = []# 针对每一列feature统计nan的个数，个数大于全量样本的rate_base的认为是异常feature，进行剔除for i in range(data.shape[1]):rate = np.isnan(np.array(data.iloc[:, i])).sum() / all_cntif rate &lt;= rate_base:avaiable_index.append(i)data_available = data.iloc[:, avaiable_index]return data_available, avaiable_index 离群点处理把空值过多的列去完之后，我们需要考虑将一些特别离群的点去掉，这边需要注意两点： 异常值分析类的场景禁止使用这步，比如信用卡评分，爬虫识别等，你如果采取了这步，还怎么去分离出这些异常啊 容忍度高的算法不建议使用这步，比如svm里面已经有了支持向量机这个东西，你如果采取了这步的离群识别的操作会改变原分布而且svm里面决定超平面的核心与离群点无关，后接函数会引发意想不到的彩蛋～ 这边采取盖帽法与额定的分位点方法，建议组合使用，用changed_feature_box定义需要采用盖帽法的列的index_num，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940# 离群点盖帽def outlier_remove(data, limit_value=10, method='box', percentile_limit_set=90, changed_feature_box=[]):# limit_value是最小处理样本个数set，当独立样本大于limit_value我们认为非可onehot字段feature_cnt = data.shape[1]feature_change = []if method == 'box':for i in range(feature_cnt):if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:q1 = np.percentile(np.array(data.iloc[:, i]), 25)q3 = np.percentile(np.array(data.iloc[:, i]), 75)# q3+3/2*qi为上截距点，详细百度分箱图top = q3 + 1.5 * (q3 - q1)data.iloc[:, i][data.iloc[:, i] &gt; top] = topfeature_change.append(i)return data, feature_changeif method == 'self_def':# 快速截断if len(changed_feature_box) == 0:# 当方法选择为自定义，且没有定义changed_feature_box则全量数据全部按照percentile_limit_set的分位点大小进行截断for i in range(feature_cnt):if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limitfeature_change.append(i)else:# 如果定义了changed_feature_box，则将changed_feature_box里面的按照box方法，changed_feature_box的feature index按照percentile_limit_set的分位点大小进行截断for i in range(feature_cnt):if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:if i in changed_feature_box:q1 = np.percentile(np.array(data.iloc[:, i]), 25)q3 = np.percentile(np.array(data.iloc[:, i]), 75)# q3+3/2*qi为上截距点，详细百度分箱图top = q3 + 1.5 * (q3 - q1)data.iloc[:, i][data.iloc[:, i] &gt; top] = topfeature_change.append(i)else:q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limitfeature_change.append(i)return data, feature_change 空值填充在此之后，我们需要对空值进行填充，这边方法就很多很多了，我这边实现的是基本的，分了连续feature和分类feature，分别针对continuous feature采取mean,min,max方式，class feature采取one_hot_encoding的方式；除此之外还可以做分层填充，差分填充等等，那个比较定制化，如果有需要，我也可以搞一套，但是个人觉得意义不大。12345678910111213141516171819202122232425262728293031323334# 空feature填充def nan_fill(data, limit_value=10, countinuous_dealed_method='mean'):feature_cnt = data.shape[1]normal_index = []continuous_feature_index = []class_feature_index = []continuous_feature_df = pd.DataFrame()class_feature_df = pd.DataFrame()# 当存在空值且每个feature下独立的样本数小于limit_value，我们认为是class feature采取one_hot_encoding；# 当存在空值且每个feature下独立的样本数大于limit_value，我们认为是continuous feature采取mean,min,max方式for i in range(feature_cnt):if np.isnan(np.array(data.iloc[:, i])).sum() &gt; 0:if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:if countinuous_dealed_method == 'mean':continuous_feature_df = pd.concat([continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].mean())], axis=1)continuous_feature_index.append(i)elif countinuous_dealed_method == 'max':continuous_feature_df = pd.concat([continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].max())], axis=1)continuous_feature_index.append(i)elif countinuous_dealed_method == 'min':continuous_feature_df = pd.concat([continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].min())], axis=1)continuous_feature_index.append(i)elif len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt; 0 and len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:class_feature_df = pd.concat([class_feature_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=1)class_feature_index.append(i)else:normal_index.append(i)data_update = pd.concat([data.iloc[:, normal_index], continuous_feature_df, class_feature_df], axis=1)return data_update one hot encoding过程分类feature的one hot encoding过程，常见操作，不多说123456789101112131415# onehotencodingdef ohe(data, limit_value=10):feature_cnt = data.shape[1]class_index = []class_df = pd.DataFrame()normal_index = []# limit_value以下的均认为是class feature，进行ohe过程for i in range(feature_cnt):if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:class_index.append(i)class_df = pd.concat([class_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=1)else:normal_index.append(i)data_update = pd.concat([data.iloc[:, normal_index], class_df], axis=1)return data_update Smote过程正负样本不平衡的解决，这边我写的是smote，理论部分建议参考：Python：SMOTE算法,其实简单的欠抽样和过抽样就可以解决，建议参考这边文章：Python:数据抽样平衡方法重写。都是一些老生常谈的问题了，不多说了，上代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# smote unbalance datasetfrom __future__ import divisionimport pandas as pdimport numpy as npfrom sklearn.neighbors import NearestNeighborsimport sysimport pickle__author__ = 'sladesal'__time__ = '20171110'"""Parameters----------data : 原始数据tag_index : 因变量所在的列数，以0开始max_amount : 少类别类想要达到的数据量std_rate : 多类:少类想要达到的比例#如果max_amount和std_rate同时定义优先考虑max_amount的定义kneighbor : 生成数据依赖kneighbor个附近的同类点，建议不超过5个kdistinctvalue : 认为每列不同元素大于kdistinctvalue及为连续变量，否则为class变量method ： 生成方法"""# smote unbalance datasetdef smote(data, tag_index=None, max_amount=0, std_rate=5, kneighbor=5, kdistinctvalue=10, method='mean'):try:data = pd.DataFrame(data)except:raise ValueErrorcase_state = data.iloc[:, tag_index].groupby(data.iloc[:, tag_index]).count()case_rate = max(case_state) / min(case_state)location = []if case_rate &lt; 5:print('不需要smote过程')return dataelse:# 拆分不同大小的数据集合less_data = np.array(data[data.iloc[:, tag_index] == np.array(case_state[case_state == min(case_state)].index)[0]])more_data = np.array(data[data.iloc[:, tag_index] == np.array(case_state[case_state == max(case_state)].index)[0]])# 找出每个少量数据中每条数据k个邻居neighbors = NearestNeighbors(n_neighbors=kneighbor).fit(less_data)for i in range(len(less_data)):point = less_data[i, :]location_set = neighbors.kneighbors([less_data[i]], return_distance=False)[0]location.append(location_set)# 确定需要将少量数据补充到上限额度# 判断有没有设定生成数据个数，如果没有按照std_rate(预期正负样本比)比例生成if max_amount &gt; 0:amount = max_amountelse:amount = int(max(case_state) / std_rate)# 初始化，判断连续还是分类变量采取不同的生成逻辑times = 0continue_index = [] # 连续变量class_index = [] # 分类变量for i in range(less_data.shape[1]):if len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; kdistinctvalue:continue_index.append(i)else:class_index.append(i)case_update = list()location_transform = np.array(location)while times &lt; amount:# 连续变量取附近k个点的重心，认为少数样本的附近也是少数样本new_case = []pool = np.random.permutation(len(location))[1]neighbor_group = location_transform[pool]if method == 'mean':new_case1 = less_data[list(neighbor_group), :][:, continue_index].mean(axis=0)# 连续样本的附近点向量上的点也是异常点if method == 'random':away_index = np.random.permutation(len(neighbor_group) - 1)[1]neighbor_group_removeorigin = neighbor_group[1:][away_index]new_case1 = less_data[pool][continue_index] + np.random.rand() * (less_data[pool][continue_index] - less_data[neighbor_group_removeorigin][continue_index])# 分类变量取modenew_case2 = np.array(pd.DataFrame(less_data[neighbor_group, :][:, class_index]).mode().iloc[0, :])new_case = list(new_case1) + list(new_case2)if times == 0:case_update = new_caseelse:case_update = np.c_[case_update, new_case]print('已经生成了%s条新数据，完成百分之%.2f' % (times, times * 100 / amount))times = times + 1less_origin_data = np.hstack((less_data[:, continue_index], less_data[:, class_index]))more_origin_data = np.hstack((more_data[:, continue_index], more_data[:, class_index]))data_res = np.vstack((more_origin_data, less_origin_data, np.array(case_update.T)))label_columns = [0] * more_origin_data.shape[0] + [1] * (less_origin_data.shape[0] + np.array(case_update.T).shape[0])data_res = pd.DataFrame(data_res)return data_res 总体整合一期的内容就这样吧，我感觉也没有啥好说的，都是数据分析挖掘的一些基本操作，我只是为了以后能够复用模版化了，下面贴一个全量我做预处理的过程，没啥差异，整合了一下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255from __future__ import divisionimport numpy as npimport pandas as pdfrom sklearn import preprocessingfrom sklearn.cross_validation import train_test_splitfrom sklearn.neighbors import NearestNeighborsimport sys__author__ = 'slade_sal'__time__ = '20171128'def change_data_format(data):# 以下预处理都是基于dataframe格式进行的data_new = pd.DataFrame(data)return data_new# 去除空值过多的featuredef nan_remove(data, rate_base=0.4):all_cnt = data.shape[0]avaiable_index = []# 针对每一列feature统计nan的个数，个数大于全量样本的rate_base的认为是异常feature，进行剔除for i in range(data.shape[1]):rate = np.isnan(np.array(data.iloc[:, i])).sum() / all_cntif rate &lt;= rate_base:avaiable_index.append(i)data_available = data.iloc[:, avaiable_index]return data_available, avaiable_index# 离群点盖帽def outlier_remove(data, limit_value=10, method='box', percentile_limit_set=90, changed_feature_box=[]):# limit_value是最小处理样本个数set，当独立样本大于limit_value我们认为非可onehot字段feature_cnt = data.shape[1]feature_change = []if method == 'box':for i in range(feature_cnt):if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:q1 = np.percentile(np.array(data.iloc[:, i]), 25)q3 = np.percentile(np.array(data.iloc[:, i]), 75)# q3+3/2*qi为上截距点，详细百度分箱图top = q3 + 1.5 * (q3 - q1)data.iloc[:, i][data.iloc[:, i] &gt; top] = topfeature_change.append(i)return data, feature_changeif method == 'self_def':# 快速截断if len(changed_feature_box) == 0:# 当方法选择为自定义，且没有定义changed_feature_box则全量数据全部按照percentile_limit_set的分位点大小进行截断for i in range(feature_cnt):if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limitfeature_change.append(i)else:# 如果定义了changed_feature_box，则将changed_feature_box里面的按照box方法，changed_feature_box的feature index按照percentile_limit_set的分位点大小进行截断for i in range(feature_cnt):if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:if i in changed_feature_box:q1 = np.percentile(np.array(data.iloc[:, i]), 25)q3 = np.percentile(np.array(data.iloc[:, i]), 75)# q3+3/2*qi为上截距点，详细百度分箱图top = q3 + 1.5 * (q3 - q1)data.iloc[:, i][data.iloc[:, i] &gt; top] = topfeature_change.append(i)else:q_limit = np.percentile(np.array(data.iloc[:, i]), percentile_limit_set)data.iloc[:, i][data.iloc[:, i] &gt; q_limit] = q_limitfeature_change.append(i)return data, feature_change# 空feature填充def nan_fill(data, limit_value=10, countinuous_dealed_method='mean'):feature_cnt = data.shape[1]normal_index = []continuous_feature_index = []class_feature_index = []continuous_feature_df = pd.DataFrame()class_feature_df = pd.DataFrame()# 当存在空值且每个feature下独立的样本数小于limit_value，我们认为是class feature采取one_hot_encoding；# 当存在空值且每个feature下独立的样本数大于limit_value，我们认为是continuous feature采取mean,min,max方式for i in range(feature_cnt):if np.isnan(np.array(data.iloc[:, i])).sum() &gt; 0:if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt;= limit_value:if countinuous_dealed_method == 'mean':continuous_feature_df = pd.concat([continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].mean())], axis=1)continuous_feature_index.append(i)elif countinuous_dealed_method == 'max':continuous_feature_df = pd.concat([continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].max())], axis=1)continuous_feature_index.append(i)elif countinuous_dealed_method == 'min':continuous_feature_df = pd.concat([continuous_feature_df, data.iloc[:, i].fillna(data.iloc[:, i].min())], axis=1)continuous_feature_index.append(i)elif len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &gt; 0 and len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:class_feature_df = pd.concat([class_feature_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=1)class_feature_index.append(i)else:normal_index.append(i)data_update = pd.concat([data.iloc[:, normal_index], continuous_feature_df, class_feature_df], axis=1)return data_update# onehotencodingdef ohe(data, limit_value=10):feature_cnt = data.shape[1]class_index = []class_df = pd.DataFrame()normal_index = []# limit_value以下的均认为是class feature，进行ohe过程for i in range(feature_cnt):if len(pd.DataFrame(data.iloc[:, i]).drop_duplicates()) &lt; limit_value:class_index.append(i)class_df = pd.concat([class_df, pd.get_dummies(data.iloc[:, i], prefix=data.columns[i])], axis=1)else:normal_index.append(i)data_update = pd.concat([data.iloc[:, normal_index], class_df], axis=1)return data_update# smote unbalance datasetfrom __future__ import divisionimport pandas as pdimport numpy as npfrom sklearn.neighbors import NearestNeighborsimport sysimport pickle__author__ = 'sladesal'__time__ = '20171110'"""Parameters----------data : 原始数据tag_index : 因变量所在的列数，以0开始max_amount : 少类别类想要达到的数据量std_rate : 多类:少类想要达到的比例#如果max_amount和std_rate同时定义优先考虑max_amount的定义kneighbor : 生成数据依赖kneighbor个附近的同类点，建议不超过5个kdistinctvalue : 认为每列不同元素大于kdistinctvalue及为连续变量，否则为class变量method ： 生成方法"""# smote unbalance datasetdef smote(data, tag_index=None, max_amount=0, std_rate=5, kneighbor=5, kdistinctvalue=10, method='mean'):try:data = pd.DataFrame(data)except:raise ValueErrorcase_state = data.iloc[:, tag_index].groupby(data.iloc[:, tag_index]).count()case_rate = max(case_state) / min(case_state)location = []if case_rate &lt; 5:print('不需要smote过程')return dataelse:# 拆分不同大小的数据集合less_data = np.array(data[data.iloc[:, tag_index] == np.array(case_state[case_state == min(case_state)].index)[0]])more_data = np.array(data[data.iloc[:, tag_index] == np.array(case_state[case_state == max(case_state)].index)[0]])# 找出每个少量数据中每条数据k个邻居neighbors = NearestNeighbors(n_neighbors=kneighbor).fit(less_data)for i in range(len(less_data)):point = less_data[i, :]location_set = neighbors.kneighbors([less_data[i]], return_distance=False)[0]location.append(location_set)# 确定需要将少量数据补充到上限额度# 判断有没有设定生成数据个数，如果没有按照std_rate(预期正负样本比)比例生成if max_amount &gt; 0:amount = max_amountelse:amount = int(max(case_state) / std_rate)# 初始化，判断连续还是分类变量采取不同的生成逻辑times = 0continue_index = [] # 连续变量class_index = [] # 分类变量for i in range(less_data.shape[1]):if len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; kdistinctvalue:continue_index.append(i)else:class_index.append(i)case_update = list()location_transform = np.array(location)while times &lt; amount:# 连续变量取附近k个点的重心，认为少数样本的附近也是少数样本new_case = []pool = np.random.permutation(len(location))[1]neighbor_group = location_transform[pool]if method == 'mean':new_case1 = less_data[list(neighbor_group), :][:, continue_index].mean(axis=0)# 连续样本的附近点向量上的点也是异常点if method == 'random':away_index = np.random.permutation(len(neighbor_group) - 1)[1]neighbor_group_removeorigin = neighbor_group[1:][away_index]new_case1 = less_data[pool][continue_index] + np.random.rand() * (less_data[pool][continue_index] - less_data[neighbor_group_removeorigin][continue_index])# 分类变量取modenew_case2 = np.array(pd.DataFrame(less_data[neighbor_group, :][:, class_index]).mode().iloc[0, :])new_case = list(new_case1) + list(new_case2)if times == 0:case_update = new_caseelse:case_update = np.c_[case_update, new_case]print('已经生成了%s条新数据，完成百分之%.2f' % (times, times * 100 / amount))times = times + 1less_origin_data = np.hstack((less_data[:, continue_index], less_data[:, class_index]))more_origin_data = np.hstack((more_data[:, continue_index], more_data[:, class_index]))data_res = np.vstack((more_origin_data, less_origin_data, np.array(case_update.T)))label_columns = [0] * more_origin_data.shape[0] + [1] * (less_origin_data.shape[0] + np.array(case_update.T).shape[0])data_res = pd.DataFrame(data_res)return data_res# 数据分列def reload(data):feature = pd.concat([data.iloc[:, :2], data.iloc[:, 4:]], axis=1)tag = data.iloc[:, 3]return feature, tag# 数据切割def split_data(feature, tag):X_train, X_test, y_train, y_test = train_test_split(feature, tag, test_size=0.33, random_state=42)return X_train, X_test, y_train, y_testif __name__ == '__main__':path = sys.argv[0]data_all = pd.read_table(str(path))print('数据读取完成！')# 更改数据格式data_all = change_data_format(data_all)# 删除电话号码列data_all = data_all.iloc[:, 1:]data_all, data_avaiable_index = nan_remove(data_all)print('空值列处理完毕！')data_all, _ = outlier_remove(data_all)print('异常点处理完成！')data_all = nan_fill(data_all)print('空值填充完成！')data_all = ohe(data_all)print('onehotencoding 完成！')data_all = smote(data_all,tag_index=1)print('smote过程完成！')feature, tag = reload(data_all)X_train, X_test, y_train, y_test = split_data(feature, tag)print('数据预处理完成！') 大家自取自用，这个也没啥好转载的，没啥干货，只是方便大家日常工作，就别转了，谢谢各位编辑大哥了。 最后，感谢大家阅读，谢谢。]]></content>
      <categories>
        <category>代码集合</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[能够快速实现的协同推荐]]></title>
    <url>%2F2017%2F12%2F01%2F%E8%83%BD%E5%A4%9F%E5%BF%AB%E9%80%9F%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%8D%8F%E5%90%8C%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[对于中小型的公司，用户的数据量及公司产品的个数都是较小规模的，需要提供给用户的推荐系统实现的重心也从人性化变成了实现成本，协同推荐就是非常常见、有效且可以快速实现的方法，也是本文想介绍的。 常规的快速简单推荐系统实现方法不排除以下几种： 热门推荐所有人打开浏览的内容都一致，惊喜性会有所缺失，但是实现特别简单，稍加逻辑带给用户的体验感满足了基本需求。 SVD+推荐之前也讨论过实现方法了，附上链接：SVD及扩展的矩阵分解方法 基于模型推荐这个比较偏向业务场景，可以说是经典的场景化模型，之前写过一篇基于用户特征的偏好推荐，可以参考一下：苏宁易购的用户交叉推荐 协同推荐这个也是几乎每个公司都会用的，也是非常非常常见有效的算法之一 协同推荐介绍首先，我们先来了解一下什么叫做协同推荐。基于用户的协同过滤推荐算法是最早诞生的，1992年提出并用于邮件过滤系统，两年后1994年被 GroupLens 用于新闻过滤。一直到2000年左右，该算法都是推荐系统领域最著名的算法。算是非常古董级别的算法之一了，但是古董归古董，它的效果以及实现的成本却奠定了它在每个公司不可取代的地位。 基于用户的协同推荐123用户u1喜欢的电影是A，B，C用户u2喜欢的电影是A, C, E, F用户u3喜欢的电影是B，D 假设u1、u2、u3用户喜欢的电影分布如上，基于用户的协同推荐干了这么一件事情，它根据每个用户看的电影（A、B、C、…）相似程度，来计算用户之间的相似程度，将高相似的用户看过但是目标用户还没有看过的电影推荐给目标用户。 基于商品的协同推荐123456电影A被u1，u2看过电影B被u1，u3看过电影C被u1，u2看过电影D被u3看过电影E被u2看过电影F被u2看过 假设A～F电影被用户观影的分布如上，基于商品的协同推荐干了这么一件事情，它根据电影（A、B、C、…）被不同用户观看相似程度，来计算电影之间的相似程度，根据目标用户看过的电影的高相似度的电影推荐给目标用户。 看起来以上的逻辑是非常简单的，其实本来也是非常简单的，我看了下，网上关于以上的代码实现还是比较林散和有问题的，优化了python版本的code，并详细解释了每一步，希望，对初学者有所帮助。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#time 2017-09-17#author：shataowei#based-item#所要的基础包比较简单from collections import defaultdictimport mathimport timestartTime = time.time()#读取数据的过程#/Users/slade/Desktop/machine learning/data/recommender/u1.basedef readdata(location):list2item = &#123;&#125; #商品对应的用户列表(1:[[1,2],[2,3]]代表商品1对应用户1的行为程度为2,商品1对应的用户2的行为程度为3)list2user = &#123;&#125; #用户对应的商品列表(1:[[1,2],[2,3]]代表用户1对应商品1的行为程度为2,用户1对应的商品2的行为程度为3)f = open(location,'r')data = f.readlines()data = [x.split('\t') for x in data]f.close()for i in data:if int(i[1]) not in list2item.keys():list2item[int(i[1])] = [[int(i[0]),int(i[2])]]else:list2item[int(i[1])].append([int(i[0]),int(i[2])])if int(i[0]) not in list2user.keys():list2user[int(i[0])] = [[int(i[1]),int(i[2])]]else:list2user[int(i[0])].append([int(i[1]),int(i[2])])return list2item,list2user#list2item,list2user=readdata('/Users/slade/Desktop/machine learning/data/recommender/u1.base')## 基于item的协同推荐#0.将用户行为程度离散化：浏览：1，搜索：2，收藏：3，加车：4，下单未支付5#1.计算item之间的相似度：item共同观看次数/单item次数连乘#2.寻找目标用户观看过的item相关的其他item列表#3.计算其他item的得分：相似度*用户行为程度，求和#0 hive操作# 1.1统计各商品出现次数def itemcf_itemall(userlist = list2user):I=&#123;&#125;for key in userlist:for item in userlist[key]:if item[0] not in I.keys():I[item[0]] = 0I[item[0]] = I[item[0]] + 1return I# 1.2计算相似矩阵def itemcf_matrix(userlist = list2user):C=defaultdict(defaultdict)W=defaultdict(defaultdict)#根据用户的已购商品来形成对应相似度矩阵for key in userlist:for item1 in userlist[key]:for item2 in userlist[key]:if item1[0] == item2[0]:continueif item2 not in C[item1[0]].keys():C[item1[0]][item2[0]] = 0C[item1[0]][item2[0]] = C[item1[0]][item2[0]] + 1#计算相似度，并填充上面对应的相似度矩阵for i , j in C.items():for z , k in j.items():W[i][z] = k/math.sqrt(I[i]*I[z])#k/math.sqrt(I[i]*I[z])计算相似度，其中k为不同商品交集，sqrt(I[i]*I[z])用来压缩那些热门商品必然有高交集的问题return W#2.寻找用户观看的其他itemdef recommendation(userid,k):score_final = defaultdict(int)useriditem = []for item,score in list2user[userid]:#3.计算用户的item得分，k来控制用多少个相似商品来计算最后的推荐商品for i , smimilarity in sorted(W[item].items() , key = lambda x:x[1] ,reverse =True)[0:k]:for j in list2user[userid]:useriditem.append(j[0])if i not in useriditem:score_final[i] = score_final[i] + smimilarity * score#累加每一个商品用户的评分与其它商品的相似度积的和作为衡量#最后的10控制输出多少个推荐商品l = sorted(score_final.items() , key = lambda x : x[1] , reverse = True)[0:10]return l#I = itemcf_itemall()#W = itemcf_matrix()#result_userid = recommendation(2,k=20)endTime = time.time()print endTime-startTime python来实现基于item的协同推荐就完成了，核心的相似度计算可以根据实际问题进行修改，整体流程同上即可，当然数据量大的时候分布式去写也是可以的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#time 2017-09-17#author：shataowei#based-userfrom collections import defaultdictimport mathimport timestartTime = time.time()#读取数据#/Users/slade/Desktop/machine learning/data/recommender/u1.basedef readdata(location):list2item = &#123;&#125; #商品对应的用户列表list2user = &#123;&#125; #用户对应的商品列表f = open(location,'r')data = f.readlines()data = [x.split('\t') for x in data]f.close()for i in data:if int(i[1]) not in list2item.keys():list2item[int(i[1])] = [[int(i[0]),int(i[2])]]else:list2item[int(i[1])].append([int(i[0]),int(i[2])])if int(i[0]) not in list2user.keys():list2user[int(i[0])] = [[int(i[1]),int(i[2])]]else:list2user[int(i[0])].append([int(i[1]),int(i[2])])return list2item,list2user#list2item,list2user=readdata('/Users/slade/Desktop/machine learning/data/recommender/u1.base')#基于用户的协同推荐#0.先通过hive求出近一段时间（根据业务频率定义），用户商品的对应表#1.求出目标用户的邻居，并计算目标用户与邻居之间的相似度#2.列出邻居所以购买的商品列表#3.针对第二步求出了商品列表，累加所对应的用户相似度，并排序求top#0.hive操作#1.1求出目标用户的邻居，及对应的相关程度def neighbour(userid,user_group = list2user,item_group = list2item):neighbours = &#123;&#125;for item in list2user[userid]:for user in list2item[item[0]]:if user[0] not in neighbours.keys():neighbours[user[0]] = 0neighbours[user[0]] = neighbours[user[0]] + 1return neighbors#通常来说，基于item的推荐对于商品量较大的业务会构成一个巨大的商品矩阵，这时候如果用户人均购买量较低的时候，可以考虑使用基于user的推荐，它在每次计算的时候会只考虑相关用户，也就是这边的neighbours(有点支持向量基的意思)，大大的降低了计算量。#neighbours = neighbour(userid=2)#1.2就算用户直接的相似程度,这边用的余弦相似度：点积/模的连乘def similarity(user1,user2):x=0y=0z=0for item1 in list2user[user1]:for item2 in list2user[user2]:if item1[0]==item2[0]:x1 = item1[1]*item1[1]y1 = item2[1]*item2[1]z1 = item1[1]*item2[1]x = x + x1y = y + y1z = z + z1#避免分母为0if x * y == 0 :simi = 0else:simi = z / math.sqrt(x * y)return simi#1.3计算目标用户与邻居之间的相似度：def N_neighbour(userid,neighbours,k):neighbour = neighbours.keys()M = []for user in neighbour:simi = similarity(userid,user)M.append((user,simi))M = sorted(M,key = lambda x:x[1] ,reverse = True)[0:k]return M#M = N_neighbour(userid,neighbours,k=200)#2.列出邻居所购买过的商品并计算商品对应的推荐指数def neighbour_item(M=M):R = &#123;&#125;M1 = dict(M)for neighbour in M1:for item in list2user[neighbour]:if item[0] not in R.keys():R[item[0]] = M1[neighbour] * item[1]else:R[item[0]] = R[item[0]] + M1[neighbour] * item[1]#根据邻居买过什么及与邻居的相似度，计算邻居买过商品的推荐度return R# R = neighbour_item(M)#3.排序得到推荐商品Rank = sorted(R.items(),key=lambda x:x[1],reverse = True)[0:50]endTime = time.time()print endTime-startTime python来实现基于user的协同推荐就完成了，核心的相似度计算可以根据实际问题进行修改，基于user的实现过程中，用了邻居这个概念，大大降低了计算量，我用了大概20万用户，2千的商品数，基于user的推荐实现速度大概为基于商品的10分之一，效果差异却相差不大。 协同推荐是非常简单的推荐入门算法之一，也是必须要手动快速代码实现的算法之一，希望能给大家一些帮助。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于SSD下的图像内容识别（二）]]></title>
    <url>%2F2017%2F12%2F01%2F%E5%9F%BA%E4%BA%8ESSD%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[上一节粗略的描述了如何关于图像识别，抠图，分类的理论相关，本节主要用代码，来和大家一起分析每一步骤。看完本节，希望你也能独立完成自己的图片、视频的内容实时定位。 首先，我们需要安装TensorFlow环境，建议利用conda进行安装，配置，90%尝试单独安装的人最后都挂了。 其次，我们需要安装从git上下载训练好的模型，git clone https://github.com/balancap/SSD-Tensorflow如果没有安装git的朋友，请自行百度安装。 最后找到你下载的位置进行解压，unzip ./SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt.zip这边务必注意，网上90%的教程这边就结束了，其实你这样是最后跑不通代码的，你需要把解压的文件进行移动到checkpoint的文件夹下面，这个问题git上这个同学解释了，详细的去看下https://github.com/balancap/SSD-Tensorflow/issues/150 最后的最后，下载你需要检测的网路图片，就ok了 预处理步骤完成了，下面让我们看代码。加载相关的包：123456789101112import osimport mathimport randomimport sysimport numpy as npimport tensorflow as tfimport cv2import matplotlib.pyplot as pltimport matplotlib.cm as mpcmsys.path.append('./SSD-Tensorflow/')from nets import ssd_vgg_300, ssd_common, np_methodsfrom preprocessing import ssd_vgg_preprocessing 配置相关TensorFlow环境123gpu_options = tf.GPUOptions(allow_growth=True)config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)isess = tf.InteractiveSession(config=config) 做图片的格式的处理，使他满足input的条件123456789101112131415161718192021222324#我们用的TensorFlow下的一个集成包slim，比tensor要更加轻便slim = tf.contrib.slim#训练数据中包含了一下已知的类别，也就是我们可以识别出以下的东西，不过后续我们将自己自己训练自己的模型，来识别自己想识别的东西l_VOC_CLASS = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle','bus', 'car', 'cat', 'chair', 'cow','diningTable', 'dog', 'horse', 'motorbike', 'person','pottedPlant', 'sheep', 'sofa', 'train', 'TV']# 定义数据格式net_shape = (300, 300)data_format = 'NHWC' # [Number, height, width, color]，Tensorflow backend 的格式# 预处理将输入图片大小改成 300x300，作为下一步输入img_input = tf.placeholder(tf.uint8, shape=(None, None, 3))image_pre, labels_pre, bboxes_pre, bbox_img = ssd_vgg_preprocessing.preprocess_for_eval(img_input,None,None,net_shape,data_format,resize=ssd_vgg_preprocessing.Resize.WARP_RESIZE)image_4d = tf.expand_dims(image_pre, 0) 下面我们来载入SSD作者已经搞定的模型123456789101112# 定义 SSD 模型结构reuse = True if 'ssd_net' in locals() else Nonessd_net = ssd_vgg_300.SSDNet()with slim.arg_scope(ssd_net.arg_scope(data_format=data_format)):predictions, localisations, _, _ = ssd_net.net(image_4d, is_training=False, reuse=reuse)# 导入官方给出的 SSD 模型参数#这边修改成你自己的路径ckpt_filename = '/Users/slade/SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt'isess.run(tf.global_variables_initializer())saver = tf.train.Saver()saver.restore(isess, ckpt_filename)ssd_anchors = ssd_net.anchors(net_shape) 下面让我们把SSD识别出来的结果在图片中表示出来1234567891011121314151617181920212223242526#不同类别，我们以不同的颜色表示def colors_subselect(colors, num_classes=21):dt = len(colors) // num_classessub_colors = []for i in range(num_classes):color = colors[i*dt]if isinstance(color[0], float):sub_colors.append([int(c * 255) for c in color])else:sub_colors.append([c for c in color])return sub_colors#画出在图中的位置def bboxes_draw_on_img(img, classes, scores, bboxes, colors, thickness=5):shape = img.shapefor i in range(bboxes.shape[0]):bbox = bboxes[i]color = colors[classes[i]]# Draw bounding box...p1 = (int(bbox[0] * shape[0]), int(bbox[1] * shape[1]))p2 = (int(bbox[2] * shape[0]), int(bbox[3] * shape[1]))cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)# Draw text...s = '%s:%.3f' % ( l_VOC_CLASS[int(classes[i])-1], scores[i])p1 = (p1[0]-5, p1[1])cv2.putText(img, s, p1[::-1], cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)colors_plasma = colors_subselect(mpcm.plasma.colors, num_classes=21) 让我们开始训练吧123456789101112131415def process_image(img, select_threshold=0.3, nms_threshold=.8, net_shape=(300, 300)):#先获取SSD网络的层相关的参数rimg, rpredictions, rlocalisations, rbbox_img = isess.run([image_4d, predictions, localisations, bbox_img],feed_dict=&#123;img_input: img&#125;)#获取分类结果，位置rclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(rpredictions, rlocalisations, ssd_anchors,select_threshold=select_threshold, img_shape=net_shape, num_classes=21, decode=True)rbboxes = np_methods.bboxes_clip(rbbox_img, rbboxes)rclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=400)rclasses, rscores, rbboxes = np_methods.bboxes_nms(rclasses, rscores, rbboxes, nms_threshold=nms_threshold)# 让我们在图中画出来就行了rbboxes = np_methods.bboxes_resize(rbbox_img, rbboxes)bboxes_draw_on_img(img, rclasses, rscores, rbboxes, colors_plasma, thickness=2)return img 预处理的函数都写完了，我们就可以执行了。12345#读取数据img = cv2.imread("/Users/slade/Documents/Yoho/picture_recognize/test7.jpg")img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)plt.imshow(process_image(img))plt.show() img的数据形式如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950In [8]: imgOut[8]:array([[[ 35, 59, 43],[ 37, 60, 44],[ 38, 61, 45],...,[ 73, 99, 62],[ 74, 99, 60],[ 72, 97, 57]],[[ 37, 60, 44],[ 37, 60, 44],[ 37, 60, 44],...,[ 66, 92, 57],[ 67, 93, 56],[ 67, 92, 53]],[[ 37, 60, 44],[ 36, 59, 43],[ 37, 58, 43],...,[ 56, 83, 48],[ 60, 86, 51],[ 61, 87, 50]],...,[[ 96, 101, 95],[107, 109, 104],[ 98, 97, 95],...,[ 84, 126, 76],[ 72, 118, 72],[ 78, 126, 86]],[[ 98, 103, 96],[114, 116, 111],[112, 113, 108],...,[ 94, 137, 84],[ 87, 133, 86],[105, 153, 111]],[[ 99, 105, 95],[110, 113, 106],[134, 135, 129],...,[127, 170, 116],[121, 167, 118],[131, 180, 135]]], dtype=uint8) 处理后的结果如下： 是不是非常无脑，上面的代码直接复制就可以完成。 下面在拓展一下视频的处理方式，其实相关的内容是一致的。利用moviepy.editor包里面的VideoFileClip的切片的功能，然后对每一次切片的结果进行process_image过程就可以了，这边就不贴代码了，需要的朋友私密我。 最后感谢大家阅读。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>图像识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[订单需求预估]]></title>
    <url>%2F2017%2F07%2F12%2F%E8%AE%A2%E5%8D%95%E9%9C%80%E6%B1%82%E9%A2%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[之前写了一篇以基于elastic的需求预估的文章，只不过用的是R语言开发的，最近在学python，就仿照逻辑写了一篇python的，主要修改点如下： 用决策树替换了elastic算法 用分层抽样替换了组合抽样 需要看详细理论及思考过程参考链接：商品需求预估 python code如下：123456789# -*- coding:utf-8 -*-import pandas as pdimport numpy as npimport random as rdfrom sklearn import tree# 读取数据data_orgin = pd.read_table(&quot;C:/Users/17031877/Desktop/supermarket_second_hair_washing_train.txt&quot;)data_deal_1 = data_orgin.drop([&apos;aimed_date&apos;, &apos;member_id&apos;, &apos;age&apos;, &apos;gender&apos;, &apos;diff_rgst&apos;], axis=1) 这边是常规的数据读取，删除了不必要的列 12345678910111213141516171819202122232425#因变量单列label = data_deal_1[&apos;label&apos;]# 用户分量级value00 = [&apos;max_date_diff&apos;, &apos;aimed_max_date_diff&apos;]data00 = data_deal_1[value00]value01 = [&apos;max_pay&apos;, &apos;per_pay&apos;, &apos;six_month_max_pay&apos;, &apos;six_month_per_pay&apos;, &apos;three_month_max_pay&apos;, &apos;three_month_per_pay&apos;,&apos;one_month_max_pay&apos;, &apos;one_month_per_pay&apos;, &apos;fifteen_day_max_pay&apos;, &apos;fifteen_day_per_pay&apos;, &apos;aimed_max_pay&apos;,&apos;aimed_per_pay&apos;, &apos;aimed_six_month_max_pay&apos;, &apos;aimed_six_month_per_pay&apos;, &apos;aimed_three_month_max_pay&apos;,&apos;aimed_three_month_per_pay&apos;, &apos;aimed_one_month_max_pay&apos;, &apos;aimed_one_month_per_pay&apos;,&apos;aimed_fifteen_day_max_pay&apos;, &apos;aimed_fifteen_day_per_pay&apos;, &apos;qty_drtn_seven&apos;, &apos;qty_drtn_fourteen&apos;]data01 = data_deal_1[value01]value02 = [&apos;cnt_time&apos;, &apos;six_month_cnt_time&apos;, &apos;three_month_cnt_time&apos;, &apos;one_month_cnt_time&apos;, &apos;fifteen_day_cnt_time&apos;,&apos;aimed_cnt_time&apos;, &apos;aimed_six_month_cnt_time&apos;, &apos;aimed_three_month_cnt_time&apos;, &apos;aimed_one_month_cnt_time&apos;,&apos;aimed_fifteen_day_cnt_time&apos;, &apos;pv_times_seven&apos;, &apos;pv_times_fourteen&apos;, &apos;search_times_seven&apos;,&apos;search_times_fourteen&apos;, &apos;clc_times_seven&apos;, &apos;clc_times_fourteen&apos;, &apos;cart2_times_seven&apos;,&apos;cart2_times_fourteen&apos;, &apos;cart1_times_seven&apos;, &apos;cart1_times_fourteen&apos;, &apos;unpay_times_seven&apos;,&apos;unpay_times_fourteen&apos;]data02 = data_deal_1[value02]value03 = [&apos;pv_visit_last_period&apos;, &apos;search_last_period&apos;, &apos;clc_last_period&apos;, &apos;cart2_last_period&apos;, &apos;cart1_last_period&apos;,&apos;unpay_last_period&apos;]data03 = data_deal_1[value03] 因为不同量级的数据之后做异常点处理的时候截断位置不同，所有需要分割数据处理 12345678910111213141516171819202122232425262728293031def test_function_one(x, l):k = x.dropna(how=&apos;any&apos;)y = k.quantile(l)z = k.max()x[x &gt; y] = yx = x.fillna(value=z)return xfor i in range(len(data00.columns)):data00.iloc[:, i] = test_function_one(data00.iloc[:, i], 0.98)def test_function_two(x, l):k = x.dropna(how=&apos;any&apos;)y = k.quantile(l)z = 0x[x &gt; y] = yx = x.fillna(value=z)return xfor i in range(len(data01.columns)):data01.iloc[:, i] = test_function_two(data01.iloc[:, i], 0.95)for i in range(len(data02.columns)):data02.iloc[:, i] = test_function_two(data02.iloc[:, i], 0.99)def test_function_three(x):z = 14x[x &gt; z] = zx = x.fillna(value=z)return xfor i in range(len(data03.columns)):data03.iloc[:, i] = test_function_three(data03.iloc[:, i])# 数据合并data_train = pd.concat([label, data00, data01, data02, data03], axis=1) 根据数据量的不同做数据分割，跑上面写完的code函数就可以 123456789#数量级对比zero_case = data_train[data_train[&apos;label&apos;] == 0][&apos;label&apos;].count()print &apos;负样本数：%d&apos; % zero_caseone_case = data_train[data_train[&apos;label&apos;] == 1][&apos;label&apos;].count()print &apos;正样本数: %d&apos; % (one_case)负样本数：292936正样本数: 3973Backend TkAgg is interactive backend. Turning interactive mode on. 实际看下来，正负样本的差异的确还是很大，这个其实做多了就有经验，常规的来看，潜在的浏览、搜索到最后的成单，普遍自然转化不到1%，也正是这么低的转化，才需要一些算法来做信息抓去。 12345678910111213141516171819202122def case_sample(x, y, z):diff_case = pd.DataFrame(x[y]).drop_duplicates([y])result = []result = pd.DataFrame(result)for i in range(len(diff_case)):k = np.array(diff_case)[i]data_set = x[x[y] == k[0]]nrow_nb = data_set.iloc[:, 0].count()data_set.index = range(nrow_nb)index_id = rd.sample(range(nrow_nb), int(nrow_nb * z))result = pd.concat([result, data_set.iloc[index_id, :]], axis=0)return resultzero_case = data_train[data_train[&apos;label&apos;] == 0]one_case = data_train[data_train[&apos;label&apos;] == 1]# 开始分层抽样new_zero_case = case_sample(zero_case, &apos;unpay_last_period&apos;, 0.1)# 新数量级对比new_zero_case_count = new_zero_case[new_zero_case[&apos;label&apos;] == 0][&apos;label&apos;].count()# 数据集合并new_data_train = pd.concat([new_zero_case, one_case], axis=0) case_sample是一个简单的分层抽样的小函数，x是数据集，y是分层变量，z是抽样占比；新的样本new_data_train中正负样本比例在1:10左右，这边的样本比是我自己设置的，不一定是最合理的；且此处也不一定要求一定用分层抽样，只是我用来练练手的；推荐还是遵从奥卡姆原理，在未知的情况下，尽可能简单的解决问题，比如组合抽样就是很不错的方法。 123456789101112131415#函数设置clf = tree.DecisionTreeRegressor(criterion=&apos;mse&apos;, max_features=&apos;log2&apos;, random_state=1)#函数拟合y = new_data_train[&apos;label&apos;]x = new_data_train.drop(&apos;label&apos;, 1)clf.fit(x, y)#数据预测y_predict = clf.predict(x)# 结果对比y.index = range(len(y))combined_date = pd.concat([y, pd.DataFrame(y_predict)], axis=1)combined_date.columns = [&apos;actual&apos;, &apos;predict&apos;] 这边稍微讲解一下，我认为的sklearn中DecisionTreeRegressor中比较终于的参数设置，criterion这边为模型优化的标准，常规的有mse和mae，建议在数据量差异不大的时候多考虑mse；max_features是每次训练用的特征个数，综合特征量级考虑，一般有log2，sqrt，尽可能是抽取比例在70%；max_depth刚开始可以默认，第一类模型出来后，可在结果附近迭代，寻找out of bag最小的error下的值；另外，我没有发现有weight设置，可能是我不熟悉，但是如果sklearn这边不提供weight的化，我们在做数据预处理的时候一定要平衡数据，不然当数据集过偏的时候最后的结果会以“牺牲”少类的判断正确率去完善整体正确率。 123456789101112131415# case 1x = []y = []for i in range(1, 10):test_data = combined_datei = i / float(10)for j in range(combined_date[&apos;actual&apos;].count()):if test_data.iloc[j, 1] &gt; i:test_data.iloc[j, 1] = 1else:continuez = test_data[test_data[&apos;actual&apos;] == test_data[&apos;predict&apos;]][&apos;actual&apos;].count() / float(test_data[&apos;actual&apos;].count())x.append(i)y.append(z) 这边写了检查函数，检查了分别0.1~1，以0.1为间隔的情况下的分割点，每个分割点下预测正确的数量/所有统计的样本数，也就是下面的accuracy. 12345678# case 2test_data = combined_dateaimed_data = test_data[test_data[&apos;predict&apos;]&gt;0]k1=aimed_data[aimed_data[&apos;actual&apos;]==1][&apos;predict&apos;].count()k2=float(aimed_data[&apos;predict&apos;].count())print &apos;所有预测可能下单用户中真实下单用户数：%d&apos; %(k1)print &apos;所有预测可能下单用户数：%d&apos; %(k2) 因为这边需要对用户营销，所以更关系topN的转化率，需要看一下实际正样本被覆盖了多数，以上即为code，这边的效果值为98.7%，还是比较高的，但是应该是过拟合了，所有一般不建议单纯使用决策树模型 所有的python code到这里就结束了，后续我做项目的同时会同时更新R及python两种code的思考，和大家讨论分享学习，谢谢。 参考文献： sklearn.tree.DecisionTreeRegressor 维基百科中对ROC的介绍 决策树常见问题及面试关键点介绍]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>预测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统-威尔逊区间法]]></title>
    <url>%2F2017%2F06%2F21%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%A8%81%E5%B0%94%E9%80%8A%E5%8C%BA%E9%97%B4%E6%B3%95%2F</url>
    <content type="text"><![CDATA[我推荐一种之前在惠普做过一种排序方法：威尔逊区间法 我们先做如下设定： （1）每个用户的打分都是独立事件。 （2）用户只有两个选择，要么投喜欢’1’，要么投不喜欢’0’。 （3）如果总人数为n，其中喜欢的为k，那么喜欢的比例p就等于k/n。 这是一种统计分布，叫做”二项分布”（binomial distribution） 理论上讲，p越大应该越好，但是n的不同，导致p的可信性有差异。100个人投票，50个人投喜欢；10个人投票，6个人喜欢，我们不能说后者比前者要好。 所以这边同时要考虑（p，n） 刚才说满足二项分布，这里p可以看作”二项分布”中某个事件的发生概率，因此我们可以计算出p的置信区间。 所谓”置信区间”，就是说，以某个概率而言，p会落在的那个区间。 置信区间展现的是这个参数的真实值有一定概率落在测量结果的周围的程度。置信区间给出的是被测量参数的测量值的可信程度，即前面所要求的“一个概率”，也就是结论的可信程度。 二项分布的置信区间有多种计算公式，最常见的是”正态区间”（Normal approximation interval）。但是，它只适用于样本较多的情况（np &gt; 5 且 n(1 − p) &gt; 5），对于小样本，它的准确性很差。 这边，我推荐用t检验来衡量小样本的数据，可以解决数据过少准确率不高的问题。 这样一来，排名算法就比较清晰了： 第一步，计算每个case的p（好评率）。 第二步，计算每个”好评率”的置信区间（参考z Test或者t Test，以95%的概率来处理）。 第三步，根据置信区间的下限值，进行排名。这个值越大，排名就越高。 解释一下，n为评价数，p为好评率，z为对应检验对应概率区间下的统计量 比如t-分布： 可以看到，当n的值足够大时，这个下限值会趋向p，如果n非常小，这个下限值会大大小于p，更加符合实际。 Reddit的评论排名，目前就使用这个算法。国内的化，滴滴也有部分业务涉及，效果也不错。 更新一下，没想到这个话题还是有高达9个人关注，所以这边我再说一些更细化的过程吧 在计算排名的时候，我们通常会考虑三个事情 1.上文讲到的，次数+好评率的分布，次数越多好评率越可靠，好评率越高该项越值得推荐 2.时间因素，如果一个项目是10天前推送的，一个项目是昨天推送的，很明显前者的次数远大于后者 3.影响权重，你这边只考虑了喜欢和不喜欢，其实所有的排序不可能只以1个维度考虑，通常会考虑多个维度，比如浏览次数，搜索次数等，你需要考虑每个的重要性或者说权重大小 1这里就不讲了，其他方法也有很多，比如贝叶斯平均的优化版本、再比如经典的Hacker公式： 2.时间因素： 时间越久，代表之前的投票结果对当前的影响越小，这边有很多不同的影响方式，举几个例子： 比如艾宾浩斯遗忘规律： 这里的c、k决定下降速度，业务运用过程中，c值一般在[1,2],k值一般在[1.5,2.5] 比如时效衰减： 这里就是比较常见的移动窗口式的，永远只看近期某一段时间，而且时间内呈线性下降，不过可以改变变化方式 3.不同种的属性对于结果的影响自然不同 举个例子，用户主动搜索和用户浏览相比，用户主动搜索的情况下，用户的需求更为强烈 通常需要判断这些强烈程度都是通过： 相关性：看因变量与自变量之间的相关系数，如：cor函数 importance：看删除或者修改自变量，对应变量的判断影响大小，如：randomForest的重要性 离散程度：看自变量的数据分布是否足够分散，是否具有判断依据，如：变异系数或者pca 等等]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类算法思路总结]]></title>
    <url>%2F2017%2F06%2F20%2F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1.cost function 1.1 距离 常见的为欧式距离（L1 norm）&amp;&amp;p=2，拓展的可以有闵可夫斯基距离（L2 norm）&amp;&amp;p=1： 当p趋向于无穷的时候，切比雪夫距离（Chebyshev distance）： 红色的时候为切比雪夫距离，蓝色为闵可夫斯基距离，绿色为欧式距离。 1.2相似系数 夹角余弦及相关系数，相关系数不受线性变换的影响，但是计算速度远慢于距离计算。 1.3dynamic time warping动态时间规整 举例子： 序列A：1,1,1,10,2,3，序列B：1,1,1,2,10,3 欧式距离：distance[i][j]=(b[j]-a[i])*(b[j]-a[i])来计算的话，总的距离和应该是128 应该说这个距离是非常大的，而实际上这个序列的图像是十分相似的。因为序列A中的10对应得是B中的2，A中的2对应的B中的10，导致计算膨胀，现在将A中的10对应B中的10，A中的1对应B中的2再计算，膨胀因素会小很多（时间前推一步）。 2.聚类算法 2.1分层聚类： 自上而下：所有点先聚为一类，然后分层次的一步一步筛出与当前类别差异最大的点 自下而上：所有点先各自为一类，组合成n个类的集合，然后寻找出最靠近的两者聚为新的一类，循环往复 数值类分类：（适用于计算量巨大或者数据量巨大的时候） BIRCH算法，层次平衡迭代规约和聚类， 主要参数包含：聚类特征和聚类特征树： 聚类特征： 给定N个d维的数据点{x1,x2,….,xn}，CF定义如下：CF=（N，LS，SS）,其中，N为子类中的节点的个数，LS是子类中的N个节点的线性和，SS是N个节点的平方和 存在计算定义：CF1+CF2=（n1+n2, LS1+LS2, SS1+SS2） 假设簇C1中有三个数据点：（2,3），（4,5），（5,6），则CF1={3，（2+4+5,3+5+6），（2^2+4^2+5^2,3^2+5^2+6^2）}={3，（11,14），（45,70）} 假设一个簇中，存在质心C和半径R，若有xi，i=1…n个点属于该簇，质心为：C=(X1+X2+…+Xn)/n，R=(|X1-C|^2+|X2-C|^2+…+|Xn-C|^2)/n 其中，簇半径表示簇中所有点到簇质心的平均距离。当有一个新点加入的时候，属性会变成CF=（N，LS，SS）的统计值，会压缩数据。 聚类特征树： 内节点的平衡因子B，子节点的平衡因子L，簇半径T。 B=6，深度为3，T为每个子节点中簇的范围最大不能超过的值，T越大簇越少，T越小簇越多。 名义分类： ROCK算法：凝聚型的层次聚类算法 1.如果两个样本点的相似度达到了阈值（θ），这两个样本点就是邻居。阈值（θ）有用户指定，相似度也是通过用户指定的相似度函数计算。常用的分类属性的相似度计算方法有：Jaccard系数，余弦相似度 Jaccard系数：J=|A∩B|/|A∪B|，一般用于分类变量之间的相似度 余弦相似度：【-1，1】之间，越趋近于0的时候，方向越一致，越趋向同一。 2.目标函数（criterion function）：最终簇之间的链接总数最小，而簇内的链接总数最大 3.相似度合并：遵循最终簇之间的链接总数最小，而簇内的链接总数最大的规则计算所有对象的两两相似度，将相似性最高的两个对象合并。通过该相似性度量不断的凝聚对象至k个簇，最终计算上面目标函数值必然是最大的。 load(‘country.RData’) d&lt;-dist(countries[,-1]) x&lt;-as.matrix(d) library(cba) rc &lt;-rockCluster(x, n=4, theta=0.2, debug=TRUE) KNN算法： 先确定K的大小，计算出每个点之外的所有点到这个目标点的距离，选出K个最近的作为一类。一般类别之间的归类的话，投票和加权为常用的，投票及少数服从多数，投票的及越靠近的点赋予越大的权重值。 2.2分隔聚类： 需要先确定分成的类数，在根据类内的点都足够近，类间的点都足够远的目标去做迭代。 常用的有K-means，K-medoids，K-modes等，只能针对数值类的分类，且只能对中等量级数据划分，只能对凸函数进行聚类，凹函数效果很差。 2.3密度聚类： 有效的避免了对分隔聚类下对凹函数聚类效果不好的情况，有效的判别入参主要有1:单点外的半径2：单点外半径内包含的点的个数 DBSCAN为主要常见的算法，可优化的角度是现在密度较高的地方进行聚类，再往密度较低的地方衍生，优化算法：OPTICS。 2.4网格聚类： 将n个点映射到n维上，在不同的网格中，计算点的密度，将点更加密集的网格归为一类。 优点是：超快，超级快，不论多少数据，计算速度只和维度相关。 缺点：n维的n难取，受分布影响较大（部分行业数据分布及其不规则） 2.5模型聚类： 基于概率和神经网络聚类，常见的为GMM，高斯混合模型。缺点为，计算量较大，效率较低。 GMM：每个点出现的概率：将k个高斯模型混合在一起，每个点出现的概率是几个高斯混合的结果 假设有K个高斯分布，每个高斯对data points的影响因子为πk，数据点为x，高斯参数为theta，则： 利用极大似然的方法去求解均值Uk，协方差矩阵（Σk），影响因子πk，但是普通的梯度下降的方法在这里求解会很麻烦，这边就以EM算法代替估计求解。 3.优化数据结构： 1.数据变换： logit处理，对所有数据进行log变换 傅里叶变换 小波变换 2.降维： PCA： 利用降维（线性变换)的思想，整体方差最大的情况下（在损失很少信息的前提下），把多个指标转化为几个不相关的综合指标（主成分),将变量线性组合代替原变量，保持代替后的数据信息量最大（方差最大）。 LLE： (1) 寻找每个样本点的k个近邻点； (2)由每个样本点的近邻点计算出该样本点的局部重建权值矩阵； (3)由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值。 (换句话说，就是由周围N个点构成改点的一个向量矩阵表示）]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用R语言包介绍]]></title>
    <url>%2F2017%2F06%2F19%2F%E5%B8%B8%E7%94%A8R%E8%AF%AD%E8%A8%80%E5%8C%85%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[r与python差异比较大的一个地方就是，python的机器学习算法集中程度比较高，比如sklearn，就集成了很多的算法，而R语言更多时候需要一个包一个包去了解，比较费时费力，对于python转过来的朋友非常不友好，抽空整理了工作中常用的R包如下： 常用检验函数： 基本上分布中常见的都罗列了： 常用作图函数包： ggplot2：万能，基本上excel能画的图它都能画 rattle：fancyRpartPlot函数，决策树画图函数 基础包函数：barplot、pie、dotchart、hist、densityplot、boxplot、contour等等 正态检验：qqplot、qqline、qqnorm 连续分类回归模型： stats包 lm函数，实现多元线性回归；glm函数，实现广义线性回归；nls函数，实现非线性最小二乘回归；knn函数，k最近邻算法 rpart包 rpart函数，基于CART算法的分类回归树模型 randomForest包 randomForest函数，基于rpart算法的集成算法 e1071包 svm函数，支持向量机算法 kernlab包 ksvm函数，基于核函数的支持向量机 nnet包 nnet函数，单隐藏层的神经网络算法 neuralnet包 neuralnet函数，多隐藏层多节点的神经网络算法 RSNNS包 mlp函数，多层感知器神经网络；rbf函数，基于径向基函数的神经网络 离散分类回归模型： stats包 glm函数，实现Logistic回归，选择logit连接函数 kknn包 kknn函数，加权的k最近邻算法 rpart包 rpart函数，基于CART算法的分类回归树模型 adabag包bagging函数，基于rpart算法的集成算法；boosting函数，基于rpart算法的集成算法 party包ctree函数，条件分类树算法 RWeka包OneR函数，一维的学习规则算法；JPip函数，多维的学习规则算法；J48函数，基于C4.5算法的决策树 C50包C5.0函数，基于C5.0算法的决策树 e1071包naiveBayes函数，贝叶斯分类器算法 klaR包NaiveBayes函数，贝叶斯分类器算分 MASS包lda函数，线性判别分析；qda函数，二次判别分析 聚类：Nbclust包Nbclust函数可以确定应该聚为几类 stats包kmeans函数，k均值聚类算法；hclust函数，层次聚类算法 cluster包pam函数，k中心点聚类算法 fpc包dbscan函数，密度聚类算法；kmeansruns函数，相比于kmeans函数更加稳定，而且还可以估计聚为几类；pamk函数，相比于pam函数，可以给出参考的聚类个数 mclust包Mclust函数，期望最大（EM）算法 关联规则：arules包apriori函数 Apriori关联规则算法 recommenderlab协调过滤 DRM：重复关联 ECLAT算法： 采用等价类，RST深度搜索和集合的交集： eclat 降维算法： psych包prcomp函数、factanal函数 时序分析： ts时序构建函数 timsac包时序分析 holtwinter包时序分析 decomp、tsr、stl成分分解 zoo 时间序列数据的预处理 统计及预处理： 常用的包 Base R, nlme aov, anova 方差分析 density 密度分析 t.test, prop.test, anova, aov:假设检验 rootSolve非线性求根 reshape2数据预处理 plyr及dplyr数据预处理大杀器 最后剩下常用的就是读入和写出了： RODBC 连接ODBC数据库接口 jsonlite 读写json文件 yaml 读写yaml文件 rmakdown写文档 knitr自动文档生成 一般业务中使用比较多的就是上面这些了，当然R里面有很多冷门的包，也很好用滴~]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>R语言工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树及衍射指标]]></title>
    <url>%2F2017%2F06%2F02%2F%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%8A%E8%A1%8D%E5%B0%84%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[一、常用的决策树节点枝剪的衡量指标： 熵： 如果一件事有k种可的结果，每种结果的概率为 pi（i＝1…k） 该事情的信息量： 熵越大，随机变量的不确定性越大。 信息增益： 特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下的经验条件熵H(D|A)之差 换句话说，就是原信息集下的信息量－在A特征条件下的信息集的信息量 信息增益越大，信息增多，不确定性减小 信息增益率： 信息增益率定义:特征A对训练数据集D的信息增益比定义为其信息增益与训练数据D关于特征A的值的熵HA(D)之比 注：p：每个唯独上，每个变量的个数／总变量个数 二、常用的决策树介绍： ID3算法： ID3算法的核心是在决策树各个子节点上应用信息增益准则选择特征，递归的构建决策树，具体方法是:从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。 解释：在做每次选择差分枝的时候，以不确定性最小点作为loss fuction，直到无法细分 缺点： 1.ID3算法只有树的生成，所以该算法生成的树容易产生过拟合，分得太细，考虑条件太多。 2.不能处理连续属性 3.选择具有较多分枝的属性，而分枝多的属性不一定是最优的选择。 4.局部最优化，整体熵值最小，贪心算法算子节点的分支 C4.5算法： 基于ID3算法，用信息增益比来选择属性，对非离散数据也能处理，能够对不完整数据进行处理。 采用增益率（GainRate）来选择分裂属性。计算方式如下： CART算法： CART算法选择分裂属性的方式是比较有意思的，首先计算不纯度，然后利用不纯度计算Gini指标。 计算每个子集最小的Gini指标作为分裂指标。 不纯度的计算方式为： pi表示按某个变量划分中，目标变量不同类别的概率。 某个自变量的Gini指标的计算方式如下： 计算出每个每个子集的Gini指标，选取其中最小的Gini指标作为树的分支（Gini（D）越小，则数据集D的纯度越高）。连续型变量的离散方式与信息增益中的离散方式相同。 三、基于决策树的一些集成算法： 随机森林： 随机生成n颗树，树之间不存在关联，取结果的时候，以众数衡量分类结果；除了分类，变量分析，无监督学习，离群点分析也可以。 生成过程： 1.n个样本，随机选择n个样本（有放回），训练一颗树 从原始训练数据集中,应用bootstrap方法有放回地随机抽取 K个新的自助样本集,并由此构建 K棵分类回归树,每次未被抽到的样本组成了 K个袋外数据(Out-of-bag,OOB) 2.每个样本有M个属性，随机选m个，采取校验函数（比如信息增益、熵啊之类的），选择最佳分类点 3.注意，每个树不存在枝剪 4.将生成的多棵树组成随机森林,用随机森林对新的数据进行分类,分类结果按树分类器的投票多少而定 树的个数随机选取，一般500，看三个误差函数是否收敛；变量的个数一般取均方作为mtry GBDT： DT步骤： GBDT里面的树是回归树！ GBDT做每个节点上的分支的时候，都会以最小均方误差作为衡量（真实值－预测值）的平方和／N，换句话说，就是存在真实线l1，预测线l2，两条线之间的间距越小越好。 BT步骤： GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。 换句话说，就是第一次预测的差值记为下一次预测的初始值，一直到某一次计算出的差值为0，把前n次的结果相加，就是一个真实预测。 Adaboost： 步骤： 1.初始化所有训练样例的权重为1 / N,其中N是样本数 2.对其中第1~m个样本: a.训练m个弱分类器，使其最小化bias： b.接下来计算该弱分类器的权重α，降低错判的分类器的权重，： c.更新权重： 3.最后得到组合分类器： 核心的思想如下图： 全量数据集在若干次训练后，降低训练正确的样本的权重，提高训练错误样本的权重，得到若干个Y对应的分类器，在组合投票得到最终的分类器 四、惠普实验室-集成并行化的随机森林：]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>树划分问题</tag>
      </tags>
  </entry>
</search>
